{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "from ray.rllib.policy.rnn_sequencing import add_time_dimension\n",
    "\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, self.policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.policy_hidden_dim, self.policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.time_major = self.model_config.get(\"_time_major\", False)\n",
    "        self.gru = nn.GRU(self.policy_hidden_dim, self.policy_hidden_dim, batch_first=not self.time_major)\n",
    "        \n",
    "        self.action_head = nn.Linear(self.policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(self.policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.gru.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        \n",
    "        if isinstance(seq_lens, np.ndarray):\n",
    "            seq_lens = torch.Tensor(seq_lens).int()\n",
    "        max_seq_len = features.shape[0] // seq_lens.shape[0]    \n",
    "        inputs = add_time_dimension(\n",
    "            features,\n",
    "            max_seq_len=max_seq_len,\n",
    "            framework=\"torch\",\n",
    "            time_major=self.time_major,\n",
    "        )\n",
    "        \n",
    "        h = state[0].permute(1, 0, 2)\n",
    "        output, new_h = self.gru(inputs, h)\n",
    "        new_state = [new_h.permute(1, 0, 2)]\n",
    "        \n",
    "        gru_output = output.reshape(-1, self.policy_hidden_dim)\n",
    "        \n",
    "        action = self.action_head(gru_output)\n",
    "        self.last_value = self.value_head(gru_output).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def get_initial_state(self):\n",
    "        return [torch.zeros(1, self.policy_hidden_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592760ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362368"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_features_dim = 512\n",
    "target_features_dim = 9 * 11 * 11\n",
    "policy_hidden_dim = 256 \n",
    "\n",
    "policy_network = nn.Sequential(\n",
    "    nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    #nn.ELU(),\n",
    ")\n",
    "\n",
    "sum(p.numel() for p in policy_network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tasks = []\n",
    "for i in range(1,156):\n",
    "    if ('C'+str(i)) == 'C38': continue\n",
    "    tasks.append('C'+str(i))\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        if abs(rew) == 1:\n",
    "            rew /= 10\n",
    "            \n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=250)\n",
    "    env.update_taskset(TaskSet(preset=tasks))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-11-07 12:16:50,713\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-11-07 12:16:50,727\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO All Tasks pretrained (AngelaCNN+GRU) (3 noops after placement) r: -0.01 div10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/960ce_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/960ce_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211107_121651-960ce_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m 2021-11-07 12:16:54,107\tWARNING ppo.py:143 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1666.\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m 2021-11-07 12:16:54,107\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m 2021-11-07 12:16:54,107\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m 2021-11-07 12:17:02,169\tINFO trainable.py:109 -- Trainable.setup took 10.476 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=550421)\u001b[0m 2021-11-07 12:17:02,169\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 9996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-20-00\n",
      "  done: false\n",
      "  episode_len_mean: 101.70103092783505\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.590000000000001\n",
      "  episode_reward_mean: -0.7984536082474234\n",
      "  episode_reward_min: -1.5900000000000005\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 97\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8843685247959234\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005787131851031565\n",
      "          policy_loss: -0.015160348295019223\n",
      "          total_loss: 0.0048022730944630426\n",
      "          vf_explained_var: -0.13828356564044952\n",
      "          vf_loss: 0.04764888057369305\n",
      "    num_agent_steps_sampled: 9996\n",
      "    num_agent_steps_trained: 9996\n",
      "    num_steps_sampled: 9996\n",
      "    num_steps_trained: 9996\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.33647058823527\n",
      "    ram_util_percent: 46.45137254901962\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05117176368150769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.88895869207942\n",
      "    mean_inference_ms: 2.91024042773677\n",
      "    mean_raw_obs_processing_ms: 0.6029087113092092\n",
      "  time_since_restore: 178.23534417152405\n",
      "  time_this_iter_s: 178.23534417152405\n",
      "  time_total_s: 178.23534417152405\n",
      "  timers:\n",
      "    learn_throughput: 801.746\n",
      "    learn_time_ms: 12467.787\n",
      "    load_throughput: 14569.011\n",
      "    load_time_ms: 686.114\n",
      "    sample_throughput: 60.563\n",
      "    sample_time_ms: 165052.401\n",
      "    update_time_ms: 14.297\n",
      "  timestamp: 1636287600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9996\n",
      "  training_iteration: 1\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         178.235</td><td style=\"text-align: right;\">9996</td><td style=\"text-align: right;\">-0.798454</td><td style=\"text-align: right;\">                2.59</td><td style=\"text-align: right;\">               -1.59</td><td style=\"text-align: right;\">           101.701</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 19992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 98.39603960396039\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.5200000000000022\n",
      "  episode_reward_mean: -0.7946534653465352\n",
      "  episode_reward_min: -1.5400000000000005\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 198\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8792491413589216\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.007026159120763405\n",
      "          policy_loss: -0.019986279375112864\n",
      "          total_loss: 0.006758365585999419\n",
      "          vf_explained_var: -0.4184056520462036\n",
      "          vf_loss: 0.054131903732892\n",
      "    num_agent_steps_sampled: 19992\n",
      "    num_agent_steps_trained: 19992\n",
      "    num_steps_sampled: 19992\n",
      "    num_steps_trained: 19992\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.09878048780487\n",
      "    ram_util_percent: 52.87743902439023\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05057405812929475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.82835685290122\n",
      "    mean_inference_ms: 2.9284729063322725\n",
      "    mean_raw_obs_processing_ms: 0.590306384237268\n",
      "  time_since_restore: 293.6380708217621\n",
      "  time_this_iter_s: 115.40272665023804\n",
      "  time_total_s: 293.6380708217621\n",
      "  timers:\n",
      "    learn_throughput: 804.03\n",
      "    learn_time_ms: 12432.366\n",
      "    load_throughput: 16918.146\n",
      "    load_time_ms: 590.845\n",
      "    sample_throughput: 74.726\n",
      "    sample_time_ms: 133768.508\n",
      "    update_time_ms: 12.435\n",
      "  timestamp: 1636287715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19992\n",
      "  training_iteration: 2\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         293.638</td><td style=\"text-align: right;\">19992</td><td style=\"text-align: right;\">-0.794653</td><td style=\"text-align: right;\">                2.52</td><td style=\"text-align: right;\">               -1.54</td><td style=\"text-align: right;\">            98.396</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 29988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 98.57843137254902\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.11\n",
      "  episode_reward_mean: -0.699411764705883\n",
      "  episode_reward_min: -1.9100000000000008\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 300\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.873959662567856\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008215863863404236\n",
      "          policy_loss: -0.02320211596914336\n",
      "          total_loss: 0.012691705124791012\n",
      "          vf_explained_var: -0.1844165325164795\n",
      "          vf_loss: 0.06299024441828713\n",
      "    num_agent_steps_sampled: 29988\n",
      "    num_agent_steps_trained: 29988\n",
      "    num_steps_sampled: 29988\n",
      "    num_steps_trained: 29988\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.02168674698794\n",
      "    ram_util_percent: 53.18674698795181\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04979328818074949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.83471973074873\n",
      "    mean_inference_ms: 2.927450216177017\n",
      "    mean_raw_obs_processing_ms: 0.5846781799178019\n",
      "  time_since_restore: 409.7674870491028\n",
      "  time_this_iter_s: 116.1294162273407\n",
      "  time_total_s: 409.7674870491028\n",
      "  timers:\n",
      "    learn_throughput: 805.388\n",
      "    learn_time_ms: 12411.411\n",
      "    load_throughput: 17914.869\n",
      "    load_time_ms: 557.972\n",
      "    sample_throughput: 80.878\n",
      "    sample_time_ms: 123593.52\n",
      "    update_time_ms: 11.943\n",
      "  timestamp: 1636287831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29988\n",
      "  training_iteration: 3\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         409.767</td><td style=\"text-align: right;\">29988</td><td style=\"text-align: right;\">-0.699412</td><td style=\"text-align: right;\">                3.11</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">           98.5784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 39984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 96.57281553398059\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.770000000000001\n",
      "  episode_reward_mean: -0.5916504854368936\n",
      "  episode_reward_min: -1.4700000000000006\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 403\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.871249017959986\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008111144584388687\n",
      "          policy_loss: -0.01956814361306337\n",
      "          total_loss: 0.038457083821686736\n",
      "          vf_explained_var: -0.09535246342420578\n",
      "          vf_loss: 0.08511548809484284\n",
      "    num_agent_steps_sampled: 39984\n",
      "    num_agent_steps_trained: 39984\n",
      "    num_steps_sampled: 39984\n",
      "    num_steps_trained: 39984\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.04328358208957\n",
      "    ram_util_percent: 53.1950248756219\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0493754715128279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.79844876939818\n",
      "    mean_inference_ms: 2.925300733913639\n",
      "    mean_raw_obs_processing_ms: 1.9330527535219888\n",
      "  time_since_restore: 550.4422965049744\n",
      "  time_this_iter_s: 140.67480945587158\n",
      "  time_total_s: 550.4422965049744\n",
      "  timers:\n",
      "    learn_throughput: 802.816\n",
      "    learn_time_ms: 12451.175\n",
      "    load_throughput: 18475.174\n",
      "    load_time_ms: 541.05\n",
      "    sample_throughput: 80.228\n",
      "    sample_time_ms: 124594.276\n",
      "    update_time_ms: 10.167\n",
      "  timestamp: 1636287972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39984\n",
      "  training_iteration: 4\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         550.442</td><td style=\"text-align: right;\">39984</td><td style=\"text-align: right;\">-0.59165</td><td style=\"text-align: right;\">                4.77</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">           96.5728</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 49980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-28-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.710000000000004\n",
      "  episode_reward_mean: -0.8549000000000007\n",
      "  episode_reward_min: -1.8600000000000008\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 503\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8667098827851123\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008871607625514715\n",
      "          policy_loss: -0.022918244961322817\n",
      "          total_loss: -7.148530547164826e-05\n",
      "          vf_explained_var: 0.17319510877132416\n",
      "          vf_loss: 0.04973953606154865\n",
      "    num_agent_steps_sampled: 49980\n",
      "    num_agent_steps_trained: 49980\n",
      "    num_steps_sampled: 49980\n",
      "    num_steps_trained: 49980\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.80174418604652\n",
      "    ram_util_percent: 54.12267441860465\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04898856334572945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.104274842284195\n",
      "    mean_inference_ms: 2.9195360244161166\n",
      "    mean_raw_obs_processing_ms: 1.6651089369882215\n",
      "  time_since_restore: 670.7818713188171\n",
      "  time_this_iter_s: 120.33957481384277\n",
      "  time_total_s: 670.7818713188171\n",
      "  timers:\n",
      "    learn_throughput: 802.07\n",
      "    learn_time_ms: 12462.747\n",
      "    load_throughput: 18811.718\n",
      "    load_time_ms: 531.371\n",
      "    sample_throughput: 82.516\n",
      "    sample_time_ms: 121139.642\n",
      "    update_time_ms: 9.227\n",
      "  timestamp: 1636288093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49980\n",
      "  training_iteration: 5\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         670.782</td><td style=\"text-align: right;\">49980</td><td style=\"text-align: right;\"> -0.8549</td><td style=\"text-align: right;\">                4.71</td><td style=\"text-align: right;\">               -1.86</td><td style=\"text-align: right;\">            100.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 59976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 98.29411764705883\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.9000000000000035\n",
      "  episode_reward_mean: -0.6413725490196083\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 605\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.858221541306911\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.010438789652350048\n",
      "          policy_loss: -0.030405953164614388\n",
      "          total_loss: 0.018758949520798703\n",
      "          vf_explained_var: 0.14639359712600708\n",
      "          vf_loss: 0.07565935982040997\n",
      "    num_agent_steps_sampled: 59976\n",
      "    num_agent_steps_trained: 59976\n",
      "    num_steps_sampled: 59976\n",
      "    num_steps_trained: 59976\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.70346820809249\n",
      "    ram_util_percent: 54.29653179190751\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.048637510673786626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.691626512824364\n",
      "    mean_inference_ms: 2.915991993933947\n",
      "    mean_raw_obs_processing_ms: 1.4858376319586386\n",
      "  time_since_restore: 792.3758232593536\n",
      "  time_this_iter_s: 121.5939519405365\n",
      "  time_total_s: 792.3758232593536\n",
      "  timers:\n",
      "    learn_throughput: 801.261\n",
      "    learn_time_ms: 12475.335\n",
      "    load_throughput: 19047.655\n",
      "    load_time_ms: 524.789\n",
      "    sample_throughput: 83.971\n",
      "    sample_time_ms: 119040.772\n",
      "    update_time_ms: 8.732\n",
      "  timestamp: 1636288214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59976\n",
      "  training_iteration: 6\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         792.376</td><td style=\"text-align: right;\">59976</td><td style=\"text-align: right;\">-0.641373</td><td style=\"text-align: right;\">                 2.9</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">           98.2941</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 69972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 99.83168316831683\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.6900000000000075\n",
      "  episode_reward_mean: -0.3659405940594059\n",
      "  episode_reward_min: -1.880000000000001\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 706\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8370240290959674\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014191464562780963\n",
      "          policy_loss: -0.037628927366817014\n",
      "          total_loss: 0.041279713383728524\n",
      "          vf_explained_var: 0.3577720522880554\n",
      "          vf_loss: 0.10444058780837008\n",
      "    num_agent_steps_sampled: 69972\n",
      "    num_agent_steps_trained: 69972\n",
      "    num_steps_sampled: 69972\n",
      "    num_steps_trained: 69972\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.55804597701149\n",
      "    ram_util_percent: 54.588505747126455\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04876933405954502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.366634462388614\n",
      "    mean_inference_ms: 2.9153068830263775\n",
      "    mean_raw_obs_processing_ms: 1.361944660442085\n",
      "  time_since_restore: 914.2079155445099\n",
      "  time_this_iter_s: 121.83209228515625\n",
      "  time_total_s: 914.2079155445099\n",
      "  timers:\n",
      "    learn_throughput: 799.746\n",
      "    learn_time_ms: 12498.972\n",
      "    load_throughput: 19216.558\n",
      "    load_time_ms: 520.176\n",
      "    sample_throughput: 85.029\n",
      "    sample_time_ms: 117559.829\n",
      "    update_time_ms: 9.58\n",
      "  timestamp: 1636288336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69972\n",
      "  training_iteration: 7\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 26.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         914.208</td><td style=\"text-align: right;\">69972</td><td style=\"text-align: right;\">-0.365941</td><td style=\"text-align: right;\">                4.69</td><td style=\"text-align: right;\">               -1.88</td><td style=\"text-align: right;\">           99.8317</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550418)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550414)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=550417)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_960ce_00000:\n",
      "  agent_timesteps_total: 79968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-34-39\n",
      "  done: false\n",
      "  episode_len_mean: 99.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.920000000000009\n",
      "  episode_reward_mean: -0.001399999999999455\n",
      "  episode_reward_min: -2.0100000000000007\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 805\n",
      "  experiment_id: d03564bf33354e528816561f1ba7167b\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.803601525991391\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0197072519793406\n",
      "          policy_loss: -0.03646253860746132\n",
      "          total_loss: 0.12053933267991067\n",
      "          vf_explained_var: 0.2784615755081177\n",
      "          vf_loss: 0.18109643551258323\n",
      "    num_agent_steps_sampled: 79968\n",
      "    num_agent_steps_trained: 79968\n",
      "    num_steps_sampled: 79968\n",
      "    num_steps_trained: 79968\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.47941176470586\n",
      "    ram_util_percent: 54.38774509803922\n",
      "  pid: 550421\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04873721924805494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.23488922714423\n",
      "    mean_inference_ms: 2.91564648670363\n",
      "    mean_raw_obs_processing_ms: 1.9021852738311935\n",
      "  time_since_restore: 1057.1198997497559\n",
      "  time_this_iter_s: 142.91198420524597\n",
      "  time_total_s: 1057.1198997497559\n",
      "  timers:\n",
      "    learn_throughput: 800.085\n",
      "    learn_time_ms: 12493.675\n",
      "    load_throughput: 19371.385\n",
      "    load_time_ms: 516.019\n",
      "    sample_throughput: 83.923\n",
      "    sample_time_ms: 119108.54\n",
      "    update_time_ms: 9.096\n",
      "  timestamp: 1636288479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79968\n",
      "  training_iteration: 8\n",
      "  trial_id: 960ce_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 25.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/21.25 GiB heap, 0.0/10.62 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_12-16-50<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_960ce_00000</td><td>RUNNING </td><td>192.168.3.5:550421</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1057.12</td><td style=\"text-align: right;\">79968</td><td style=\"text-align: right;\"> -0.0014</td><td style=\"text-align: right;\">                4.92</td><td style=\"text-align: right;\">               -2.01</td><td style=\"text-align: right;\">             99.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 3,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 5_000,\n",
    "             \"lr\": 1e-4,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO All Tasks pretrained (AngelaCNN+GRU) (3 noops after placement) r: -0.01 div10\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        local_dir=\"/IGLU-Minecraft/checkpoints/all_tasks\",\n",
    "        keep_checkpoints_num=50,\n",
    "        checkpoint_freq=5,\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
