{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C8']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 07:52:45,934\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-02 07:52:46,167\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m 2021-10-02 07:52:52,665\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m 2021-10-02 07:52:52,665\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C8 pretrained (AnnaCNN)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/ba7ae_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/ba7ae_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211002_075249-ba7ae_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m 2021-10-02 07:55:59,270\tINFO trainable.py:109 -- Trainable.setup took 191.149 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=50048)\u001b[0m 2021-10-02 07:55:59,336\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=50041)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7033940553665161\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0057295713520358026\n",
      "          policy_loss: -0.08215594333079126\n",
      "          total_loss: -0.0870968044632011\n",
      "          vf_explained_var: -0.010583942756056786\n",
      "          vf_loss: 0.010947168166361128\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.19748603351954\n",
      "    ram_util_percent: 30.251396648044693\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07399312266103038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 243.8798698631081\n",
      "    mean_inference_ms: 2.6542826013250664\n",
      "    mean_raw_obs_processing_ms: 0.2546889203173536\n",
      "  time_since_restore: 250.37637758255005\n",
      "  time_this_iter_s: 250.37637758255005\n",
      "  time_total_s: 250.37637758255005\n",
      "  timers:\n",
      "    learn_throughput: 330.349\n",
      "    learn_time_ms: 3027.101\n",
      "    load_throughput: 27324.812\n",
      "    load_time_ms: 36.597\n",
      "    sample_throughput: 4.044\n",
      "    sample_time_ms: 247300.548\n",
      "    update_time_ms: 4.783\n",
      "  timestamp: 1633161609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         250.376</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-00-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6928278128306071\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00904520250513509\n",
      "          policy_loss: -0.09347861359516779\n",
      "          total_loss: -0.10446699799762832\n",
      "          vf_explained_var: 0.20475783944129944\n",
      "          vf_loss: 0.0041308524901978675\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.46190476190476\n",
      "    ram_util_percent: 31.08571428571429\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07442881442317661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 185.5629746099113\n",
      "    mean_inference_ms: 2.661499193813736\n",
      "    mean_raw_obs_processing_ms: 0.24417301926016852\n",
      "  time_since_restore: 265.59167551994324\n",
      "  time_this_iter_s: 15.215297937393188\n",
      "  time_total_s: 265.59167551994324\n",
      "  timers:\n",
      "    learn_throughput: 433.478\n",
      "    learn_time_ms: 2306.923\n",
      "    load_throughput: 31473.174\n",
      "    load_time_ms: 31.773\n",
      "    sample_throughput: 7.666\n",
      "    sample_time_ms: 130440.934\n",
      "    update_time_ms: 6.458\n",
      "  timestamp: 1633161624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         265.592</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6625326010915968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015246788401227462\n",
      "          policy_loss: -0.13549536143740018\n",
      "          total_loss: -0.14701449589596854\n",
      "          vf_explained_var: 0.182473286986351\n",
      "          vf_loss: 0.0020568322244798763\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.21904761904761\n",
      "    ram_util_percent: 31.147619047619052\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0740400462432456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 153.11572030831329\n",
      "    mean_inference_ms: 2.640624624194471\n",
      "    mean_raw_obs_processing_ms: 0.23760421400804174\n",
      "  time_since_restore: 280.30226039886475\n",
      "  time_this_iter_s: 14.710584878921509\n",
      "  time_total_s: 280.30226039886475\n",
      "  timers:\n",
      "    learn_throughput: 471.319\n",
      "    learn_time_ms: 2121.705\n",
      "    load_throughput: 32788.407\n",
      "    load_time_ms: 30.499\n",
      "    sample_throughput: 10.957\n",
      "    sample_time_ms: 91267.675\n",
      "    update_time_ms: 5.678\n",
      "  timestamp: 1633161639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         280.302</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.711222055223253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01555327559668432\n",
      "          policy_loss: -0.14178306518329514\n",
      "          total_loss: -0.15419599894020292\n",
      "          vf_explained_var: 0.1656806766986847\n",
      "          vf_loss: 0.0015886313563290362\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.64285714285715\n",
      "    ram_util_percent: 31.16666666666667\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07354928899551914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 132.0071547167108\n",
      "    mean_inference_ms: 2.6131506925635035\n",
      "    mean_raw_obs_processing_ms: 0.2318161306737143\n",
      "  time_since_restore: 294.94624376296997\n",
      "  time_this_iter_s: 14.643983364105225\n",
      "  time_total_s: 294.94624376296997\n",
      "  timers:\n",
      "    learn_throughput: 481.215\n",
      "    learn_time_ms: 2078.074\n",
      "    load_throughput: 40863.926\n",
      "    load_time_ms: 24.471\n",
      "    sample_throughput: 13.963\n",
      "    sample_time_ms: 71620.319\n",
      "    update_time_ms: 5.079\n",
      "  timestamp: 1633161654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         294.946</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.414307369126214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011918093467656253\n",
      "          policy_loss: -0.22832251009013918\n",
      "          total_loss: -0.24024085861941177\n",
      "          vf_explained_var: 0.20854517817497253\n",
      "          vf_loss: 0.009841103280066616\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.476\n",
      "    ram_util_percent: 31.203999999999997\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07296967704665933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 117.0959841052439\n",
      "    mean_inference_ms: 2.5893253455522194\n",
      "    mean_raw_obs_processing_ms: 0.22770998275589643\n",
      "  time_since_restore: 312.0155739784241\n",
      "  time_this_iter_s: 17.0693302154541\n",
      "  time_total_s: 312.0155739784241\n",
      "  timers:\n",
      "    learn_throughput: 492.496\n",
      "    learn_time_ms: 2030.475\n",
      "    load_throughput: 46323.174\n",
      "    load_time_ms: 21.587\n",
      "    sample_throughput: 16.573\n",
      "    sample_time_ms: 60338.258\n",
      "    update_time_ms: 4.916\n",
      "  timestamp: 1633161671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         312.016</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-01-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.350110077857971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012982086003341959\n",
      "          policy_loss: -0.06267721951007843\n",
      "          total_loss: -0.057134075545602375\n",
      "          vf_explained_var: 0.3407261371612549\n",
      "          vf_loss: 0.026447828233035073\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.2217391304348\n",
      "    ram_util_percent: 31.239130434782613\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07242086201137711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 105.89118498792153\n",
      "    mean_inference_ms: 2.5689680875278187\n",
      "    mean_raw_obs_processing_ms: 0.22436492205796577\n",
      "  time_since_restore: 328.3955702781677\n",
      "  time_this_iter_s: 16.379996299743652\n",
      "  time_total_s: 328.3955702781677\n",
      "  timers:\n",
      "    learn_throughput: 504.357\n",
      "    learn_time_ms: 1982.721\n",
      "    load_throughput: 44458.188\n",
      "    load_time_ms: 22.493\n",
      "    sample_throughput: 18.97\n",
      "    sample_time_ms: 52714.884\n",
      "    update_time_ms: 5.033\n",
      "  timestamp: 1633161687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         328.396</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3456860780715942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011152195521835691\n",
      "          policy_loss: -0.10105257810403903\n",
      "          total_loss: -0.10958368144929409\n",
      "          vf_explained_var: 0.49182116985321045\n",
      "          vf_loss: 0.012695318750209279\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.14399999999999\n",
      "    ram_util_percent: 31.26\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07192025191448913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 97.13103671052647\n",
      "    mean_inference_ms: 2.552658057202341\n",
      "    mean_raw_obs_processing_ms: 0.22150710083172442\n",
      "  time_since_restore: 345.5972361564636\n",
      "  time_this_iter_s: 17.2016658782959\n",
      "  time_total_s: 345.5972361564636\n",
      "  timers:\n",
      "    learn_throughput: 515.391\n",
      "    learn_time_ms: 1940.276\n",
      "    load_throughput: 44273.536\n",
      "    load_time_ms: 22.587\n",
      "    sample_throughput: 21.099\n",
      "    sample_time_ms: 47395.462\n",
      "    update_time_ms: 5.12\n",
      "  timestamp: 1633161705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         345.597</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-02-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9639955202738444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026395138365796277\n",
      "          policy_loss: -0.008113656839769747\n",
      "          total_loss: -0.0015758781849096219\n",
      "          vf_explained_var: 0.19546976685523987\n",
      "          vf_loss: 0.020898711740867132\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.4\n",
      "    ram_util_percent: 31.18\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07155106272007003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 90.06673880725307\n",
      "    mean_inference_ms: 2.540650820991499\n",
      "    mean_raw_obs_processing_ms: 0.21924616941354108\n",
      "  time_since_restore: 363.09135222435\n",
      "  time_this_iter_s: 17.494116067886353\n",
      "  time_total_s: 363.09135222435\n",
      "  timers:\n",
      "    learn_throughput: 522.888\n",
      "    learn_time_ms: 1912.454\n",
      "    load_throughput: 48363.751\n",
      "    load_time_ms: 20.677\n",
      "    sample_throughput: 23.02\n",
      "    sample_time_ms: 43439.544\n",
      "    update_time_ms: 6.003\n",
      "  timestamp: 1633161722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         363.091</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9588770164383782\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007259550089954203\n",
      "          policy_loss: -0.1982526576353444\n",
      "          total_loss: -0.21398147319753966\n",
      "          vf_explained_var: 0.23233485221862793\n",
      "          vf_loss: 0.0016820880480938488\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.92857142857142\n",
      "    ram_util_percent: 31.219047619047615\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07119931288212353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 84.19543160647599\n",
      "    mean_inference_ms: 2.529919750015809\n",
      "    mean_raw_obs_processing_ms: 0.21725174187924362\n",
      "  time_since_restore: 377.59684586524963\n",
      "  time_this_iter_s: 14.505493640899658\n",
      "  time_total_s: 377.59684586524963\n",
      "  timers:\n",
      "    learn_throughput: 529.359\n",
      "    learn_time_ms: 1889.076\n",
      "    load_throughput: 44825.52\n",
      "    load_time_ms: 22.309\n",
      "    sample_throughput: 24.981\n",
      "    sample_time_ms: 40029.952\n",
      "    update_time_ms: 5.759\n",
      "  timestamp: 1633161737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         377.597</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.296233571900262\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010914303352281262\n",
      "          policy_loss: -0.14011111992100875\n",
      "          total_loss: -0.15559267128507295\n",
      "          vf_explained_var: 0.19625301659107208\n",
      "          vf_loss: 0.004206494155288156\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.81363636363638\n",
      "    ram_util_percent: 31.240909090909096\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0708638169618673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 79.23900194568904\n",
      "    mean_inference_ms: 2.521188939546426\n",
      "    mean_raw_obs_processing_ms: 0.21536998559028256\n",
      "  time_since_restore: 393.40155363082886\n",
      "  time_this_iter_s: 15.804707765579224\n",
      "  time_total_s: 393.40155363082886\n",
      "  timers:\n",
      "    learn_throughput: 533.471\n",
      "    learn_time_ms: 1874.517\n",
      "    load_throughput: 46235.854\n",
      "    load_time_ms: 21.628\n",
      "    sample_throughput: 26.716\n",
      "    sample_time_ms: 37430.287\n",
      "    update_time_ms: 5.676\n",
      "  timestamp: 1633161753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         393.402</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.386650392744276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009933997771297559\n",
      "          policy_loss: -0.09689303172959221\n",
      "          total_loss: -0.11367242965433333\n",
      "          vf_explained_var: 0.22492121160030365\n",
      "          vf_loss: 0.004106906537587444\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.175\n",
      "    ram_util_percent: 31.175\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07056747084109062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 74.99685201352716\n",
      "    mean_inference_ms: 2.5132555298018797\n",
      "    mean_raw_obs_processing_ms: 0.21372515837083436\n",
      "  time_since_restore: 410.033323764801\n",
      "  time_this_iter_s: 16.631770133972168\n",
      "  time_total_s: 410.033323764801\n",
      "  timers:\n",
      "    learn_throughput: 567.899\n",
      "    learn_time_ms: 1760.877\n",
      "    load_throughput: 48209.449\n",
      "    load_time_ms: 20.743\n",
      "    sample_throughput: 70.578\n",
      "    sample_time_ms: 14168.732\n",
      "    update_time_ms: 7.058\n",
      "  timestamp: 1633161769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         410.033</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8740591565767923\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005029126855457831\n",
      "          policy_loss: -0.201232731466492\n",
      "          total_loss: -0.21762040766576926\n",
      "          vf_explained_var: 0.23509421944618225\n",
      "          vf_loss: 0.0008441742221798955\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.15\n",
      "    ram_util_percent: 31.16363636363637\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07032682044600475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 71.30936478226596\n",
      "    mean_inference_ms: 2.5065249927536626\n",
      "    mean_raw_obs_processing_ms: 0.21222314902284925\n",
      "  time_since_restore: 425.1595788002014\n",
      "  time_this_iter_s: 15.12625503540039\n",
      "  time_total_s: 425.1595788002014\n",
      "  timers:\n",
      "    learn_throughput: 563.31\n",
      "    learn_time_ms: 1775.22\n",
      "    load_throughput: 48227.409\n",
      "    load_time_ms: 20.735\n",
      "    sample_throughput: 70.691\n",
      "    sample_time_ms: 14146.121\n",
      "    update_time_ms: 6.784\n",
      "  timestamp: 1633161784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          425.16</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.380464103486803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00878413366902536\n",
      "          policy_loss: -0.16162001540263493\n",
      "          total_loss: -0.1798016674609648\n",
      "          vf_explained_var: -0.2627602219581604\n",
      "          vf_loss: 0.0029877474738491906\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.768\n",
      "    ram_util_percent: 31.184\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07013622889369085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 68.08458090883266\n",
      "    mean_inference_ms: 2.5009313929980843\n",
      "    mean_raw_obs_processing_ms: 0.2109809760443079\n",
      "  time_since_restore: 442.9325969219208\n",
      "  time_this_iter_s: 17.77301812171936\n",
      "  time_total_s: 442.9325969219208\n",
      "  timers:\n",
      "    learn_throughput: 559.655\n",
      "    learn_time_ms: 1786.816\n",
      "    load_throughput: 45420.92\n",
      "    load_time_ms: 22.016\n",
      "    sample_throughput: 69.254\n",
      "    sample_time_ms: 14439.563\n",
      "    update_time_ms: 6.825\n",
      "  timestamp: 1633161802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         442.933</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.350667651494344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01220372760733852\n",
      "          policy_loss: -0.09368460807535384\n",
      "          total_loss: -0.11101704790360398\n",
      "          vf_explained_var: -0.06730862706899643\n",
      "          vf_loss: 0.002513117877404309\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.16250000000001\n",
      "    ram_util_percent: 31.175\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06997028379212966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 65.23308409871422\n",
      "    mean_inference_ms: 2.4964225734389403\n",
      "    mean_raw_obs_processing_ms: 0.20983391569000975\n",
      "  time_since_restore: 459.43463373184204\n",
      "  time_this_iter_s: 16.502036809921265\n",
      "  time_total_s: 459.43463373184204\n",
      "  timers:\n",
      "    learn_throughput: 576.453\n",
      "    learn_time_ms: 1734.746\n",
      "    load_throughput: 45918.378\n",
      "    load_time_ms: 21.778\n",
      "    sample_throughput: 68.13\n",
      "    sample_time_ms: 14677.811\n",
      "    update_time_ms: 6.895\n",
      "  timestamp: 1633161819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         459.435</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3587653080622357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010714324040161502\n",
      "          policy_loss: -0.10776974144909117\n",
      "          total_loss: -0.12615874136487643\n",
      "          vf_explained_var: -0.5301680564880371\n",
      "          vf_loss: 0.001984355960222375\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.94782608695652\n",
      "    ram_util_percent: 31.195652173913043\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06983991874611736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 62.69050053843478\n",
      "    mean_inference_ms: 2.492210215686321\n",
      "    mean_raw_obs_processing_ms: 0.2088130270889304\n",
      "  time_since_restore: 475.6058533191681\n",
      "  time_this_iter_s: 16.17121958732605\n",
      "  time_total_s: 475.6058533191681\n",
      "  timers:\n",
      "    learn_throughput: 593.684\n",
      "    learn_time_ms: 1684.397\n",
      "    load_throughput: 47244.964\n",
      "    load_time_ms: 21.166\n",
      "    sample_throughput: 68.312\n",
      "    sample_time_ms: 14638.72\n",
      "    update_time_ms: 6.93\n",
      "  timestamp: 1633161835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         475.606</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-04-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2662361515892875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010066390428302666\n",
      "          policy_loss: -0.09792334909240405\n",
      "          total_loss: -0.11547860784663094\n",
      "          vf_explained_var: -0.9567546248435974\n",
      "          vf_loss: 0.002087186795607623\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.71304347826089\n",
      "    ram_util_percent: 31.26521739130434\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06975237846229418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 60.407242211272546\n",
      "    mean_inference_ms: 2.4883495851921014\n",
      "    mean_raw_obs_processing_ms: 0.2079252351992583\n",
      "  time_since_restore: 492.13618445396423\n",
      "  time_this_iter_s: 16.530331134796143\n",
      "  time_total_s: 492.13618445396423\n",
      "  timers:\n",
      "    learn_throughput: 596.096\n",
      "    learn_time_ms: 1677.583\n",
      "    load_throughput: 53206.884\n",
      "    load_time_ms: 18.795\n",
      "    sample_throughput: 68.199\n",
      "    sample_time_ms: 14663.027\n",
      "    update_time_ms: 6.756\n",
      "  timestamp: 1633161852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         492.136</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-04-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.295942846934001\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009359804366588955\n",
      "          policy_loss: -0.09024037793278694\n",
      "          total_loss: -0.1084925083650483\n",
      "          vf_explained_var: -0.9236060976982117\n",
      "          vf_loss: 0.0018993563342115118\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0625\n",
      "    ram_util_percent: 31.320833333333336\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06969718234458411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 58.34481180448192\n",
      "    mean_inference_ms: 2.484831914091109\n",
      "    mean_raw_obs_processing_ms: 0.20715436431654266\n",
      "  time_since_restore: 508.9597680568695\n",
      "  time_this_iter_s: 16.823583602905273\n",
      "  time_total_s: 508.9597680568695\n",
      "  timers:\n",
      "    learn_throughput: 595.402\n",
      "    learn_time_ms: 1679.538\n",
      "    load_throughput: 53875.907\n",
      "    load_time_ms: 18.561\n",
      "    sample_throughput: 68.385\n",
      "    sample_time_ms: 14623.175\n",
      "    update_time_ms: 6.895\n",
      "  timestamp: 1633161869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          508.96</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-04-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.308965635299683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010724465343371833\n",
      "          policy_loss: -0.1465073295144571\n",
      "          total_loss: -0.1646737790770001\n",
      "          vf_explained_var: -0.9383972883224487\n",
      "          vf_loss: 0.001705866341944784\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.16521739130434\n",
      "    ram_util_percent: 31.278260869565226\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06966274719309032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.470950374149815\n",
      "    mean_inference_ms: 2.481649429510186\n",
      "    mean_raw_obs_processing_ms: 0.20645135283582913\n",
      "  time_since_restore: 525.1087436676025\n",
      "  time_this_iter_s: 16.148975610733032\n",
      "  time_total_s: 525.1087436676025\n",
      "  timers:\n",
      "    learn_throughput: 613.746\n",
      "    learn_time_ms: 1629.339\n",
      "    load_throughput: 55003.442\n",
      "    load_time_ms: 18.181\n",
      "    sample_throughput: 68.775\n",
      "    sample_time_ms: 14540.075\n",
      "    update_time_ms: 6.3\n",
      "  timestamp: 1633161885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         525.109</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.215860652923584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009488277918171306\n",
      "          policy_loss: -0.2925342020061281\n",
      "          total_loss: -0.31007658309406705\n",
      "          vf_explained_var: -0.35835760831832886\n",
      "          vf_loss: 0.0017697355173166014\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5304347826087\n",
      "    ram_util_percent: 31.23913043478261\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06964524103714972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.75755228060671\n",
      "    mean_inference_ms: 2.4787968001444485\n",
      "    mean_raw_obs_processing_ms: 0.20582610700852513\n",
      "  time_since_restore: 540.7893631458282\n",
      "  time_this_iter_s: 15.680619478225708\n",
      "  time_total_s: 540.7893631458282\n",
      "  timers:\n",
      "    learn_throughput: 619.825\n",
      "    learn_time_ms: 1613.358\n",
      "    load_throughput: 57575.471\n",
      "    load_time_ms: 17.369\n",
      "    sample_throughput: 68.145\n",
      "    sample_time_ms: 14674.641\n",
      "    update_time_ms: 6.329\n",
      "  timestamp: 1633161900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         540.789</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.24553022119734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009003435538894104\n",
      "          policy_loss: -0.05972740418381161\n",
      "          total_loss: -0.0773909698964821\n",
      "          vf_explained_var: -0.9506805539131165\n",
      "          vf_loss: 0.002090705519852539\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.50434782608696\n",
      "    ram_util_percent: 31.195652173913054\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0696099816610984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 53.185663204121724\n",
      "    mean_inference_ms: 2.475979283127598\n",
      "    mean_raw_obs_processing_ms: 0.20523699963936348\n",
      "  time_since_restore: 557.2432179450989\n",
      "  time_this_iter_s: 16.45385479927063\n",
      "  time_total_s: 557.2432179450989\n",
      "  timers:\n",
      "    learn_throughput: 616.642\n",
      "    learn_time_ms: 1621.687\n",
      "    load_throughput: 61495.55\n",
      "    load_time_ms: 16.261\n",
      "    sample_throughput: 67.877\n",
      "    sample_time_ms: 14732.54\n",
      "    update_time_ms: 6.162\n",
      "  timestamp: 1633161917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         557.243</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.189465970463223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009367137870199235\n",
      "          policy_loss: -0.03264522407617834\n",
      "          total_loss: -0.049852876075439985\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001876864143155722\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.94800000000001\n",
      "    ram_util_percent: 31.22\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06957022610441102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.739337366113595\n",
      "    mean_inference_ms: 2.4733247716297377\n",
      "    mean_raw_obs_processing_ms: 0.20467632932865573\n",
      "  time_since_restore: 574.1926875114441\n",
      "  time_this_iter_s: 16.949469566345215\n",
      "  time_total_s: 574.1926875114441\n",
      "  timers:\n",
      "    learn_throughput: 629.524\n",
      "    learn_time_ms: 1588.503\n",
      "    load_throughput: 59892.533\n",
      "    load_time_ms: 16.697\n",
      "    sample_throughput: 67.573\n",
      "    sample_time_ms: 14798.807\n",
      "    update_time_ms: 4.801\n",
      "  timestamp: 1633161934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         574.193</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.247083483801948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009809034730543557\n",
      "          policy_loss: -0.12085900281866392\n",
      "          total_loss: -0.13882948611345555\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015576437076864143\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.93333333333334\n",
      "    ram_util_percent: 31.183333333333334\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06954520378014428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 50.40316783475448\n",
      "    mean_inference_ms: 2.4710516422234465\n",
      "    mean_raw_obs_processing_ms: 0.20420302203128282\n",
      "  time_since_restore: 591.0596399307251\n",
      "  time_this_iter_s: 16.866952419281006\n",
      "  time_total_s: 591.0596399307251\n",
      "  timers:\n",
      "    learn_throughput: 639.003\n",
      "    learn_time_ms: 1564.937\n",
      "    load_throughput: 68822.489\n",
      "    load_time_ms: 14.53\n",
      "    sample_throughput: 66.671\n",
      "    sample_time_ms: 14999.086\n",
      "    update_time_ms: 4.627\n",
      "  timestamp: 1633161951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">          591.06</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-06-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.29186880853441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012569230207493062\n",
      "          policy_loss: -0.060284621434079276\n",
      "          total_loss: -0.07803077167934841\n",
      "          vf_explained_var: -0.48386430740356445\n",
      "          vf_loss: 0.0014017721528135653\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.26818181818182\n",
      "    ram_util_percent: 31.259090909090897\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06951444172911925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.16262152235843\n",
      "    mean_inference_ms: 2.468884912463686\n",
      "    mean_raw_obs_processing_ms: 0.20374669510543128\n",
      "  time_since_restore: 606.647251367569\n",
      "  time_this_iter_s: 15.587611436843872\n",
      "  time_total_s: 606.647251367569\n",
      "  timers:\n",
      "    learn_throughput: 658.567\n",
      "    learn_time_ms: 1518.448\n",
      "    load_throughput: 75183.176\n",
      "    load_time_ms: 13.301\n",
      "    sample_throughput: 67.443\n",
      "    sample_time_ms: 14827.362\n",
      "    update_time_ms: 5.291\n",
      "  timestamp: 1633161966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         606.647</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-06-20\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7910295168558756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007871447651408816\n",
      "          policy_loss: -0.16238004378974438\n",
      "          total_loss: -0.17768344949516984\n",
      "          vf_explained_var: 0.29554057121276855\n",
      "          vf_loss: 0.00024546026638240113\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.84\n",
      "    ram_util_percent: 31.325\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06947708053283437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.00516020177039\n",
      "    mean_inference_ms: 2.466778286907047\n",
      "    mean_raw_obs_processing_ms: 0.2033216139583728\n",
      "  time_since_restore: 620.454656124115\n",
      "  time_this_iter_s: 13.80740475654602\n",
      "  time_total_s: 620.454656124115\n",
      "  timers:\n",
      "    learn_throughput: 680.164\n",
      "    learn_time_ms: 1470.233\n",
      "    load_throughput: 73568.922\n",
      "    load_time_ms: 13.593\n",
      "    sample_throughput: 68.465\n",
      "    sample_time_ms: 14605.899\n",
      "    update_time_ms: 5.239\n",
      "  timestamp: 1633161980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         620.455</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-06-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7241903556717766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004136660007062155\n",
      "          policy_loss: -0.15480173548890486\n",
      "          total_loss: -0.1703178840999802\n",
      "          vf_explained_var: 0.04485338553786278\n",
      "          vf_loss: 0.00048475229840631883\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.04499999999999\n",
      "    ram_util_percent: 31.32\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06943479770583823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.92282115447816\n",
      "    mean_inference_ms: 2.4647087227340396\n",
      "    mean_raw_obs_processing_ms: 0.20290880802933337\n",
      "  time_since_restore: 634.8762974739075\n",
      "  time_this_iter_s: 14.42164134979248\n",
      "  time_total_s: 634.8762974739075\n",
      "  timers:\n",
      "    learn_throughput: 682.016\n",
      "    learn_time_ms: 1466.242\n",
      "    load_throughput: 73646.428\n",
      "    load_time_ms: 13.578\n",
      "    sample_throughput: 69.282\n",
      "    sample_time_ms: 14433.855\n",
      "    update_time_ms: 6.161\n",
      "  timestamp: 1633161995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         634.876</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ba7ae_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-02_08-06-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: e6b91d6d56dc46e0b618e1e217a489ac\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.700627491209242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008560338233513123\n",
      "          policy_loss: -0.13626314542359777\n",
      "          total_loss: -0.1515316622124778\n",
      "          vf_explained_var: 0.07711713761091232\n",
      "          vf_loss: 0.00045370558225638687\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.80000000000001\n",
      "    ram_util_percent: 31.24500000000001\n",
      "  pid: 50048\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06938372126741713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.907266656413654\n",
      "    mean_inference_ms: 2.4626044289700606\n",
      "    mean_raw_obs_processing_ms: 0.20251036280683415\n",
      "  time_since_restore: 648.7012867927551\n",
      "  time_this_iter_s: 13.824989318847656\n",
      "  time_total_s: 648.7012867927551\n",
      "  timers:\n",
      "    learn_throughput: 694.599\n",
      "    learn_time_ms: 1439.68\n",
      "    load_throughput: 71115.331\n",
      "    load_time_ms: 14.062\n",
      "    sample_throughput: 70.476\n",
      "    sample_time_ms: 14189.301\n",
      "    update_time_ms: 6.268\n",
      "  timestamp: 1633162009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: ba7ae_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 34.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.24 GiB heap, 0.0/32.81 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-02_07-52-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ba7ae_00000</td><td>RUNNING </td><td>192.168.1.96:50048</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         648.701</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C8 pretrained (AnnaCNN)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
