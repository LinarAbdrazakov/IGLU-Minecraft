{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ray torch torchvision tabulate tensorboard\n",
    "#!pip3 install 'ray[rllib]'\n",
    "#!pip3 install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder(features_dim)\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/src/Visual Autoencoder weights and models/IGLU_encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C22']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 33.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 08:54:00,356\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id f02b1_00000 but id ccae9_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m 2021-09-17 08:54:06,313\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m 2021-09-17 08:54:06,313\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C22 pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/ccae9_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/ccae9_00000</a><br/>\n",
       "                Run data is saved locally in <code>/src/wandb/run-20210917_085401-ccae9_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m 2021-09-17 08:54:32,399\tINFO trainable.py:109 -- Trainable.setup took 30.337 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=177)\u001b[0m 2021-09-17 08:54:32,400\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 2.0\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2638701505131191\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007371294528260108\n",
      "          policy_loss: -0.14228328110443222\n",
      "          total_loss: -0.055882537023474774\n",
      "          vf_explained_var: -0.2014622986316681\n",
      "          vf_loss: 0.09756518351948923\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.715025906735754\n",
      "    ram_util_percent: 33.03575129533679\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08549032868681612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 130.24627221571458\n",
      "    mean_inference_ms: 2.875944236656288\n",
      "    mean_raw_obs_processing_ms: 0.28199034851866883\n",
      "  time_since_restore: 135.2741413116455\n",
      "  time_this_iter_s: 135.2741413116455\n",
      "  time_total_s: 135.2741413116455\n",
      "  timers:\n",
      "    learn_throughput: 663.648\n",
      "    learn_time_ms: 1506.823\n",
      "    load_throughput: 38390.042\n",
      "    load_time_ms: 26.048\n",
      "    sample_throughput: 7.478\n",
      "    sample_time_ms: 133727.906\n",
      "    update_time_ms: 5.35\n",
      "  timestamp: 1631869007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 37.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         135.274</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3267443339029947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029203277595236444\n",
      "          policy_loss: -0.1886763532956441\n",
      "          total_loss: -0.14949229419645335\n",
      "          vf_explained_var: -0.2841486632823944\n",
      "          vf_loss: 0.0518674381594691\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.01304347826088\n",
      "    ram_util_percent: 34.50869565217391\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08469062990408475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 100.62983586152279\n",
      "    mean_inference_ms: 2.8316732417807\n",
      "    mean_raw_obs_processing_ms: 0.2708362524270682\n",
      "  time_since_restore: 150.984876871109\n",
      "  time_this_iter_s: 15.710735559463501\n",
      "  time_total_s: 150.984876871109\n",
      "  timers:\n",
      "    learn_throughput: 842.205\n",
      "    learn_time_ms: 1187.359\n",
      "    load_throughput: 62484.976\n",
      "    load_time_ms: 16.004\n",
      "    sample_throughput: 13.463\n",
      "    sample_time_ms: 74275.217\n",
      "    update_time_ms: 6.215\n",
      "  timestamp: 1631869023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         150.985</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.3333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0641983840200635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009514609790496802\n",
      "          policy_loss: -0.13647007072965303\n",
      "          total_loss: -0.09078157742818196\n",
      "          vf_explained_var: 0.5495882034301758\n",
      "          vf_loss: 0.05537901787708203\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.62727272727273\n",
      "    ram_util_percent: 34.577272727272735\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08436540393451371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 84.11480647930281\n",
      "    mean_inference_ms: 2.8131984868572357\n",
      "    mean_raw_obs_processing_ms: 0.2644510755523308\n",
      "  time_since_restore: 166.204443693161\n",
      "  time_this_iter_s: 15.219566822052002\n",
      "  time_total_s: 166.204443693161\n",
      "  timers:\n",
      "    learn_throughput: 927.157\n",
      "    learn_time_ms: 1078.566\n",
      "    load_throughput: 82826.999\n",
      "    load_time_ms: 12.073\n",
      "    sample_throughput: 18.417\n",
      "    sample_time_ms: 54298.893\n",
      "    update_time_ms: 5.216\n",
      "  timestamp: 1631869038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         166.204</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> 1.33333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1945028980573018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00492661648780681\n",
      "          policy_loss: -0.056288139977388914\n",
      "          total_loss: -0.03694659185906251\n",
      "          vf_explained_var: -0.05728582665324211\n",
      "          vf_loss: 0.030793915544119147\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.17619047619047\n",
      "    ram_util_percent: 34.53809523809524\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0840893177988426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 73.3626435785676\n",
      "    mean_inference_ms: 2.8006810783182328\n",
      "    mean_raw_obs_processing_ms: 0.25950975956384825\n",
      "  time_since_restore: 181.4100923538208\n",
      "  time_this_iter_s: 15.20564866065979\n",
      "  time_total_s: 181.4100923538208\n",
      "  timers:\n",
      "    learn_throughput: 957.279\n",
      "    learn_time_ms: 1044.628\n",
      "    load_throughput: 93624.947\n",
      "    load_time_ms: 10.681\n",
      "    sample_throughput: 22.581\n",
      "    sample_time_ms: 44285.701\n",
      "    update_time_ms: 4.981\n",
      "  timestamp: 1631869053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          181.41</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.2\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5039758493502935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010314543527916461\n",
      "          policy_loss: -0.14678724780678748\n",
      "          total_loss: -0.12443516792522537\n",
      "          vf_explained_var: 0.11248816549777985\n",
      "          vf_loss: 0.026876108850248985\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.622727272727275\n",
      "    ram_util_percent: 34.54545454545455\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08386614059660238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 65.70962623043225\n",
      "    mean_inference_ms: 2.79085212908216\n",
      "    mean_raw_obs_processing_ms: 0.25623810386966694\n",
      "  time_since_restore: 196.37723803520203\n",
      "  time_this_iter_s: 14.967145681381226\n",
      "  time_total_s: 196.37723803520203\n",
      "  timers:\n",
      "    learn_throughput: 1000.069\n",
      "    learn_time_ms: 999.931\n",
      "    load_throughput: 106105.936\n",
      "    load_time_ms: 9.425\n",
      "    sample_throughput: 26.14\n",
      "    sample_time_ms: 38255.115\n",
      "    update_time_ms: 4.604\n",
      "  timestamp: 1631869068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         196.377</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">     1.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8619716909196642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008421977148433946\n",
      "          policy_loss: -0.15512842055824067\n",
      "          total_loss: -0.14068603875736396\n",
      "          vf_explained_var: -0.275292307138443\n",
      "          vf_loss: 0.02264099792163405\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.09047619047618\n",
      "    ram_util_percent: 34.55238095238096\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08369416817562754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 59.931408892450285\n",
      "    mean_inference_ms: 2.7826759519214206\n",
      "    mean_raw_obs_processing_ms: 0.25346835509204224\n",
      "  time_since_restore: 211.06440997123718\n",
      "  time_this_iter_s: 14.687171936035156\n",
      "  time_total_s: 211.06440997123718\n",
      "  timers:\n",
      "    learn_throughput: 1022.839\n",
      "    learn_time_ms: 977.671\n",
      "    load_throughput: 112911.482\n",
      "    load_time_ms: 8.856\n",
      "    sample_throughput: 29.257\n",
      "    sample_time_ms: 34179.764\n",
      "    update_time_ms: 4.66\n",
      "  timestamp: 1631869083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         211.064</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-58-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.8571428571428571\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.828158105744256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00547120779050153\n",
      "          policy_loss: -0.17185391618145837\n",
      "          total_loss: -0.16623784115331042\n",
      "          vf_explained_var: -0.3235822021961212\n",
      "          vf_loss: 0.013624096366887292\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.63333333333334\n",
      "    ram_util_percent: 34.53333333333333\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0834722442036822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 55.395970127196755\n",
      "    mean_inference_ms: 2.7741329583348926\n",
      "    mean_raw_obs_processing_ms: 0.25098643907599133\n",
      "  time_since_restore: 226.00385761260986\n",
      "  time_this_iter_s: 14.93944764137268\n",
      "  time_total_s: 226.00385761260986\n",
      "  timers:\n",
      "    learn_throughput: 1030.353\n",
      "    learn_time_ms: 970.541\n",
      "    load_throughput: 123187.97\n",
      "    load_time_ms: 8.118\n",
      "    sample_throughput: 31.953\n",
      "    sample_time_ms: 31295.906\n",
      "    update_time_ms: 5.244\n",
      "  timestamp: 1631869098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         226.004</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">0.857143</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 1.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.35305360158284504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0037894813533234303\n",
      "          policy_loss: -0.18278620996408992\n",
      "          total_loss: -0.17254396403829256\n",
      "          vf_explained_var: 0.5013548731803894\n",
      "          vf_loss: 0.013583308297933803\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.25714285714286\n",
      "    ram_util_percent: 34.55714285714286\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08307508532930774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.73018873342425\n",
      "    mean_inference_ms: 2.76138991860429\n",
      "    mean_raw_obs_processing_ms: 0.248624940758913\n",
      "  time_since_restore: 240.7696123123169\n",
      "  time_this_iter_s: 14.765754699707031\n",
      "  time_total_s: 240.7696123123169\n",
      "  timers:\n",
      "    learn_throughput: 1048.375\n",
      "    learn_time_ms: 953.858\n",
      "    load_throughput: 130536.088\n",
      "    load_time_ms: 7.661\n",
      "    sample_throughput: 34.337\n",
      "    sample_time_ms: 29123.09\n",
      "    update_time_ms: 5.178\n",
      "  timestamp: 1631869113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          240.77</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-58-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.8888888888888888\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4959735459751553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005207974388619855\n",
      "          policy_loss: -0.15570432568589845\n",
      "          total_loss: -0.14782057479023933\n",
      "          vf_explained_var: -0.3123420476913452\n",
      "          vf_loss: 0.012713286562615798\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.489999999999995\n",
      "    ram_util_percent: 34.57\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08256466459275183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.68884258994579\n",
      "    mean_inference_ms: 2.7460536311006125\n",
      "    mean_raw_obs_processing_ms: 0.24601107725464447\n",
      "  time_since_restore: 254.8672366142273\n",
      "  time_this_iter_s: 14.0976243019104\n",
      "  time_total_s: 254.8672366142273\n",
      "  timers:\n",
      "    learn_throughput: 1054.637\n",
      "    learn_time_ms: 948.193\n",
      "    load_throughput: 137378.033\n",
      "    load_time_ms: 7.279\n",
      "    sample_throughput: 36.561\n",
      "    sample_time_ms: 27351.855\n",
      "    update_time_ms: 5.019\n",
      "  timestamp: 1631869127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         254.867</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">0.888889</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-59-02\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.8\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4483474761247635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0008588785514054583\n",
      "          policy_loss: -0.16754555934005314\n",
      "          total_loss: -0.16417291255460845\n",
      "          vf_explained_var: -0.3616377115249634\n",
      "          vf_loss: 0.007834648139153917\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.10476190476191\n",
      "    ram_util_percent: 34.58095238095239\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08210904139357203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.11880780219763\n",
      "    mean_inference_ms: 2.7336026813257677\n",
      "    mean_raw_obs_processing_ms: 0.24367139666481727\n",
      "  time_since_restore: 269.40007281303406\n",
      "  time_this_iter_s: 14.532836198806763\n",
      "  time_total_s: 269.40007281303406\n",
      "  timers:\n",
      "    learn_throughput: 1056.682\n",
      "    learn_time_ms: 946.359\n",
      "    load_throughput: 142880.638\n",
      "    load_time_ms: 6.999\n",
      "    sample_throughput: 38.498\n",
      "    sample_time_ms: 25975.576\n",
      "    update_time_ms: 4.916\n",
      "  timestamp: 1631869142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">           269.4</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">     0.8</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.7272727272727273\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4803800665669971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0022142929054215048\n",
      "          policy_loss: -0.16848707497119902\n",
      "          total_loss: -0.16770227940546142\n",
      "          vf_explained_var: -0.277459979057312\n",
      "          vf_loss: 0.005560916120238188\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.19047619047619\n",
      "    ram_util_percent: 34.609523809523814\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08165170652371828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.91553874859331\n",
      "    mean_inference_ms: 2.720449829798037\n",
      "    mean_raw_obs_processing_ms: 0.24137581358082824\n",
      "  time_since_restore: 283.7113857269287\n",
      "  time_this_iter_s: 14.311312913894653\n",
      "  time_total_s: 283.7113857269287\n",
      "  timers:\n",
      "    learn_throughput: 1128.229\n",
      "    learn_time_ms: 886.345\n",
      "    load_throughput: 206774.861\n",
      "    load_time_ms: 4.836\n",
      "    sample_throughput: 71.727\n",
      "    sample_time_ms: 13941.796\n",
      "    update_time_ms: 4.891\n",
      "  timestamp: 1631869156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         283.711</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">0.727273</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6666666666666666\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.45173031522168056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004001243547247668\n",
      "          policy_loss: -0.17224202288521662\n",
      "          total_loss: -0.17275281217363145\n",
      "          vf_explained_var: -0.2397492229938507\n",
      "          vf_loss: 0.003981506581314736\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.894999999999996\n",
      "    ram_util_percent: 34.63000000000001\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08119357468570793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.00342807808108\n",
      "    mean_inference_ms: 2.7075401974305024\n",
      "    mean_raw_obs_processing_ms: 0.2392225436164075\n",
      "  time_since_restore: 298.1924967765808\n",
      "  time_this_iter_s: 14.4811110496521\n",
      "  time_total_s: 298.1924967765808\n",
      "  timers:\n",
      "    learn_throughput: 1120.703\n",
      "    learn_time_ms: 892.297\n",
      "    load_throughput: 212580.789\n",
      "    load_time_ms: 4.704\n",
      "    sample_throughput: 72.395\n",
      "    sample_time_ms: 13813.128\n",
      "    update_time_ms: 4.665\n",
      "  timestamp: 1631869171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         298.192</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">0.666667</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6153846153846154\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4392571965853373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001312520665117805\n",
      "          policy_loss: -0.17031776673263974\n",
      "          total_loss: -0.17190882083442477\n",
      "          vf_explained_var: -0.315489798784256\n",
      "          vf_loss: 0.002797417849716213\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.26666666666667\n",
      "    ram_util_percent: 34.62857142857143\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08073417247549838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.32768269419921\n",
      "    mean_inference_ms: 2.694822301986894\n",
      "    mean_raw_obs_processing_ms: 0.23720977086686867\n",
      "  time_since_restore: 312.8666253089905\n",
      "  time_this_iter_s: 14.674128532409668\n",
      "  time_total_s: 312.8666253089905\n",
      "  timers:\n",
      "    learn_throughput: 1116.086\n",
      "    learn_time_ms: 895.988\n",
      "    load_throughput: 212655.158\n",
      "    load_time_ms: 4.702\n",
      "    sample_throughput: 72.702\n",
      "    sample_time_ms: 13754.713\n",
      "    update_time_ms: 4.806\n",
      "  timestamp: 1631869185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         312.867</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">0.615385</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_08-59-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5714285714285714\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4425276666879654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001342836949564546\n",
      "          policy_loss: -0.16975600288973913\n",
      "          total_loss: -0.1721155012647311\n",
      "          vf_explained_var: -0.3182525932788849\n",
      "          vf_loss: 0.002063680479639313\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.96\n",
      "    ram_util_percent: 34.605000000000004\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08028840427031689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.84290021806127\n",
      "    mean_inference_ms: 2.6823216067934497\n",
      "    mean_raw_obs_processing_ms: 0.23524775967665948\n",
      "  time_since_restore: 326.90350341796875\n",
      "  time_this_iter_s: 14.036878108978271\n",
      "  time_total_s: 326.90350341796875\n",
      "  timers:\n",
      "    learn_throughput: 1135.36\n",
      "    learn_time_ms: 880.778\n",
      "    load_throughput: 227977.323\n",
      "    load_time_ms: 4.386\n",
      "    sample_throughput: 73.244\n",
      "    sample_time_ms: 13653.077\n",
      "    update_time_ms: 5.124\n",
      "  timestamp: 1631869199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         326.904</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">0.571429</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-00-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5333333333333333\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3317614522245195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011419471095434928\n",
      "          policy_loss: -0.1702460648285018\n",
      "          total_loss: -0.1719860452744696\n",
      "          vf_explained_var: -0.33333316445350647\n",
      "          vf_loss: 0.0015687122889277009\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.93000000000001\n",
      "    ram_util_percent: 34.60000000000001\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07986405450245672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.515787309644644\n",
      "    mean_inference_ms: 2.6704526939750495\n",
      "    mean_raw_obs_processing_ms: 0.2334033122768148\n",
      "  time_since_restore: 340.8605315685272\n",
      "  time_this_iter_s: 13.957028150558472\n",
      "  time_total_s: 340.8605315685272\n",
      "  timers:\n",
      "    learn_throughput: 1127.269\n",
      "    learn_time_ms: 887.1\n",
      "    load_throughput: 223949.425\n",
      "    load_time_ms: 4.465\n",
      "    sample_throughput: 73.826\n",
      "    sample_time_ms: 13545.452\n",
      "    update_time_ms: 5.24\n",
      "  timestamp: 1631869213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         340.861</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">0.533333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.625\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5499187615182665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029796191280851217\n",
      "          policy_loss: -0.13256903667416836\n",
      "          total_loss: -0.13108960866100258\n",
      "          vf_explained_var: 0.5581504106521606\n",
      "          vf_loss: 0.006955334433991488\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.35999999999999\n",
      "    ram_util_percent: 34.6\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794598178334066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.32206833338893\n",
      "    mean_inference_ms: 2.659198842401483\n",
      "    mean_raw_obs_processing_ms: 0.23172167409155184\n",
      "  time_since_restore: 354.8851616382599\n",
      "  time_this_iter_s: 14.024630069732666\n",
      "  time_total_s: 354.8851616382599\n",
      "  timers:\n",
      "    learn_throughput: 1135.352\n",
      "    learn_time_ms: 880.784\n",
      "    load_throughput: 226041.012\n",
      "    load_time_ms: 4.424\n",
      "    sample_throughput: 74.152\n",
      "    sample_time_ms: 13485.865\n",
      "    update_time_ms: 5.062\n",
      "  timestamp: 1631869227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         354.885</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   0.625</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.7058823529411765\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001171875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5476481669478946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009714132739995307\n",
      "          policy_loss: -0.16685405688153374\n",
      "          total_loss: -0.16519847677813637\n",
      "          vf_explained_var: 0.5546317100524902\n",
      "          vf_loss: 0.007120680441019229\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.4904761904762\n",
      "    ram_util_percent: 34.61428571428572\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07907672397056736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.24141054273664\n",
      "    mean_inference_ms: 2.6485261565595395\n",
      "    mean_raw_obs_processing_ms: 0.23014759080042024\n",
      "  time_since_restore: 368.92122507095337\n",
      "  time_this_iter_s: 14.036063432693481\n",
      "  time_total_s: 368.92122507095337\n",
      "  timers:\n",
      "    learn_throughput: 1133.57\n",
      "    learn_time_ms: 882.169\n",
      "    load_throughput: 213037.52\n",
      "    load_time_ms: 4.694\n",
      "    sample_throughput: 74.658\n",
      "    sample_time_ms: 13394.419\n",
      "    update_time_ms: 4.656\n",
      "  timestamp: 1631869241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         368.921</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">0.705882</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6666666666666666\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001171875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41486725012461345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0022755200169686735\n",
      "          policy_loss: -0.12327586867743068\n",
      "          total_loss: -0.12263859469029638\n",
      "          vf_explained_var: -0.34633469581604004\n",
      "          vf_loss: 0.004783279329745306\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.73157894736841\n",
      "    ram_util_percent: 34.58947368421053\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07870990393860788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.256872789113075\n",
      "    mean_inference_ms: 2.6382728463773453\n",
      "    mean_raw_obs_processing_ms: 0.22862701391013143\n",
      "  time_since_restore: 382.4635269641876\n",
      "  time_this_iter_s: 13.542301893234253\n",
      "  time_total_s: 382.4635269641876\n",
      "  timers:\n",
      "    learn_throughput: 1135.224\n",
      "    learn_time_ms: 880.883\n",
      "    load_throughput: 214525.944\n",
      "    load_time_ms: 4.661\n",
      "    sample_throughput: 75.338\n",
      "    sample_time_ms: 13273.486\n",
      "    update_time_ms: 4.604\n",
      "  timestamp: 1631869255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         382.464</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">0.666667</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5789473684210527\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5045332570870718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060724824405753\n",
      "          policy_loss: -0.12294840498103035\n",
      "          total_loss: -0.11282028042607838\n",
      "          vf_explained_var: 0.35318514704704285\n",
      "          vf_loss: 0.015169900636505595\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.135000000000005\n",
      "    ram_util_percent: 34.625000000000014\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07836323768413074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.35693266514404\n",
      "    mean_inference_ms: 2.628541415024094\n",
      "    mean_raw_obs_processing_ms: 0.2272233995814542\n",
      "  time_since_restore: 396.464150428772\n",
      "  time_this_iter_s: 14.00062346458435\n",
      "  time_total_s: 396.464150428772\n",
      "  timers:\n",
      "    learn_throughput: 1153.285\n",
      "    learn_time_ms: 867.088\n",
      "    load_throughput: 191737.858\n",
      "    load_time_ms: 5.215\n",
      "    sample_throughput: 75.318\n",
      "    sample_time_ms: 13277.005\n",
      "    update_time_ms: 4.527\n",
      "  timestamp: 1631869269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         396.464</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">0.578947</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-01-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.45\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.001783232556449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07560953322294447\n",
      "          policy_loss: 0.08567170715994304\n",
      "          total_loss: 0.10726093161437246\n",
      "          vf_explained_var: 0.6416773200035095\n",
      "          vf_loss: 0.03156275768867797\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.73\n",
      "    ram_util_percent: 34.6\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07803508372990947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.5304245944982\n",
      "    mean_inference_ms: 2.6193606414161925\n",
      "    mean_raw_obs_processing_ms: 0.22593428034974342\n",
      "  time_since_restore: 410.5541641712189\n",
      "  time_this_iter_s: 14.0900137424469\n",
      "  time_total_s: 410.5541641712189\n",
      "  timers:\n",
      "    learn_throughput: 1151.797\n",
      "    learn_time_ms: 868.208\n",
      "    load_throughput: 181547.238\n",
      "    load_time_ms: 5.508\n",
      "    sample_throughput: 75.58\n",
      "    sample_time_ms: 13230.947\n",
      "    update_time_ms: 4.582\n",
      "  timestamp: 1631869283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         410.554</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">    0.45</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.42857142857142855\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0008789062500000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1558389630582597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.034036156612104504\n",
      "          policy_loss: -0.017310090073280864\n",
      "          total_loss: 0.005914414218730396\n",
      "          vf_explained_var: 0.19238783419132233\n",
      "          vf_loss: 0.034752979760782586\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.464999999999996\n",
      "    ram_util_percent: 34.60000000000001\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07772447239187767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.768511948616514\n",
      "    mean_inference_ms: 2.610671910850585\n",
      "    mean_raw_obs_processing_ms: 0.22470090310247404\n",
      "  time_since_restore: 424.6963224411011\n",
      "  time_this_iter_s: 14.142158269882202\n",
      "  time_total_s: 424.6963224411011\n",
      "  timers:\n",
      "    learn_throughput: 1144.553\n",
      "    learn_time_ms: 873.704\n",
      "    load_throughput: 175047.849\n",
      "    load_time_ms: 5.713\n",
      "    sample_throughput: 75.71\n",
      "    sample_time_ms: 13208.321\n",
      "    update_time_ms: 4.532\n",
      "  timestamp: 1631869297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         424.696</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">0.428571</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0013183593749999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.099529633257124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03837734177244413\n",
      "          policy_loss: -0.23386686763001813\n",
      "          total_loss: -0.22659129401048025\n",
      "          vf_explained_var: 0.6783127188682556\n",
      "          vf_loss: 0.018220275888840357\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.96\n",
      "    ram_util_percent: 34.61\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07742608074923091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.062870506899028\n",
      "    mean_inference_ms: 2.6023388248373687\n",
      "    mean_raw_obs_processing_ms: 0.2235410655967592\n",
      "  time_since_restore: 438.2837793827057\n",
      "  time_this_iter_s: 13.587456941604614\n",
      "  time_total_s: 438.2837793827057\n",
      "  timers:\n",
      "    learn_throughput: 1159.913\n",
      "    learn_time_ms: 862.134\n",
      "    load_throughput: 175006.217\n",
      "    load_time_ms: 5.714\n",
      "    sample_throughput: 76.157\n",
      "    sample_time_ms: 13130.852\n",
      "    update_time_ms: 4.494\n",
      "  timestamp: 1631869311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         438.284</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">     0.5</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5652173913043478\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1219972531000773\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026085655658302738\n",
      "          policy_loss: -0.09167659220596154\n",
      "          total_loss: -0.07817328551577198\n",
      "          vf_explained_var: 0.7809060215950012\n",
      "          vf_loss: 0.0246716967318207\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.37894736842105\n",
      "    ram_util_percent: 34.61578947368421\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07714034029717672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.407203015783406\n",
      "    mean_inference_ms: 2.5943904557663333\n",
      "    mean_raw_obs_processing_ms: 0.22245263661798148\n",
      "  time_since_restore: 451.9238340854645\n",
      "  time_this_iter_s: 13.640054702758789\n",
      "  time_total_s: 451.9238340854645\n",
      "  timers:\n",
      "    learn_throughput: 1165.688\n",
      "    learn_time_ms: 857.863\n",
      "    load_throughput: 171349.247\n",
      "    load_time_ms: 5.836\n",
      "    sample_throughput: 76.736\n",
      "    sample_time_ms: 13031.706\n",
      "    update_time_ms: 4.408\n",
      "  timestamp: 1631869325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         451.924</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">0.565217</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.625\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0029663085937500014\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9722165875964695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03491051173355623\n",
      "          policy_loss: -0.11382555663585663\n",
      "          total_loss: -0.10519747601615058\n",
      "          vf_explained_var: 0.3942827582359314\n",
      "          vf_loss: 0.018246692211121424\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.82631578947369\n",
      "    ram_util_percent: 34.62105263157895\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07686876575635761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.79576446144721\n",
      "    mean_inference_ms: 2.586779829272484\n",
      "    mean_raw_obs_processing_ms: 0.22142742081744524\n",
      "  time_since_restore: 465.32267928123474\n",
      "  time_this_iter_s: 13.398845195770264\n",
      "  time_total_s: 465.32267928123474\n",
      "  timers:\n",
      "    learn_throughput: 1155.288\n",
      "    learn_time_ms: 865.585\n",
      "    load_throughput: 171572.14\n",
      "    load_time_ms: 5.828\n",
      "    sample_throughput: 77.16\n",
      "    sample_time_ms: 12960.096\n",
      "    update_time_ms: 4.356\n",
      "  timestamp: 1631869338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         465.323</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   0.625</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.68\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004449462890624999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5410435696442922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003997754675962284\n",
      "          policy_loss: -0.12563635839356316\n",
      "          total_loss: -0.1239927505246467\n",
      "          vf_explained_var: -0.0812818855047226\n",
      "          vf_loss: 0.007036255724314187\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.05999999999999\n",
      "    ram_util_percent: 34.595000000000006\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0766082427669586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.224397211810295\n",
      "    mean_inference_ms: 2.5794830354329843\n",
      "    mean_raw_obs_processing_ms: 0.22044763721863408\n",
      "  time_since_restore: 478.8322150707245\n",
      "  time_this_iter_s: 13.509535789489746\n",
      "  time_total_s: 478.8322150707245\n",
      "  timers:\n",
      "    learn_throughput: 1171.333\n",
      "    learn_time_ms: 853.728\n",
      "    load_throughput: 169110.841\n",
      "    load_time_ms: 5.913\n",
      "    sample_throughput: 77.354\n",
      "    sample_time_ms: 12927.514\n",
      "    update_time_ms: 4.202\n",
      "  timestamp: 1631869352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         478.832</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">    0.68</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6538461538461539\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0022247314453124993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8724494652615653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03265438811278736\n",
      "          policy_loss: -0.20590276138650046\n",
      "          total_loss: -0.17742193076345655\n",
      "          vf_explained_var: 0.6193729043006897\n",
      "          vf_loss: 0.03713267985213962\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.49473684210527\n",
      "    ram_util_percent: 34.60526315789474\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07635909750873601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.6889674609345\n",
      "    mean_inference_ms: 2.572491715653542\n",
      "    mean_raw_obs_processing_ms: 0.2194966421177587\n",
      "  time_since_restore: 492.2030429840088\n",
      "  time_this_iter_s: 13.370827913284302\n",
      "  time_total_s: 492.2030429840088\n",
      "  timers:\n",
      "    learn_throughput: 1177.929\n",
      "    learn_time_ms: 848.947\n",
      "    load_throughput: 176051.51\n",
      "    load_time_ms: 5.68\n",
      "    sample_throughput: 77.717\n",
      "    sample_time_ms: 12867.134\n",
      "    update_time_ms: 4.269\n",
      "  timestamp: 1631869365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         492.203</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">0.653846</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.7037037037037037\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00333709716796875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5583961400720808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0026772820438552003\n",
      "          policy_loss: -0.011963810357782576\n",
      "          total_loss: -0.011113236678971185\n",
      "          vf_explained_var: 0.9051923751831055\n",
      "          vf_loss: 0.0064255994149587225\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.878947368421045\n",
      "    ram_util_percent: 34.61578947368421\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0761200034312787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.186217372254532\n",
      "    mean_inference_ms: 2.5658001806893935\n",
      "    mean_raw_obs_processing_ms: 0.218593976360496\n",
      "  time_since_restore: 505.7560782432556\n",
      "  time_this_iter_s: 13.553035259246826\n",
      "  time_total_s: 505.7560782432556\n",
      "  timers:\n",
      "    learn_throughput: 1198.43\n",
      "    learn_time_ms: 834.425\n",
      "    load_throughput: 183984.103\n",
      "    load_time_ms: 5.435\n",
      "    sample_throughput: 77.92\n",
      "    sample_time_ms: 12833.732\n",
      "    update_time_ms: 4.166\n",
      "  timestamp: 1631869379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         505.756</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">0.703704</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6785714285714286\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7061865376101599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005068314965082739\n",
      "          policy_loss: -0.16883482897861135\n",
      "          total_loss: -0.15232352591637108\n",
      "          vf_explained_var: 0.5424697399139404\n",
      "          vf_loss: 0.02356471338102387\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.64\n",
      "    ram_util_percent: 34.555\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07589261756775871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.71314673603094\n",
      "    mean_inference_ms: 2.559439382028693\n",
      "    mean_raw_obs_processing_ms: 0.21773922477996246\n",
      "  time_since_restore: 519.3985579013824\n",
      "  time_this_iter_s: 13.642479658126831\n",
      "  time_total_s: 519.3985579013824\n",
      "  timers:\n",
      "    learn_throughput: 1199.069\n",
      "    learn_time_ms: 833.981\n",
      "    load_throughput: 185245.231\n",
      "    load_time_ms: 5.398\n",
      "    sample_throughput: 77.857\n",
      "    sample_time_ms: 12844.122\n",
      "    update_time_ms: 4.127\n",
      "  timestamp: 1631869392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         519.399</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">0.678571</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-03-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6551724137931034\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9651034759150611\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008039568173862774\n",
      "          policy_loss: -0.19940556548535823\n",
      "          total_loss: -0.19593876428488227\n",
      "          vf_explained_var: 0.29955098032951355\n",
      "          vf_loss: 0.013104423236412307\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.41052631578947\n",
      "    ram_util_percent: 34.55263157894737\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.075673360295518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.26711432863361\n",
      "    mean_inference_ms: 2.5533198345223966\n",
      "    mean_raw_obs_processing_ms: 0.21691239588475922\n",
      "  time_since_restore: 532.9212963581085\n",
      "  time_this_iter_s: 13.522738456726074\n",
      "  time_total_s: 532.9212963581085\n",
      "  timers:\n",
      "    learn_throughput: 1198.682\n",
      "    learn_time_ms: 834.25\n",
      "    load_throughput: 209779.183\n",
      "    load_time_ms: 4.767\n",
      "    sample_throughput: 78.146\n",
      "    sample_time_ms: 12796.5\n",
      "    update_time_ms: 4.179\n",
      "  timestamp: 1631869406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         532.921</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">0.655172</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6333333333333333\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9800361189577315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009448464897438037\n",
      "          policy_loss: -0.05048242857058843\n",
      "          total_loss: -0.053174827992916104\n",
      "          vf_explained_var: 0.5123451352119446\n",
      "          vf_loss: 0.00709219681771679\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.64318181818182\n",
      "    ram_util_percent: 34.42045454545455\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0754632400702308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.845801543998615\n",
      "    mean_inference_ms: 2.5474623445621267\n",
      "    mean_raw_obs_processing_ms: 0.23517137621214626\n",
      "  time_since_restore: 563.6804175376892\n",
      "  time_this_iter_s: 30.75912117958069\n",
      "  time_total_s: 563.6804175376892\n",
      "  timers:\n",
      "    learn_throughput: 1222.699\n",
      "    learn_time_ms: 817.863\n",
      "    load_throughput: 155161.846\n",
      "    load_time_ms: 6.445\n",
      "    sample_throughput: 69.067\n",
      "    sample_time_ms: 14478.614\n",
      "    update_time_ms: 4.033\n",
      "  timestamp: 1631869437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          563.68</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">0.633333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.2258064516129\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6129032258064516\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2579226202434963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018370994522461753\n",
      "          policy_loss: 0.0061354890051815245\n",
      "          total_loss: 0.0002695647378762563\n",
      "          vf_explained_var: -0.2760816216468811\n",
      "          vf_loss: 0.006682649190123711\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.19583333333333\n",
      "    ram_util_percent: 34.54583333333333\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07526412669518123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.4506294487745\n",
      "    mean_inference_ms: 2.5419171652120975\n",
      "    mean_raw_obs_processing_ms: 0.25165934186941097\n",
      "  time_since_restore: 580.7335774898529\n",
      "  time_this_iter_s: 17.053159952163696\n",
      "  time_total_s: 580.7335774898529\n",
      "  timers:\n",
      "    learn_throughput: 1249.871\n",
      "    learn_time_ms: 800.083\n",
      "    load_throughput: 150155.873\n",
      "    load_time_ms: 6.66\n",
      "    sample_throughput: 67.626\n",
      "    sample_time_ms: 14787.304\n",
      "    update_time_ms: 3.929\n",
      "  timestamp: 1631869454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         580.734</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">0.612903</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.226</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-04-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.34375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.59375\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.52997324930297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013705651420265141\n",
      "          policy_loss: -0.06156104418138663\n",
      "          total_loss: -0.06858886174029774\n",
      "          vf_explained_var: -0.1084279790520668\n",
      "          vf_loss: 0.008249044956432448\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.26818181818181\n",
      "    ram_util_percent: 34.75454545454545\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0750747258712522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.077104076990498\n",
      "    mean_inference_ms: 2.536629576506752\n",
      "    mean_raw_obs_processing_ms: 0.2665738301914639\n",
      "  time_since_restore: 595.557133436203\n",
      "  time_this_iter_s: 14.823555946350098\n",
      "  time_total_s: 595.557133436203\n",
      "  timers:\n",
      "    learn_throughput: 1258.836\n",
      "    learn_time_ms: 794.384\n",
      "    load_throughput: 153534.028\n",
      "    load_time_ms: 6.513\n",
      "    sample_throughput: 67.038\n",
      "    sample_time_ms: 14916.952\n",
      "    update_time_ms: 3.751\n",
      "  timestamp: 1631869469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         595.557</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 0.59375</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.344</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.6363636363636364\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5048811793327332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01766805714609983\n",
      "          policy_loss: 0.05744072389271524\n",
      "          total_loss: 0.09846798450582557\n",
      "          vf_explained_var: 0.47142961621284485\n",
      "          vf_loss: 0.056046592221698825\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.50952380952382\n",
      "    ram_util_percent: 34.86190476190475\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07489448157915865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.723443357454638\n",
      "    mean_inference_ms: 2.5315777634863457\n",
      "    mean_raw_obs_processing_ms: 0.2800998569996918\n",
      "  time_since_restore: 610.5025980472565\n",
      "  time_this_iter_s: 14.945464611053467\n",
      "  time_total_s: 610.5025980472565\n",
      "  timers:\n",
      "    learn_throughput: 1259.036\n",
      "    learn_time_ms: 794.259\n",
      "    load_throughput: 159185.08\n",
      "    load_time_ms: 6.282\n",
      "    sample_throughput: 66.455\n",
      "    sample_time_ms: 15047.75\n",
      "    update_time_ms: 3.64\n",
      "  timestamp: 1631869484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         610.503</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">0.636364</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.5588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5588235294117647\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.001668548583984375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9909116268157959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020540787177666706\n",
      "          policy_loss: -0.10518386099073622\n",
      "          total_loss: -0.043079135070244474\n",
      "          vf_explained_var: 0.31721174716949463\n",
      "          vf_loss: 0.07197956755343411\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.233333333333334\n",
      "    ram_util_percent: 34.93809523809524\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07472326913059903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.387876761994157\n",
      "    mean_inference_ms: 2.526731904565443\n",
      "    mean_raw_obs_processing_ms: 0.29238310138569235\n",
      "  time_since_restore: 625.1339037418365\n",
      "  time_this_iter_s: 14.631305694580078\n",
      "  time_total_s: 625.1339037418365\n",
      "  timers:\n",
      "    learn_throughput: 1274.058\n",
      "    learn_time_ms: 784.894\n",
      "    load_throughput: 159113.219\n",
      "    load_time_ms: 6.285\n",
      "    sample_throughput: 65.872\n",
      "    sample_time_ms: 15180.944\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1631869498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         625.134</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">0.558824</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-05-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.6571428571428\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5428571428571428\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.002502822875976563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1980020549562242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014988034514400539\n",
      "          policy_loss: 0.04083139565255907\n",
      "          total_loss: 0.06585677787661552\n",
      "          vf_explained_var: 0.10480692237615585\n",
      "          vf_loss: 0.03696789021293322\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.98095238095239\n",
      "    ram_util_percent: 34.9047619047619\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07456008666022293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.06929172314977\n",
      "    mean_inference_ms: 2.5221186965034335\n",
      "    mean_raw_obs_processing_ms: 0.3035630378581286\n",
      "  time_since_restore: 640.17200922966\n",
      "  time_this_iter_s: 15.038105487823486\n",
      "  time_total_s: 640.17200922966\n",
      "  timers:\n",
      "    learn_throughput: 1274.027\n",
      "    learn_time_ms: 784.913\n",
      "    load_throughput: 162492.455\n",
      "    load_time_ms: 6.154\n",
      "    sample_throughput: 65.215\n",
      "    sample_time_ms: 15333.889\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1631869513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         640.172</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">0.542857</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-05-28\n",
      "  done: false\n",
      "  episode_len_mean: 996.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.002502822875976563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2688809441195594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022416949022988162\n",
      "          policy_loss: 0.025615384595261678\n",
      "          total_loss: 0.11645603742864397\n",
      "          vf_explained_var: 0.31916046142578125\n",
      "          vf_loss: 0.10347335769070519\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.83636363636363\n",
      "    ram_util_percent: 34.918181818181814\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07440355250028545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.766209710454532\n",
      "    mean_inference_ms: 2.5176837975984165\n",
      "    mean_raw_obs_processing_ms: 0.31375063919967194\n",
      "  time_since_restore: 654.9552583694458\n",
      "  time_this_iter_s: 14.783249139785767\n",
      "  time_total_s: 654.9552583694458\n",
      "  timers:\n",
      "    learn_throughput: 1267.203\n",
      "    learn_time_ms: 789.14\n",
      "    load_throughput: 161958.498\n",
      "    load_time_ms: 6.174\n",
      "    sample_throughput: 64.639\n",
      "    sample_time_ms: 15470.469\n",
      "    update_time_ms: 3.43\n",
      "  timestamp: 1631869528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         654.955</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">     0.5</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-05-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.8378378378378\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.43243243243243246\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0037542343139648424\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3352697014808654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019109469266848065\n",
      "          policy_loss: -0.010520291659567091\n",
      "          total_loss: 0.04413586917022864\n",
      "          vf_explained_var: -0.029090436175465584\n",
      "          vf_loss: 0.06793711843589942\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.110000000000014\n",
      "    ram_util_percent: 34.919999999999995\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07425281239040893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.477407186193417\n",
      "    mean_inference_ms: 2.5134036733142846\n",
      "    mean_raw_obs_processing_ms: 0.32304301850652467\n",
      "  time_since_restore: 669.5644021034241\n",
      "  time_this_iter_s: 14.609143733978271\n",
      "  time_total_s: 669.5644021034241\n",
      "  timers:\n",
      "    learn_throughput: 1273.298\n",
      "    learn_time_ms: 785.362\n",
      "    load_throughput: 163411.683\n",
      "    load_time_ms: 6.12\n",
      "    sample_throughput: 64.185\n",
      "    sample_time_ms: 15579.983\n",
      "    update_time_ms: 3.357\n",
      "  timestamp: 1631869543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         669.564</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">0.432432</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.838</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-05-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.921052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.42105263157894735\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0037542343139648424\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.423187584347195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012306355295905015\n",
      "          policy_loss: 0.0008854208721054925\n",
      "          total_loss: 0.10031969083680047\n",
      "          vf_explained_var: 0.2470291405916214\n",
      "          vf_loss: 0.1136199451982975\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.18636363636363\n",
      "    ram_util_percent: 34.92727272727272\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07410848322360927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.20219706274729\n",
      "    mean_inference_ms: 2.5093019315889635\n",
      "    mean_raw_obs_processing_ms: 0.3315296314681679\n",
      "  time_since_restore: 684.8078935146332\n",
      "  time_this_iter_s: 15.243491411209106\n",
      "  time_total_s: 684.8078935146332\n",
      "  timers:\n",
      "    learn_throughput: 1265.809\n",
      "    learn_time_ms: 790.008\n",
      "    load_throughput: 161199.725\n",
      "    load_time_ms: 6.203\n",
      "    sample_throughput: 63.55\n",
      "    sample_time_ms: 15735.582\n",
      "    update_time_ms: 3.33\n",
      "  timestamp: 1631869558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         684.808</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">0.421053</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           996.921</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.4358974358974359\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0037542343139648424\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0268018358283573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007222873609032337\n",
      "          policy_loss: -0.1643661199344529\n",
      "          total_loss: -0.16513575828737684\n",
      "          vf_explained_var: 0.05441375449299812\n",
      "          vf_loss: 0.009471259998261101\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.04285714285714\n",
      "    ram_util_percent: 34.94285714285714\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0739695207902757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.939253311408613\n",
      "    mean_inference_ms: 2.505347187486269\n",
      "    mean_raw_obs_processing_ms: 0.3392918648716179\n",
      "  time_since_restore: 699.4286279678345\n",
      "  time_this_iter_s: 14.620734453201294\n",
      "  time_total_s: 699.4286279678345\n",
      "  timers:\n",
      "    learn_throughput: 1257.739\n",
      "    learn_time_ms: 795.077\n",
      "    load_throughput: 161612.774\n",
      "    load_time_ms: 6.188\n",
      "    sample_throughput: 63.13\n",
      "    sample_time_ms: 15840.245\n",
      "    update_time_ms: 3.445\n",
      "  timestamp: 1631869573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         699.429</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">0.435897</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-06-28\n",
      "  done: false\n",
      "  episode_len_mean: 997.075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.425\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0037542343139648424\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2724582890669505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020299708089104584\n",
      "          policy_loss: -0.06701360613935524\n",
      "          total_loss: -0.0738712535964118\n",
      "          vf_explained_var: -0.6941999793052673\n",
      "          vf_loss: 0.005790725600026134\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.504761904761914\n",
      "    ram_util_percent: 34.93809523809524\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0738365657425415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.687841776191473\n",
      "    mean_inference_ms: 2.5015645341607247\n",
      "    mean_raw_obs_processing_ms: 0.3463815005154799\n",
      "  time_since_restore: 714.3078999519348\n",
      "  time_this_iter_s: 14.879271984100342\n",
      "  time_total_s: 714.3078999519348\n",
      "  timers:\n",
      "    learn_throughput: 1239.273\n",
      "    learn_time_ms: 806.925\n",
      "    load_throughput: 241405.738\n",
      "    load_time_ms: 4.142\n",
      "    sample_throughput: 70.213\n",
      "    sample_time_ms: 14242.411\n",
      "    update_time_ms: 3.458\n",
      "  timestamp: 1631869588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         714.308</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   0.425</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.075</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 997.1463414634146\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.4146341463414634\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005631351470947267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2221750464704302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012888935152703675\n",
      "          policy_loss: 0.014162208181288508\n",
      "          total_loss: 0.16635196142726474\n",
      "          vf_explained_var: 0.3370513319969177\n",
      "          vf_loss: 0.16433891728520394\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.13636363636363\n",
      "    ram_util_percent: 34.949999999999996\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07370938936533701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.447454428442704\n",
      "    mean_inference_ms: 2.4979438311729507\n",
      "    mean_raw_obs_processing_ms: 0.35285828624534676\n",
      "  time_since_restore: 729.5292632579803\n",
      "  time_this_iter_s: 15.221363306045532\n",
      "  time_total_s: 729.5292632579803\n",
      "  timers:\n",
      "    learn_throughput: 1235.648\n",
      "    learn_time_ms: 809.292\n",
      "    load_throughput: 275730.627\n",
      "    load_time_ms: 3.627\n",
      "    sample_throughput: 71.136\n",
      "    sample_time_ms: 14057.526\n",
      "    update_time_ms: 3.442\n",
      "  timestamp: 1631869603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         729.529</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">0.414634</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-06-57\n",
      "  done: false\n",
      "  episode_len_mean: 997.2142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.4523809523809524\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005631351470947267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1311080932617188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011723252714345397\n",
      "          policy_loss: -0.057966456727849114\n",
      "          total_loss: 0.08820444003989299\n",
      "          vf_explained_var: 0.423430860042572\n",
      "          vf_loss: 0.15741596354378595\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.790476190476184\n",
      "    ram_util_percent: 34.93333333333333\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07358624250572819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21693878300869\n",
      "    mean_inference_ms: 2.494438478452236\n",
      "    mean_raw_obs_processing_ms: 0.3587947207978682\n",
      "  time_since_restore: 743.9200534820557\n",
      "  time_this_iter_s: 14.390790224075317\n",
      "  time_total_s: 743.9200534820557\n",
      "  timers:\n",
      "    learn_throughput: 1235.119\n",
      "    learn_time_ms: 809.639\n",
      "    load_throughput: 274955.194\n",
      "    load_time_ms: 3.637\n",
      "    sample_throughput: 71.358\n",
      "    sample_time_ms: 14013.781\n",
      "    update_time_ms: 3.515\n",
      "  timestamp: 1631869617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">          743.92</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">0.452381</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.214</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 997.2790697674419\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.4418604651162791\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005631351470947267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.14967283738984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006625710235183549\n",
      "          policy_loss: -0.09162064178122414\n",
      "          total_loss: -0.05439729541540146\n",
      "          vf_explained_var: 0.1865161955356598\n",
      "          vf_loss: 0.04868276430190437\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.69047619047618\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07346759924296829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.995830595062277\n",
      "    mean_inference_ms: 2.4910507242843285\n",
      "    mean_raw_obs_processing_ms: 0.3642336290272586\n",
      "  time_since_restore: 758.6067566871643\n",
      "  time_this_iter_s: 14.686703205108643\n",
      "  time_total_s: 758.6067566871643\n",
      "  timers:\n",
      "    learn_throughput: 1248.322\n",
      "    learn_time_ms: 801.075\n",
      "    load_throughput: 273367.442\n",
      "    load_time_ms: 3.658\n",
      "    sample_throughput: 71.446\n",
      "    sample_time_ms: 13996.497\n",
      "    update_time_ms: 3.62\n",
      "  timestamp: 1631869632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         758.607</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> 0.44186</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-07-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.3409090909091\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.4318181818181818\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.005631351470947267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5334487398465475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024723269552412653\n",
      "          policy_loss: 0.07296109489268726\n",
      "          total_loss: 0.1261789468427499\n",
      "          vf_explained_var: 0.48996394872665405\n",
      "          vf_loss: 0.0684131158515811\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.635000000000005\n",
      "    ram_util_percent: 34.885\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07335346574720007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.783448388763553\n",
      "    mean_inference_ms: 2.4877718743927697\n",
      "    mean_raw_obs_processing_ms: 0.36921136237979907\n",
      "  time_since_restore: 773.0870079994202\n",
      "  time_this_iter_s: 14.48025131225586\n",
      "  time_total_s: 773.0870079994202\n",
      "  timers:\n",
      "    learn_throughput: 1249.47\n",
      "    learn_time_ms: 800.339\n",
      "    load_throughput: 273568.922\n",
      "    load_time_ms: 3.655\n",
      "    sample_throughput: 71.521\n",
      "    sample_time_ms: 13981.817\n",
      "    update_time_ms: 3.792\n",
      "  timestamp: 1631869647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         773.087</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">0.431818</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">           997.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 997.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.3111111111111111\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0084470272064209\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5882663144005669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01239336890344597\n",
      "          policy_loss: 0.0478071649869283\n",
      "          total_loss: 0.15045236899620956\n",
      "          vf_explained_var: 0.34111928939819336\n",
      "          vf_loss: 0.11842317655682563\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.69545454545455\n",
      "    ram_util_percent: 34.886363636363626\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07324304463813844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.579510330702252\n",
      "    mean_inference_ms: 2.4846033574086825\n",
      "    mean_raw_obs_processing_ms: 0.3737817699583479\n",
      "  time_since_restore: 788.1237449645996\n",
      "  time_this_iter_s: 15.036736965179443\n",
      "  time_total_s: 788.1237449645996\n",
      "  timers:\n",
      "    learn_throughput: 1245.976\n",
      "    learn_time_ms: 802.584\n",
      "    load_throughput: 283989.925\n",
      "    load_time_ms: 3.521\n",
      "    sample_throughput: 71.535\n",
      "    sample_time_ms: 13979.127\n",
      "    update_time_ms: 3.92\n",
      "  timestamp: 1631869662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         788.124</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">0.311111</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             997.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-07-57\n",
      "  done: false\n",
      "  episode_len_mean: 997.4565217391304\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.30434782608695654\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0084470272064209\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2607011801666683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018534425575501116\n",
      "          policy_loss: -0.031787022948265076\n",
      "          total_loss: 0.0060950517654418945\n",
      "          vf_explained_var: 0.39898261427879333\n",
      "          vf_loss: 0.05033252677725007\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.4952380952381\n",
      "    ram_util_percent: 34.9095238095238\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07313834222032106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.38340248518504\n",
      "    mean_inference_ms: 2.4816661009873417\n",
      "    mean_raw_obs_processing_ms: 0.37798202924368535\n",
      "  time_since_restore: 803.18119597435\n",
      "  time_this_iter_s: 15.057451009750366\n",
      "  time_total_s: 803.18119597435\n",
      "  timers:\n",
      "    learn_throughput: 1252.0\n",
      "    learn_time_ms: 798.722\n",
      "    load_throughput: 286615.598\n",
      "    load_time_ms: 3.489\n",
      "    sample_throughput: 71.374\n",
      "    sample_time_ms: 14010.764\n",
      "    update_time_ms: 3.849\n",
      "  timestamp: 1631869677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         803.181</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">0.304348</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.457</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 997.5106382978723\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.2978723404255319\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0084470272064209\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7163839750819736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013837920375420589\n",
      "          policy_loss: 0.02865282907668087\n",
      "          total_loss: 0.10785167631175784\n",
      "          vf_explained_var: -0.07974981516599655\n",
      "          vf_loss: 0.09624579610923926\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.56363636363638\n",
      "    ram_util_percent: 34.87727272727272\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.073038096463373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.194754682199346\n",
      "    mean_inference_ms: 2.478897972919689\n",
      "    mean_raw_obs_processing_ms: 0.38183777716860506\n",
      "  time_since_restore: 818.3222477436066\n",
      "  time_this_iter_s: 15.141051769256592\n",
      "  time_total_s: 818.3222477436066\n",
      "  timers:\n",
      "    learn_throughput: 1251.792\n",
      "    learn_time_ms: 798.855\n",
      "    load_throughput: 277847.599\n",
      "    load_time_ms: 3.599\n",
      "    sample_throughput: 71.106\n",
      "    sample_time_ms: 14063.459\n",
      "    update_time_ms: 3.888\n",
      "  timestamp: 1631869692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         818.322</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">0.297872</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.5625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.3541666666666667\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0084470272064209\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5505958623356288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021654661064537083\n",
      "          policy_loss: 0.03991184590591325\n",
      "          total_loss: 0.04953656105531586\n",
      "          vf_explained_var: 0.4058457612991333\n",
      "          vf_loss: 0.024947756298610735\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.845454545454544\n",
      "    ram_util_percent: 34.86818181818181\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07294070622146523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.01312401863501\n",
      "    mean_inference_ms: 2.476214063546436\n",
      "    mean_raw_obs_processing_ms: 0.3853742870451162\n",
      "  time_since_restore: 833.2745552062988\n",
      "  time_this_iter_s: 14.95230746269226\n",
      "  time_total_s: 833.2745552062988\n",
      "  timers:\n",
      "    learn_throughput: 1266.322\n",
      "    learn_time_ms: 789.688\n",
      "    load_throughput: 269643.459\n",
      "    load_time_ms: 3.709\n",
      "    sample_throughput: 71.208\n",
      "    sample_time_ms: 14043.31\n",
      "    update_time_ms: 3.857\n",
      "  timestamp: 1631869707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         833.275</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">0.354167</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 997.6122448979592\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.3673469387755102\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6666306376457214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015180862022473837\n",
      "          policy_loss: -0.047956634602612916\n",
      "          total_loss: 0.049948445997304386\n",
      "          vf_explained_var: 0.2171301692724228\n",
      "          vf_loss: 0.1143790375182612\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.779999999999994\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07284644371147689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.837935956046252\n",
      "    mean_inference_ms: 2.473606863420742\n",
      "    mean_raw_obs_processing_ms: 0.3886171044693913\n",
      "  time_since_restore: 847.7952513694763\n",
      "  time_this_iter_s: 14.52069616317749\n",
      "  time_total_s: 847.7952513694763\n",
      "  timers:\n",
      "    learn_throughput: 1272.014\n",
      "    learn_time_ms: 786.155\n",
      "    load_throughput: 265230.621\n",
      "    load_time_ms: 3.77\n",
      "    sample_throughput: 71.24\n",
      "    sample_time_ms: 14036.979\n",
      "    update_time_ms: 3.719\n",
      "  timestamp: 1631869722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         847.795</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">0.367347</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.612</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.34\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3756437804963855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013339462082201298\n",
      "          policy_loss: 0.08645544995864232\n",
      "          total_loss: 0.17211947329342364\n",
      "          vf_explained_var: 0.2737703025341034\n",
      "          vf_loss: 0.0992514437271489\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.56666666666666\n",
      "    ram_util_percent: 34.88571428571428\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07275636400706936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.66886514994029\n",
      "    mean_inference_ms: 2.4710618004587417\n",
      "    mean_raw_obs_processing_ms: 0.39159099339730813\n",
      "  time_since_restore: 862.36359333992\n",
      "  time_this_iter_s: 14.568341970443726\n",
      "  time_total_s: 862.36359333992\n",
      "  timers:\n",
      "    learn_throughput: 1289.009\n",
      "    learn_time_ms: 775.79\n",
      "    load_throughput: 262106.322\n",
      "    load_time_ms: 3.815\n",
      "    sample_throughput: 71.347\n",
      "    sample_time_ms: 14016.083\n",
      "    update_time_ms: 3.832\n",
      "  timestamp: 1631869736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         862.364</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">    0.34</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            997.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 997.7058823529412\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.3333333333333333\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.683243719736735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0192242976854572\n",
      "          policy_loss: -0.006373980848325624\n",
      "          total_loss: 0.018687032784024876\n",
      "          vf_explained_var: 0.6795833706855774\n",
      "          vf_loss: 0.041649866311086546\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.48636363636363\n",
      "    ram_util_percent: 34.86363636363635\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0726689197358637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.505823095022464\n",
      "    mean_inference_ms: 2.4685962854176497\n",
      "    mean_raw_obs_processing_ms: 0.39431803867494275\n",
      "  time_since_restore: 877.5923829078674\n",
      "  time_this_iter_s: 15.228789567947388\n",
      "  time_total_s: 877.5923829078674\n",
      "  timers:\n",
      "    learn_throughput: 1292.421\n",
      "    learn_time_ms: 773.742\n",
      "    load_throughput: 250469.013\n",
      "    load_time_ms: 3.993\n",
      "    sample_throughput: 71.334\n",
      "    sample_time_ms: 14018.658\n",
      "    update_time_ms: 3.827\n",
      "  timestamp: 1631869751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         877.592</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">0.333333</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.706</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.2692307692307692\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1112701972325643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010377769951912728\n",
      "          policy_loss: 0.046113262263437114\n",
      "          total_loss: 0.04767000178496043\n",
      "          vf_explained_var: -0.08146780729293823\n",
      "          vf_loss: 0.022537948987964126\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.32380952380951\n",
      "    ram_util_percent: 34.87619047619047\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258383239884005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.34840902957543\n",
      "    mean_inference_ms: 2.4662038595921865\n",
      "    mean_raw_obs_processing_ms: 0.3968186252767627\n",
      "  time_since_restore: 892.6559391021729\n",
      "  time_this_iter_s: 15.06355619430542\n",
      "  time_total_s: 892.6559391021729\n",
      "  timers:\n",
      "    learn_throughput: 1280.11\n",
      "    learn_time_ms: 781.183\n",
      "    load_throughput: 249994.278\n",
      "    load_time_ms: 4.0\n",
      "    sample_throughput: 71.031\n",
      "    sample_time_ms: 14078.43\n",
      "    update_time_ms: 3.854\n",
      "  timestamp: 1631869767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         892.656</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">0.269231</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            997.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 997.7924528301887\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.24528301886792453\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6024000260565017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013060612571438708\n",
      "          policy_loss: -0.00039494501219855413\n",
      "          total_loss: 0.002832987904548645\n",
      "          vf_explained_var: -0.3462499976158142\n",
      "          vf_loss: 0.019086447546255336\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.681818181818194\n",
      "    ram_util_percent: 34.84999999999999\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07250148648882843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.196340402764626\n",
      "    mean_inference_ms: 2.4638844517468974\n",
      "    mean_raw_obs_processing_ms: 0.39911720075714024\n",
      "  time_since_restore: 907.7581415176392\n",
      "  time_this_iter_s: 15.102202415466309\n",
      "  time_total_s: 907.7581415176392\n",
      "  timers:\n",
      "    learn_throughput: 1274.049\n",
      "    learn_time_ms: 784.899\n",
      "    load_throughput: 249907.885\n",
      "    load_time_ms: 4.001\n",
      "    sample_throughput: 70.84\n",
      "    sample_time_ms: 14116.409\n",
      "    update_time_ms: 3.783\n",
      "  timestamp: 1631869782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         907.758</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">0.245283</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-09-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.25925925925925924\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7409509036276076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015094905029495169\n",
      "          policy_loss: 0.03577265996072027\n",
      "          total_loss: 0.06783842742443084\n",
      "          vf_explained_var: -0.037606339901685715\n",
      "          vf_loss: 0.049284014597328174\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.75714285714286\n",
      "    ram_util_percent: 34.86666666666666\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07242096317340065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.049136209415018\n",
      "    mean_inference_ms: 2.4616166472657874\n",
      "    mean_raw_obs_processing_ms: 0.4012187929080229\n",
      "  time_since_restore: 922.1733677387238\n",
      "  time_this_iter_s: 14.415226221084595\n",
      "  time_total_s: 922.1733677387238\n",
      "  timers:\n",
      "    learn_throughput: 1267.651\n",
      "    learn_time_ms: 788.861\n",
      "    load_throughput: 250088.186\n",
      "    load_time_ms: 3.999\n",
      "    sample_throughput: 70.891\n",
      "    sample_time_ms: 14106.184\n",
      "    update_time_ms: 3.688\n",
      "  timestamp: 1631869796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         922.173</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">0.259259</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 997.8727272727273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.2909090909090909\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012670540809631342\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1473966399828592\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02088398168716823\n",
      "          policy_loss: 0.00046231779787275524\n",
      "          total_loss: 0.039526440906855795\n",
      "          vf_explained_var: 0.5066514611244202\n",
      "          vf_loss: 0.06027347942710751\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.06\n",
      "    ram_util_percent: 34.82499999999999\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07234225824065461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.906615847293647\n",
      "    mean_inference_ms: 2.4593993425378597\n",
      "    mean_raw_obs_processing_ms: 0.4031394604897858\n",
      "  time_since_restore: 936.7441337108612\n",
      "  time_this_iter_s: 14.570765972137451\n",
      "  time_total_s: 936.7441337108612\n",
      "  timers:\n",
      "    learn_throughput: 1272.045\n",
      "    learn_time_ms: 786.136\n",
      "    load_throughput: 240284.149\n",
      "    load_time_ms: 4.162\n",
      "    sample_throughput: 71.111\n",
      "    sample_time_ms: 14062.434\n",
      "    update_time_ms: 3.545\n",
      "  timestamp: 1631869811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         936.744</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">0.290909</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-10-26\n",
      "  done: false\n",
      "  episode_len_mean: 997.9107142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.32142857142857145\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.019005811214447027\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.119206182161967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01888081276414457\n",
      "          policy_loss: -0.07501299074954457\n",
      "          total_loss: -0.05673553273081779\n",
      "          vf_explained_var: 0.3690633475780487\n",
      "          vf_loss: 0.03911067712017231\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.531818181818174\n",
      "    ram_util_percent: 34.81818181818181\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07226561350094332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.76867433222464\n",
      "    mean_inference_ms: 2.457240837229719\n",
      "    mean_raw_obs_processing_ms: 0.4048922662996051\n",
      "  time_since_restore: 951.7569653987885\n",
      "  time_this_iter_s: 15.012831687927246\n",
      "  time_total_s: 951.7569653987885\n",
      "  timers:\n",
      "    learn_throughput: 1264.109\n",
      "    learn_time_ms: 791.071\n",
      "    load_throughput: 232599.502\n",
      "    load_time_ms: 4.299\n",
      "    sample_throughput: 71.159\n",
      "    sample_time_ms: 14052.964\n",
      "    update_time_ms: 3.41\n",
      "  timestamp: 1631869826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         951.757</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">0.321429</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 997.9473684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.2631578947368421\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.019005811214447027\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1940232541826035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014459323826784025\n",
      "          policy_loss: 0.07191730116804441\n",
      "          total_loss: 0.15146883034871683\n",
      "          vf_explained_var: 0.11930849403142929\n",
      "          vf_loss: 0.10121695428258842\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.95454545454545\n",
      "    ram_util_percent: 34.818181818181806\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0721908574631553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.635106054035266\n",
      "    mean_inference_ms: 2.4551392464992956\n",
      "    mean_raw_obs_processing_ms: 0.40649524621937955\n",
      "  time_since_restore: 966.8308539390564\n",
      "  time_this_iter_s: 15.073888540267944\n",
      "  time_total_s: 966.8308539390564\n",
      "  timers:\n",
      "    learn_throughput: 1258.991\n",
      "    learn_time_ms: 794.287\n",
      "    load_throughput: 233757.12\n",
      "    load_time_ms: 4.278\n",
      "    sample_throughput: 71.209\n",
      "    sample_time_ms: 14043.263\n",
      "    update_time_ms: 3.37\n",
      "  timestamp: 1631869841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         966.831</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">0.263158</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.9827586206897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.29310344827586204\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.019005811214447027\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.002727544307709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06078847841760319\n",
      "          policy_loss: 0.12763537681765028\n",
      "          total_loss: 0.11644090306427744\n",
      "          vf_explained_var: 0.48857319355010986\n",
      "          vf_loss: 0.0076774700493034385\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.210000000000015\n",
      "    ram_util_percent: 34.83\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0721179092184674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.505584754240818\n",
      "    mean_inference_ms: 2.453094111553653\n",
      "    mean_raw_obs_processing_ms: 0.40795376948600165\n",
      "  time_since_restore: 981.5116863250732\n",
      "  time_this_iter_s: 14.680832386016846\n",
      "  time_total_s: 981.5116863250732\n",
      "  timers:\n",
      "    learn_throughput: 1255.564\n",
      "    learn_time_ms: 796.455\n",
      "    load_throughput: 244128.819\n",
      "    load_time_ms: 4.096\n",
      "    sample_throughput: 71.356\n",
      "    sample_time_ms: 14014.244\n",
      "    update_time_ms: 3.376\n",
      "  timestamp: 1631869856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         981.512</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">0.293103</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           997.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 998.0169491525423\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.288135593220339\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.028508716821670534\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.367446684837341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021795890059411724\n",
      "          policy_loss: 0.0523942273731033\n",
      "          total_loss: 0.18308414405004847\n",
      "          vf_explained_var: 0.15476839244365692\n",
      "          vf_loss: 0.1537430095175902\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.2681818181818\n",
      "    ram_util_percent: 34.8090909090909\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0720472664527648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.380079156116356\n",
      "    mean_inference_ms: 2.451108557387765\n",
      "    mean_raw_obs_processing_ms: 0.4092729678369281\n",
      "  time_since_restore: 996.6931943893433\n",
      "  time_this_iter_s: 15.18150806427002\n",
      "  time_total_s: 996.6931943893433\n",
      "  timers:\n",
      "    learn_throughput: 1259.63\n",
      "    learn_time_ms: 793.884\n",
      "    load_throughput: 247250.262\n",
      "    load_time_ms: 4.044\n",
      "    sample_throughput: 71.007\n",
      "    sample_time_ms: 14083.065\n",
      "    update_time_ms: 3.444\n",
      "  timestamp: 1631869871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         996.693</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">0.288136</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           998.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=163)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-11-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.23333333333333334\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0427630752325058\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.300815212726593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013297160648327984\n",
      "          policy_loss: 0.08462678185767597\n",
      "          total_loss: 0.11545389542977015\n",
      "          vf_explained_var: 0.17279617488384247\n",
      "          vf_loss: 0.053266639029607175\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.09777777777778\n",
      "    ram_util_percent: 34.79555555555555\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0719790218970963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.258377278372947\n",
      "    mean_inference_ms: 2.4491768803792198\n",
      "    mean_raw_obs_processing_ms: 0.4149723518720903\n",
      "  time_since_restore: 1028.1082911491394\n",
      "  time_this_iter_s: 31.415096759796143\n",
      "  time_total_s: 1028.1082911491394\n",
      "  timers:\n",
      "    learn_throughput: 1253.258\n",
      "    learn_time_ms: 797.92\n",
      "    load_throughput: 174043.288\n",
      "    load_time_ms: 5.746\n",
      "    sample_throughput: 63.443\n",
      "    sample_time_ms: 15762.168\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1631869902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1028.11</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">0.233333</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           995.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.9016393442623\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.22950819672131148\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0427630752325058\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.327588857544793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015096379353695157\n",
      "          policy_loss: 0.028094798367884425\n",
      "          total_loss: 0.17307958321438896\n",
      "          vf_explained_var: 0.2852250933647156\n",
      "          vf_loss: 0.1676151064534982\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.504\n",
      "    ram_util_percent: 34.728\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07191321318367129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14094381528901\n",
      "    mean_inference_ms: 2.447307679494126\n",
      "    mean_raw_obs_processing_ms: 0.420331459172713\n",
      "  time_since_restore: 1045.6856634616852\n",
      "  time_this_iter_s: 17.577372312545776\n",
      "  time_total_s: 1045.6856634616852\n",
      "  timers:\n",
      "    learn_throughput: 1248.119\n",
      "    learn_time_ms: 801.205\n",
      "    load_throughput: 166059.752\n",
      "    load_time_ms: 6.022\n",
      "    sample_throughput: 62.525\n",
      "    sample_time_ms: 15993.534\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1631869920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1045.69</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">0.229508</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           995.902</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-12-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.9677419354839\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.24193548387096775\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0427630752325058\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4021497276094226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04013951747441043\n",
      "          policy_loss: 0.011018658429384232\n",
      "          total_loss: 0.08925801254808903\n",
      "          vf_explained_var: -0.12355893105268478\n",
      "          vf_loss: 0.10054436191502544\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.11428571428572\n",
      "    ram_util_percent: 34.83809523809523\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07184891792476321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.026882919852504\n",
      "    mean_inference_ms: 2.4454805465720146\n",
      "    mean_raw_obs_processing_ms: 0.425371372541446\n",
      "  time_since_restore: 1060.4696998596191\n",
      "  time_this_iter_s: 14.78403639793396\n",
      "  time_total_s: 1060.4696998596191\n",
      "  timers:\n",
      "    learn_throughput: 1266.511\n",
      "    learn_time_ms: 789.571\n",
      "    load_throughput: 164425.122\n",
      "    load_time_ms: 6.082\n",
      "    sample_throughput: 62.589\n",
      "    sample_time_ms: 15977.188\n",
      "    update_time_ms: 3.262\n",
      "  timestamp: 1631869935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1060.47</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">0.241935</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           995.968</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.031746031746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.2698412698412698\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06414461284875872\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.530714217821757\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028014880789343857\n",
      "          policy_loss: 0.04063880424946546\n",
      "          total_loss: 0.03212589598778221\n",
      "          vf_explained_var: -0.3227521777153015\n",
      "          vf_loss: 0.014997230496050583\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.222727272727276\n",
      "    ram_util_percent: 34.8590909090909\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07178641232946263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.916136347460345\n",
      "    mean_inference_ms: 2.4437048910930255\n",
      "    mean_raw_obs_processing_ms: 0.43011968216153973\n",
      "  time_since_restore: 1075.730857372284\n",
      "  time_this_iter_s: 15.261157512664795\n",
      "  time_total_s: 1075.730857372284\n",
      "  timers:\n",
      "    learn_throughput: 1273.612\n",
      "    learn_time_ms: 785.168\n",
      "    load_throughput: 164159.984\n",
      "    load_time_ms: 6.092\n",
      "    sample_throughput: 62.511\n",
      "    sample_time_ms: 15997.162\n",
      "    update_time_ms: 3.381\n",
      "  timestamp: 1631869950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1075.73</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">0.269841</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.032</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.09375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.25\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09621691927313802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0882544848654003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013362699800084154\n",
      "          policy_loss: 0.09828603019316991\n",
      "          total_loss: 0.11111870276638203\n",
      "          vf_explained_var: -0.03814312070608139\n",
      "          vf_loss: 0.0324295015177793\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.877272727272725\n",
      "    ram_util_percent: 34.90454545454544\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07172539108239692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.808576987084425\n",
      "    mean_inference_ms: 2.441971625675089\n",
      "    mean_raw_obs_processing_ms: 0.4345897968738176\n",
      "  time_since_restore: 1091.0682036876678\n",
      "  time_this_iter_s: 15.337346315383911\n",
      "  time_total_s: 1091.0682036876678\n",
      "  timers:\n",
      "    learn_throughput: 1273.295\n",
      "    learn_time_ms: 785.364\n",
      "    load_throughput: 161804.175\n",
      "    load_time_ms: 6.18\n",
      "    sample_throughput: 62.154\n",
      "    sample_time_ms: 16089.151\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1631869965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1091.07</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">    0.25</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.1538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.24615384615384617\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09621691927313802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.454525521066454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016052547004071768\n",
      "          policy_loss: -0.020937303288115397\n",
      "          total_loss: 0.0569317452609539\n",
      "          vf_explained_var: -0.11801651120185852\n",
      "          vf_loss: 0.10086977941294512\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.74761904761905\n",
      "    ram_util_percent: 34.96190476190476\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0716656107867875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.70397454205847\n",
      "    mean_inference_ms: 2.4402739065211145\n",
      "    mean_raw_obs_processing_ms: 0.43879847194052546\n",
      "  time_since_restore: 1105.9620289802551\n",
      "  time_this_iter_s: 14.89382529258728\n",
      "  time_total_s: 1105.9620289802551\n",
      "  timers:\n",
      "    learn_throughput: 1271.61\n",
      "    learn_time_ms: 786.405\n",
      "    load_throughput: 164157.414\n",
      "    load_time_ms: 6.092\n",
      "    sample_throughput: 62.033\n",
      "    sample_time_ms: 16120.355\n",
      "    update_time_ms: 3.506\n",
      "  timestamp: 1631869980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1105.96</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">0.246154</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ccae9_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_09-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.2121212121212\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.2727272727272727\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 92f384b1516d4da09515d85b259ef30e\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09621691927313802\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.242608322037591\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02377127594185347\n",
      "          policy_loss: 0.04443291384312842\n",
      "          total_loss: 0.059659829032089974\n",
      "          vf_explained_var: 0.1345871090888977\n",
      "          vf_loss: 0.035365797069648076\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.695454545454545\n",
      "    ram_util_percent: 34.945454545454545\n",
      "  pid: 177\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07160724743703567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.602185744154724\n",
      "    mean_inference_ms: 2.4386188022188144\n",
      "    mean_raw_obs_processing_ms: 0.4427577433778787\n",
      "  time_since_restore: 1120.8164370059967\n",
      "  time_this_iter_s: 14.854408025741577\n",
      "  time_total_s: 1120.8164370059967\n",
      "  timers:\n",
      "    learn_throughput: 1273.129\n",
      "    learn_time_ms: 785.466\n",
      "    load_throughput: 166506.048\n",
      "    load_time_ms: 6.006\n",
      "    sample_throughput: 62.091\n",
      "    sample_time_ms: 16105.522\n",
      "    update_time_ms: 3.5\n",
      "  timestamp: 1631869995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: ccae9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.5/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/67.33 GiB heap, 0.0/32.85 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_08-53-59<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ccae9_00000</td><td>RUNNING </td><td>192.168.1.96:177</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1120.82</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">0.272727</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.212</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C22 pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])#callbacks=[\n",
    "        #    CustomLoggerCallback(),\n",
    "        #])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
