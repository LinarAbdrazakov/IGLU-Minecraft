{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(features_dim, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(256, 256),\n",
    "            #nn.ELU(),\n",
    "            #nn.Linear(256, 256),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(256, action_space.n)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.mlp.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(obs)\n",
    "        features = self.mlp(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C32']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-23 19:44:41,765\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-23 19:44:41,777\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=343315)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343315)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C32 pretrained (frozen AngelaCNN + MLP) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/aa44d_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/aa44d_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211023_194442-aa44d_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343315)\u001b[0m 2021-10-23 19:44:45,328\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=343315)\u001b[0m 2021-10-23 19:44:45,328\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=343315)\u001b[0m 2021-10-23 19:44:51,382\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 387.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -3.8699999999999615\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.878217119640774\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009090882563476023\n",
      "          policy_loss: 0.030945457393924396\n",
      "          total_loss: 0.006289673762189018\n",
      "          vf_explained_var: 0.16710948944091797\n",
      "          vf_loss: 0.0023082126488184764\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.46352941176471\n",
      "    ram_util_percent: 30.755294117647058\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03968252168668734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.51100174887674\n",
      "    mean_inference_ms: 1.7834719601687377\n",
      "    mean_raw_obs_processing_ms: 0.1631993037480098\n",
      "  time_since_restore: 59.23599457740784\n",
      "  time_this_iter_s: 59.23599457740784\n",
      "  time_total_s: 59.23599457740784\n",
      "  timers:\n",
      "    learn_throughput: 1624.279\n",
      "    learn_time_ms: 615.658\n",
      "    load_throughput: 63693.854\n",
      "    load_time_ms: 15.7\n",
      "    sample_throughput: 17.065\n",
      "    sample_time_ms: 58597.858\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1635018350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          59.236</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   -3.87</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">               387</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 398.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -5.127999999999959\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 5\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8717774285210504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007649655589335996\n",
      "          policy_loss: 0.022169906728797487\n",
      "          total_loss: 0.2890206274886926\n",
      "          vf_explained_var: -0.033184196799993515\n",
      "          vf_loss: 0.29403856550860735\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.544117647058826\n",
      "    ram_util_percent: 32.37941176470588\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03958046570665699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.97216930605402\n",
      "    mean_inference_ms: 1.7648311605660134\n",
      "    mean_raw_obs_processing_ms: 0.17178083609998457\n",
      "  time_since_restore: 83.2505533695221\n",
      "  time_this_iter_s: 24.014558792114258\n",
      "  time_total_s: 83.2505533695221\n",
      "  timers:\n",
      "    learn_throughput: 1589.552\n",
      "    learn_time_ms: 629.108\n",
      "    load_throughput: 63689.502\n",
      "    load_time_ms: 15.701\n",
      "    sample_throughput: 24.406\n",
      "    sample_time_ms: 40973.948\n",
      "    update_time_ms: 3.154\n",
      "  timestamp: 1635018374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         83.2506</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  -5.128</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">             398.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 401.7142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.834285714285672\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 7\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.866729998588562\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007617120640092286\n",
      "          policy_loss: 0.0348946451726887\n",
      "          total_loss: 0.019126118885146245\n",
      "          vf_explained_var: -0.21593870222568512\n",
      "          vf_loss: 0.011375348306157523\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.38064516129033\n",
      "    ram_util_percent: 32.67741935483872\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03947873962730287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.023705857378324\n",
      "    mean_inference_ms: 1.7572934828677624\n",
      "    mean_raw_obs_processing_ms: 0.17138558708064316\n",
      "  time_since_restore: 104.44129633903503\n",
      "  time_this_iter_s: 21.19074296951294\n",
      "  time_total_s: 104.44129633903503\n",
      "  timers:\n",
      "    learn_throughput: 1546.65\n",
      "    learn_time_ms: 646.559\n",
      "    load_throughput: 66997.737\n",
      "    load_time_ms: 14.926\n",
      "    sample_throughput: 29.286\n",
      "    sample_time_ms: 34146.14\n",
      "    update_time_ms: 2.811\n",
      "  timestamp: 1635018395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         104.441</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-4.83429</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           401.714</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 402.1111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.656666666666625\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8545502185821534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007762734919247608\n",
      "          policy_loss: -0.05023912841247188\n",
      "          total_loss: -0.0650040199359258\n",
      "          vf_explained_var: 0.41888394951820374\n",
      "          vf_loss: 0.012228064358027445\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.31785714285714\n",
      "    ram_util_percent: 32.68928571428572\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03934421341597046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.999279008464256\n",
      "    mean_inference_ms: 1.7503762393082878\n",
      "    mean_raw_obs_processing_ms: 0.16956382446090804\n",
      "  time_since_restore: 124.1626787185669\n",
      "  time_this_iter_s: 19.72138237953186\n",
      "  time_total_s: 124.1626787185669\n",
      "  timers:\n",
      "    learn_throughput: 1568.971\n",
      "    learn_time_ms: 637.361\n",
      "    load_throughput: 64136.031\n",
      "    load_time_ms: 15.592\n",
      "    sample_throughput: 32.914\n",
      "    sample_time_ms: 30381.794\n",
      "    update_time_ms: 2.633\n",
      "  timestamp: 1635018415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         124.163</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-4.65667</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           402.111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 406.3333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.539999999999957\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8450461864471435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008237960288403349\n",
      "          policy_loss: 0.010300463934739431\n",
      "          total_loss: -0.00767212708791097\n",
      "          vf_explained_var: 0.4035176634788513\n",
      "          vf_loss: 0.00883028249308053\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.68571428571429\n",
      "    ram_util_percent: 32.55714285714286\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917836709570538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.80411070980461\n",
      "    mean_inference_ms: 1.7430114612692067\n",
      "    mean_raw_obs_processing_ms: 0.1684791851386579\n",
      "  time_since_restore: 144.22958302497864\n",
      "  time_this_iter_s: 20.066904306411743\n",
      "  time_total_s: 144.22958302497864\n",
      "  timers:\n",
      "    learn_throughput: 1555.074\n",
      "    learn_time_ms: 643.056\n",
      "    load_throughput: 63058.207\n",
      "    load_time_ms: 15.858\n",
      "    sample_throughput: 35.486\n",
      "    sample_time_ms: 28180.386\n",
      "    update_time_ms: 3.12\n",
      "  timestamp: 1635018435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          144.23</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">   -4.54</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           406.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 409.14285714285717\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.4999999999999565\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.834255743026733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0067657954882664965\n",
      "          policy_loss: -0.07383443415164948\n",
      "          total_loss: -0.09498233000437419\n",
      "          vf_explained_var: 0.5223467946052551\n",
      "          vf_loss: 0.00584150311899268\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.22068965517241\n",
      "    ram_util_percent: 33.09310344827587\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911517276672659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.22081295136515\n",
      "    mean_inference_ms: 1.739973950655451\n",
      "    mean_raw_obs_processing_ms: 0.16769087097339366\n",
      "  time_since_restore: 164.0374825000763\n",
      "  time_this_iter_s: 19.807899475097656\n",
      "  time_total_s: 164.0374825000763\n",
      "  timers:\n",
      "    learn_throughput: 1570.296\n",
      "    learn_time_ms: 636.823\n",
      "    load_throughput: 62579.447\n",
      "    load_time_ms: 15.98\n",
      "    sample_throughput: 37.481\n",
      "    sample_time_ms: 26680.341\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1635018455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         164.037</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">    -4.5</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           409.143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 409.29411764705884\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.429411764705839\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 17\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.815087464120653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007929158434775666\n",
      "          policy_loss: -0.06051536053419113\n",
      "          total_loss: -0.08177518033319049\n",
      "          vf_explained_var: 0.11704941838979721\n",
      "          vf_loss: 0.005305221888961063\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.59999999999999\n",
      "    ram_util_percent: 33.03214285714286\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039023948377393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.36005035772207\n",
      "    mean_inference_ms: 1.7359473422426515\n",
      "    mean_raw_obs_processing_ms: 0.16703046701809143\n",
      "  time_since_restore: 184.01518297195435\n",
      "  time_this_iter_s: 19.97770047187805\n",
      "  time_total_s: 184.01518297195435\n",
      "  timers:\n",
      "    learn_throughput: 1575.011\n",
      "    learn_time_ms: 634.916\n",
      "    load_throughput: 61904.761\n",
      "    load_time_ms: 16.154\n",
      "    sample_throughput: 39.016\n",
      "    sample_time_ms: 25630.549\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1635018475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         184.015</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-4.42941</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           409.294</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-48-15\n",
      "  done: false\n",
      "  episode_len_mean: 409.42105263157896\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.395263157894694\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 19\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.800937543974982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008380999908755252\n",
      "          policy_loss: 0.018658138066530227\n",
      "          total_loss: -0.004579783934685919\n",
      "          vf_explained_var: 0.41173985600471497\n",
      "          vf_loss: 0.0030952507921028884\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.03571428571429\n",
      "    ram_util_percent: 33.12500000000001\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038964736680439246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.361357066232447\n",
      "    mean_inference_ms: 1.7334987973096836\n",
      "    mean_raw_obs_processing_ms: 0.1665116576756325\n",
      "  time_since_restore: 203.79430627822876\n",
      "  time_this_iter_s: 19.779123306274414\n",
      "  time_total_s: 203.79430627822876\n",
      "  timers:\n",
      "    learn_throughput: 1582.51\n",
      "    learn_time_ms: 631.908\n",
      "    load_throughput: 61683.776\n",
      "    load_time_ms: 16.212\n",
      "    sample_throughput: 40.29\n",
      "    sample_time_ms: 24820.019\n",
      "    update_time_ms: 2.79\n",
      "  timestamp: 1635018495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         203.794</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-4.39526</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           409.421</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 408.6818181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.34681818181814\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 22\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7836355288823444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009291714395237112\n",
      "          policy_loss: 0.026655024538437527\n",
      "          total_loss: 0.007051481886042489\n",
      "          vf_explained_var: 0.38125932216644287\n",
      "          vf_loss: 0.006374472229638033\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03793103448276\n",
      "    ram_util_percent: 33.1448275862069\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038885673624812926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.12178280201373\n",
      "    mean_inference_ms: 1.7303446594482994\n",
      "    mean_raw_obs_processing_ms: 0.16619422774494177\n",
      "  time_since_restore: 223.85446333885193\n",
      "  time_this_iter_s: 20.06015706062317\n",
      "  time_total_s: 223.85446333885193\n",
      "  timers:\n",
      "    learn_throughput: 1578.711\n",
      "    learn_time_ms: 633.428\n",
      "    load_throughput: 63367.851\n",
      "    load_time_ms: 15.781\n",
      "    sample_throughput: 41.292\n",
      "    sample_time_ms: 24217.484\n",
      "    update_time_ms: 2.706\n",
      "  timestamp: 1635018515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         223.854</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-4.34682</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           408.682</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-48-54\n",
      "  done: false\n",
      "  episode_len_mean: 409.6666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.334999999999957\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 24\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.785961969693502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011949275585753583\n",
      "          policy_loss: 0.14223714901341333\n",
      "          total_loss: 0.1197340795563327\n",
      "          vf_explained_var: 0.8202787041664124\n",
      "          vf_loss: 0.0029666936406607014\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.32962962962962\n",
      "    ram_util_percent: 33.22222222222223\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884795071148311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.41983043313694\n",
      "    mean_inference_ms: 1.7286550111900676\n",
      "    mean_raw_obs_processing_ms: 0.16587045349638563\n",
      "  time_since_restore: 242.98696970939636\n",
      "  time_this_iter_s: 19.132506370544434\n",
      "  time_total_s: 242.98696970939636\n",
      "  timers:\n",
      "    learn_throughput: 1573.886\n",
      "    learn_time_ms: 635.37\n",
      "    load_throughput: 64549.999\n",
      "    load_time_ms: 15.492\n",
      "    sample_throughput: 42.298\n",
      "    sample_time_ms: 23641.899\n",
      "    update_time_ms: 2.643\n",
      "  timestamp: 1635018534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         242.987</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">  -4.335</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           409.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 411.6923076923077\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.336923076923034\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 26\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7294529941346912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011171176208441599\n",
      "          policy_loss: -0.0838297329015202\n",
      "          total_loss: -0.10368211054139667\n",
      "          vf_explained_var: 0.6892042756080627\n",
      "          vf_loss: 0.005207913873406748\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53\n",
      "    ram_util_percent: 33.226666666666674\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882302372122429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.797975303266753\n",
      "    mean_inference_ms: 1.7273042319556142\n",
      "    mean_raw_obs_processing_ms: 0.16558214180644198\n",
      "  time_since_restore: 263.3307764530182\n",
      "  time_this_iter_s: 20.343806743621826\n",
      "  time_total_s: 263.3307764530182\n",
      "  timers:\n",
      "    learn_throughput: 1563.126\n",
      "    learn_time_ms: 639.744\n",
      "    load_throughput: 63952.379\n",
      "    load_time_ms: 15.637\n",
      "    sample_throughput: 50.637\n",
      "    sample_time_ms: 19748.268\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1635018554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         263.331</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-4.33692</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           411.692</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-49-34\n",
      "  done: false\n",
      "  episode_len_mean: 414.7857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.352142857142814\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 28\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7021663268407186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010727903101116675\n",
      "          policy_loss: -0.07177570462226868\n",
      "          total_loss: -0.08795975413587358\n",
      "          vf_explained_var: 0.48596832156181335\n",
      "          vf_loss: 0.00869203625091662\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.007407407407406\n",
      "    ram_util_percent: 33.14814814814815\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038804007734684655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.238576144825963\n",
      "    mean_inference_ms: 1.726199395215726\n",
      "    mean_raw_obs_processing_ms: 0.16534551181605445\n",
      "  time_since_restore: 282.8631942272186\n",
      "  time_this_iter_s: 19.53241777420044\n",
      "  time_total_s: 282.8631942272186\n",
      "  timers:\n",
      "    learn_throughput: 1561.361\n",
      "    learn_time_ms: 640.467\n",
      "    load_throughput: 64436.551\n",
      "    load_time_ms: 15.519\n",
      "    sample_throughput: 51.816\n",
      "    sample_time_ms: 19299.107\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1635018574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         282.863</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-4.35214</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           414.786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-50-14\n",
      "  done: false\n",
      "  episode_len_mean: 416.61290322580646\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.350645161290279\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 31\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.713387293285794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00821484380528409\n",
      "          policy_loss: -0.015414113468594021\n",
      "          total_loss: -0.03565204524331623\n",
      "          vf_explained_var: 0.6229920387268066\n",
      "          vf_loss: 0.005252974281190998\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.03859649122807\n",
      "    ram_util_percent: 32.91228070175438\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879376915016746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.517171247311765\n",
      "    mean_inference_ms: 1.7250384477363387\n",
      "    mean_raw_obs_processing_ms: 0.30184580586145876\n",
      "  time_since_restore: 322.48935556411743\n",
      "  time_this_iter_s: 39.626161336898804\n",
      "  time_total_s: 322.48935556411743\n",
      "  timers:\n",
      "    learn_throughput: 1567.693\n",
      "    learn_time_ms: 637.88\n",
      "    load_throughput: 63483.403\n",
      "    load_time_ms: 15.752\n",
      "    sample_throughput: 47.293\n",
      "    sample_time_ms: 21144.892\n",
      "    update_time_ms: 2.579\n",
      "  timestamp: 1635018614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         322.489</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-4.35065</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           416.613</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 419.6666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.369999999999955\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 33\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7082553678088717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010467954907779574\n",
      "          policy_loss: 0.03292229092783398\n",
      "          total_loss: 0.00982908486492104\n",
      "          vf_explained_var: 0.7748987674713135\n",
      "          vf_loss: 0.0018957547882261375\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.30689655172414\n",
      "    ram_util_percent: 33.20344827586207\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879536905310203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.094552826514537\n",
      "    mean_inference_ms: 1.7244897169303874\n",
      "    mean_raw_obs_processing_ms: 0.37294613002572774\n",
      "  time_since_restore: 342.5506999492645\n",
      "  time_this_iter_s: 20.061344385147095\n",
      "  time_total_s: 342.5506999492645\n",
      "  timers:\n",
      "    learn_throughput: 1559.308\n",
      "    learn_time_ms: 641.31\n",
      "    load_throughput: 64063.05\n",
      "    load_time_ms: 15.61\n",
      "    sample_throughput: 47.224\n",
      "    sample_time_ms: 21175.603\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1635018634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         342.551</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">   -4.37</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           419.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 420.9428571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.3728571428570975\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 35\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6755916357040403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009193660526080037\n",
      "          policy_loss: 0.04141766611072752\n",
      "          total_loss: 0.019485084836681685\n",
      "          vf_explained_var: 0.9250808954238892\n",
      "          vf_loss: 0.0029846012365952546\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99333333333333\n",
      "    ram_util_percent: 32.93333333333333\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879939393200126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.713665703196384\n",
      "    mean_inference_ms: 1.7240789860675059\n",
      "    mean_raw_obs_processing_ms: 0.4309246405963034\n",
      "  time_since_restore: 364.00581979751587\n",
      "  time_this_iter_s: 21.455119848251343\n",
      "  time_total_s: 364.00581979751587\n",
      "  timers:\n",
      "    learn_throughput: 1565.901\n",
      "    learn_time_ms: 638.61\n",
      "    load_throughput: 65883.95\n",
      "    load_time_ms: 15.178\n",
      "    sample_throughput: 46.909\n",
      "    sample_time_ms: 21317.948\n",
      "    update_time_ms: 2.282\n",
      "  timestamp: 1635018655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         364.006</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.37286</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           420.943</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-51-15\n",
      "  done: false\n",
      "  episode_len_mean: 423.2162162162162\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.386756756756712\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 37\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6199710687001545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008861371269091634\n",
      "          policy_loss: -0.09029238025347391\n",
      "          total_loss: -0.11030173699061076\n",
      "          vf_explained_var: 0.847046434879303\n",
      "          vf_loss: 0.004418079408868733\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.45862068965518\n",
      "    ram_util_percent: 32.83103448275861\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880469600797783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.36347669149735\n",
      "    mean_inference_ms: 1.7237782974037963\n",
      "    mean_raw_obs_processing_ms: 0.4785447513388032\n",
      "  time_since_restore: 383.9722180366516\n",
      "  time_this_iter_s: 19.966398239135742\n",
      "  time_total_s: 383.9722180366516\n",
      "  timers:\n",
      "    learn_throughput: 1556.865\n",
      "    learn_time_ms: 642.316\n",
      "    load_throughput: 68260.223\n",
      "    load_time_ms: 14.65\n",
      "    sample_throughput: 46.881\n",
      "    sample_time_ms: 21330.657\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1635018675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         383.972</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-4.38676</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           423.216</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 424.64102564102564\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.393076923076878\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 39\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6060307926601833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01159370513129631\n",
      "          policy_loss: -0.08199029150936339\n",
      "          total_loss: -0.1017773429552714\n",
      "          vf_explained_var: 0.876018762588501\n",
      "          vf_loss: 0.003954512021866524\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.18965517241379\n",
      "    ram_util_percent: 32.87931034482758\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038811418428633566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.041542622671336\n",
      "    mean_inference_ms: 1.723542059284448\n",
      "    mean_raw_obs_processing_ms: 0.5178145648890974\n",
      "  time_since_restore: 404.3355760574341\n",
      "  time_this_iter_s: 20.36335802078247\n",
      "  time_total_s: 404.3355760574341\n",
      "  timers:\n",
      "    learn_throughput: 1549.591\n",
      "    learn_time_ms: 645.332\n",
      "    load_throughput: 70667.451\n",
      "    load_time_ms: 14.151\n",
      "    sample_throughput: 46.802\n",
      "    sample_time_ms: 21366.438\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1635018696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         404.336</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.39308</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           424.641</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 426.5238095238095\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.401428571428525\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.562554793887668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008552329102289072\n",
      "          policy_loss: -0.06480226375990444\n",
      "          total_loss: -0.08488140867816077\n",
      "          vf_explained_var: 0.9009752869606018\n",
      "          vf_loss: 0.003835937565761722\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.60714285714287\n",
      "    ram_util_percent: 32.93214285714286\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882023208181166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.604655024456957\n",
      "    mean_inference_ms: 1.7232397561919224\n",
      "    mean_raw_obs_processing_ms: 0.5655630896048819\n",
      "  time_since_restore: 424.18223571777344\n",
      "  time_this_iter_s: 19.846659660339355\n",
      "  time_total_s: 424.18223571777344\n",
      "  timers:\n",
      "    learn_throughput: 1541.036\n",
      "    learn_time_ms: 648.914\n",
      "    load_throughput: 73762.864\n",
      "    load_time_ms: 13.557\n",
      "    sample_throughput: 46.794\n",
      "    sample_time_ms: 21370.211\n",
      "    update_time_ms: 2.487\n",
      "  timestamp: 1635018715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         424.182</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-4.40143</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           426.524</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 427.79545454545456\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.407954545454499\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 44\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5560142676035564\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011619525023541695\n",
      "          policy_loss: 0.10558415297418833\n",
      "          total_loss: 0.08566944706771108\n",
      "          vf_explained_var: 0.8137038350105286\n",
      "          vf_loss: 0.0033215316103046965\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14137931034483\n",
      "    ram_util_percent: 33.07241379310345\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882675684242258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.34115531612164\n",
      "    mean_inference_ms: 1.7230836986630296\n",
      "    mean_raw_obs_processing_ms: 0.5913197465107514\n",
      "  time_since_restore: 444.51552271842957\n",
      "  time_this_iter_s: 20.333287000656128\n",
      "  time_total_s: 444.51552271842957\n",
      "  timers:\n",
      "    learn_throughput: 1536.701\n",
      "    learn_time_ms: 650.745\n",
      "    load_throughput: 74245.193\n",
      "    load_time_ms: 13.469\n",
      "    sample_throughput: 46.738\n",
      "    sample_time_ms: 21395.718\n",
      "    update_time_ms: 2.493\n",
      "  timestamp: 1635018736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         444.516</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-4.40795</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           427.795</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-52-35\n",
      "  done: false\n",
      "  episode_len_mean: 428.4782608695652\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.409130434782561\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 46\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5528295093112523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009442039382388353\n",
      "          policy_loss: 0.029559416737821368\n",
      "          total_loss: 0.011490584082073636\n",
      "          vf_explained_var: 0.6533993482589722\n",
      "          vf_loss: 0.005571053334925738\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.964285714285715\n",
      "    ram_util_percent: 32.35714285714287\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883178396719693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.094125539477883\n",
      "    mean_inference_ms: 1.7229361051796244\n",
      "    mean_raw_obs_processing_ms: 0.6127267412851104\n",
      "  time_since_restore: 463.9397323131561\n",
      "  time_this_iter_s: 19.424209594726562\n",
      "  time_total_s: 463.9397323131561\n",
      "  timers:\n",
      "    learn_throughput: 1547.277\n",
      "    learn_time_ms: 646.297\n",
      "    load_throughput: 72142.32\n",
      "    load_time_ms: 13.861\n",
      "    sample_throughput: 46.666\n",
      "    sample_time_ms: 21428.935\n",
      "    update_time_ms: 2.489\n",
      "  timestamp: 1635018755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          463.94</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-4.40913</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           428.478</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-52-53\n",
      "  done: false\n",
      "  episode_len_mean: 429.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.416666666666619\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 48\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.52110681798723\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008294742653086088\n",
      "          policy_loss: -0.16767918053600522\n",
      "          total_loss: -0.18557664371199079\n",
      "          vf_explained_var: 0.8260522484779358\n",
      "          vf_loss: 0.0056546550502793655\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.79999999999999\n",
      "    ram_util_percent: 32.188461538461546\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883466788313262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.859908978065132\n",
      "    mean_inference_ms: 1.7227538654330357\n",
      "    mean_raw_obs_processing_ms: 0.6305127880891596\n",
      "  time_since_restore: 482.21987748146057\n",
      "  time_this_iter_s: 18.280145168304443\n",
      "  time_total_s: 482.21987748146057\n",
      "  timers:\n",
      "    learn_throughput: 1561.76\n",
      "    learn_time_ms: 640.303\n",
      "    load_throughput: 72745.537\n",
      "    load_time_ms: 13.747\n",
      "    sample_throughput: 47.106\n",
      "    sample_time_ms: 21228.695\n",
      "    update_time_ms: 2.482\n",
      "  timestamp: 1635018773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          482.22</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-4.41667</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">            429.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 431.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.426199999999953\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 50\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4885539531707765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010789211548255828\n",
      "          policy_loss: -0.15868818163871765\n",
      "          total_loss: -0.17491533988051944\n",
      "          vf_explained_var: 0.4452061951160431\n",
      "          vf_loss: 0.006500538298860192\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.71199999999999\n",
      "    ram_util_percent: 32.228\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883507762069072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.635914940524593\n",
      "    mean_inference_ms: 1.7225274057386781\n",
      "    mean_raw_obs_processing_ms: 0.6453078596307982\n",
      "  time_since_restore: 499.58996844291687\n",
      "  time_this_iter_s: 17.3700909614563\n",
      "  time_total_s: 499.58996844291687\n",
      "  timers:\n",
      "    learn_throughput: 1570.49\n",
      "    learn_time_ms: 636.744\n",
      "    load_throughput: 74362.082\n",
      "    load_time_ms: 13.448\n",
      "    sample_throughput: 47.586\n",
      "    sample_time_ms: 21014.635\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1635018791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">          499.59</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\"> -4.4262</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">            431.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-53-29\n",
      "  done: false\n",
      "  episode_len_mean: 433.8490566037736\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.446415094339574\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 53\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4855319446987574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011116304088179405\n",
      "          policy_loss: -0.07276245761248801\n",
      "          total_loss: -0.08999257741702928\n",
      "          vf_explained_var: 0.7027759552001953\n",
      "          vf_loss: 0.005401940670950959\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.034615384615385\n",
      "    ram_util_percent: 32.3\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883389085329616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.32286148391071\n",
      "    mean_inference_ms: 1.7221494806117055\n",
      "    mean_raw_obs_processing_ms: 0.6634820290874246\n",
      "  time_since_restore: 517.8841443061829\n",
      "  time_this_iter_s: 18.29417586326599\n",
      "  time_total_s: 517.8841443061829\n",
      "  timers:\n",
      "    learn_throughput: 1584.049\n",
      "    learn_time_ms: 631.294\n",
      "    load_throughput: 74019.698\n",
      "    load_time_ms: 13.51\n",
      "    sample_throughput: 52.947\n",
      "    sample_time_ms: 18886.942\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1635018809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         517.884</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-4.44642</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           433.849</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-53-48\n",
      "  done: false\n",
      "  episode_len_mean: 434.3090909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.447090909090861\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 55\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4416831970214843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011761002998574079\n",
      "          policy_loss: 0.07479316372838285\n",
      "          total_loss: 0.056353772928317385\n",
      "          vf_explained_var: 0.8078038692474365\n",
      "          vf_loss: 0.0036252363817766307\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.51111111111111\n",
      "    ram_util_percent: 32.37037037037037\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883365557613358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.12872027128212\n",
      "    mean_inference_ms: 1.7219249819759619\n",
      "    mean_raw_obs_processing_ms: 0.673285630009089\n",
      "  time_since_restore: 536.6741051673889\n",
      "  time_this_iter_s: 18.789960861206055\n",
      "  time_total_s: 536.6741051673889\n",
      "  timers:\n",
      "    learn_throughput: 1589.519\n",
      "    learn_time_ms: 629.121\n",
      "    load_throughput: 73606.233\n",
      "    load_time_ms: 13.586\n",
      "    sample_throughput: 53.299\n",
      "    sample_time_ms: 18761.92\n",
      "    update_time_ms: 2.34\n",
      "  timestamp: 1635018828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         536.674</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-4.44709</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.309</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-54-08\n",
      "  done: false\n",
      "  episode_len_mean: 434.42105263157896\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.4445614035087235\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 57\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.481146952841017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010945687448970521\n",
      "          policy_loss: -0.02685449090268877\n",
      "          total_loss: -0.0422958160440127\n",
      "          vf_explained_var: 0.5427663326263428\n",
      "          vf_loss: 0.007181010805329101\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.19642857142858\n",
      "    ram_util_percent: 32.34642857142856\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038833131978648086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.945432986975767\n",
      "    mean_inference_ms: 1.721712455868037\n",
      "    mean_raw_obs_processing_ms: 0.6813215030101412\n",
      "  time_since_restore: 556.276917219162\n",
      "  time_this_iter_s: 19.60281205177307\n",
      "  time_total_s: 556.276917219162\n",
      "  timers:\n",
      "    learn_throughput: 1592.854\n",
      "    learn_time_ms: 627.804\n",
      "    load_throughput: 71944.082\n",
      "    load_time_ms: 13.9\n",
      "    sample_throughput: 53.828\n",
      "    sample_time_ms: 18577.7\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1635018848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         556.277</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-4.44456</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.421</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 434.20338983050846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.438983050847409\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 59\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3590919944975113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008136656733831163\n",
      "          policy_loss: -0.10379506465461519\n",
      "          total_loss: -0.11820235401391983\n",
      "          vf_explained_var: 0.6632922887802124\n",
      "          vf_loss: 0.007556301454314962\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.74074074074073\n",
      "    ram_util_percent: 32.31481481481482\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883205871403292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.77148998200419\n",
      "    mean_inference_ms: 1.7214993036078292\n",
      "    mean_raw_obs_processing_ms: 0.6878568181705352\n",
      "  time_since_restore: 575.3957633972168\n",
      "  time_this_iter_s: 19.11884617805481\n",
      "  time_total_s: 575.3957633972168\n",
      "  timers:\n",
      "    learn_throughput: 1599.64\n",
      "    learn_time_ms: 625.141\n",
      "    load_throughput: 70132.764\n",
      "    load_time_ms: 14.259\n",
      "    sample_throughput: 54.068\n",
      "    sample_time_ms: 18495.232\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1635018867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         575.396</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-4.43898</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.203</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 433.9032258064516\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.431290322580597\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 62\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3308644851048785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008920217118296984\n",
      "          policy_loss: 0.05101245252622499\n",
      "          total_loss: 0.03725331781638993\n",
      "          vf_explained_var: 0.6061708331108093\n",
      "          vf_loss: 0.007765467651188374\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.105555555555554\n",
      "    ram_util_percent: 32.20185185185185\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882940575543461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.52973020214163\n",
      "    mean_inference_ms: 1.7211866199505326\n",
      "    mean_raw_obs_processing_ms: 0.7261910249183271\n",
      "  time_since_restore: 612.7988994121552\n",
      "  time_this_iter_s: 37.403136014938354\n",
      "  time_total_s: 612.7988994121552\n",
      "  timers:\n",
      "    learn_throughput: 1609.584\n",
      "    learn_time_ms: 621.279\n",
      "    load_throughput: 68677.12\n",
      "    load_time_ms: 14.561\n",
      "    sample_throughput: 49.498\n",
      "    sample_time_ms: 20203.01\n",
      "    update_time_ms: 2.108\n",
      "  timestamp: 1635018904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         612.799</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-4.43129</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           433.903</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 434.28125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.4321874999999515\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 64\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3821584966447618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008633569892946823\n",
      "          policy_loss: 0.11343564556704627\n",
      "          total_loss: 0.09655558301342858\n",
      "          vf_explained_var: 0.3652201294898987\n",
      "          vf_loss: 0.005214814822668106\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.25769230769231\n",
      "    ram_util_percent: 32.315384615384616\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882739386321535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.37800753942351\n",
      "    mean_inference_ms: 1.7209714910347893\n",
      "    mean_raw_obs_processing_ms: 0.7482997037523675\n",
      "  time_since_restore: 631.3351621627808\n",
      "  time_this_iter_s: 18.53626275062561\n",
      "  time_total_s: 631.3351621627808\n",
      "  timers:\n",
      "    learn_throughput: 1618.065\n",
      "    learn_time_ms: 618.022\n",
      "    load_throughput: 65789.495\n",
      "    load_time_ms: 15.2\n",
      "    sample_throughput: 49.814\n",
      "    sample_time_ms: 20074.59\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1635018923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         631.335</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-4.43219</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 434.7878787878788\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.434545454545406\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 66\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.308122817675273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011663353597190548\n",
      "          policy_loss: -0.07617642978827159\n",
      "          total_loss: -0.08680824248327149\n",
      "          vf_explained_var: 0.3994485139846802\n",
      "          vf_loss: 0.01011674489062797\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.56153846153846\n",
      "    ram_util_percent: 32.315384615384616\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038824991632922815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.23248857720525\n",
      "    mean_inference_ms: 1.7207522439541159\n",
      "    mean_raw_obs_processing_ms: 0.767745640776788\n",
      "  time_since_restore: 649.7367134094238\n",
      "  time_this_iter_s: 18.401551246643066\n",
      "  time_total_s: 649.7367134094238\n",
      "  timers:\n",
      "    learn_throughput: 1623.713\n",
      "    learn_time_ms: 615.872\n",
      "    load_throughput: 65818.092\n",
      "    load_time_ms: 15.193\n",
      "    sample_throughput: 50.293\n",
      "    sample_time_ms: 19883.584\n",
      "    update_time_ms: 2.132\n",
      "  timestamp: 1635018941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         649.737</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-4.43455</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.788</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 434.5735294117647\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.429852941176422\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 68\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.304282988442315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0107477344045306\n",
      "          policy_loss: -0.1283126112487581\n",
      "          total_loss: -0.13840490447150336\n",
      "          vf_explained_var: 0.24112474918365479\n",
      "          vf_loss: 0.010800988744530412\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.375\n",
      "    ram_util_percent: 32.310714285714276\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882319925828579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.093617911280393\n",
      "    mean_inference_ms: 1.720554098785205\n",
      "    mean_raw_obs_processing_ms: 0.7848529084543022\n",
      "  time_since_restore: 668.9877393245697\n",
      "  time_this_iter_s: 19.251025915145874\n",
      "  time_total_s: 668.9877393245697\n",
      "  timers:\n",
      "    learn_throughput: 1614.725\n",
      "    learn_time_ms: 619.301\n",
      "    load_throughput: 66065.453\n",
      "    load_time_ms: 15.137\n",
      "    sample_throughput: 50.345\n",
      "    sample_time_ms: 19862.89\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1635018960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         668.988</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-4.42985</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.574</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-56-21\n",
      "  done: false\n",
      "  episode_len_mean: 434.49295774647885\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.425492957746431\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 71\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3144882864422267\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0082653286962029\n",
      "          policy_loss: 0.04817105664147271\n",
      "          total_loss: 0.03645745457874404\n",
      "          vf_explained_var: 0.5554531216621399\n",
      "          vf_loss: 0.009778217032241325\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.044827586206885\n",
      "    ram_util_percent: 32.279310344827586\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882073375786426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.8991443184427\n",
      "    mean_inference_ms: 1.7202713821540292\n",
      "    mean_raw_obs_processing_ms: 0.807166427654047\n",
      "  time_since_restore: 689.5090506076813\n",
      "  time_this_iter_s: 20.521311283111572\n",
      "  time_total_s: 689.5090506076813\n",
      "  timers:\n",
      "    learn_throughput: 1612.024\n",
      "    learn_time_ms: 620.338\n",
      "    load_throughput: 66344.47\n",
      "    load_time_ms: 15.073\n",
      "    sample_throughput: 49.786\n",
      "    sample_time_ms: 20086.06\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1635018981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         689.509</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-4.42549</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           434.493</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-56-40\n",
      "  done: false\n",
      "  episode_len_mean: 434.8904109589041\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.4272602739725535\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 73\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.338136445151435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008405245413227114\n",
      "          policy_loss: -0.05098868658145269\n",
      "          total_loss: -0.06251714080572128\n",
      "          vf_explained_var: 0.5227544903755188\n",
      "          vf_loss: 0.010171856784856775\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.37777777777778\n",
      "    ram_util_percent: 32.28888888888888\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038818750228255365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.776276363027225\n",
      "    mean_inference_ms: 1.7200888074428955\n",
      "    mean_raw_obs_processing_ms: 0.8200425715066111\n",
      "  time_since_restore: 708.1264250278473\n",
      "  time_this_iter_s: 18.617374420166016\n",
      "  time_total_s: 708.1264250278473\n",
      "  timers:\n",
      "    learn_throughput: 1611.019\n",
      "    learn_time_ms: 620.725\n",
      "    load_throughput: 64022.567\n",
      "    load_time_ms: 15.619\n",
      "    sample_throughput: 49.476\n",
      "    sample_time_ms: 20211.981\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1635019000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         708.126</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-4.42726</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">            434.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 435.3466666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.429733333333284\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 75\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.322149059507582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00837809552291853\n",
      "          policy_loss: -0.06784811433818605\n",
      "          total_loss: -0.0790189661913448\n",
      "          vf_explained_var: 0.45956292748451233\n",
      "          vf_loss: 0.010375020685347004\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.62307692307692\n",
      "    ram_util_percent: 32.33461538461538\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881676357939528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.657615768023575\n",
      "    mean_inference_ms: 1.7199081034586803\n",
      "    mean_raw_obs_processing_ms: 0.831332241493659\n",
      "  time_since_restore: 726.2201972007751\n",
      "  time_this_iter_s: 18.093772172927856\n",
      "  time_total_s: 726.2201972007751\n",
      "  timers:\n",
      "    learn_throughput: 1604.043\n",
      "    learn_time_ms: 623.425\n",
      "    load_throughput: 63956.963\n",
      "    load_time_ms: 15.636\n",
      "    sample_throughput: 49.531\n",
      "    sample_time_ms: 20189.228\n",
      "    update_time_ms: 2.118\n",
      "  timestamp: 1635019018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          726.22</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-4.42973</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           435.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-57-17\n",
      "  done: false\n",
      "  episode_len_mean: 435.8181818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.432467532467483\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 77\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2834779103597005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008530085170790845\n",
      "          policy_loss: -0.13576504190762836\n",
      "          total_loss: -0.14635975360870362\n",
      "          vf_explained_var: 0.28741344809532166\n",
      "          vf_loss: 0.010534047802341066\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.199999999999996\n",
      "    ram_util_percent: 32.407407407407405\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881451984447334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.54367624686422\n",
      "    mean_inference_ms: 1.719725985123026\n",
      "    mean_raw_obs_processing_ms: 0.841210944284255\n",
      "  time_since_restore: 745.2153029441833\n",
      "  time_this_iter_s: 18.995105743408203\n",
      "  time_total_s: 745.2153029441833\n",
      "  timers:\n",
      "    learn_throughput: 1608.069\n",
      "    learn_time_ms: 621.864\n",
      "    load_throughput: 63592.938\n",
      "    load_time_ms: 15.725\n",
      "    sample_throughput: 49.478\n",
      "    sample_time_ms: 20211.175\n",
      "    update_time_ms: 2.132\n",
      "  timestamp: 1635019037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         745.215</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-4.43247</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           435.818</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 436.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.434499999999952\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 80\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2537445704142254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007133612985875424\n",
      "          policy_loss: 0.029798848927021025\n",
      "          total_loss: 0.016907573325766458\n",
      "          vf_explained_var: 0.5491291284561157\n",
      "          vf_loss: 0.008219449791229433\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.733333333333334\n",
      "    ram_util_percent: 32.41481481481481\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038810723182796374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.381289490825708\n",
      "    mean_inference_ms: 1.7194524729377811\n",
      "    mean_raw_obs_processing_ms: 0.8540231386035275\n",
      "  time_since_restore: 764.0667839050293\n",
      "  time_this_iter_s: 18.851480960845947\n",
      "  time_total_s: 764.0667839050293\n",
      "  timers:\n",
      "    learn_throughput: 1612.756\n",
      "    learn_time_ms: 620.057\n",
      "    load_throughput: 62810.985\n",
      "    load_time_ms: 15.921\n",
      "    sample_throughput: 49.658\n",
      "    sample_time_ms: 20137.651\n",
      "    update_time_ms: 2.146\n",
      "  timestamp: 1635019056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         764.067</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\"> -4.4345</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">             436.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 436.0609756097561\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.430365853658488\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 82\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.183645706706577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008987793154797172\n",
      "          policy_loss: -0.04155415611134635\n",
      "          total_loss: -0.05139942251973682\n",
      "          vf_explained_var: 0.4208368957042694\n",
      "          vf_loss: 0.01019362923818537\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.146428571428565\n",
      "    ram_util_percent: 32.482142857142854\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880797470037674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.279184787294962\n",
      "    mean_inference_ms: 1.7192745239707095\n",
      "    mean_raw_obs_processing_ms: 0.8613506992332329\n",
      "  time_since_restore: 784.1842665672302\n",
      "  time_this_iter_s: 20.117482662200928\n",
      "  time_total_s: 784.1842665672302\n",
      "  timers:\n",
      "    learn_throughput: 1613.161\n",
      "    learn_time_ms: 619.901\n",
      "    load_throughput: 61350.73\n",
      "    load_time_ms: 16.3\n",
      "    sample_throughput: 49.414\n",
      "    sample_time_ms: 20237.289\n",
      "    update_time_ms: 2.157\n",
      "  timestamp: 1635019076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         784.184</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-4.43037</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           436.061</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 435.98809523809524\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.427976190476142\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 84\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.222153674231635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010766922967580052\n",
      "          policy_loss: -0.11927219405770302\n",
      "          total_loss: -0.13105402431554264\n",
      "          vf_explained_var: 0.5142672061920166\n",
      "          vf_loss: 0.008286324281814611\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.667741935483875\n",
      "    ram_util_percent: 32.509677419354844\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880496936005725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.18233270193942\n",
      "    mean_inference_ms: 1.71910092102292\n",
      "    mean_raw_obs_processing_ms: 0.867691566528459\n",
      "  time_since_restore: 805.5462708473206\n",
      "  time_this_iter_s: 21.362004280090332\n",
      "  time_total_s: 805.5462708473206\n",
      "  timers:\n",
      "    learn_throughput: 1615.931\n",
      "    learn_time_ms: 618.838\n",
      "    load_throughput: 60620.615\n",
      "    load_time_ms: 16.496\n",
      "    sample_throughput: 53.665\n",
      "    sample_time_ms: 18634.068\n",
      "    update_time_ms: 2.127\n",
      "  timestamp: 1635019097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         805.546</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-4.42798</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           435.988</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 436.14942528735634\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.8599999999999617\n",
      "  episode_reward_mean: -4.427241379310296\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 87\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.275034189224243\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00595967333335964\n",
      "          policy_loss: -0.011781069636344909\n",
      "          total_loss: -0.024211448265446557\n",
      "          vf_explained_var: 0.5969095230102539\n",
      "          vf_loss: 0.00912802927972128\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.903703703703705\n",
      "    ram_util_percent: 32.53333333333333\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038800571797127106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.043832977974656\n",
      "    mean_inference_ms: 1.7188532579156126\n",
      "    mean_raw_obs_processing_ms: 0.875838395533403\n",
      "  time_since_restore: 824.6558661460876\n",
      "  time_this_iter_s: 19.10959529876709\n",
      "  time_total_s: 824.6558661460876\n",
      "  timers:\n",
      "    learn_throughput: 1618.064\n",
      "    learn_time_ms: 618.022\n",
      "    load_throughput: 60768.96\n",
      "    load_time_ms: 16.456\n",
      "    sample_throughput: 53.498\n",
      "    sample_time_ms: 18692.257\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1635019116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         824.656</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-4.42724</td><td style=\"text-align: right;\">               -3.86</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           436.149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 435.8426966292135\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.400112359550514\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 89\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.289850730366177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012392546961491427\n",
      "          policy_loss: -0.04330721406473054\n",
      "          total_loss: -0.029145468026399612\n",
      "          vf_explained_var: 0.1712469905614853\n",
      "          vf_loss: 0.034581739041540355\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.679310344827584\n",
      "    ram_util_percent: 32.51379310344828\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879743883511632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.95612350158025\n",
      "    mean_inference_ms: 1.7186894713537126\n",
      "    mean_raw_obs_processing_ms: 0.8804311835152951\n",
      "  time_since_restore: 844.4229600429535\n",
      "  time_this_iter_s: 19.767093896865845\n",
      "  time_total_s: 844.4229600429535\n",
      "  timers:\n",
      "    learn_throughput: 1625.59\n",
      "    learn_time_ms: 615.161\n",
      "    load_throughput: 58925.73\n",
      "    load_time_ms: 16.971\n",
      "    sample_throughput: 53.103\n",
      "    sample_time_ms: 18831.159\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1635019136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         844.423</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-4.40011</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           435.843</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 436.38461538461536\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.404615384615337\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 91\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.279676389694214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011365624880625072\n",
      "          policy_loss: -0.06780133959319856\n",
      "          total_loss: -0.07792381130986743\n",
      "          vf_explained_var: 0.5251362919807434\n",
      "          vf_loss: 0.010401167423050436\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.56\n",
      "    ram_util_percent: 32.46000000000001\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879440236954921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.870669680609534\n",
      "    mean_inference_ms: 1.7185302489685548\n",
      "    mean_raw_obs_processing_ms: 0.8939996862964358\n",
      "  time_since_restore: 879.8853976726532\n",
      "  time_this_iter_s: 35.46243762969971\n",
      "  time_total_s: 879.8853976726532\n",
      "  timers:\n",
      "    learn_throughput: 1622.326\n",
      "    learn_time_ms: 616.399\n",
      "    load_throughput: 58621.502\n",
      "    load_time_ms: 17.059\n",
      "    sample_throughput: 48.897\n",
      "    sample_time_ms: 20450.974\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1635019172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         879.885</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-4.40462</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           436.385</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_19-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 436.31182795698925\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.4030107526881235\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 93\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2330397420459325\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009429774579956005\n",
      "          policy_loss: -0.11293440775738822\n",
      "          total_loss: -0.12432260281509823\n",
      "          vf_explained_var: 0.5818276405334473\n",
      "          vf_loss: 0.009056247741035703\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.634375000000006\n",
      "    ram_util_percent: 35.865624999999994\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038792437303893375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.789560781559473\n",
      "    mean_inference_ms: 1.718406445999581\n",
      "    mean_raw_obs_processing_ms: 0.906289338495565\n",
      "  time_since_restore: 901.912565946579\n",
      "  time_this_iter_s: 22.02716827392578\n",
      "  time_total_s: 901.912565946579\n",
      "  timers:\n",
      "    learn_throughput: 1612.714\n",
      "    learn_time_ms: 620.073\n",
      "    load_throughput: 61450.052\n",
      "    load_time_ms: 16.273\n",
      "    sample_throughput: 48.547\n",
      "    sample_time_ms: 20598.665\n",
      "    update_time_ms: 2.099\n",
      "  timestamp: 1635019194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         901.913</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-4.40301</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">           436.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-00-24\n",
      "  done: false\n",
      "  episode_len_mean: 436.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.398645833333286\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 96\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.233578191863166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007511988361391237\n",
      "          policy_loss: -0.005438137302796046\n",
      "          total_loss: -0.019096689919630688\n",
      "          vf_explained_var: 0.6500407457351685\n",
      "          vf_loss: 0.007174827655156453\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.7547619047619\n",
      "    ram_util_percent: 41.56428571428572\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038791122617217094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.680916744939257\n",
      "    mean_inference_ms: 1.7182903359389623\n",
      "    mean_raw_obs_processing_ms: 0.922840607514089\n",
      "  time_since_restore: 931.896160364151\n",
      "  time_this_iter_s: 29.98359441757202\n",
      "  time_total_s: 931.896160364151\n",
      "  timers:\n",
      "    learn_throughput: 1582.395\n",
      "    learn_time_ms: 631.954\n",
      "    load_throughput: 61742.957\n",
      "    load_time_ms: 16.196\n",
      "    sample_throughput: 46.034\n",
      "    sample_time_ms: 21722.898\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1635019224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         931.896</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-4.39865</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">               436</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 435.98979591836735\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.397755102040769\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 98\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1640738964080812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008139213958745846\n",
      "          policy_loss: -0.01217754301097658\n",
      "          total_loss: -0.025674344930383893\n",
      "          vf_explained_var: 0.7681093811988831\n",
      "          vf_loss: 0.006516095623373986\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.50256410256408\n",
      "    ram_util_percent: 43.76153846153848\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879202666152662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.61495993846882\n",
      "    mean_inference_ms: 1.7182763043884968\n",
      "    mean_raw_obs_processing_ms: 0.9327167095243533\n",
      "  time_since_restore: 958.8760738372803\n",
      "  time_this_iter_s: 26.979913473129272\n",
      "  time_total_s: 958.8760738372803\n",
      "  timers:\n",
      "    learn_throughput: 1570.294\n",
      "    learn_time_ms: 636.823\n",
      "    load_throughput: 61864.807\n",
      "    load_time_ms: 16.164\n",
      "    sample_throughput: 44.236\n",
      "    sample_time_ms: 22606.157\n",
      "    update_time_ms: 2.98\n",
      "  timestamp: 1635019251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         958.876</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-4.39776</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">            435.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 435.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.414899999999951\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 101\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1595092799928453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010073970001347899\n",
      "          policy_loss: 0.02578423014945454\n",
      "          total_loss: 0.034863235221968755\n",
      "          vf_explained_var: 0.6495180130004883\n",
      "          vf_loss: 0.02865930088640501\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.15405405405404\n",
      "    ram_util_percent: 43.964864864864865\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878596771099072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.174039871192345\n",
      "    mean_inference_ms: 1.7176644768711566\n",
      "    mean_raw_obs_processing_ms: 0.9538318823454717\n",
      "  time_since_restore: 984.7229413986206\n",
      "  time_this_iter_s: 25.846867561340332\n",
      "  time_total_s: 984.7229413986206\n",
      "  timers:\n",
      "    learn_throughput: 1523.966\n",
      "    learn_time_ms: 656.183\n",
      "    load_throughput: 61556.47\n",
      "    load_time_ms: 16.245\n",
      "    sample_throughput: 42.971\n",
      "    sample_time_ms: 23271.52\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1635019276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         984.723</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -4.4149</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">             435.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 435.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.415099999999952\n",
      "  episode_reward_min: -9.859999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 103\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1553941620720756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004990846914951171\n",
      "          policy_loss: 0.01546290549967024\n",
      "          total_loss: 0.004057837526003519\n",
      "          vf_explained_var: 0.7196251153945923\n",
      "          vf_loss: 0.009150704681976802\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.3918918918919\n",
      "    ram_util_percent: 44.15135135135135\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877293739301452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.5924987874473\n",
      "    mean_inference_ms: 1.7167374564235942\n",
      "    mean_raw_obs_processing_ms: 0.9774932468856905\n",
      "  time_since_restore: 1010.6368288993835\n",
      "  time_this_iter_s: 25.91388750076294\n",
      "  time_total_s: 1010.6368288993835\n",
      "  timers:\n",
      "    learn_throughput: 1508.383\n",
      "    learn_time_ms: 662.962\n",
      "    load_throughput: 65875.982\n",
      "    load_time_ms: 15.18\n",
      "    sample_throughput: 41.715\n",
      "    sample_time_ms: 23972.041\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1635019302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1010.64</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\"> -4.4151</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -9.86</td><td style=\"text-align: right;\">            435.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 435.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.352599999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 106\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.218172033627828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009678379322517867\n",
      "          policy_loss: -0.05322426466478242\n",
      "          total_loss: -0.0645353032482995\n",
      "          vf_explained_var: 0.696117103099823\n",
      "          vf_loss: 0.009902840425881246\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.55945945945946\n",
      "    ram_util_percent: 44.41621621621622\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876194208344054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.054739308932955\n",
      "    mean_inference_ms: 1.716028024785544\n",
      "    mean_raw_obs_processing_ms: 1.01210780694422\n",
      "  time_since_restore: 1036.956953048706\n",
      "  time_this_iter_s: 26.32012414932251\n",
      "  time_total_s: 1036.956953048706\n",
      "  timers:\n",
      "    learn_throughput: 1494.884\n",
      "    learn_time_ms: 668.948\n",
      "    load_throughput: 71185.939\n",
      "    load_time_ms: 14.048\n",
      "    sample_throughput: 40.672\n",
      "    sample_time_ms: 24587.058\n",
      "    update_time_ms: 3.516\n",
      "  timestamp: 1635019329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1036.96</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\"> -4.3526</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            435.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 435.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.355599999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 108\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1891521215438843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008879403202508001\n",
      "          policy_loss: 0.05700274035334587\n",
      "          total_loss: 0.04059322037630611\n",
      "          vf_explained_var: 0.771797239780426\n",
      "          vf_loss: 0.004594059765157807\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.88857142857142\n",
      "    ram_util_percent: 44.494285714285716\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876300090319206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.8254271228012\n",
      "    mean_inference_ms: 1.715905986682726\n",
      "    mean_raw_obs_processing_ms: 1.0348654952059533\n",
      "  time_since_restore: 1061.367699623108\n",
      "  time_this_iter_s: 24.410746574401855\n",
      "  time_total_s: 1061.367699623108\n",
      "  timers:\n",
      "    learn_throughput: 1482.169\n",
      "    learn_time_ms: 674.687\n",
      "    load_throughput: 76314.186\n",
      "    load_time_ms: 13.104\n",
      "    sample_throughput: 40.182\n",
      "    sample_time_ms: 24886.881\n",
      "    update_time_ms: 3.612\n",
      "  timestamp: 1635019353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1061.37</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\"> -4.3556</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            435.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-02-58\n",
      "  done: false\n",
      "  episode_len_mean: 435.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.356499999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 110\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.17584527598487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009575842308693143\n",
      "          policy_loss: -0.12981241138445007\n",
      "          total_loss: -0.1441402830183506\n",
      "          vf_explained_var: 0.7631569504737854\n",
      "          vf_loss: 0.006472995740154551\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.80857142857143\n",
      "    ram_util_percent: 44.55714285714285\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877013195222166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.65671194058418\n",
      "    mean_inference_ms: 1.7159922299427695\n",
      "    mean_raw_obs_processing_ms: 1.0572062284015622\n",
      "  time_since_restore: 1085.9507575035095\n",
      "  time_this_iter_s: 24.58305788040161\n",
      "  time_total_s: 1085.9507575035095\n",
      "  timers:\n",
      "    learn_throughput: 1471.201\n",
      "    learn_time_ms: 679.717\n",
      "    load_throughput: 80990.664\n",
      "    load_time_ms: 12.347\n",
      "    sample_throughput: 39.325\n",
      "    sample_time_ms: 25429.344\n",
      "    update_time_ms: 3.831\n",
      "  timestamp: 1635019378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1085.95</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -4.3565</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            435.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 435.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.356399999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 113\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.139356944296095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014269520725157806\n",
      "          policy_loss: -0.008985245476166408\n",
      "          total_loss: -0.02292207951347033\n",
      "          vf_explained_var: 0.8264049291610718\n",
      "          vf_loss: 0.00602978366595279\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.68\n",
      "    ram_util_percent: 44.620000000000005\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038783978306189425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.4529208681032\n",
      "    mean_inference_ms: 1.7162243101520431\n",
      "    mean_raw_obs_processing_ms: 1.0900649681169483\n",
      "  time_since_restore: 1109.871096611023\n",
      "  time_this_iter_s: 23.920339107513428\n",
      "  time_total_s: 1109.871096611023\n",
      "  timers:\n",
      "    learn_throughput: 1460.951\n",
      "    learn_time_ms: 684.486\n",
      "    load_throughput: 83629.171\n",
      "    load_time_ms: 11.958\n",
      "    sample_throughput: 38.7\n",
      "    sample_time_ms: 25839.844\n",
      "    update_time_ms: 4.23\n",
      "  timestamp: 1635019402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1109.87</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> -4.3564</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            435.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 436.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.364999999999951\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 115\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1486356205410426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007830625393956462\n",
      "          policy_loss: 0.11732956502172683\n",
      "          total_loss: 0.10307966768741608\n",
      "          vf_explained_var: 0.34841614961624146\n",
      "          vf_loss: 0.006453398668155488\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.90322580645162\n",
      "    ram_util_percent: 44.522580645161284\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038794760892376316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.34763859892574\n",
      "    mean_inference_ms: 1.716439438589004\n",
      "    mean_raw_obs_processing_ms: 1.1115622560968825\n",
      "  time_since_restore: 1131.961033821106\n",
      "  time_this_iter_s: 22.089937210083008\n",
      "  time_total_s: 1131.961033821106\n",
      "  timers:\n",
      "    learn_throughput: 1459.05\n",
      "    learn_time_ms: 685.377\n",
      "    load_throughput: 91142.678\n",
      "    load_time_ms: 10.972\n",
      "    sample_throughput: 40.812\n",
      "    sample_time_ms: 24502.576\n",
      "    update_time_ms: 4.283\n",
      "  timestamp: 1635019424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1131.96</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">  -4.365</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            436.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 436.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.3663999999999525\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.08079170121087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010453668653037065\n",
      "          policy_loss: -0.1222158753209644\n",
      "          total_loss: -0.13340416567193136\n",
      "          vf_explained_var: 0.4413968324661255\n",
      "          vf_loss: 0.008574259513989092\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.69428571428573\n",
      "    ram_util_percent: 44.519999999999996\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880787514342893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.253925301996322\n",
      "    mean_inference_ms: 1.7167265562354663\n",
      "    mean_raw_obs_processing_ms: 1.1326412851916061\n",
      "  time_since_restore: 1156.5410397052765\n",
      "  time_this_iter_s: 24.580005884170532\n",
      "  time_total_s: 1156.5410397052765\n",
      "  timers:\n",
      "    learn_throughput: 1455.997\n",
      "    learn_time_ms: 686.815\n",
      "    load_throughput: 88053.66\n",
      "    load_time_ms: 11.357\n",
      "    sample_throughput: 40.395\n",
      "    sample_time_ms: 24755.678\n",
      "    update_time_ms: 4.519\n",
      "  timestamp: 1635019448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1156.54</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -4.3664</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            436.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 436.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.368099999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.072778908411662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00922540269739426\n",
      "          policy_loss: 0.014142849544684092\n",
      "          total_loss: 0.003514394329653846\n",
      "          vf_explained_var: 0.5955776572227478\n",
      "          vf_loss: 0.009176795076604726\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.32931034482758\n",
      "    ram_util_percent: 44.44827586206897\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883333215169524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.144524794519143\n",
      "    mean_inference_ms: 1.7173680436709078\n",
      "    mean_raw_obs_processing_ms: 1.173431158950965\n",
      "  time_since_restore: 1196.9678869247437\n",
      "  time_this_iter_s: 40.42684721946716\n",
      "  time_total_s: 1196.9678869247437\n",
      "  timers:\n",
      "    learn_throughput: 1472.904\n",
      "    learn_time_ms: 678.931\n",
      "    load_throughput: 95206.732\n",
      "    load_time_ms: 10.503\n",
      "    sample_throughput: 38.747\n",
      "    sample_time_ms: 25808.716\n",
      "    update_time_ms: 4.452\n",
      "  timestamp: 1635019489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1196.97</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -4.3681</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            436.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-05-13\n",
      "  done: false\n",
      "  episode_len_mean: 437.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.371999999999951\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 122\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.137690846125285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010285055192131468\n",
      "          policy_loss: -0.061753562009996835\n",
      "          total_loss: -0.07394151596559419\n",
      "          vf_explained_var: 0.543520450592041\n",
      "          vf_loss: 0.008160451526701864\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7942857142857\n",
      "    ram_util_percent: 43.982857142857156\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885158316359434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.080693116735603\n",
      "    mean_inference_ms: 1.717837511699093\n",
      "    mean_raw_obs_processing_ms: 1.2000854351066728\n",
      "  time_since_restore: 1221.5233056545258\n",
      "  time_this_iter_s: 24.555418729782104\n",
      "  time_total_s: 1221.5233056545258\n",
      "  timers:\n",
      "    learn_throughput: 1483.147\n",
      "    learn_time_ms: 674.242\n",
      "    load_throughput: 95047.724\n",
      "    load_time_ms: 10.521\n",
      "    sample_throughput: 39.106\n",
      "    sample_time_ms: 25571.354\n",
      "    update_time_ms: 4.121\n",
      "  timestamp: 1635019513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1221.52</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">  -4.372</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            437.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 437.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.369699999999951\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 125\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1552549733055963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009697169464143377\n",
      "          policy_loss: 0.020159436265627544\n",
      "          total_loss: 0.010165009316470889\n",
      "          vf_explained_var: 0.46772316098213196\n",
      "          vf_loss: 0.010588403115333576\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.37142857142857\n",
      "    ram_util_percent: 43.95142857142857\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038876973363800325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00724537578114\n",
      "    mean_inference_ms: 1.7185591559179534\n",
      "    mean_raw_obs_processing_ms: 1.2394147365653898\n",
      "  time_since_restore: 1246.069155216217\n",
      "  time_this_iter_s: 24.545849561691284\n",
      "  time_total_s: 1246.069155216217\n",
      "  timers:\n",
      "    learn_throughput: 1508.021\n",
      "    learn_time_ms: 663.121\n",
      "    load_throughput: 105365.197\n",
      "    load_time_ms: 9.491\n",
      "    sample_throughput: 39.288\n",
      "    sample_time_ms: 25452.975\n",
      "    update_time_ms: 4.522\n",
      "  timestamp: 1635019538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1246.07</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> -4.3697</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">               437</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-06-00\n",
      "  done: false\n",
      "  episode_len_mean: 437.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.376299999999951\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 127\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.114791488647461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010898284687231946\n",
      "          policy_loss: 0.0941600567764706\n",
      "          total_loss: 0.07779857880539365\n",
      "          vf_explained_var: 0.5364037752151489\n",
      "          vf_loss: 0.003696608901049735\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.65312499999999\n",
      "    ram_util_percent: 44.35\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03889285309192468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.966970741288602\n",
      "    mean_inference_ms: 1.7190433894673607\n",
      "    mean_raw_obs_processing_ms: 1.2651632932657084\n",
      "  time_since_restore: 1268.3811893463135\n",
      "  time_this_iter_s: 22.312034130096436\n",
      "  time_total_s: 1268.3811893463135\n",
      "  timers:\n",
      "    learn_throughput: 1513.593\n",
      "    learn_time_ms: 660.68\n",
      "    load_throughput: 103767.009\n",
      "    load_time_ms: 9.637\n",
      "    sample_throughput: 39.849\n",
      "    sample_time_ms: 25094.91\n",
      "    update_time_ms: 4.521\n",
      "  timestamp: 1635019560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1268.38</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -4.3763</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            437.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-06-25\n",
      "  done: false\n",
      "  episode_len_mean: 437.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.370599999999951\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 129\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106161361270481\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008702019156740053\n",
      "          policy_loss: 0.0018217076857884726\n",
      "          total_loss: -0.011060031255086263\n",
      "          vf_explained_var: 0.1770768165588379\n",
      "          vf_loss: 0.00730967154312465\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71176470588237\n",
      "    ram_util_percent: 44.464705882352945\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038907360448456385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.933013785787885\n",
      "    mean_inference_ms: 1.7195097369305932\n",
      "    mean_raw_obs_processing_ms: 1.2763270065907708\n",
      "  time_since_restore: 1292.4933166503906\n",
      "  time_this_iter_s: 24.11212730407715\n",
      "  time_total_s: 1292.4933166503906\n",
      "  timers:\n",
      "    learn_throughput: 1517.424\n",
      "    learn_time_ms: 659.012\n",
      "    load_throughput: 96463.58\n",
      "    load_time_ms: 10.367\n",
      "    sample_throughput: 40.201\n",
      "    sample_time_ms: 24875.281\n",
      "    update_time_ms: 4.455\n",
      "  timestamp: 1635019585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1292.49</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -4.3706</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            437.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-06-49\n",
      "  done: false\n",
      "  episode_len_mean: 436.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.368299999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 131\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9894014636675517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01015785820253308\n",
      "          policy_loss: -0.09869575045175022\n",
      "          total_loss: -0.11147152731815974\n",
      "          vf_explained_var: 0.6715469360351562\n",
      "          vf_loss: 0.006102451361301872\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.34857142857143\n",
      "    ram_util_percent: 44.51714285714286\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892099730618412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.90183850421555\n",
      "    mean_inference_ms: 1.7199703217949787\n",
      "    mean_raw_obs_processing_ms: 1.2729161342389514\n",
      "  time_since_restore: 1317.136278629303\n",
      "  time_this_iter_s: 24.642961978912354\n",
      "  time_total_s: 1317.136278629303\n",
      "  timers:\n",
      "    learn_throughput: 1516.463\n",
      "    learn_time_ms: 659.429\n",
      "    load_throughput: 96301.899\n",
      "    load_time_ms: 10.384\n",
      "    sample_throughput: 40.163\n",
      "    sample_time_ms: 24898.265\n",
      "    update_time_ms: 4.412\n",
      "  timestamp: 1635019609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1317.14</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -4.3683</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            436.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 435.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.350999999999953\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 134\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9731889830695257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009742258518797758\n",
      "          policy_loss: 0.052127044730716283\n",
      "          total_loss: 0.03730539754033089\n",
      "          vf_explained_var: 0.5481957793235779\n",
      "          vf_loss: 0.003936015563603077\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.49444444444444\n",
      "    ram_util_percent: 44.55833333333333\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03894002938437423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.864786537672423\n",
      "    mean_inference_ms: 1.7206216359946387\n",
      "    mean_raw_obs_processing_ms: 1.2710818879491201\n",
      "  time_since_restore: 1341.7607471942902\n",
      "  time_this_iter_s: 24.624468564987183\n",
      "  time_total_s: 1341.7607471942902\n",
      "  timers:\n",
      "    learn_throughput: 1515.932\n",
      "    learn_time_ms: 659.66\n",
      "    load_throughput: 98034.635\n",
      "    load_time_ms: 10.2\n",
      "    sample_throughput: 40.156\n",
      "    sample_time_ms: 24902.763\n",
      "    update_time_ms: 4.229\n",
      "  timestamp: 1635019634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1341.76</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">  -4.351</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            435.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-07-39\n",
      "  done: false\n",
      "  episode_len_mean: 434.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.347799999999952\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 136\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0157004568311905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01042776109250449\n",
      "          policy_loss: -0.12529418153895272\n",
      "          total_loss: -0.13667507320642472\n",
      "          vf_explained_var: 0.5395908355712891\n",
      "          vf_loss: 0.007733336059997479\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.47999999999999\n",
      "    ram_util_percent: 44.60285714285714\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038952151786399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.844494357273415\n",
      "    mean_inference_ms: 1.7210425261005573\n",
      "    mean_raw_obs_processing_ms: 1.2713721009747738\n",
      "  time_since_restore: 1366.5460028648376\n",
      "  time_this_iter_s: 24.785255670547485\n",
      "  time_total_s: 1366.5460028648376\n",
      "  timers:\n",
      "    learn_throughput: 1513.991\n",
      "    learn_time_ms: 660.506\n",
      "    load_throughput: 102037.786\n",
      "    load_time_ms: 9.8\n",
      "    sample_throughput: 40.019\n",
      "    sample_time_ms: 24988.186\n",
      "    update_time_ms: 4.443\n",
      "  timestamp: 1635019659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1366.55</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> -4.3478</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            434.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-08-03\n",
      "  done: false\n",
      "  episode_len_mean: 433.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.331699999999953\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 139\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9730835517247518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006269380225958473\n",
      "          policy_loss: 0.046831805258989334\n",
      "          total_loss: 0.03586910826464494\n",
      "          vf_explained_var: 0.45432335138320923\n",
      "          vf_loss: 0.008141200239252713\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.42571428571429\n",
      "    ram_util_percent: 44.631428571428565\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896972079462806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.821130134011952\n",
      "    mean_inference_ms: 1.7216646915554232\n",
      "    mean_raw_obs_processing_ms: 1.2737199793000973\n",
      "  time_since_restore: 1390.9835844039917\n",
      "  time_this_iter_s: 24.437581539154053\n",
      "  time_total_s: 1390.9835844039917\n",
      "  timers:\n",
      "    learn_throughput: 1513.954\n",
      "    learn_time_ms: 660.522\n",
      "    load_throughput: 102234.767\n",
      "    load_time_ms: 9.781\n",
      "    sample_throughput: 39.646\n",
      "    sample_time_ms: 25222.931\n",
      "    update_time_ms: 4.395\n",
      "  timestamp: 1635019683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1390.98</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -4.3317</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">             433.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 432.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.3225999999999525\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 141\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9777796732054815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008644253088080254\n",
      "          policy_loss: -0.12626225335730445\n",
      "          total_loss: -0.1397722616791725\n",
      "          vf_explained_var: 0.8373710513114929\n",
      "          vf_loss: 0.005403364095319476\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72162162162162\n",
      "    ram_util_percent: 44.67297297297298\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038981969572375416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.810945505276308\n",
      "    mean_inference_ms: 1.7220882662779733\n",
      "    mean_raw_obs_processing_ms: 1.2765117961989916\n",
      "  time_since_restore: 1416.691914319992\n",
      "  time_this_iter_s: 25.708329916000366\n",
      "  time_total_s: 1416.691914319992\n",
      "  timers:\n",
      "    learn_throughput: 1512.338\n",
      "    learn_time_ms: 661.228\n",
      "    load_throughput: 103618.069\n",
      "    load_time_ms: 9.651\n",
      "    sample_throughput: 39.471\n",
      "    sample_time_ms: 25335.191\n",
      "    update_time_ms: 4.454\n",
      "  timestamp: 1635019709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1416.69</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -4.3226</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            432.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 431.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.313299999999953\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 144\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.088556151919895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01218184892330119\n",
      "          policy_loss: -0.04659910549720128\n",
      "          total_loss: -0.051874770555231306\n",
      "          vf_explained_var: -0.08240556716918945\n",
      "          vf_loss: 0.014391709728321682\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34857142857143\n",
      "    ram_util_percent: 44.69428571428571\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900195848063673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.799092040194253\n",
      "    mean_inference_ms: 1.722764970600784\n",
      "    mean_raw_obs_processing_ms: 1.2812490242497843\n",
      "  time_since_restore: 1441.1856458187103\n",
      "  time_this_iter_s: 24.49373149871826\n",
      "  time_total_s: 1441.1856458187103\n",
      "  timers:\n",
      "    learn_throughput: 1510.111\n",
      "    learn_time_ms: 662.203\n",
      "    load_throughput: 99398.392\n",
      "    load_time_ms: 10.061\n",
      "    sample_throughput: 42.122\n",
      "    sample_time_ms: 23740.824\n",
      "    update_time_ms: 4.13\n",
      "  timestamp: 1635019733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1441.19</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\"> -4.3133</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            431.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-09-17\n",
      "  done: false\n",
      "  episode_len_mean: 431.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.314599999999953\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 146\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9973971684773764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008682389822048843\n",
      "          policy_loss: 0.07460441295471457\n",
      "          total_loss: 0.05877807471487257\n",
      "          vf_explained_var: 0.7874733805656433\n",
      "          vf_loss: 0.003279393924943482\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.30588235294118\n",
      "    ram_util_percent: 44.61470588235293\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901653108337027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.795450697649347\n",
      "    mean_inference_ms: 1.7232274449578597\n",
      "    mean_raw_obs_processing_ms: 1.2853784243227073\n",
      "  time_since_restore: 1464.9833199977875\n",
      "  time_this_iter_s: 23.79767417907715\n",
      "  time_total_s: 1464.9833199977875\n",
      "  timers:\n",
      "    learn_throughput: 1505.284\n",
      "    learn_time_ms: 664.326\n",
      "    load_throughput: 101962.13\n",
      "    load_time_ms: 9.808\n",
      "    sample_throughput: 42.261\n",
      "    sample_time_ms: 23662.699\n",
      "    update_time_ms: 4.354\n",
      "  timestamp: 1635019757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1464.98</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -4.3146</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            431.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 430.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.309299999999953\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 148\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.051754613717397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013698685625042926\n",
      "          policy_loss: -0.06835680305957795\n",
      "          total_loss: -0.08103034049272537\n",
      "          vf_explained_var: 0.5799527168273926\n",
      "          vf_loss: 0.0064741406654421655\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.51176470588236\n",
      "    ram_util_percent: 44.57647058823529\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03903229102894305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.796231376708583\n",
      "    mean_inference_ms: 1.7237262782259075\n",
      "    mean_raw_obs_processing_ms: 1.290044269799836\n",
      "  time_since_restore: 1489.415952205658\n",
      "  time_this_iter_s: 24.432632207870483\n",
      "  time_total_s: 1489.415952205658\n",
      "  timers:\n",
      "    learn_throughput: 1512.564\n",
      "    learn_time_ms: 661.129\n",
      "    load_throughput: 91983.175\n",
      "    load_time_ms: 10.872\n",
      "    sample_throughput: 42.276\n",
      "    sample_time_ms: 23654.195\n",
      "    update_time_ms: 3.743\n",
      "  timestamp: 1635019782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1489.42</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -4.3093</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            430.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 428.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.285399999999954\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 151\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.948362746503618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007245887915321999\n",
      "          policy_loss: 0.020531770338614783\n",
      "          total_loss: 0.005188081165154775\n",
      "          vf_explained_var: 0.7554931044578552\n",
      "          vf_loss: 0.0034153508591569133\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.75151515151515\n",
      "    ram_util_percent: 44.55454545454546\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905856342675529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80838216070355\n",
      "    mean_inference_ms: 1.7245639459625\n",
      "    mean_raw_obs_processing_ms: 1.3058419123167007\n",
      "  time_since_restore: 1535.1785588264465\n",
      "  time_this_iter_s: 45.762606620788574\n",
      "  time_total_s: 1535.1785588264465\n",
      "  timers:\n",
      "    learn_throughput: 1509.934\n",
      "    learn_time_ms: 662.281\n",
      "    load_throughput: 84948.445\n",
      "    load_time_ms: 11.772\n",
      "    sample_throughput: 38.466\n",
      "    sample_time_ms: 25997.173\n",
      "    update_time_ms: 3.84\n",
      "  timestamp: 1635019827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 20.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1535.18</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\"> -4.2854</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            428.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-10-54\n",
      "  done: false\n",
      "  episode_len_mean: 426.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.269399999999955\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 153\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.049194338586595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01267501610823951\n",
      "          policy_loss: -0.10163272354337904\n",
      "          total_loss: -0.10950944390561845\n",
      "          vf_explained_var: 0.12567538022994995\n",
      "          vf_loss: 0.011347725548936675\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.49473684210527\n",
      "    ram_util_percent: 44.8\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907728818744383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.819861092750024\n",
      "    mean_inference_ms: 1.7251561728811793\n",
      "    mean_raw_obs_processing_ms: 1.316418512631179\n",
      "  time_since_restore: 1561.9449875354767\n",
      "  time_this_iter_s: 26.76642870903015\n",
      "  time_total_s: 1561.9449875354767\n",
      "  timers:\n",
      "    learn_throughput: 1494.405\n",
      "    learn_time_ms: 669.162\n",
      "    load_throughput: 90390.587\n",
      "    load_time_ms: 11.063\n",
      "    sample_throughput: 38.087\n",
      "    sample_time_ms: 26255.676\n",
      "    update_time_ms: 4.327\n",
      "  timestamp: 1635019854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1561.94</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -4.2694</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            426.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 426.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.2611999999999535\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 156\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9278341624471875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010113807790558009\n",
      "          policy_loss: -0.014488451679547628\n",
      "          total_loss: -0.022353045807944404\n",
      "          vf_explained_var: 0.5158191323280334\n",
      "          vf_loss: 0.010402361965841718\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.6975\n",
      "    ram_util_percent: 45.255\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910661211943036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.843986646980607\n",
      "    mean_inference_ms: 1.7260872118635362\n",
      "    mean_raw_obs_processing_ms: 1.3330361290088077\n",
      "  time_since_restore: 1590.0785014629364\n",
      "  time_this_iter_s: 28.133513927459717\n",
      "  time_total_s: 1590.0785014629364\n",
      "  timers:\n",
      "    learn_throughput: 1489.506\n",
      "    learn_time_ms: 671.363\n",
      "    load_throughput: 90953.928\n",
      "    load_time_ms: 10.995\n",
      "    sample_throughput: 37.592\n",
      "    sample_time_ms: 26601.517\n",
      "    update_time_ms: 5.343\n",
      "  timestamp: 1635019882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1590.08</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\"> -4.2612</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            426.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-11-51\n",
      "  done: false\n",
      "  episode_len_mean: 425.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.253899999999954\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 158\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7641051994429695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009075877399056489\n",
      "          policy_loss: -0.08164861649274827\n",
      "          total_loss: -0.09408221542835235\n",
      "          vf_explained_var: 0.8700914978981018\n",
      "          vf_loss: 0.004299866171075134\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.20975609756097\n",
      "    ram_util_percent: 45.48048780487805\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912684650249813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8638768742951\n",
      "    mean_inference_ms: 1.7267326923247837\n",
      "    mean_raw_obs_processing_ms: 1.3444305210275838\n",
      "  time_since_restore: 1618.959089756012\n",
      "  time_this_iter_s: 28.88058829307556\n",
      "  time_total_s: 1618.959089756012\n",
      "  timers:\n",
      "    learn_throughput: 1472.05\n",
      "    learn_time_ms: 679.325\n",
      "    load_throughput: 90166.153\n",
      "    load_time_ms: 11.091\n",
      "    sample_throughput: 37.012\n",
      "    sample_time_ms: 27018.513\n",
      "    update_time_ms: 5.6\n",
      "  timestamp: 1635019911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1618.96</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -4.2539</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            425.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-12-19\n",
      "  done: false\n",
      "  episode_len_mean: 425.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.2569999999999535\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 160\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0851427965694005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009275961638974619\n",
      "          policy_loss: -0.09826806022061242\n",
      "          total_loss: -0.10885693149434196\n",
      "          vf_explained_var: 0.44432204961776733\n",
      "          vf_loss: 0.009334958953938136\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.30487804878048\n",
      "    ram_util_percent: 45.5219512195122\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03915088832457564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.886642563721676\n",
      "    mean_inference_ms: 1.7274119198192666\n",
      "    mean_raw_obs_processing_ms: 1.3496857521709158\n",
      "  time_since_restore: 1647.0966589450836\n",
      "  time_this_iter_s: 28.137569189071655\n",
      "  time_total_s: 1647.0966589450836\n",
      "  timers:\n",
      "    learn_throughput: 1471.457\n",
      "    learn_time_ms: 679.599\n",
      "    load_throughput: 87996.945\n",
      "    load_time_ms: 11.364\n",
      "    sample_throughput: 36.558\n",
      "    sample_time_ms: 27353.921\n",
      "    update_time_ms: 5.167\n",
      "  timestamp: 1635019939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">          1647.1</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">  -4.257</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            425.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 424.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.248099999999954\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 163\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.008443432384067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012428222154738716\n",
      "          policy_loss: -0.031208917415804335\n",
      "          total_loss: -0.043738798300425215\n",
      "          vf_explained_var: 0.8006059527397156\n",
      "          vf_loss: 0.006311731445344372\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.63714285714288\n",
      "    ram_util_percent: 45.408571428571435\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0391878760492794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.923400032174943\n",
      "    mean_inference_ms: 1.7284700196991967\n",
      "    mean_raw_obs_processing_ms: 1.3484254223852599\n",
      "  time_since_restore: 1672.208999156952\n",
      "  time_this_iter_s: 25.112340211868286\n",
      "  time_total_s: 1672.208999156952\n",
      "  timers:\n",
      "    learn_throughput: 1473.437\n",
      "    learn_time_ms: 678.685\n",
      "    load_throughput: 87725.84\n",
      "    load_time_ms: 11.399\n",
      "    sample_throughput: 36.467\n",
      "    sample_time_ms: 27422.141\n",
      "    update_time_ms: 5.373\n",
      "  timestamp: 1635019965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1672.21</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\"> -4.2481</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            424.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 424.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.242999999999954\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 165\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.860249518023597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009752995474158215\n",
      "          policy_loss: -0.13587028247614702\n",
      "          total_loss: -0.15018923303319348\n",
      "          vf_explained_var: 0.8783450722694397\n",
      "          vf_loss: 0.003308243526973658\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.24210526315791\n",
      "    ram_util_percent: 45.51052631578947\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039212902745680414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95113660959929\n",
      "    mean_inference_ms: 1.7292028716150525\n",
      "    mean_raw_obs_processing_ms: 1.3482930473931862\n",
      "  time_since_restore: 1698.2132437229156\n",
      "  time_this_iter_s: 26.004244565963745\n",
      "  time_total_s: 1698.2132437229156\n",
      "  timers:\n",
      "    learn_throughput: 1459.733\n",
      "    learn_time_ms: 685.057\n",
      "    load_throughput: 87593.565\n",
      "    load_time_ms: 11.416\n",
      "    sample_throughput: 36.436\n",
      "    sample_time_ms: 27445.305\n",
      "    update_time_ms: 5.414\n",
      "  timestamp: 1635019991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1698.21</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">  -4.243</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            424.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 423.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.234199999999955\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.980401372909546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01226245025439287\n",
      "          policy_loss: -0.01804441875881619\n",
      "          total_loss: -0.0285046539372868\n",
      "          vf_explained_var: 0.5289773941040039\n",
      "          vf_loss: 0.008117532603339188\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.5027027027027\n",
      "    ram_util_percent: 45.559459459459454\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03925079484668679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99699460329933\n",
      "    mean_inference_ms: 1.7303221631955303\n",
      "    mean_raw_obs_processing_ms: 1.3490751720669067\n",
      "  time_since_restore: 1724.5630073547363\n",
      "  time_this_iter_s: 26.34976363182068\n",
      "  time_total_s: 1724.5630073547363\n",
      "  timers:\n",
      "    learn_throughput: 1460.244\n",
      "    learn_time_ms: 684.817\n",
      "    load_throughput: 91153.77\n",
      "    load_time_ms: 10.97\n",
      "    sample_throughput: 36.191\n",
      "    sample_time_ms: 27631.48\n",
      "    update_time_ms: 5.583\n",
      "  timestamp: 1635020017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1724.56</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -4.2342</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            423.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-14-05\n",
      "  done: false\n",
      "  episode_len_mean: 422.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.2202999999999555\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 170\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8833481152852376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010864808690368832\n",
      "          policy_loss: -0.10135316091279188\n",
      "          total_loss: -0.11567855262094073\n",
      "          vf_explained_var: 0.8659747838973999\n",
      "          vf_loss: 0.0034216089795033136\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.6125\n",
      "    ram_util_percent: 45.6025\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039276165463715235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.029864866867992\n",
      "    mean_inference_ms: 1.7310799592969772\n",
      "    mean_raw_obs_processing_ms: 1.3502633161609834\n",
      "  time_since_restore: 1752.7085721492767\n",
      "  time_this_iter_s: 28.145564794540405\n",
      "  time_total_s: 1752.7085721492767\n",
      "  timers:\n",
      "    learn_throughput: 1457.551\n",
      "    learn_time_ms: 686.082\n",
      "    load_throughput: 96900.831\n",
      "    load_time_ms: 10.32\n",
      "    sample_throughput: 35.631\n",
      "    sample_time_ms: 28065.836\n",
      "    update_time_ms: 5.647\n",
      "  timestamp: 1635020045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1752.71</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -4.2203</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            422.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 420.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.1997999999999545\n",
      "  episode_reward_min: -6.149999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 173\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9188411328527661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012618127034296675\n",
      "          policy_loss: -0.0895100115901894\n",
      "          total_loss: -0.09756068260305457\n",
      "          vf_explained_var: 0.6629371047019958\n",
      "          vf_loss: 0.00987592625638677\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.39024390243902\n",
      "    ram_util_percent: 45.52195121951219\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039314821806717595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.08281553742381\n",
      "    mean_inference_ms: 1.7322365721261201\n",
      "    mean_raw_obs_processing_ms: 1.3522990729712623\n",
      "  time_since_restore: 1780.8528981208801\n",
      "  time_this_iter_s: 28.144325971603394\n",
      "  time_total_s: 1780.8528981208801\n",
      "  timers:\n",
      "    learn_throughput: 1457.396\n",
      "    learn_time_ms: 686.155\n",
      "    load_throughput: 108232.283\n",
      "    load_time_ms: 9.239\n",
      "    sample_throughput: 35.164\n",
      "    sample_time_ms: 28437.816\n",
      "    update_time_ms: 5.729\n",
      "  timestamp: 1635020073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1780.85</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -4.1998</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -6.15</td><td style=\"text-align: right;\">            420.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 418.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.248699999999954\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 176\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9646844956609937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012176157260704517\n",
      "          policy_loss: -0.007155405150519477\n",
      "          total_loss: 0.05461708969540066\n",
      "          vf_explained_var: 0.35940635204315186\n",
      "          vf_loss: 0.08020171970387714\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.63333333333334\n",
      "    ram_util_percent: 45.45384615384617\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393540693224139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.141482356462546\n",
      "    mean_inference_ms: 1.7334517039052348\n",
      "    mean_raw_obs_processing_ms: 1.3555724672188605\n",
      "  time_since_restore: 1808.299192905426\n",
      "  time_this_iter_s: 27.4462947845459\n",
      "  time_total_s: 1808.299192905426\n",
      "  timers:\n",
      "    learn_throughput: 1457.321\n",
      "    learn_time_ms: 686.191\n",
      "    load_throughput: 121943.039\n",
      "    load_time_ms: 8.201\n",
      "    sample_throughput: 37.584\n",
      "    sample_time_ms: 26607.234\n",
      "    update_time_ms: 5.656\n",
      "  timestamp: 1635020101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">          1808.3</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\"> -4.2487</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            418.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 416.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.245099999999955\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 178\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0226288795471192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013476103359133499\n",
      "          policy_loss: -0.0908059181438552\n",
      "          total_loss: -0.09596127855281035\n",
      "          vf_explained_var: 0.6901037693023682\n",
      "          vf_loss: 0.013723314698371623\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.49487179487178\n",
      "    ram_util_percent: 45.38461538461538\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03938070225497071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.18297755245372\n",
      "    mean_inference_ms: 1.734285639825291\n",
      "    mean_raw_obs_processing_ms: 1.3581646403845573\n",
      "  time_since_restore: 1835.3872663974762\n",
      "  time_this_iter_s: 27.08807349205017\n",
      "  time_total_s: 1835.3872663974762\n",
      "  timers:\n",
      "    learn_throughput: 1471.408\n",
      "    learn_time_ms: 679.621\n",
      "    load_throughput: 110288.771\n",
      "    load_time_ms: 9.067\n",
      "    sample_throughput: 37.529\n",
      "    sample_time_ms: 26645.883\n",
      "    update_time_ms: 5.093\n",
      "  timestamp: 1635020128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1835.39</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -4.2451</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">             416.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 414.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.243799999999956\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 181\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.107645716932085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01503162713928307\n",
      "          policy_loss: -0.0060857413543595204\n",
      "          total_loss: 0.028540202561351986\n",
      "          vf_explained_var: 0.6484020948410034\n",
      "          vf_loss: 0.05419923489292463\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.37166666666667\n",
      "    ram_util_percent: 45.23833333333333\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039420789712559155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.247431021267335\n",
      "    mean_inference_ms: 1.7355372589040075\n",
      "    mean_raw_obs_processing_ms: 1.3687220343627684\n",
      "  time_since_restore: 1877.8036003112793\n",
      "  time_this_iter_s: 42.4163339138031\n",
      "  time_total_s: 1877.8036003112793\n",
      "  timers:\n",
      "    learn_throughput: 1475.204\n",
      "    learn_time_ms: 677.872\n",
      "    load_throughput: 99297.44\n",
      "    load_time_ms: 10.071\n",
      "    sample_throughput: 35.618\n",
      "    sample_time_ms: 28075.501\n",
      "    update_time_ms: 4.458\n",
      "  timestamp: 1635020170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">          1877.8</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -4.2438</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            414.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 413.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.2309999999999555\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 184\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.082269628842672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015308540536104311\n",
      "          policy_loss: -0.05773127741283841\n",
      "          total_loss: -0.05231473983989821\n",
      "          vf_explained_var: 0.473812073469162\n",
      "          vf_loss: 0.024708381936781935\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96666666666667\n",
      "    ram_util_percent: 45.030769230769224\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03946206477895825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.313355829739685\n",
      "    mean_inference_ms: 1.7368184089775942\n",
      "    mean_raw_obs_processing_ms: 1.3798912266633117\n",
      "  time_since_restore: 1904.7008402347565\n",
      "  time_this_iter_s: 26.897239923477173\n",
      "  time_total_s: 1904.7008402347565\n",
      "  timers:\n",
      "    learn_throughput: 1493.482\n",
      "    learn_time_ms: 669.576\n",
      "    load_throughput: 90919.228\n",
      "    load_time_ms: 10.999\n",
      "    sample_throughput: 35.861\n",
      "    sample_time_ms: 27885.072\n",
      "    update_time_ms: 4.276\n",
      "  timestamp: 1635020197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1904.7</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">  -4.231</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            413.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 412.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2899999999999965\n",
      "  episode_reward_mean: -4.2187999999999555\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 186\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1205252488454183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012746691520429445\n",
      "          policy_loss: -0.12080534994602203\n",
      "          total_loss: -0.13127856486373476\n",
      "          vf_explained_var: 0.7498727440834045\n",
      "          vf_loss: 0.009457361687802606\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7\n",
      "    ram_util_percent: 45.11388888888889\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03949030938162471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.358670997417676\n",
      "    mean_inference_ms: 1.7376853856220347\n",
      "    mean_raw_obs_processing_ms: 1.387656450434964\n",
      "  time_since_restore: 1930.0927910804749\n",
      "  time_this_iter_s: 25.391950845718384\n",
      "  time_total_s: 1930.0927910804749\n",
      "  timers:\n",
      "    learn_throughput: 1497.121\n",
      "    learn_time_ms: 667.949\n",
      "    load_throughput: 85839.618\n",
      "    load_time_ms: 11.65\n",
      "    sample_throughput: 36.218\n",
      "    sample_time_ms: 27610.394\n",
      "    update_time_ms: 5.484\n",
      "  timestamp: 1635020223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1930.09</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\"> -4.2188</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            412.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 411.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4399999999999706\n",
      "  episode_reward_mean: -4.232299999999956\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 189\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.056717982557085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011866953175714885\n",
      "          policy_loss: 0.017162138554784986\n",
      "          total_loss: 0.007729502684540219\n",
      "          vf_explained_var: 0.4333215355873108\n",
      "          vf_loss: 0.009947845745935208\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.97777777777779\n",
      "    ram_util_percent: 45.20833333333334\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03953340044484642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.42789374530784\n",
      "    mean_inference_ms: 1.7390087525511226\n",
      "    mean_raw_obs_processing_ms: 1.3993101294847898\n",
      "  time_since_restore: 1955.3686699867249\n",
      "  time_this_iter_s: 25.27587890625\n",
      "  time_total_s: 1955.3686699867249\n",
      "  timers:\n",
      "    learn_throughput: 1488.25\n",
      "    learn_time_ms: 671.93\n",
      "    load_throughput: 78535.606\n",
      "    load_time_ms: 12.733\n",
      "    sample_throughput: 36.204\n",
      "    sample_time_ms: 27621.557\n",
      "    update_time_ms: 5.47\n",
      "  timestamp: 1635020248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1955.37</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -4.2323</td><td style=\"text-align: right;\">               -3.44</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            411.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 409.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4399999999999706\n",
      "  episode_reward_mean: -4.2152999999999565\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 191\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9330639084180197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008581248702140234\n",
      "          policy_loss: -0.10621350291702482\n",
      "          total_loss: -0.11619161069393158\n",
      "          vf_explained_var: 0.6715477705001831\n",
      "          vf_loss: 0.008494405566145563\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.68648648648646\n",
      "    ram_util_percent: 45.281081081081076\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03956243454657977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.47626857785588\n",
      "    mean_inference_ms: 1.7399070696018941\n",
      "    mean_raw_obs_processing_ms: 1.3985702154382267\n",
      "  time_since_restore: 1981.0540175437927\n",
      "  time_this_iter_s: 25.68534755706787\n",
      "  time_total_s: 1981.0540175437927\n",
      "  timers:\n",
      "    learn_throughput: 1504.111\n",
      "    learn_time_ms: 664.845\n",
      "    load_throughput: 74081.144\n",
      "    load_time_ms: 13.499\n",
      "    sample_throughput: 36.237\n",
      "    sample_time_ms: 27596.008\n",
      "    update_time_ms: 5.367\n",
      "  timestamp: 1635020274\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1981.05</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -4.2153</td><td style=\"text-align: right;\">               -3.44</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            409.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-18-19\n",
      "  done: false\n",
      "  episode_len_mean: 408.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4399999999999706\n",
      "  episode_reward_mean: -4.199799999999956\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 194\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8152099609375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008347796412510681\n",
      "          policy_loss: 0.07708863673938646\n",
      "          total_loss: 0.06732451137569216\n",
      "          vf_explained_var: 0.5629695653915405\n",
      "          vf_loss: 0.007553193099900253\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03714285714284\n",
      "    ram_util_percent: 45.48571428571428\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03960465984715221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.54640842797172\n",
      "    mean_inference_ms: 1.7412138469401988\n",
      "    mean_raw_obs_processing_ms: 1.3982765462197626\n",
      "  time_since_restore: 2006.1126964092255\n",
      "  time_this_iter_s: 25.05867886543274\n",
      "  time_total_s: 2006.1126964092255\n",
      "  timers:\n",
      "    learn_throughput: 1509.249\n",
      "    learn_time_ms: 662.581\n",
      "    load_throughput: 73809.594\n",
      "    load_time_ms: 13.548\n",
      "    sample_throughput: 36.404\n",
      "    sample_time_ms: 27469.168\n",
      "    update_time_ms: 5.402\n",
      "  timestamp: 1635020299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         2006.11</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\"> -4.1998</td><td style=\"text-align: right;\">               -3.44</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            408.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 407.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4399999999999706\n",
      "  episode_reward_mean: -4.194299999999957\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 196\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7945300300916036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011680508472430597\n",
      "          policy_loss: -0.1279475533299976\n",
      "          total_loss: -0.13547181694044008\n",
      "          vf_explained_var: 0.47419288754463196\n",
      "          vf_loss: 0.009252985667747756\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.52439024390245\n",
      "    ram_util_percent: 45.71951219512195\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039632842959301764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.591535134837226\n",
      "    mean_inference_ms: 1.7420820094670342\n",
      "    mean_raw_obs_processing_ms: 1.3981785478534219\n",
      "  time_since_restore: 2034.4811837673187\n",
      "  time_this_iter_s: 28.36848735809326\n",
      "  time_total_s: 2034.4811837673187\n",
      "  timers:\n",
      "    learn_throughput: 1518.289\n",
      "    learn_time_ms: 658.636\n",
      "    load_throughput: 73199.661\n",
      "    load_time_ms: 13.661\n",
      "    sample_throughput: 36.371\n",
      "    sample_time_ms: 27494.425\n",
      "    update_time_ms: 5.84\n",
      "  timestamp: 1635020327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         2034.48</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -4.1943</td><td style=\"text-align: right;\">               -3.44</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            407.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 405.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4399999999999706\n",
      "  episode_reward_mean: -4.175099999999957\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 199\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6659393588701883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074699603837724265\n",
      "          policy_loss: -0.10012543317344454\n",
      "          total_loss: -0.10498916506767272\n",
      "          vf_explained_var: 0.4111643433570862\n",
      "          vf_loss: 0.01104866291085879\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.47179487179486\n",
      "    ram_util_percent: 45.764102564102565\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03967244032078781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.655462982414875\n",
      "    mean_inference_ms: 1.7432862450289774\n",
      "    mean_raw_obs_processing_ms: 1.398752628738209\n",
      "  time_since_restore: 2061.5708804130554\n",
      "  time_this_iter_s: 27.089696645736694\n",
      "  time_total_s: 2061.5708804130554\n",
      "  timers:\n",
      "    learn_throughput: 1516.545\n",
      "    learn_time_ms: 659.393\n",
      "    load_throughput: 71219.421\n",
      "    load_time_ms: 14.041\n",
      "    sample_throughput: 36.512\n",
      "    sample_time_ms: 27388.076\n",
      "    update_time_ms: 5.659\n",
      "  timestamp: 1635020354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         2061.57</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -4.1751</td><td style=\"text-align: right;\">               -3.44</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            405.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 403.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.135699999999957\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 202\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6172287146250406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052688235348109435\n",
      "          policy_loss: -0.07094322856929568\n",
      "          total_loss: -0.07569107537468274\n",
      "          vf_explained_var: 0.32514384388923645\n",
      "          vf_loss: 0.010897559848510556\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.82\n",
      "    ram_util_percent: 45.82\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039710564355946006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.71806202364339\n",
      "    mean_inference_ms: 1.7444362939946527\n",
      "    mean_raw_obs_processing_ms: 1.3997156319664084\n",
      "  time_since_restore: 2089.8058009147644\n",
      "  time_this_iter_s: 28.234920501708984\n",
      "  time_total_s: 2089.8058009147644\n",
      "  timers:\n",
      "    learn_throughput: 1515.133\n",
      "    learn_time_ms: 660.008\n",
      "    load_throughput: 70638.887\n",
      "    load_time_ms: 14.157\n",
      "    sample_throughput: 36.409\n",
      "    sample_time_ms: 27466.107\n",
      "    update_time_ms: 5.729\n",
      "  timestamp: 1635020383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         2089.81</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\"> -4.1357</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">             403.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-20-09\n",
      "  done: false\n",
      "  episode_len_mean: 402.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.126299999999957\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 205\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.657235242260827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005424683364210223\n",
      "          policy_loss: 0.03086310132510132\n",
      "          total_loss: 0.024021255928609105\n",
      "          vf_explained_var: 0.5021877884864807\n",
      "          vf_loss: 0.009188038097151244\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67894736842106\n",
      "    ram_util_percent: 45.78947368421053\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03974566943007686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.77766984535169\n",
      "    mean_inference_ms: 1.7455139220363143\n",
      "    mean_raw_obs_processing_ms: 1.401296182792867\n",
      "  time_since_restore: 2116.575587272644\n",
      "  time_this_iter_s: 26.76978635787964\n",
      "  time_total_s: 2116.575587272644\n",
      "  timers:\n",
      "    learn_throughput: 1516.227\n",
      "    learn_time_ms: 659.532\n",
      "    load_throughput: 74797.71\n",
      "    load_time_ms: 13.369\n",
      "    sample_throughput: 36.449\n",
      "    sample_time_ms: 27435.312\n",
      "    update_time_ms: 5.801\n",
      "  timestamp: 1635020409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         2116.58</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\"> -4.1263</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            402.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-20-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.3499999999999726\n",
      "  episode_reward_mean: -4.1073999999999575\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 208\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7541700548595853\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007607002050742562\n",
      "          policy_loss: 0.054009326299031574\n",
      "          total_loss: 0.04664392082227601\n",
      "          vf_explained_var: 0.400435209274292\n",
      "          vf_loss: 0.009415592020377516\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84102564102565\n",
      "    ram_util_percent: 45.69487179487179\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03977946122788222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.836401600123164\n",
      "    mean_inference_ms: 1.7465412435340988\n",
      "    mean_raw_obs_processing_ms: 1.40320895292078\n",
      "  time_since_restore: 2144.1523394584656\n",
      "  time_this_iter_s: 27.576752185821533\n",
      "  time_total_s: 2144.1523394584656\n",
      "  timers:\n",
      "    learn_throughput: 1516.706\n",
      "    learn_time_ms: 659.324\n",
      "    load_throughput: 78287.725\n",
      "    load_time_ms: 12.773\n",
      "    sample_throughput: 38.533\n",
      "    sample_time_ms: 25952.001\n",
      "    update_time_ms: 5.983\n",
      "  timestamp: 1635020437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         2144.15</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -4.1074</td><td style=\"text-align: right;\">               -3.35</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            400.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 398.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -4.083399999999958\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 211\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5784203860494825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011551709079408133\n",
      "          policy_loss: 0.058495956824885474\n",
      "          total_loss: 0.05326485070917342\n",
      "          vf_explained_var: 0.4204216003417969\n",
      "          vf_loss: 0.00939792726551079\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.76718749999999\n",
      "    ram_util_percent: 45.5609375\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03981146235591174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.89371813319719\n",
      "    mean_inference_ms: 1.7474855842564887\n",
      "    mean_raw_obs_processing_ms: 1.4114869354170219\n",
      "  time_since_restore: 2188.5532870292664\n",
      "  time_this_iter_s: 44.40094757080078\n",
      "  time_total_s: 2188.5532870292664\n",
      "  timers:\n",
      "    learn_throughput: 1525.578\n",
      "    learn_time_ms: 655.489\n",
      "    load_throughput: 78762.279\n",
      "    load_time_ms: 12.696\n",
      "    sample_throughput: 36.093\n",
      "    sample_time_ms: 27706.218\n",
      "    update_time_ms: 6.133\n",
      "  timestamp: 1635020481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         2188.55</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -4.0834</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            398.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 395.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -4.054499999999958\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 214\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5537443929248387\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008563164172598666\n",
      "          policy_loss: 0.03573604838715659\n",
      "          total_loss: 0.03046239283349779\n",
      "          vf_explained_var: 0.5202913284301758\n",
      "          vf_loss: 0.009407472891163908\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15365853658535\n",
      "    ram_util_percent: 45.509756097560974\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03984278514318104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.95187219376187\n",
      "    mean_inference_ms: 1.748405316374581\n",
      "    mean_raw_obs_processing_ms: 1.4199609180658763\n",
      "  time_since_restore: 2217.6510598659515\n",
      "  time_this_iter_s: 29.09777283668518\n",
      "  time_total_s: 2217.6510598659515\n",
      "  timers:\n",
      "    learn_throughput: 1510.935\n",
      "    learn_time_ms: 661.842\n",
      "    load_throughput: 79254.749\n",
      "    load_time_ms: 12.618\n",
      "    sample_throughput: 35.623\n",
      "    sample_time_ms: 28071.966\n",
      "    update_time_ms: 4.728\n",
      "  timestamp: 1635020510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         2217.65</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\"> -4.0545</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            395.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 393.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -4.028699999999959\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 217\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.654174440436893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007829043435568221\n",
      "          policy_loss: 0.0350532054901123\n",
      "          total_loss: 0.028885805937978955\n",
      "          vf_explained_var: 0.4424043297767639\n",
      "          vf_loss: 0.009591441344107604\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.40714285714287\n",
      "    ram_util_percent: 45.61904761904761\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03987296003649322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.010658319452176\n",
      "    mean_inference_ms: 1.7492873794455863\n",
      "    mean_raw_obs_processing_ms: 1.42883045322405\n",
      "  time_since_restore: 2247.0611062049866\n",
      "  time_this_iter_s: 29.410046339035034\n",
      "  time_total_s: 2247.0611062049866\n",
      "  timers:\n",
      "    learn_throughput: 1518.381\n",
      "    learn_time_ms: 658.596\n",
      "    load_throughput: 86455.004\n",
      "    load_time_ms: 11.567\n",
      "    sample_throughput: 35.1\n",
      "    sample_time_ms: 28489.869\n",
      "    update_time_ms: 4.55\n",
      "  timestamp: 1635020540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         2247.06</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\"> -4.0287</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">             393.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 390.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -4.00539999999996\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 220\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6166292760107253\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004768213438634916\n",
      "          policy_loss: 0.060320100602176456\n",
      "          total_loss: 0.051702542851368584\n",
      "          vf_explained_var: 0.3882598876953125\n",
      "          vf_loss: 0.007071916169176499\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.86829268292682\n",
      "    ram_util_percent: 45.614634146341466\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039902614357199134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.069417423152267\n",
      "    mean_inference_ms: 1.7501113472191852\n",
      "    mean_raw_obs_processing_ms: 1.428356999567381\n",
      "  time_since_restore: 2275.5554349422455\n",
      "  time_this_iter_s: 28.49432873725891\n",
      "  time_total_s: 2275.5554349422455\n",
      "  timers:\n",
      "    learn_throughput: 1524.303\n",
      "    learn_time_ms: 656.037\n",
      "    load_throughput: 94206.813\n",
      "    load_time_ms: 10.615\n",
      "    sample_throughput: 34.753\n",
      "    sample_time_ms: 28774.5\n",
      "    update_time_ms: 4.437\n",
      "  timestamp: 1635020568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2275.56</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\"> -4.0054</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            390.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 388.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.98019999999996\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 223\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.702251328362359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010805581088072537\n",
      "          policy_loss: 0.016165454023414186\n",
      "          total_loss: 0.010589511030250125\n",
      "          vf_explained_var: 0.3696785271167755\n",
      "          vf_loss: 0.010906290519698006\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.6951219512195\n",
      "    ram_util_percent: 45.7\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03993197684810571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.127552265281388\n",
      "    mean_inference_ms: 1.7509196487986278\n",
      "    mean_raw_obs_processing_ms: 1.428472684439504\n",
      "  time_since_restore: 2303.909399032593\n",
      "  time_this_iter_s: 28.35396409034729\n",
      "  time_total_s: 2303.909399032593\n",
      "  timers:\n",
      "    learn_throughput: 1523.735\n",
      "    learn_time_ms: 656.282\n",
      "    load_throughput: 94343.702\n",
      "    load_time_ms: 10.6\n",
      "    sample_throughput: 34.361\n",
      "    sample_time_ms: 29103.141\n",
      "    update_time_ms: 4.919\n",
      "  timestamp: 1635020597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         2303.91</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -3.9802</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            388.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 387.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.9705999999999606\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 225\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7564063787460327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01682790576995791\n",
      "          policy_loss: -0.09833930648035473\n",
      "          total_loss: -0.10409256137079663\n",
      "          vf_explained_var: 0.5258105397224426\n",
      "          vf_loss: 0.01096941492998869\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67027027027028\n",
      "    ram_util_percent: 45.8081081081081\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03995151883584889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.166171520474673\n",
      "    mean_inference_ms: 1.7514560809094195\n",
      "    mean_raw_obs_processing_ms: 1.4285830261798032\n",
      "  time_since_restore: 2330.116124391556\n",
      "  time_this_iter_s: 26.206725358963013\n",
      "  time_total_s: 2330.116124391556\n",
      "  timers:\n",
      "    learn_throughput: 1515.683\n",
      "    learn_time_ms: 659.769\n",
      "    load_throughput: 92210.878\n",
      "    load_time_ms: 10.845\n",
      "    sample_throughput: 34.621\n",
      "    sample_time_ms: 28884.257\n",
      "    update_time_ms: 4.303\n",
      "  timestamp: 1635020623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2330.12</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\"> -3.9706</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            387.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 385.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.9483999999999617\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 228\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8053063882721796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009629658687166377\n",
      "          policy_loss: 0.041120141165124045\n",
      "          total_loss: 0.034226451565821964\n",
      "          vf_explained_var: 0.5153694152832031\n",
      "          vf_loss: 0.010677893887946589\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.37105263157896\n",
      "    ram_util_percent: 45.794736842105266\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03998073113281822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.224416583213685\n",
      "    mean_inference_ms: 1.7522377689781723\n",
      "    mean_raw_obs_processing_ms: 1.4292875180176505\n",
      "  time_since_restore: 2357.0695703029633\n",
      "  time_this_iter_s: 26.95344591140747\n",
      "  time_total_s: 2357.0695703029633\n",
      "  timers:\n",
      "    learn_throughput: 1506.789\n",
      "    learn_time_ms: 663.663\n",
      "    load_throughput: 96118.726\n",
      "    load_time_ms: 10.404\n",
      "    sample_throughput: 34.641\n",
      "    sample_time_ms: 28867.163\n",
      "    update_time_ms: 4.345\n",
      "  timestamp: 1635020650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2357.07</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\"> -3.9484</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            385.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-24-38\n",
      "  done: false\n",
      "  episode_len_mean: 383.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.9314999999999625\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 231\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6193169554074605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011282217265880412\n",
      "          policy_loss: 0.01797447486056222\n",
      "          total_loss: 0.01294412695699268\n",
      "          vf_explained_var: 0.5573305487632751\n",
      "          vf_loss: 0.010598713006280984\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80250000000001\n",
      "    ram_util_percent: 45.71999999999999\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04000933703112744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.282038672569776\n",
      "    mean_inference_ms: 1.7530014700420795\n",
      "    mean_raw_obs_processing_ms: 1.4304946793690376\n",
      "  time_since_restore: 2384.583449602127\n",
      "  time_this_iter_s: 27.51387929916382\n",
      "  time_total_s: 2384.583449602127\n",
      "  timers:\n",
      "    learn_throughput: 1507.618\n",
      "    learn_time_ms: 663.298\n",
      "    load_throughput: 92822.265\n",
      "    load_time_ms: 10.773\n",
      "    sample_throughput: 34.728\n",
      "    sample_time_ms: 28795.164\n",
      "    update_time_ms: 4.237\n",
      "  timestamp: 1635020678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2384.58</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\"> -3.9315</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            383.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-25-07\n",
      "  done: false\n",
      "  episode_len_mean: 381.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.910599999999962\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 234\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.562432630856832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006882268474120382\n",
      "          policy_loss: 0.01923524853256014\n",
      "          total_loss: 0.01350867756538921\n",
      "          vf_explained_var: 0.45818057656288147\n",
      "          vf_loss: 0.009553640382364392\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69024390243902\n",
      "    ram_util_percent: 45.6170731707317\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04003527609708348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.33945914505061\n",
      "    mean_inference_ms: 1.753733439682823\n",
      "    mean_raw_obs_processing_ms: 1.4321602093569465\n",
      "  time_since_restore: 2413.7120213508606\n",
      "  time_this_iter_s: 29.12857174873352\n",
      "  time_total_s: 2413.7120213508606\n",
      "  timers:\n",
      "    learn_throughput: 1513.921\n",
      "    learn_time_ms: 660.536\n",
      "    load_throughput: 94124.364\n",
      "    load_time_ms: 10.624\n",
      "    sample_throughput: 34.442\n",
      "    sample_time_ms: 29034.026\n",
      "    update_time_ms: 4.313\n",
      "  timestamp: 1635020707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2413.71</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -3.9106</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            381.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 378.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.8862999999999635\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 237\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.553562421268887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060620118616449715\n",
      "          policy_loss: 0.007332553135024177\n",
      "          total_loss: 0.0021660505069626703\n",
      "          vf_explained_var: 0.5605535507202148\n",
      "          vf_loss: 0.010066022436755398\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.06428571428572\n",
      "    ram_util_percent: 45.490476190476194\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040060907091714074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.396805149122734\n",
      "    mean_inference_ms: 1.7544441948424128\n",
      "    mean_raw_obs_processing_ms: 1.4342623874749634\n",
      "  time_since_restore: 2443.2870314121246\n",
      "  time_this_iter_s: 29.575010061264038\n",
      "  time_total_s: 2443.2870314121246\n",
      "  timers:\n",
      "    learn_throughput: 1520.066\n",
      "    learn_time_ms: 657.866\n",
      "    load_throughput: 94265.461\n",
      "    load_time_ms: 10.608\n",
      "    sample_throughput: 34.203\n",
      "    sample_time_ms: 29237.055\n",
      "    update_time_ms: 3.789\n",
      "  timestamp: 1635020736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2443.29</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\"> -3.8863</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            378.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 376.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.863299999999963\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 240\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5483521382013956\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054754022612807695\n",
      "          policy_loss: 0.028146551715003118\n",
      "          total_loss: 0.022537926336129506\n",
      "          vf_explained_var: 0.6109572052955627\n",
      "          vf_loss: 0.009601126271041318\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.1578125\n",
      "    ram_util_percent: 45.221875\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008610240772297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.45423402237995\n",
      "    mean_inference_ms: 1.7551372974425328\n",
      "    mean_raw_obs_processing_ms: 1.4414733653485312\n",
      "  time_since_restore: 2488.1202535629272\n",
      "  time_this_iter_s: 44.83322215080261\n",
      "  time_total_s: 2488.1202535629272\n",
      "  timers:\n",
      "    learn_throughput: 1501.371\n",
      "    learn_time_ms: 666.058\n",
      "    load_throughput: 103484.363\n",
      "    load_time_ms: 9.663\n",
      "    sample_throughput: 34.161\n",
      "    sample_time_ms: 29273.292\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1635020781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2488.12</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\"> -3.8633</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            376.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-26-54\n",
      "  done: false\n",
      "  episode_len_mean: 373.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.8359999999999634\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 243\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5532474173439874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006060584821576103\n",
      "          policy_loss: 0.018152669237719642\n",
      "          total_loss: 0.012276412381066217\n",
      "          vf_explained_var: 0.6182360649108887\n",
      "          vf_loss: 0.009353191036886225\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.28510638297873\n",
      "    ram_util_percent: 45.06808510638297\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040109733335131556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.51241341006819\n",
      "    mean_inference_ms: 1.7558029925532253\n",
      "    mean_raw_obs_processing_ms: 1.4490178357206098\n",
      "  time_since_restore: 2520.9976642131805\n",
      "  time_this_iter_s: 32.877410650253296\n",
      "  time_total_s: 2520.9976642131805\n",
      "  timers:\n",
      "    learn_throughput: 1518.472\n",
      "    learn_time_ms: 658.557\n",
      "    load_throughput: 115171.495\n",
      "    load_time_ms: 8.683\n",
      "    sample_throughput: 33.716\n",
      "    sample_time_ms: 29659.598\n",
      "    update_time_ms: 3.611\n",
      "  timestamp: 1635020814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">            2521</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">  -3.836</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            373.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 370.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.798499999999964\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 246\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4365587989489237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005685567311237784\n",
      "          policy_loss: 0.05628261135684119\n",
      "          total_loss: 0.05243414524528715\n",
      "          vf_explained_var: 0.5703794956207275\n",
      "          vf_loss: 0.010232841015224241\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38048780487804\n",
      "    ram_util_percent: 45.02439024390244\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040132906253554805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.57110694740774\n",
      "    mean_inference_ms: 1.75643352943618\n",
      "    mean_raw_obs_processing_ms: 1.456689947538379\n",
      "  time_since_restore: 2549.7289283275604\n",
      "  time_this_iter_s: 28.731264114379883\n",
      "  time_total_s: 2549.7289283275604\n",
      "  timers:\n",
      "    learn_throughput: 1518.168\n",
      "    learn_time_ms: 658.689\n",
      "    load_throughput: 102116.537\n",
      "    load_time_ms: 9.793\n",
      "    sample_throughput: 33.794\n",
      "    sample_time_ms: 29590.62\n",
      "    update_time_ms: 3.614\n",
      "  timestamp: 1635020843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2549.73</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -3.7985</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            370.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 367.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.7701999999999645\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 249\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3627658949957953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008386284736710476\n",
      "          policy_loss: 0.05407146679030524\n",
      "          total_loss: 0.0511649328801367\n",
      "          vf_explained_var: 0.5876361131668091\n",
      "          vf_loss: 0.010301813560848435\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.22558139534884\n",
      "    ram_util_percent: 45.316279069767454\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040154894650719856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.62906017345761\n",
      "    mean_inference_ms: 1.7570244027641249\n",
      "    mean_raw_obs_processing_ms: 1.4622423050119189\n",
      "  time_since_restore: 2579.3547925949097\n",
      "  time_this_iter_s: 29.625864267349243\n",
      "  time_total_s: 2579.3547925949097\n",
      "  timers:\n",
      "    learn_throughput: 1477.288\n",
      "    learn_time_ms: 676.916\n",
      "    load_throughput: 103038.456\n",
      "    load_time_ms: 9.705\n",
      "    sample_throughput: 33.686\n",
      "    sample_time_ms: 29685.582\n",
      "    update_time_ms: 3.575\n",
      "  timestamp: 1635020872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2579.35</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\"> -3.7702</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            367.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 364.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.746499999999965\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 252\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1744129604763456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005884557115698325\n",
      "          policy_loss: -0.007331374784310659\n",
      "          total_loss: -0.008494741552405887\n",
      "          vf_explained_var: 0.5232735872268677\n",
      "          vf_loss: 0.010286534846656852\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91914893617022\n",
      "    ram_util_percent: 45.48085106382978\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017622490325271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.686839990734416\n",
      "    mean_inference_ms: 1.7575960151020207\n",
      "    mean_raw_obs_processing_ms: 1.4627595700264546\n",
      "  time_since_restore: 2612.213611125946\n",
      "  time_this_iter_s: 32.85881853103638\n",
      "  time_total_s: 2612.213611125946\n",
      "  timers:\n",
      "    learn_throughput: 1474.089\n",
      "    learn_time_ms: 678.385\n",
      "    load_throughput: 103326.05\n",
      "    load_time_ms: 9.678\n",
      "    sample_throughput: 33.184\n",
      "    sample_time_ms: 30135.144\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1635020905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2612.21</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\"> -3.7465</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            364.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-28-59\n",
      "  done: false\n",
      "  episode_len_mean: 359.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -3.697599999999966\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 256\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3120519505606758\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00574711409333296\n",
      "          policy_loss: -0.019611439812514515\n",
      "          total_loss: -0.0200772139761183\n",
      "          vf_explained_var: 0.5569987893104553\n",
      "          vf_loss: 0.012367389257997274\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.29361702127659\n",
      "    ram_util_percent: 45.50851063829787\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04020201554229588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.763061155640802\n",
      "    mean_inference_ms: 1.7582906905525824\n",
      "    mean_raw_obs_processing_ms: 1.4640706293229093\n",
      "  time_since_restore: 2645.3798184394836\n",
      "  time_this_iter_s: 33.1662073135376\n",
      "  time_total_s: 2645.3798184394836\n",
      "  timers:\n",
      "    learn_throughput: 1474.111\n",
      "    learn_time_ms: 678.375\n",
      "    load_throughput: 104594.806\n",
      "    load_time_ms: 9.561\n",
      "    sample_throughput: 32.435\n",
      "    sample_time_ms: 30831.266\n",
      "    update_time_ms: 3.117\n",
      "  timestamp: 1635020939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2645.38</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\"> -3.6976</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            359.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 356.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -3.6655999999999667\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 259\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2057986444897122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0039553284808955804\n",
      "          policy_loss: 0.044641373389297065\n",
      "          total_loss: 0.04168722530206045\n",
      "          vf_explained_var: 0.6633824706077576\n",
      "          vf_loss: 0.008906074930241124\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.26739130434783\n",
      "    ram_util_percent: 45.608695652173914\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04021855958009828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.81821580459604\n",
      "    mean_inference_ms: 1.7587580240513745\n",
      "    mean_raw_obs_processing_ms: 1.4655658174589896\n",
      "  time_since_restore: 2677.270683288574\n",
      "  time_this_iter_s: 31.890864849090576\n",
      "  time_total_s: 2677.270683288574\n",
      "  timers:\n",
      "    learn_throughput: 1483.611\n",
      "    learn_time_ms: 674.031\n",
      "    load_throughput: 104697.413\n",
      "    load_time_ms: 9.551\n",
      "    sample_throughput: 31.919\n",
      "    sample_time_ms: 31329.311\n",
      "    update_time_ms: 3.088\n",
      "  timestamp: 1635020970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2677.27</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -3.6656</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            356.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-30-04\n",
      "  done: false\n",
      "  episode_len_mean: 352.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -3.627099999999968\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 262\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2499314387639364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006337801373807789\n",
      "          policy_loss: -0.1042589063445727\n",
      "          total_loss: -0.10552658918831083\n",
      "          vf_explained_var: 0.6161864399909973\n",
      "          vf_loss: 0.011073185762183534\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.26808510638298\n",
      "    ram_util_percent: 45.58085106382978\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040231681656518505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.8736367973855\n",
      "    mean_inference_ms: 1.7591786830360758\n",
      "    mean_raw_obs_processing_ms: 1.4674095521511297\n",
      "  time_since_restore: 2710.5768060684204\n",
      "  time_this_iter_s: 33.30612277984619\n",
      "  time_total_s: 2710.5768060684204\n",
      "  timers:\n",
      "    learn_throughput: 1485.959\n",
      "    learn_time_ms: 672.966\n",
      "    load_throughput: 103929.252\n",
      "    load_time_ms: 9.622\n",
      "    sample_throughput: 31.339\n",
      "    sample_time_ms: 31909.571\n",
      "    update_time_ms: 3.213\n",
      "  timestamp: 1635021004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2710.58</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> -3.6271</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            352.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 348.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -3.5789999999999695\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 266\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3009725279278226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005861298460170556\n",
      "          policy_loss: 3.6459995640648734e-05\n",
      "          total_loss: -0.002341476579507192\n",
      "          vf_explained_var: 0.5917928218841553\n",
      "          vf_loss: 0.01048525520082977\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.99574468085105\n",
      "    ram_util_percent: 45.546808510638286\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024892137389353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.948256802436678\n",
      "    mean_inference_ms: 1.7597136142827048\n",
      "    mean_raw_obs_processing_ms: 1.470322994064295\n",
      "  time_since_restore: 2743.516392469406\n",
      "  time_this_iter_s: 32.93958640098572\n",
      "  time_total_s: 2743.516392469406\n",
      "  timers:\n",
      "    learn_throughput: 1417.897\n",
      "    learn_time_ms: 705.27\n",
      "    load_throughput: 103813.753\n",
      "    load_time_ms: 9.633\n",
      "    sample_throughput: 31.0\n",
      "    sample_time_ms: 32258.454\n",
      "    update_time_ms: 3.013\n",
      "  timestamp: 1635021037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2743.52</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">  -3.579</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            348.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-31-09\n",
      "  done: false\n",
      "  episode_len_mean: 345.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -3.5558999999999696\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 269\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3358632816208733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00621878413025401\n",
      "          policy_loss: 0.07145275506708357\n",
      "          total_loss: 0.06613705307245255\n",
      "          vf_explained_var: 0.688347339630127\n",
      "          vf_loss: 0.007887462675312741\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.42608695652174\n",
      "    ram_util_percent: 45.57173913043478\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04026147979954441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.004141703052237\n",
      "    mean_inference_ms: 1.7600989382973558\n",
      "    mean_raw_obs_processing_ms: 1.4728216010514248\n",
      "  time_since_restore: 2775.6869337558746\n",
      "  time_this_iter_s: 32.170541286468506\n",
      "  time_total_s: 2775.6869337558746\n",
      "  timers:\n",
      "    learn_throughput: 1411.197\n",
      "    learn_time_ms: 708.618\n",
      "    load_throughput: 108535.319\n",
      "    load_time_ms: 9.214\n",
      "    sample_throughput: 30.755\n",
      "    sample_time_ms: 32515.125\n",
      "    update_time_ms: 3.051\n",
      "  timestamp: 1635021069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2775.69</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\"> -3.5559</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            345.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 342.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.5214999999999703\n",
      "  episode_reward_min: -10.79999999999993\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 273\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2133645216623943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005265569971383321\n",
      "          policy_loss: -0.02606847956776619\n",
      "          total_loss: -0.02674600879351298\n",
      "          vf_explained_var: 0.6032352447509766\n",
      "          vf_loss: 0.011324472839219703\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.88421052631577\n",
      "    ram_util_percent: 45.50394736842105\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028053756865612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.07875934195903\n",
      "    mean_inference_ms: 1.7605981013297662\n",
      "    mean_raw_obs_processing_ms: 1.4828716701608715\n",
      "  time_since_restore: 2828.6611602306366\n",
      "  time_this_iter_s: 52.97422647476196\n",
      "  time_total_s: 2828.6611602306366\n",
      "  timers:\n",
      "    learn_throughput: 1417.165\n",
      "    learn_time_ms: 705.634\n",
      "    load_throughput: 108279.223\n",
      "    load_time_ms: 9.235\n",
      "    sample_throughput: 30.001\n",
      "    sample_time_ms: 33331.703\n",
      "    update_time_ms: 3.468\n",
      "  timestamp: 1635021122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2828.66</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -3.5215</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -10.8</td><td style=\"text-align: right;\">            342.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 339.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.426299999999971\n",
      "  episode_reward_min: -5.849999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 276\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1952572968271045\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006214833327015551\n",
      "          policy_loss: 0.04670333787798882\n",
      "          total_loss: 0.04312052751580874\n",
      "          vf_explained_var: 0.6927326917648315\n",
      "          vf_loss: 0.008214390041151395\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.46222222222221\n",
      "    ram_util_percent: 45.42000000000001\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040294610318920586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.13431799322888\n",
      "    mean_inference_ms: 1.7609280143529455\n",
      "    mean_raw_obs_processing_ms: 1.4905883672792084\n",
      "  time_since_restore: 2860.2963650226593\n",
      "  time_this_iter_s: 31.635204792022705\n",
      "  time_total_s: 2860.2963650226593\n",
      "  timers:\n",
      "    learn_throughput: 1413.625\n",
      "    learn_time_ms: 707.401\n",
      "    load_throughput: 99222.741\n",
      "    load_time_ms: 10.078\n",
      "    sample_throughput: 30.116\n",
      "    sample_time_ms: 33204.863\n",
      "    update_time_ms: 3.458\n",
      "  timestamp: 1635021154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          2860.3</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\"> -3.4263</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -5.85</td><td style=\"text-align: right;\">            339.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-33-08\n",
      "  done: false\n",
      "  episode_len_mean: 337.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.3893999999999718\n",
      "  episode_reward_min: -5.849999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 279\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0069921685589684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004645967158639606\n",
      "          policy_loss: -0.09345941932664978\n",
      "          total_loss: -0.0920934672984812\n",
      "          vf_explained_var: 0.5490016341209412\n",
      "          vf_loss: 0.011319724428984854\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.24693877551022\n",
      "    ram_util_percent: 45.4\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040308574556737246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.19044963548362\n",
      "    mean_inference_ms: 1.7612412481735218\n",
      "    mean_raw_obs_processing_ms: 1.4963948782761718\n",
      "  time_since_restore: 2894.580672979355\n",
      "  time_this_iter_s: 34.28430795669556\n",
      "  time_total_s: 2894.580672979355\n",
      "  timers:\n",
      "    learn_throughput: 1408.322\n",
      "    learn_time_ms: 710.065\n",
      "    load_throughput: 110124.584\n",
      "    load_time_ms: 9.081\n",
      "    sample_throughput: 29.622\n",
      "    sample_time_ms: 33758.328\n",
      "    update_time_ms: 3.519\n",
      "  timestamp: 1635021188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2894.58</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\"> -3.3894</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -5.85</td><td style=\"text-align: right;\">            337.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-33-39\n",
      "  done: false\n",
      "  episode_len_mean: 332.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.3292999999999724\n",
      "  episode_reward_min: -4.339999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 283\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1231139918168387\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005088906568597472\n",
      "          policy_loss: 0.031831791748603185\n",
      "          total_loss: 0.03184889960620138\n",
      "          vf_explained_var: 0.5676871538162231\n",
      "          vf_loss: 0.01118463355426987\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.65777777777778\n",
      "    ram_util_percent: 45.35333333333334\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032681815360604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.265672297048148\n",
      "    mean_inference_ms: 1.7616490844540693\n",
      "    mean_raw_obs_processing_ms: 1.4986976010076227\n",
      "  time_since_restore: 2926.109159231186\n",
      "  time_this_iter_s: 31.528486251831055\n",
      "  time_total_s: 2926.109159231186\n",
      "  timers:\n",
      "    learn_throughput: 1446.121\n",
      "    learn_time_ms: 691.505\n",
      "    load_throughput: 100225.671\n",
      "    load_time_ms: 9.977\n",
      "    sample_throughput: 29.441\n",
      "    sample_time_ms: 33966.453\n",
      "    update_time_ms: 3.457\n",
      "  timestamp: 1635021219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2926.11</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\"> -3.3293</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -4.34</td><td style=\"text-align: right;\">            332.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-34-14\n",
      "  done: false\n",
      "  episode_len_mean: 329.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.298899999999973\n",
      "  episode_reward_min: -4.339999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 286\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0506624076101514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006033045068492271\n",
      "          policy_loss: -0.11614346330364546\n",
      "          total_loss: -0.11568058166238997\n",
      "          vf_explained_var: 0.6099490523338318\n",
      "          vf_loss: 0.010894091768811147\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.96\n",
      "    ram_util_percent: 45.29600000000001\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040339471764053496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.323411007469314\n",
      "    mean_inference_ms: 1.7619286022710312\n",
      "    mean_raw_obs_processing_ms: 1.5006417211766452\n",
      "  time_since_restore: 2960.971395254135\n",
      "  time_this_iter_s: 34.86223602294922\n",
      "  time_total_s: 2960.971395254135\n",
      "  timers:\n",
      "    learn_throughput: 1411.518\n",
      "    learn_time_ms: 708.457\n",
      "    load_throughput: 99386.851\n",
      "    load_time_ms: 10.062\n",
      "    sample_throughput: 29.282\n",
      "    sample_time_ms: 34150.118\n",
      "    update_time_ms: 3.009\n",
      "  timestamp: 1635021254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2960.97</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -3.2989</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -4.34</td><td style=\"text-align: right;\">            329.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 325.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.2549999999999737\n",
      "  episode_reward_min: -4.109999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 290\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.185866932074229\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009425950458479883\n",
      "          policy_loss: 0.06039156234926647\n",
      "          total_loss: 0.05623731497261259\n",
      "          vf_explained_var: 0.7189188599586487\n",
      "          vf_loss: 0.0075865944226582846\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.31739130434784\n",
      "    ram_util_percent: 45.53043478260869\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040355263299737446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.40122459303413\n",
      "    mean_inference_ms: 1.7622636343315896\n",
      "    mean_raw_obs_processing_ms: 1.5037826393392941\n",
      "  time_since_restore: 2993.20107960701\n",
      "  time_this_iter_s: 32.229684352874756\n",
      "  time_total_s: 2993.20107960701\n",
      "  timers:\n",
      "    learn_throughput: 1409.993\n",
      "    learn_time_ms: 709.223\n",
      "    load_throughput: 96272.058\n",
      "    load_time_ms: 10.387\n",
      "    sample_throughput: 29.365\n",
      "    sample_time_ms: 34054.403\n",
      "    update_time_ms: 3.781\n",
      "  timestamp: 1635021287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">          2993.2</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">  -3.255</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -4.11</td><td style=\"text-align: right;\">             325.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 321.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.2127999999999752\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 294\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9002747866842482\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005560922450178947\n",
      "          policy_loss: -0.01340582449403074\n",
      "          total_loss: -0.011231425611509218\n",
      "          vf_explained_var: 0.521113395690918\n",
      "          vf_loss: 0.011107637164079481\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.32448979591835\n",
      "    ram_util_percent: 45.642857142857146\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04037063687152806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.480811249757906\n",
      "    mean_inference_ms: 1.7625621048306133\n",
      "    mean_raw_obs_processing_ms: 1.5073989006190778\n",
      "  time_since_restore: 3027.9023399353027\n",
      "  time_this_iter_s: 34.70126032829285\n",
      "  time_total_s: 3027.9023399353027\n",
      "  timers:\n",
      "    learn_throughput: 1409.27\n",
      "    learn_time_ms: 709.587\n",
      "    load_throughput: 95460.688\n",
      "    load_time_ms: 10.476\n",
      "    sample_throughput: 29.125\n",
      "    sample_time_ms: 34334.764\n",
      "    update_time_ms: 3.874\n",
      "  timestamp: 1635021321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">          3027.9</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\"> -3.2128</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            321.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-35-57\n",
      "  done: false\n",
      "  episode_len_mean: 318.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.181999999999976\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 297\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9982279631826613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0045426295866692395\n",
      "          policy_loss: 0.04446436141928037\n",
      "          total_loss: 0.04255380092395677\n",
      "          vf_explained_var: 0.5489307641983032\n",
      "          vf_loss: 0.008014935203310517\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.9156862745098\n",
      "    ram_util_percent: 45.817647058823546\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04038113232689047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.5406886384774\n",
      "    mean_inference_ms: 1.762752038822219\n",
      "    mean_raw_obs_processing_ms: 1.5104657314713155\n",
      "  time_since_restore: 3063.2587995529175\n",
      "  time_this_iter_s: 35.356459617614746\n",
      "  time_total_s: 3063.2587995529175\n",
      "  timers:\n",
      "    learn_throughput: 1402.285\n",
      "    learn_time_ms: 713.122\n",
      "    load_throughput: 97311.574\n",
      "    load_time_ms: 10.276\n",
      "    sample_throughput: 28.956\n",
      "    sample_time_ms: 34535.678\n",
      "    update_time_ms: 4.44\n",
      "  timestamp: 1635021357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         3063.26</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">  -3.182</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">             318.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 314.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.1492999999999767\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 301\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9547284073299832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046782191964315385\n",
      "          policy_loss: 0.04509885377354092\n",
      "          total_loss: 0.0471082699795564\n",
      "          vf_explained_var: 0.44764506816864014\n",
      "          vf_loss: 0.011527459126793677\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.75205479452055\n",
      "    ram_util_percent: 45.65616438356164\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04039464185801827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.621699169276265\n",
      "    mean_inference_ms: 1.7629948796213006\n",
      "    mean_raw_obs_processing_ms: 1.5202846981835672\n",
      "  time_since_restore: 3114.750944375992\n",
      "  time_this_iter_s: 51.49214482307434\n",
      "  time_total_s: 3114.750944375992\n",
      "  timers:\n",
      "    learn_throughput: 1460.612\n",
      "    learn_time_ms: 684.644\n",
      "    load_throughput: 96566.853\n",
      "    load_time_ms: 10.356\n",
      "    sample_throughput: 27.458\n",
      "    sample_time_ms: 36419.462\n",
      "    update_time_ms: 4.391\n",
      "  timestamp: 1635021408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         3114.75</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -3.1493</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            314.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-37-25\n",
      "  done: false\n",
      "  episode_len_mean: 312.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.126699999999977\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 304\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8998950137032403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007846293509318869\n",
      "          policy_loss: -0.10812360809908973\n",
      "          total_loss: -0.10586231475075086\n",
      "          vf_explained_var: 0.4822577238082886\n",
      "          vf_loss: 0.011235723468578524\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.07500000000002\n",
      "    ram_util_percent: 45.56538461538461\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04040540992370142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.68369924063534\n",
      "    mean_inference_ms: 1.7631652221743963\n",
      "    mean_raw_obs_processing_ms: 1.52775596646565\n",
      "  time_since_restore: 3151.365803003311\n",
      "  time_this_iter_s: 36.614858627319336\n",
      "  time_total_s: 3151.365803003311\n",
      "  timers:\n",
      "    learn_throughput: 1453.222\n",
      "    learn_time_ms: 688.126\n",
      "    load_throughput: 95813.96\n",
      "    load_time_ms: 10.437\n",
      "    sample_throughput: 27.129\n",
      "    sample_time_ms: 36860.377\n",
      "    update_time_ms: 4.275\n",
      "  timestamp: 1635021445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         3151.37</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\"> -3.1267</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            312.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 309.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.095399999999978\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 308\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0087517466810014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004884231929074822\n",
      "          policy_loss: 0.031157403811812402\n",
      "          total_loss: 0.03135678031378322\n",
      "          vf_explained_var: 0.49735718965530396\n",
      "          vf_loss: 0.010271629510033463\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8204081632653\n",
      "    ram_util_percent: 45.59183673469388\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040419109339884705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.76739979564328\n",
      "    mean_inference_ms: 1.7633687345630886\n",
      "    mean_raw_obs_processing_ms: 1.5378308723162213\n",
      "  time_since_restore: 3185.6350045204163\n",
      "  time_this_iter_s: 34.2692015171051\n",
      "  time_total_s: 3185.6350045204163\n",
      "  timers:\n",
      "    learn_throughput: 1450.476\n",
      "    learn_time_ms: 689.429\n",
      "    load_throughput: 94959.712\n",
      "    load_time_ms: 10.531\n",
      "    sample_throughput: 28.581\n",
      "    sample_time_ms: 34988.613\n",
      "    update_time_ms: 3.855\n",
      "  timestamp: 1635021479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         3185.64</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\"> -3.0954</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            309.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 307.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.074999999999977\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 312\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.069027066230774\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007856589972188823\n",
      "          policy_loss: -0.01825839223133193\n",
      "          total_loss: -0.019202705638276207\n",
      "          vf_explained_var: 0.6112270951271057\n",
      "          vf_loss: 0.00973367929044697\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.74489795918367\n",
      "    ram_util_percent: 45.504081632653055\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040432751731933454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.850797688363773\n",
      "    mean_inference_ms: 1.7635660932995711\n",
      "    mean_raw_obs_processing_ms: 1.5407931839304654\n",
      "  time_since_restore: 3219.745756149292\n",
      "  time_this_iter_s: 34.11075162887573\n",
      "  time_total_s: 3219.745756149292\n",
      "  timers:\n",
      "    learn_throughput: 1440.456\n",
      "    learn_time_ms: 694.225\n",
      "    load_throughput: 102218.323\n",
      "    load_time_ms: 9.783\n",
      "    sample_throughput: 28.384\n",
      "    sample_time_ms: 35231.55\n",
      "    update_time_ms: 4.377\n",
      "  timestamp: 1635021513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         3219.75</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">  -3.075</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">             307.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 305.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.057299999999978\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 315\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.141917007499271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015377240431826245\n",
      "          policy_loss: 0.031553730741143225\n",
      "          total_loss: 0.027212206605407928\n",
      "          vf_explained_var: 0.6696286797523499\n",
      "          vf_loss: 0.007053617452685203\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96326530612245\n",
      "    ram_util_percent: 45.57142857142857\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040442536204821494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.913519007065606\n",
      "    mean_inference_ms: 1.7637084917791963\n",
      "    mean_raw_obs_processing_ms: 1.5431528803979506\n",
      "  time_since_restore: 3254.2080340385437\n",
      "  time_this_iter_s: 34.46227788925171\n",
      "  time_total_s: 3254.2080340385437\n",
      "  timers:\n",
      "    learn_throughput: 1451.86\n",
      "    learn_time_ms: 688.772\n",
      "    load_throughput: 102963.332\n",
      "    load_time_ms: 9.712\n",
      "    sample_throughput: 28.365\n",
      "    sample_time_ms: 35254.469\n",
      "    update_time_ms: 4.699\n",
      "  timestamp: 1635021548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         3254.21</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -3.0573</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            305.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-39-41\n",
      "  done: false\n",
      "  episode_len_mean: 303.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0383999999999793\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 319\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3498962243398032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008280241256824658\n",
      "          policy_loss: -0.02803583898478084\n",
      "          total_loss: -0.032556495981083976\n",
      "          vf_explained_var: 0.6818796992301941\n",
      "          vf_loss: 0.008965365930149953\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.97659574468085\n",
      "    ram_util_percent: 45.57021276595744\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04045427478344706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.996709805222885\n",
      "    mean_inference_ms: 1.7638814395149256\n",
      "    mean_raw_obs_processing_ms: 1.5465244956952267\n",
      "  time_since_restore: 3286.974997282028\n",
      "  time_this_iter_s: 32.7669632434845\n",
      "  time_total_s: 3286.974997282028\n",
      "  timers:\n",
      "    learn_throughput: 1451.689\n",
      "    learn_time_ms: 688.853\n",
      "    load_throughput: 114293.064\n",
      "    load_time_ms: 8.749\n",
      "    sample_throughput: 28.265\n",
      "    sample_time_ms: 35379.207\n",
      "    update_time_ms: 4.692\n",
      "  timestamp: 1635021581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         3286.97</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\"> -3.0384</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            303.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 302.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0224999999999795\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 322\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2891516023212009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008327042753141356\n",
      "          policy_loss: 0.04810272802909215\n",
      "          total_loss: 0.04189171443382899\n",
      "          vf_explained_var: 0.7841580510139465\n",
      "          vf_loss: 0.006667490440627767\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.42558139534884\n",
      "    ram_util_percent: 45.43953488372093\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040462420539400634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.058443425788184\n",
      "    mean_inference_ms: 1.763997548492267\n",
      "    mean_raw_obs_processing_ms: 1.5491946088019888\n",
      "  time_since_restore: 3317.3866209983826\n",
      "  time_this_iter_s: 30.41162371635437\n",
      "  time_total_s: 3317.3866209983826\n",
      "  timers:\n",
      "    learn_throughput: 1471.178\n",
      "    learn_time_ms: 679.727\n",
      "    load_throughput: 113500.984\n",
      "    load_time_ms: 8.81\n",
      "    sample_throughput: 28.618\n",
      "    sample_time_ms: 34942.884\n",
      "    update_time_ms: 5.097\n",
      "  timestamp: 1635021611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         3317.39</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\"> -3.0225</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">            302.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 300.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -3.0059999999999802\n",
      "  episode_reward_min: -4.079999999999957\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 325\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.181307397948371\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008263692792202527\n",
      "          policy_loss: -0.06112308435969883\n",
      "          total_loss: -0.06336073784364595\n",
      "          vf_explained_var: 0.6716057658195496\n",
      "          vf_loss: 0.00956250752011935\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31250000000001\n",
      "    ram_util_percent: 45.58541666666667\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04047066969756069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.121021237808296\n",
      "    mean_inference_ms: 1.7641105243120043\n",
      "    mean_raw_obs_processing_ms: 1.551952976814338\n",
      "  time_since_restore: 3350.886272907257\n",
      "  time_this_iter_s: 33.49965190887451\n",
      "  time_total_s: 3350.886272907257\n",
      "  timers:\n",
      "    learn_throughput: 1473.235\n",
      "    learn_time_ms: 678.779\n",
      "    load_throughput: 121003.035\n",
      "    load_time_ms: 8.264\n",
      "    sample_throughput: 28.513\n",
      "    sample_time_ms: 35072.147\n",
      "    update_time_ms: 4.615\n",
      "  timestamp: 1635021644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         3350.89</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">  -3.006</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -4.08</td><td style=\"text-align: right;\">             300.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 297.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.978699999999981\n",
      "  episode_reward_min: -3.889999999999961\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 329\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2957955294185215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006066862298625213\n",
      "          policy_loss: -0.005807141049040688\n",
      "          total_loss: -0.008054251389371023\n",
      "          vf_explained_var: 0.6710819602012634\n",
      "          vf_loss: 0.01070136736250586\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.30652173913045\n",
      "    ram_util_percent: 45.710869565217386\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040480455072684086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.204383343860087\n",
      "    mean_inference_ms: 1.7642226278337065\n",
      "    mean_raw_obs_processing_ms: 1.5560680521663834\n",
      "  time_since_restore: 3382.8748214244843\n",
      "  time_this_iter_s: 31.988548517227173\n",
      "  time_total_s: 3382.8748214244843\n",
      "  timers:\n",
      "    learn_throughput: 1473.668\n",
      "    learn_time_ms: 678.579\n",
      "    load_throughput: 122966.581\n",
      "    load_time_ms: 8.132\n",
      "    sample_throughput: 28.734\n",
      "    sample_time_ms: 34801.487\n",
      "    update_time_ms: 4.5\n",
      "  timestamp: 1635021676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         3382.87</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -2.9787</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.89</td><td style=\"text-align: right;\">            297.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 295.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.955599999999981\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 332\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1925435364246368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009759743030496024\n",
      "          policy_loss: 0.04717342373397615\n",
      "          total_loss: 0.04221992467840512\n",
      "          vf_explained_var: 0.7084634304046631\n",
      "          vf_loss: 0.006956690198017491\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.1041095890411\n",
      "    ram_util_percent: 45.73013698630138\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040487528690463244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.267303001040307\n",
      "    mean_inference_ms: 1.764306350212141\n",
      "    mean_raw_obs_processing_ms: 1.563461268149193\n",
      "  time_since_restore: 3433.882218360901\n",
      "  time_this_iter_s: 51.007396936416626\n",
      "  time_total_s: 3433.882218360901\n",
      "  timers:\n",
      "    learn_throughput: 1486.109\n",
      "    learn_time_ms: 672.898\n",
      "    load_throughput: 120028.961\n",
      "    load_time_ms: 8.331\n",
      "    sample_throughput: 27.493\n",
      "    sample_time_ms: 36372.861\n",
      "    update_time_ms: 3.863\n",
      "  timestamp: 1635021728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         3433.88</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\"> -2.9556</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            295.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-42-42\n",
      "  done: false\n",
      "  episode_len_mean: 293.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.9338999999999813\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 336\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0697135792838202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005242457397676425\n",
      "          policy_loss: -0.0028939323292838204\n",
      "          total_loss: -0.0032021626830101013\n",
      "          vf_explained_var: 0.6675970554351807\n",
      "          vf_loss: 0.01038071344503098\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.82916666666667\n",
      "    ram_util_percent: 45.625\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04049658308587892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.351162382922112\n",
      "    mean_inference_ms: 1.7644076815511185\n",
      "    mean_raw_obs_processing_ms: 1.5734508938592688\n",
      "  time_since_restore: 3468.0366790294647\n",
      "  time_this_iter_s: 34.15446066856384\n",
      "  time_total_s: 3468.0366790294647\n",
      "  timers:\n",
      "    learn_throughput: 1495.054\n",
      "    learn_time_ms: 668.872\n",
      "    load_throughput: 107335.164\n",
      "    load_time_ms: 9.317\n",
      "    sample_throughput: 28.867\n",
      "    sample_time_ms: 34641.325\n",
      "    update_time_ms: 4.675\n",
      "  timestamp: 1635021762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         3468.04</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -2.9339</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            293.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 291.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.918499999999982\n",
      "  episode_reward_min: -3.4899999999999696\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 339\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0591649545563593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008275681099250755\n",
      "          policy_loss: 0.029317817091941832\n",
      "          total_loss: 0.026270405451456705\n",
      "          vf_explained_var: 0.6027352809906006\n",
      "          vf_loss: 0.007531307482471068\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.50625000000001\n",
      "    ram_util_percent: 45.60625000000001\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04050312034177874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.413954241244095\n",
      "    mean_inference_ms: 1.7644758404998873\n",
      "    mean_raw_obs_processing_ms: 1.5777620920045212\n",
      "  time_since_restore: 3501.5182526111603\n",
      "  time_this_iter_s: 33.48157358169556\n",
      "  time_total_s: 3501.5182526111603\n",
      "  timers:\n",
      "    learn_throughput: 1490.243\n",
      "    learn_time_ms: 671.031\n",
      "    load_throughput: 104852.882\n",
      "    load_time_ms: 9.537\n",
      "    sample_throughput: 29.133\n",
      "    sample_time_ms: 34325.433\n",
      "    update_time_ms: 4.894\n",
      "  timestamp: 1635021795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         3501.52</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\"> -2.9185</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.49</td><td style=\"text-align: right;\">            291.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 290.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.9013999999999824\n",
      "  episode_reward_min: -3.429999999999971\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 343\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.104745548301273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005706491799716397\n",
      "          policy_loss: -0.01546491127875116\n",
      "          total_loss: -0.01520564125643836\n",
      "          vf_explained_var: 0.5047358870506287\n",
      "          vf_loss: 0.011297805472794506\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.51372549019608\n",
      "    ram_util_percent: 45.72549019607843\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040512139933917404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.497192882365653\n",
      "    mean_inference_ms: 1.764548180473542\n",
      "    mean_raw_obs_processing_ms: 1.581466931996981\n",
      "  time_since_restore: 3536.8590552806854\n",
      "  time_this_iter_s: 35.34080266952515\n",
      "  time_total_s: 3536.8590552806854\n",
      "  timers:\n",
      "    learn_throughput: 1466.139\n",
      "    learn_time_ms: 682.063\n",
      "    load_throughput: 105833.918\n",
      "    load_time_ms: 9.449\n",
      "    sample_throughput: 29.052\n",
      "    sample_time_ms: 34421.55\n",
      "    update_time_ms: 5.253\n",
      "  timestamp: 1635021831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         3536.86</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -2.9014</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.43</td><td style=\"text-align: right;\">            290.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 289.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.892399999999982\n",
      "  episode_reward_min: -3.429999999999971\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 346\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9944541626506381\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004239309299430932\n",
      "          policy_loss: 0.044499366482098894\n",
      "          total_loss: 0.042789509064621395\n",
      "          vf_explained_var: 0.5701876282691956\n",
      "          vf_loss: 0.00822806067751824\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75\n",
      "    ram_util_percent: 45.71458333333334\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04051757010909981\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.559342655311834\n",
      "    mean_inference_ms: 1.7646052983574878\n",
      "    mean_raw_obs_processing_ms: 1.5844252101637468\n",
      "  time_since_restore: 3570.703642845154\n",
      "  time_this_iter_s: 33.844587564468384\n",
      "  time_total_s: 3570.703642845154\n",
      "  timers:\n",
      "    learn_throughput: 1472.449\n",
      "    learn_time_ms: 679.141\n",
      "    load_throughput: 105811.224\n",
      "    load_time_ms: 9.451\n",
      "    sample_throughput: 29.071\n",
      "    sample_time_ms: 34398.282\n",
      "    update_time_ms: 4.834\n",
      "  timestamp: 1635021864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">          3570.7</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\"> -2.8924</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.43</td><td style=\"text-align: right;\">            289.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 287.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.874899999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 350\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0474731279744043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054970972704890325\n",
      "          policy_loss: -0.008987388221753968\n",
      "          total_loss: -0.008894858840439055\n",
      "          vf_explained_var: 0.4955233633518219\n",
      "          vf_loss: 0.010562964890980058\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.89791666666666\n",
      "    ram_util_percent: 45.708333333333336\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040524801656234614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.64163815566888\n",
      "    mean_inference_ms: 1.7646829121236038\n",
      "    mean_raw_obs_processing_ms: 1.5885923365832872\n",
      "  time_since_restore: 3604.715540409088\n",
      "  time_this_iter_s: 34.011897563934326\n",
      "  time_total_s: 3604.715540409088\n",
      "  timers:\n",
      "    learn_throughput: 1466.656\n",
      "    learn_time_ms: 681.823\n",
      "    load_throughput: 105759.997\n",
      "    load_time_ms: 9.455\n",
      "    sample_throughput: 29.111\n",
      "    sample_time_ms: 34350.936\n",
      "    update_time_ms: 4.639\n",
      "  timestamp: 1635021898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         3604.72</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\"> -2.8749</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            287.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 287.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8702999999999834\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 353\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9779867404037051\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004113490452341513\n",
      "          policy_loss: -0.028196162233750027\n",
      "          total_loss: -0.028558612118164697\n",
      "          vf_explained_var: 0.48503243923187256\n",
      "          vf_loss: 0.009414205410414272\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64081632653061\n",
      "    ram_util_percent: 45.71836734693877\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405303467801612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.7023392033077\n",
      "    mean_inference_ms: 1.7647314039642226\n",
      "    mean_raw_obs_processing_ms: 1.5917998566132656\n",
      "  time_since_restore: 3638.529372692108\n",
      "  time_this_iter_s: 33.81383228302002\n",
      "  time_total_s: 3638.529372692108\n",
      "  timers:\n",
      "    learn_throughput: 1460.046\n",
      "    learn_time_ms: 684.91\n",
      "    load_throughput: 104642.037\n",
      "    load_time_ms: 9.556\n",
      "    sample_throughput: 29.026\n",
      "    sample_time_ms: 34451.96\n",
      "    update_time_ms: 5.094\n",
      "  timestamp: 1635021932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         3638.53</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\"> -2.8703</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            287.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 286.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.867499999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 357\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906249999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0709841185145907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00757297766144889\n",
      "          policy_loss: -0.0020407485879129833\n",
      "          total_loss: -0.001028180287943946\n",
      "          vf_explained_var: 0.46572473645210266\n",
      "          vf_loss: 0.01171945347968075\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.05555555555556\n",
      "    ram_util_percent: 45.68222222222221\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040537680519277156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.781836710741942\n",
      "    mean_inference_ms: 1.764793120921704\n",
      "    mean_raw_obs_processing_ms: 1.5961040086631864\n",
      "  time_since_restore: 3670.0736377239227\n",
      "  time_this_iter_s: 31.544265031814575\n",
      "  time_total_s: 3670.0736377239227\n",
      "  timers:\n",
      "    learn_throughput: 1476.494\n",
      "    learn_time_ms: 677.28\n",
      "    load_throughput: 106129.026\n",
      "    load_time_ms: 9.422\n",
      "    sample_throughput: 28.925\n",
      "    sample_time_ms: 34572.503\n",
      "    update_time_ms: 5.531\n",
      "  timestamp: 1635021964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         3670.07</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -2.8675</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            286.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 286.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.863899999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 360\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906249999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.05011418528027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005574738627327531\n",
      "          policy_loss: -0.09128139913082123\n",
      "          total_loss: -0.09005866870284081\n",
      "          vf_explained_var: 0.4533504545688629\n",
      "          vf_loss: 0.0117216973255078\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.32054794520549\n",
      "    ram_util_percent: 45.654794520547945\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040543382024592234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.840829656168047\n",
      "    mean_inference_ms: 1.7648468317241393\n",
      "    mean_raw_obs_processing_ms: 1.6032295778851529\n",
      "  time_since_restore: 3721.544302225113\n",
      "  time_this_iter_s: 51.470664501190186\n",
      "  time_total_s: 3721.544302225113\n",
      "  timers:\n",
      "    learn_throughput: 1473.331\n",
      "    learn_time_ms: 678.734\n",
      "    load_throughput: 105294.044\n",
      "    load_time_ms: 9.497\n",
      "    sample_throughput: 27.497\n",
      "    sample_time_ms: 36367.978\n",
      "    update_time_ms: 5.654\n",
      "  timestamp: 1635022015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         3721.54</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\"> -2.8639</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            286.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 285.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.857899999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 364\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906249999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1006082309616936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004995693863303174\n",
      "          policy_loss: 0.033707687507073085\n",
      "          total_loss: 0.03299728367063734\n",
      "          vf_explained_var: 0.5386841893196106\n",
      "          vf_loss: 0.010293728744404183\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.54081632653062\n",
      "    ram_util_percent: 45.4265306122449\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04055193871480611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.91792175849262\n",
      "    mean_inference_ms: 1.7649204428530976\n",
      "    mean_raw_obs_processing_ms: 1.6128417059920686\n",
      "  time_since_restore: 3755.4192972183228\n",
      "  time_this_iter_s: 33.87499499320984\n",
      "  time_total_s: 3755.4192972183228\n",
      "  timers:\n",
      "    learn_throughput: 1473.25\n",
      "    learn_time_ms: 678.771\n",
      "    load_throughput: 104337.218\n",
      "    load_time_ms: 9.584\n",
      "    sample_throughput: 27.355\n",
      "    sample_time_ms: 36556.507\n",
      "    update_time_ms: 5.709\n",
      "  timestamp: 1635022049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3755.42</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\"> -2.8579</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            285.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 285.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.852699999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 367\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531249999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8750174562136332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007067322249096522\n",
      "          policy_loss: -0.08954561998446782\n",
      "          total_loss: -0.08748951016200913\n",
      "          vf_explained_var: 0.48682597279548645\n",
      "          vf_loss: 0.010804904086722269\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.47800000000001\n",
      "    ram_util_percent: 45.57999999999999\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0405582332834089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.975790636790222\n",
      "    mean_inference_ms: 1.7649647225188696\n",
      "    mean_raw_obs_processing_ms: 1.620004426748551\n",
      "  time_since_restore: 3790.390208721161\n",
      "  time_this_iter_s: 34.970911502838135\n",
      "  time_total_s: 3790.390208721161\n",
      "  timers:\n",
      "    learn_throughput: 1467.193\n",
      "    learn_time_ms: 681.573\n",
      "    load_throughput: 110146.563\n",
      "    load_time_ms: 9.079\n",
      "    sample_throughput: 28.612\n",
      "    sample_time_ms: 34950.217\n",
      "    update_time_ms: 5.897\n",
      "  timestamp: 1635022084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3790.39</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\"> -2.8527</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            285.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 284.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8477999999999835\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 371\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531249999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.006188366148207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003956554941638125\n",
      "          policy_loss: 0.03021787264280849\n",
      "          total_loss: 0.03068714141845703\n",
      "          vf_explained_var: 0.543079137802124\n",
      "          vf_loss: 0.010530381380683846\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.53260869565216\n",
      "    ram_util_percent: 45.64130434782608\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040565234697696226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.05077125207587\n",
      "    mean_inference_ms: 1.7650129149388614\n",
      "    mean_raw_obs_processing_ms: 1.6265345142645302\n",
      "  time_since_restore: 3823.0598170757294\n",
      "  time_this_iter_s: 32.66960835456848\n",
      "  time_total_s: 3823.0598170757294\n",
      "  timers:\n",
      "    learn_throughput: 1460.919\n",
      "    learn_time_ms: 684.501\n",
      "    load_throughput: 124618.699\n",
      "    load_time_ms: 8.024\n",
      "    sample_throughput: 28.735\n",
      "    sample_time_ms: 34800.402\n",
      "    update_time_ms: 5.394\n",
      "  timestamp: 1635022117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3823.06</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -2.8478</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            284.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 283.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8387999999999836\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 375\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8775059256288741\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0029854191530691624\n",
      "          policy_loss: 0.005993819567892286\n",
      "          total_loss: 0.008168569869465299\n",
      "          vf_explained_var: 0.49452459812164307\n",
      "          vf_loss: 0.01094952066325479\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.05961538461537\n",
      "    ram_util_percent: 45.71923076923079\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057107392350189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.12516024540321\n",
      "    mean_inference_ms: 1.765057691964102\n",
      "    mean_raw_obs_processing_ms: 1.6299960549012331\n",
      "  time_since_restore: 3859.2001984119415\n",
      "  time_this_iter_s: 36.14038133621216\n",
      "  time_total_s: 3859.2001984119415\n",
      "  timers:\n",
      "    learn_throughput: 1468.255\n",
      "    learn_time_ms: 681.081\n",
      "    load_throughput: 128994.384\n",
      "    load_time_ms: 7.752\n",
      "    sample_throughput: 28.514\n",
      "    sample_time_ms: 35070.104\n",
      "    update_time_ms: 5.262\n",
      "  timestamp: 1635022153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">          3859.2</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\"> -2.8388</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-49-48\n",
      "  done: false\n",
      "  episode_len_mean: 283.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.833499999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 378\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.882812499999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9951496773295933\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004427126754461222\n",
      "          policy_loss: -0.01316325060195393\n",
      "          total_loss: -0.014735568480359184\n",
      "          vf_explained_var: 0.528319776058197\n",
      "          vf_loss: 0.008378963997691042\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.29199999999999\n",
      "    ram_util_percent: 45.736000000000004\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04057590997933223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.180065953081925\n",
      "    mean_inference_ms: 1.7650965099645826\n",
      "    mean_raw_obs_processing_ms: 1.6327199372642756\n",
      "  time_since_restore: 3894.074788570404\n",
      "  time_this_iter_s: 34.874590158462524\n",
      "  time_total_s: 3894.074788570404\n",
      "  timers:\n",
      "    learn_throughput: 1492.949\n",
      "    learn_time_ms: 669.815\n",
      "    load_throughput: 125396.252\n",
      "    load_time_ms: 7.975\n",
      "    sample_throughput: 28.543\n",
      "    sample_time_ms: 35034.602\n",
      "    update_time_ms: 5.289\n",
      "  timestamp: 1635022188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3894.07</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\"> -2.8335</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 283.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8321999999999834\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 382\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.029045448700587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008114386512434476\n",
      "          policy_loss: 0.005348288102282418\n",
      "          total_loss: 0.007305599169598685\n",
      "          vf_explained_var: 0.42727237939834595\n",
      "          vf_loss: 0.012247570055640407\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12244897959184\n",
      "    ram_util_percent: 45.828571428571436\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04058265001632727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.252688417688677\n",
      "    mean_inference_ms: 1.7651504898830688\n",
      "    mean_raw_obs_processing_ms: 1.6364623680763914\n",
      "  time_since_restore: 3928.9617977142334\n",
      "  time_this_iter_s: 34.887009143829346\n",
      "  time_total_s: 3928.9617977142334\n",
      "  timers:\n",
      "    learn_throughput: 1496.771\n",
      "    learn_time_ms: 668.105\n",
      "    load_throughput: 126800.411\n",
      "    load_time_ms: 7.886\n",
      "    sample_throughput: 28.457\n",
      "    sample_time_ms: 35140.922\n",
      "    update_time_ms: 5.118\n",
      "  timestamp: 1635022223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3928.96</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\"> -2.8322</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 283.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.832499999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 385\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9733583039707607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005971279789129744\n",
      "          policy_loss: -0.08449785924620098\n",
      "          total_loss: -0.08197464396556219\n",
      "          vf_explained_var: 0.4190077483654022\n",
      "          vf_loss: 0.012256655873109897\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.56666666666666\n",
      "    ram_util_percent: 45.750980392156855\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040587786419222595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.30675035243821\n",
      "    mean_inference_ms: 1.7651935747336729\n",
      "    mean_raw_obs_processing_ms: 1.6392946124127161\n",
      "  time_since_restore: 3964.095625638962\n",
      "  time_this_iter_s: 35.133827924728394\n",
      "  time_total_s: 3964.095625638962\n",
      "  timers:\n",
      "    learn_throughput: 1492.469\n",
      "    learn_time_ms: 670.031\n",
      "    load_throughput: 127728.312\n",
      "    load_time_ms: 7.829\n",
      "    sample_throughput: 28.368\n",
      "    sample_time_ms: 35250.54\n",
      "    update_time_ms: 5.737\n",
      "  timestamp: 1635022258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">          3964.1</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -2.8325</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-51-31\n",
      "  done: false\n",
      "  episode_len_mean: 283.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.831599999999984\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 389\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.19374977350235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014758127081899235\n",
      "          policy_loss: 0.03273546910948223\n",
      "          total_loss: 0.03208401319053438\n",
      "          vf_explained_var: 0.4842647612094879\n",
      "          vf_loss: 0.011285683409207397\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75106382978724\n",
      "    ram_util_percent: 45.72553191489361\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04059491372241841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.377628405814384\n",
      "    mean_inference_ms: 1.765259198510343\n",
      "    mean_raw_obs_processing_ms: 1.643178402558052\n",
      "  time_since_restore: 3997.4590213298798\n",
      "  time_this_iter_s: 33.36339569091797\n",
      "  time_total_s: 3997.4590213298798\n",
      "  timers:\n",
      "    learn_throughput: 1494.664\n",
      "    learn_time_ms: 669.047\n",
      "    load_throughput: 128842.62\n",
      "    load_time_ms: 7.761\n",
      "    sample_throughput: 28.404\n",
      "    sample_time_ms: 35206.844\n",
      "    update_time_ms: 5.451\n",
      "  timestamp: 1635022291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3997.46</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\"> -2.8316</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-52-25\n",
      "  done: false\n",
      "  episode_len_mean: 283.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8304999999999843\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 392\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9981088777383168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006893086262272716\n",
      "          policy_loss: -0.1105089783668518\n",
      "          total_loss: -0.10850081791480383\n",
      "          vf_explained_var: 0.48961180448532104\n",
      "          vf_loss: 0.011989083689534002\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.20263157894738\n",
      "    ram_util_percent: 45.54342105263157\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406002583812849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.430612380231683\n",
      "    mean_inference_ms: 1.76531002603825\n",
      "    mean_raw_obs_processing_ms: 1.6497148180477048\n",
      "  time_since_restore: 4050.7791464328766\n",
      "  time_this_iter_s: 53.320125102996826\n",
      "  time_total_s: 4050.7791464328766\n",
      "  timers:\n",
      "    learn_throughput: 1494.177\n",
      "    learn_time_ms: 669.265\n",
      "    load_throughput: 125617.455\n",
      "    load_time_ms: 7.961\n",
      "    sample_throughput: 26.749\n",
      "    sample_time_ms: 37384.645\n",
      "    update_time_ms: 4.707\n",
      "  timestamp: 1635022345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         4050.78</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\"> -2.8305</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-52-58\n",
      "  done: false\n",
      "  episode_len_mean: 283.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.8335999999999832\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 396\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1174957970778148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00516655007707993\n",
      "          policy_loss: 0.05290031863583459\n",
      "          total_loss: 0.05155102262894313\n",
      "          vf_explained_var: 0.5419435501098633\n",
      "          vf_loss: 0.009825533312848873\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92553191489361\n",
      "    ram_util_percent: 45.62978723404254\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04060753616762963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.50007776441243\n",
      "    mean_inference_ms: 1.7653804117127736\n",
      "    mean_raw_obs_processing_ms: 1.6583881528975548\n",
      "  time_since_restore: 4083.840222120285\n",
      "  time_this_iter_s: 33.06107568740845\n",
      "  time_total_s: 4083.840222120285\n",
      "  timers:\n",
      "    learn_throughput: 1492.789\n",
      "    learn_time_ms: 669.887\n",
      "    load_throughput: 120085.319\n",
      "    load_time_ms: 8.327\n",
      "    sample_throughput: 28.135\n",
      "    sample_time_ms: 35542.895\n",
      "    update_time_ms: 4.468\n",
      "  timestamp: 1635022378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         4083.84</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\"> -2.8336</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 283.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.429999999999992\n",
      "  episode_reward_mean: -2.833799999999984\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 399\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9982407079802619\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004833850991847655\n",
      "          policy_loss: -0.11146353334188461\n",
      "          total_loss: -0.10984368994832039\n",
      "          vf_explained_var: 0.5132634043693542\n",
      "          vf_loss: 0.011602138520942794\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.78039215686275\n",
      "    ram_util_percent: 45.562745098039215\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406130236097095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.55092446198395\n",
      "    mean_inference_ms: 1.7654348312216215\n",
      "    mean_raw_obs_processing_ms: 1.6622082542697008\n",
      "  time_since_restore: 4119.59525847435\n",
      "  time_this_iter_s: 35.75503635406494\n",
      "  time_total_s: 4119.59525847435\n",
      "  timers:\n",
      "    learn_throughput: 1487.38\n",
      "    learn_time_ms: 672.323\n",
      "    load_throughput: 120612.277\n",
      "    load_time_ms: 8.291\n",
      "    sample_throughput: 27.989\n",
      "    sample_time_ms: 35728.339\n",
      "    update_time_ms: 4.515\n",
      "  timestamp: 1635022414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">          4119.6</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -2.8338</td><td style=\"text-align: right;\">               -2.43</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-54-06\n",
      "  done: false\n",
      "  episode_len_mean: 284.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -2.8434999999999833\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 403\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2207031249999997e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0785119407706791\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005008976682392764\n",
      "          policy_loss: 0.04093268190821012\n",
      "          total_loss: 0.04035544652077887\n",
      "          vf_explained_var: 0.5434441566467285\n",
      "          vf_loss: 0.010207818696896236\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8468085106383\n",
      "    ram_util_percent: 45.61489361702128\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0406197308842739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.617079761320152\n",
      "    mean_inference_ms: 1.7655058884118922\n",
      "    mean_raw_obs_processing_ms: 1.665450878661592\n",
      "  time_since_restore: 4151.913305044174\n",
      "  time_this_iter_s: 32.31804656982422\n",
      "  time_total_s: 4151.913305044174\n",
      "  timers:\n",
      "    learn_throughput: 1489.413\n",
      "    learn_time_ms: 671.405\n",
      "    load_throughput: 116363.959\n",
      "    load_time_ms: 8.594\n",
      "    sample_throughput: 28.198\n",
      "    sample_time_ms: 35463.827\n",
      "    update_time_ms: 4.454\n",
      "  timestamp: 1635022446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         4151.91</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\"> -2.8435</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            284.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 284.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -2.8436999999999824\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 406\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2207031249999997e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8548009335994721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0068077180613358755\n",
      "          policy_loss: -0.10355153133471807\n",
      "          total_loss: -0.10043562476833662\n",
      "          vf_explained_var: 0.47991448640823364\n",
      "          vf_loss: 0.011663831853204304\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.50599999999999\n",
      "    ram_util_percent: 45.672\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04062411251231546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.665426250382016\n",
      "    mean_inference_ms: 1.765555820958594\n",
      "    mean_raw_obs_processing_ms: 1.6679937329958463\n",
      "  time_since_restore: 4187.024334907532\n",
      "  time_this_iter_s: 35.111029863357544\n",
      "  time_total_s: 4187.024334907532\n",
      "  timers:\n",
      "    learn_throughput: 1473.975\n",
      "    learn_time_ms: 678.438\n",
      "    load_throughput: 116032.71\n",
      "    load_time_ms: 8.618\n",
      "    sample_throughput: 28.01\n",
      "    sample_time_ms: 35701.054\n",
      "    update_time_ms: 4.227\n",
      "  timestamp: 1635022481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         4187.02</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\"> -2.8437</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            284.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 284.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -2.843199999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 410\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2207031249999997e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8326170265674591\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00294246937628052\n",
      "          policy_loss: 0.03263414576649666\n",
      "          total_loss: 0.0361057761642668\n",
      "          vf_explained_var: 0.431357204914093\n",
      "          vf_loss: 0.011797771789133549\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.19591836734695\n",
      "    ram_util_percent: 45.79591836734695\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063005731656925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.729401384138544\n",
      "    mean_inference_ms: 1.7656235915854053\n",
      "    mean_raw_obs_processing_ms: 1.671368757189785\n",
      "  time_since_restore: 4221.214910745621\n",
      "  time_this_iter_s: 34.19057583808899\n",
      "  time_total_s: 4221.214910745621\n",
      "  timers:\n",
      "    learn_throughput: 1490.517\n",
      "    learn_time_ms: 670.908\n",
      "    load_throughput: 110144.249\n",
      "    load_time_ms: 9.079\n",
      "    sample_throughput: 28.159\n",
      "    sample_time_ms: 35512.998\n",
      "    update_time_ms: 4.162\n",
      "  timestamp: 1635022515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         4221.21</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -2.8432</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            284.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 284.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -2.8414999999999835\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 414\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8466679824723138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005617746481387363\n",
      "          policy_loss: 0.020825385881794823\n",
      "          total_loss: 0.022434148854679533\n",
      "          vf_explained_var: 0.5027387142181396\n",
      "          vf_loss: 0.01007541493098769\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.5142857142857\n",
      "    ram_util_percent: 45.818367346938764\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04063656081548064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.79242985594882\n",
      "    mean_inference_ms: 1.765684494810151\n",
      "    mean_raw_obs_processing_ms: 1.6748097754801028\n",
      "  time_since_restore: 4255.63723731041\n",
      "  time_this_iter_s: 34.42232656478882\n",
      "  time_total_s: 4255.63723731041\n",
      "  timers:\n",
      "    learn_throughput: 1490.631\n",
      "    learn_time_ms: 670.857\n",
      "    load_throughput: 112278.057\n",
      "    load_time_ms: 8.906\n",
      "    sample_throughput: 28.194\n",
      "    sample_time_ms: 35468.365\n",
      "    update_time_ms: 3.781\n",
      "  timestamp: 1635022550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         4255.64</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -2.8415</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            284.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 283.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -2.837699999999984\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 417\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8194616662131415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005100386791795586\n",
      "          policy_loss: 0.025871543751822577\n",
      "          total_loss: 0.026928832630316417\n",
      "          vf_explained_var: 0.4632547199726105\n",
      "          vf_loss: 0.009251881126935283\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.48775510204081\n",
      "    ram_util_percent: 45.71836734693877\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064151475657611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.838922619312857\n",
      "    mean_inference_ms: 1.7657233148125682\n",
      "    mean_raw_obs_processing_ms: 1.6774906003634065\n",
      "  time_since_restore: 4290.257950544357\n",
      "  time_this_iter_s: 34.620713233947754\n",
      "  time_total_s: 4290.257950544357\n",
      "  timers:\n",
      "    learn_throughput: 1490.576\n",
      "    learn_time_ms: 670.882\n",
      "    load_throughput: 109145.376\n",
      "    load_time_ms: 9.162\n",
      "    sample_throughput: 28.216\n",
      "    sample_time_ms: 35441.34\n",
      "    update_time_ms: 3.798\n",
      "  timestamp: 1635022584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         4290.26</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\"> -2.8377</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            283.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-57-19\n",
      "  done: false\n",
      "  episode_len_mean: 282.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.826199999999983\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 421\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8803310016791026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00639419656737086\n",
      "          policy_loss: 0.01668236955172486\n",
      "          total_loss: 0.02051345482468605\n",
      "          vf_explained_var: 0.40551719069480896\n",
      "          vf_loss: 0.012634360883384942\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.75641025641026\n",
      "    ram_util_percent: 45.58846153846154\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04064825559396928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.90208083868981\n",
      "    mean_inference_ms: 1.7657846984404983\n",
      "    mean_raw_obs_processing_ms: 1.6856401059138215\n",
      "  time_since_restore: 4344.9074010849\n",
      "  time_this_iter_s: 54.6494505405426\n",
      "  time_total_s: 4344.9074010849\n",
      "  timers:\n",
      "    learn_throughput: 1498.384\n",
      "    learn_time_ms: 667.386\n",
      "    load_throughput: 109370.215\n",
      "    load_time_ms: 9.143\n",
      "    sample_throughput: 26.74\n",
      "    sample_time_ms: 37396.856\n",
      "    update_time_ms: 3.394\n",
      "  timestamp: 1635022639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         4344.91</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> -2.8262</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            282.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 282.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.8207999999999833\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 425\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.121380211247338\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007292591994450288\n",
      "          policy_loss: -0.01521387245092127\n",
      "          total_loss: -0.01414076027770837\n",
      "          vf_explained_var: 0.4785309135913849\n",
      "          vf_loss: 0.01228687535557482\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.94468085106384\n",
      "    ram_util_percent: 45.59999999999999\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040654654671707945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.964036057093825\n",
      "    mean_inference_ms: 1.765838717195788\n",
      "    mean_raw_obs_processing_ms: 1.693950208367234\n",
      "  time_since_restore: 4377.638142824173\n",
      "  time_this_iter_s: 32.73074173927307\n",
      "  time_total_s: 4377.638142824173\n",
      "  timers:\n",
      "    learn_throughput: 1508.242\n",
      "    learn_time_ms: 663.024\n",
      "    load_throughput: 99704.379\n",
      "    load_time_ms: 10.03\n",
      "    sample_throughput: 26.783\n",
      "    sample_time_ms: 37337.243\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1635022672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         4377.64</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\"> -2.8208</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            282.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-58-24\n",
      "  done: false\n",
      "  episode_len_mean: 282.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.8253999999999837\n",
      "  episode_reward_min: -3.329999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 428\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4802750057644314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011056762570985276\n",
      "          policy_loss: 0.02076067907942666\n",
      "          total_loss: 0.013996441000037723\n",
      "          vf_explained_var: 0.6810218095779419\n",
      "          vf_loss: 0.008038448015900536\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.98222222222222\n",
      "    ram_util_percent: 45.5311111111111\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04065925794946217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.009424608383775\n",
      "    mean_inference_ms: 1.7658815273542163\n",
      "    mean_raw_obs_processing_ms: 1.700263437131159\n",
      "  time_since_restore: 4409.467223405838\n",
      "  time_this_iter_s: 31.82908058166504\n",
      "  time_total_s: 4409.467223405838\n",
      "  timers:\n",
      "    learn_throughput: 1478.165\n",
      "    learn_time_ms: 676.514\n",
      "    load_throughput: 99576.794\n",
      "    load_time_ms: 10.043\n",
      "    sample_throughput: 28.429\n",
      "    sample_time_ms: 35174.796\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1635022704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         4409.47</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -2.8254</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">            282.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-58-55\n",
      "  done: false\n",
      "  episode_len_mean: 283.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.8306999999999833\n",
      "  episode_reward_min: -3.2499999999999747\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 431\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.103515624999999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.492064082622528\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02727181914111879\n",
      "          policy_loss: 0.038672267480029\n",
      "          total_loss: 0.03253387047184838\n",
      "          vf_explained_var: 0.5777721405029297\n",
      "          vf_loss: 0.008782080840319395\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.42826086956522\n",
      "    ram_util_percent: 45.50652173913044\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066392811339758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.05443699625419\n",
      "    mean_inference_ms: 1.7659181707305824\n",
      "    mean_raw_obs_processing_ms: 1.7036806417511121\n",
      "  time_since_restore: 4441.1239359378815\n",
      "  time_this_iter_s: 31.656712532043457\n",
      "  time_total_s: 4441.1239359378815\n",
      "  timers:\n",
      "    learn_throughput: 1483.926\n",
      "    learn_time_ms: 673.888\n",
      "    load_throughput: 101999.324\n",
      "    load_time_ms: 9.804\n",
      "    sample_throughput: 28.541\n",
      "    sample_time_ms: 35036.957\n",
      "    update_time_ms: 3.38\n",
      "  timestamp: 1635022735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         4441.12</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\"> -2.8307</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.25</td><td style=\"text-align: right;\">            283.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_20-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 282.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.828599999999983\n",
      "  episode_reward_min: -3.2499999999999747\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 435\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1545261078410678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01713055703611938\n",
      "          policy_loss: -0.0074344423082139755\n",
      "          total_loss: -0.0067059030135472614\n",
      "          vf_explained_var: 0.4937335252761841\n",
      "          vf_loss: 0.012273650543971195\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.55306122448981\n",
      "    ram_util_percent: 45.506122448979596\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04066994821235682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.113561536094185\n",
      "    mean_inference_ms: 1.7659700600533461\n",
      "    mean_raw_obs_processing_ms: 1.7064558503510439\n",
      "  time_since_restore: 4475.752586364746\n",
      "  time_this_iter_s: 34.628650426864624\n",
      "  time_total_s: 4475.752586364746\n",
      "  timers:\n",
      "    learn_throughput: 1489.027\n",
      "    learn_time_ms: 671.579\n",
      "    load_throughput: 102118.526\n",
      "    load_time_ms: 9.793\n",
      "    sample_throughput: 28.632\n",
      "    sample_time_ms: 34926.203\n",
      "    update_time_ms: 3.8\n",
      "  timestamp: 1635022770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         4475.75</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\"> -2.8286</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.25</td><td style=\"text-align: right;\">            282.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 283.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.832399999999983\n",
      "  episode_reward_min: -3.2499999999999747\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 438\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5989307694964938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015377903562008853\n",
      "          policy_loss: 0.027444574568006728\n",
      "          total_loss: 0.019204015036424\n",
      "          vf_explained_var: 0.6849798560142517\n",
      "          vf_loss: 0.007748607689023225\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.42666666666666\n",
      "    ram_util_percent: 45.60222222222222\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04067428452873645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.15700165829466\n",
      "    mean_inference_ms: 1.7660060509570639\n",
      "    mean_raw_obs_processing_ms: 1.7085556198943213\n",
      "  time_since_restore: 4507.112004518509\n",
      "  time_this_iter_s: 31.359418153762817\n",
      "  time_total_s: 4507.112004518509\n",
      "  timers:\n",
      "    learn_throughput: 1484.999\n",
      "    learn_time_ms: 673.401\n",
      "    load_throughput: 105119.084\n",
      "    load_time_ms: 9.513\n",
      "    sample_throughput: 28.712\n",
      "    sample_time_ms: 34828.94\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1635022801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         4507.11</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\"> -2.8324</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.25</td><td style=\"text-align: right;\">            283.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 283.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.837999999999983\n",
      "  episode_reward_min: -3.2499999999999747\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 441\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4075118674172296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009255638915295784\n",
      "          policy_loss: -0.053690113416976395\n",
      "          total_loss: -0.05757428854703903\n",
      "          vf_explained_var: 0.6933321952819824\n",
      "          vf_loss: 0.010190859923346176\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.82340425531915\n",
      "    ram_util_percent: 45.78085106382979\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04067827506074071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.199531276155724\n",
      "    mean_inference_ms: 1.766046810614506\n",
      "    mean_raw_obs_processing_ms: 1.7106944966866862\n",
      "  time_since_restore: 4540.306910991669\n",
      "  time_this_iter_s: 33.19490647315979\n",
      "  time_total_s: 4540.306910991669\n",
      "  timers:\n",
      "    learn_throughput: 1499.836\n",
      "    learn_time_ms: 666.74\n",
      "    load_throughput: 104118.877\n",
      "    load_time_ms: 9.604\n",
      "    sample_throughput: 28.865\n",
      "    sample_time_ms: 34644.057\n",
      "    update_time_ms: 3.547\n",
      "  timestamp: 1635022835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         4540.31</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  -2.838</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.25</td><td style=\"text-align: right;\">             283.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 284.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.849699999999982\n",
      "  episode_reward_min: -3.279999999999974\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 444\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.1552734375e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7903712140189276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02462240391108226\n",
      "          policy_loss: -0.09965616568095154\n",
      "          total_loss: -0.110416480857465\n",
      "          vf_explained_var: 0.8127371072769165\n",
      "          vf_loss: 0.007143173465091321\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.10465116279069\n",
      "    ram_util_percent: 45.788372093023256\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040681842872862674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.241289080389652\n",
      "    mean_inference_ms: 1.7660827955480638\n",
      "    mean_raw_obs_processing_ms: 1.712767811701115\n",
      "  time_since_restore: 4570.600663423538\n",
      "  time_this_iter_s: 30.293752431869507\n",
      "  time_total_s: 4570.600663423538\n",
      "  timers:\n",
      "    learn_throughput: 1498.852\n",
      "    learn_time_ms: 667.177\n",
      "    load_throughput: 109148.216\n",
      "    load_time_ms: 9.162\n",
      "    sample_throughput: 29.193\n",
      "    sample_time_ms: 34254.672\n",
      "    update_time_ms: 3.518\n",
      "  timestamp: 1635022865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">          4570.6</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\"> -2.8497</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.28</td><td style=\"text-align: right;\">            284.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 286.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.389999999999993\n",
      "  episode_reward_mean: -2.865699999999983\n",
      "  episode_reward_min: -3.509999999999969\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 447\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6926487671004402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016602096840959903\n",
      "          policy_loss: -0.019549724459648133\n",
      "          total_loss: -0.030619422760274677\n",
      "          vf_explained_var: 0.8775721192359924\n",
      "          vf_loss: 0.005856565639583601\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.45714285714287\n",
      "    ram_util_percent: 45.74047619047619\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068527833356205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.281509962037955\n",
      "    mean_inference_ms: 1.7661144242807192\n",
      "    mean_raw_obs_processing_ms: 1.7148853832526776\n",
      "  time_since_restore: 4599.428379774094\n",
      "  time_this_iter_s: 28.82771635055542\n",
      "  time_total_s: 4599.428379774094\n",
      "  timers:\n",
      "    learn_throughput: 1500.947\n",
      "    learn_time_ms: 666.246\n",
      "    load_throughput: 110571.376\n",
      "    load_time_ms: 9.044\n",
      "    sample_throughput: 29.677\n",
      "    sample_time_ms: 33696.26\n",
      "    update_time_ms: 3.511\n",
      "  timestamp: 1635022894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         4599.43</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\"> -2.8657</td><td style=\"text-align: right;\">               -2.39</td><td style=\"text-align: right;\">               -3.51</td><td style=\"text-align: right;\">            286.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-02-24\n",
      "  done: false\n",
      "  episode_len_mean: 287.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.05000000000000231\n",
      "  episode_reward_mean: -2.8492999999999826\n",
      "  episode_reward_min: -3.509999999999969\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 451\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6324779431025187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01975761071489525\n",
      "          policy_loss: -0.08191949625809987\n",
      "          total_loss: -0.0280760467880302\n",
      "          vf_explained_var: 0.55033940076828\n",
      "          vf_loss: 0.07016795857602523\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.21690140845068\n",
      "    ram_util_percent: 45.656338028169024\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04068969824490146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.33442975230425\n",
      "    mean_inference_ms: 1.7661540226224894\n",
      "    mean_raw_obs_processing_ms: 1.721967755249119\n",
      "  time_since_restore: 4649.625935554504\n",
      "  time_this_iter_s: 50.19755578041077\n",
      "  time_total_s: 4649.625935554504\n",
      "  timers:\n",
      "    learn_throughput: 1499.453\n",
      "    learn_time_ms: 666.91\n",
      "    load_throughput: 108019.326\n",
      "    load_time_ms: 9.258\n",
      "    sample_throughput: 28.366\n",
      "    sample_time_ms: 35252.884\n",
      "    update_time_ms: 3.712\n",
      "  timestamp: 1635022944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         4649.63</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\"> -2.8493</td><td style=\"text-align: right;\">               -0.05</td><td style=\"text-align: right;\">               -3.51</td><td style=\"text-align: right;\">            287.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 288.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.05000000000000231\n",
      "  episode_reward_mean: -2.8520999999999828\n",
      "  episode_reward_min: -3.509999999999969\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 454\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6740461150805155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011952111454866474\n",
      "          policy_loss: -0.11398539323773649\n",
      "          total_loss: -0.11876474426438412\n",
      "          vf_explained_var: 0.7207739949226379\n",
      "          vf_loss: 0.011960945591434008\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.38913043478261\n",
      "    ram_util_percent: 45.61086956521738\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040693583411632685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.373228660471888\n",
      "    mean_inference_ms: 1.766184074698594\n",
      "    mean_raw_obs_processing_ms: 1.727326832629882\n",
      "  time_since_restore: 4681.580249071121\n",
      "  time_this_iter_s: 31.95431351661682\n",
      "  time_total_s: 4681.580249071121\n",
      "  timers:\n",
      "    learn_throughput: 1469.266\n",
      "    learn_time_ms: 680.612\n",
      "    load_throughput: 107439.918\n",
      "    load_time_ms: 9.308\n",
      "    sample_throughput: 30.331\n",
      "    sample_time_ms: 32970.099\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1635022976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         4681.58</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -2.8521</td><td style=\"text-align: right;\">               -0.05</td><td style=\"text-align: right;\">               -3.51</td><td style=\"text-align: right;\">            288.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 288.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.010000000000002307\n",
      "  episode_reward_mean: -2.836199999999982\n",
      "  episode_reward_min: -3.9199999999999595\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 457\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.228187574280633\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011336155759983058\n",
      "          policy_loss: -0.08480489146378305\n",
      "          total_loss: 0.00015027709305286406\n",
      "          vf_explained_var: 0.4136126935482025\n",
      "          vf_loss: 0.09723689134957061\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.15\n",
      "    ram_util_percent: 45.77692307692309\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040697822890133975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.412901154018126\n",
      "    mean_inference_ms: 1.7662221144440569\n",
      "    mean_raw_obs_processing_ms: 1.732604127565256\n",
      "  time_since_restore: 4718.273320913315\n",
      "  time_this_iter_s: 36.6930718421936\n",
      "  time_total_s: 4718.273320913315\n",
      "  timers:\n",
      "    learn_throughput: 1442.884\n",
      "    learn_time_ms: 693.057\n",
      "    load_throughput: 110074.007\n",
      "    load_time_ms: 9.085\n",
      "    sample_throughput: 29.982\n",
      "    sample_time_ms: 33353.891\n",
      "    update_time_ms: 3.487\n",
      "  timestamp: 1635023013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         4718.27</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\"> -2.8362</td><td style=\"text-align: right;\">               -0.01</td><td style=\"text-align: right;\">               -3.92</td><td style=\"text-align: right;\">            288.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-04-06\n",
      "  done: false\n",
      "  episode_len_mean: 289.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.130000000000025\n",
      "  episode_reward_mean: -2.810199999999982\n",
      "  episode_reward_min: -3.9199999999999595\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 461\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4272215525309244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010785974032455946\n",
      "          policy_loss: -0.069745624727673\n",
      "          total_loss: 0.07002708295153247\n",
      "          vf_explained_var: 0.2176138162612915\n",
      "          vf_loss: 0.15404477778615222\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58125\n",
      "    ram_util_percent: 45.76458333333333\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070311272835053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.464260745254816\n",
      "    mean_inference_ms: 1.7662629379804267\n",
      "    mean_raw_obs_processing_ms: 1.7347026469787619\n",
      "  time_since_restore: 4751.943695068359\n",
      "  time_this_iter_s: 33.670374155044556\n",
      "  time_total_s: 4751.943695068359\n",
      "  timers:\n",
      "    learn_throughput: 1468.289\n",
      "    learn_time_ms: 681.065\n",
      "    load_throughput: 102347.277\n",
      "    load_time_ms: 9.771\n",
      "    sample_throughput: 29.808\n",
      "    sample_time_ms: 33548.319\n",
      "    update_time_ms: 4.33\n",
      "  timestamp: 1635023046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         4751.94</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\"> -2.8102</td><td style=\"text-align: right;\">                1.13</td><td style=\"text-align: right;\">               -3.92</td><td style=\"text-align: right;\">            289.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 289.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.9900000000000404\n",
      "  episode_reward_mean: -2.7927999999999815\n",
      "  episode_reward_min: -6.839999999999961\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 464\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6031615283754137\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014439007899354945\n",
      "          policy_loss: 0.1372931925786866\n",
      "          total_loss: 0.42180516918500266\n",
      "          vf_explained_var: 0.24463015794754028\n",
      "          vf_loss: 0.3005433910836776\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84583333333335\n",
      "    ram_util_percent: 45.708333333333336\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070633228581183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.502632499128158\n",
      "    mean_inference_ms: 1.7662917646816263\n",
      "    mean_raw_obs_processing_ms: 1.736242433705958\n",
      "  time_since_restore: 4785.631815433502\n",
      "  time_this_iter_s: 33.68812036514282\n",
      "  time_total_s: 4785.631815433502\n",
      "  timers:\n",
      "    learn_throughput: 1473.152\n",
      "    learn_time_ms: 678.817\n",
      "    load_throughput: 103929.509\n",
      "    load_time_ms: 9.622\n",
      "    sample_throughput: 29.626\n",
      "    sample_time_ms: 33753.976\n",
      "    update_time_ms: 4.088\n",
      "  timestamp: 1635023080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         4785.63</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\"> -2.7928</td><td style=\"text-align: right;\">                1.99</td><td style=\"text-align: right;\">               -6.84</td><td style=\"text-align: right;\">            289.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 289.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8006999999999804\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 467\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3732910156250005e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4081464277373419\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02435766332787832\n",
      "          policy_loss: 0.004251981816358037\n",
      "          total_loss: 0.6614922708935208\n",
      "          vf_explained_var: 0.3941268026828766\n",
      "          vf_loss: 0.6713214175568687\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.46938775510205\n",
      "    ram_util_percent: 45.77346938775511\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04070950788812808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.540294331810273\n",
      "    mean_inference_ms: 1.766321508363169\n",
      "    mean_raw_obs_processing_ms: 1.7378381648745267\n",
      "  time_since_restore: 4819.8346037864685\n",
      "  time_this_iter_s: 34.20278835296631\n",
      "  time_total_s: 4819.8346037864685\n",
      "  timers:\n",
      "    learn_throughput: 1479.917\n",
      "    learn_time_ms: 675.714\n",
      "    load_throughput: 104046.299\n",
      "    load_time_ms: 9.611\n",
      "    sample_throughput: 29.661\n",
      "    sample_time_ms: 33714.348\n",
      "    update_time_ms: 4.323\n",
      "  timestamp: 1635023114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         4819.83</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -2.8007</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            289.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 290.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.778599999999981\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 471\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0599365234374995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.655023729801178\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023019411809865806\n",
      "          policy_loss: -0.014559244447284275\n",
      "          total_loss: 0.9201898902654648\n",
      "          vf_explained_var: 0.6911690831184387\n",
      "          vf_loss: 0.9512989044189453\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6204081632653\n",
      "    ram_util_percent: 45.977551020408164\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071422161293901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.590176148304153\n",
      "    mean_inference_ms: 1.7663706948715585\n",
      "    mean_raw_obs_processing_ms: 1.740029137062403\n",
      "  time_since_restore: 4853.7572231292725\n",
      "  time_this_iter_s: 33.922619342803955\n",
      "  time_total_s: 4853.7572231292725\n",
      "  timers:\n",
      "    learn_throughput: 1479.584\n",
      "    learn_time_ms: 675.866\n",
      "    load_throughput: 103843.31\n",
      "    load_time_ms: 9.63\n",
      "    sample_throughput: 29.437\n",
      "    sample_time_ms: 33970.346\n",
      "    update_time_ms: 4.323\n",
      "  timestamp: 1635023148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         4853.76</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\"> -2.7786</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            290.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 293.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.81279999999998\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 473\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.08990478515625e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.18338414033254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019499213920965947\n",
      "          policy_loss: -0.04436359682844745\n",
      "          total_loss: 0.3624800162182914\n",
      "          vf_explained_var: 0.7300177812576294\n",
      "          vf_loss: 0.4286768512593375\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.79230769230769\n",
      "    ram_util_percent: 46.05641025641025\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04071654633482059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.61366287064604\n",
      "    mean_inference_ms: 1.7663996862016944\n",
      "    mean_raw_obs_processing_ms: 1.7411462118802052\n",
      "  time_since_restore: 4880.992712974548\n",
      "  time_this_iter_s: 27.23548984527588\n",
      "  time_total_s: 4880.992712974548\n",
      "  timers:\n",
      "    learn_throughput: 1483.189\n",
      "    learn_time_ms: 674.223\n",
      "    load_throughput: 104700.811\n",
      "    load_time_ms: 9.551\n",
      "    sample_throughput: 29.962\n",
      "    sample_time_ms: 33375.878\n",
      "    update_time_ms: 4.457\n",
      "  timestamp: 1635023176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         4880.99</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\"> -2.8128</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            293.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 296.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8495999999999797\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 476\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.08990478515625e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.170417000187768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021282693707756154\n",
      "          policy_loss: -0.010070914568172561\n",
      "          total_loss: 0.5019467497865359\n",
      "          vf_explained_var: 0.727685809135437\n",
      "          vf_loss: 0.5337211754586961\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.3125\n",
      "    ram_util_percent: 45.995000000000005\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040719891364055495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.64835004916254\n",
      "    mean_inference_ms: 1.7664396887528013\n",
      "    mean_raw_obs_processing_ms: 1.7426587562490652\n",
      "  time_since_restore: 4909.152667045593\n",
      "  time_this_iter_s: 28.159954071044922\n",
      "  time_total_s: 4909.152667045593\n",
      "  timers:\n",
      "    learn_throughput: 1472.727\n",
      "    learn_time_ms: 679.012\n",
      "    load_throughput: 105186.043\n",
      "    load_time_ms: 9.507\n",
      "    sample_throughput: 30.159\n",
      "    sample_time_ms: 33157.276\n",
      "    update_time_ms: 4.551\n",
      "  timestamp: 1635023204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         4909.15</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\"> -2.8496</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            296.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-07-13\n",
      "  done: false\n",
      "  episode_len_mean: 298.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8653999999999793\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 478\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.634857177734377e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.142497566011217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019964642372510423\n",
      "          policy_loss: -0.07920310679409238\n",
      "          total_loss: 0.512857964883248\n",
      "          vf_explained_var: 0.5569603443145752\n",
      "          vf_loss: 0.6134851232171059\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.05365853658537\n",
      "    ram_util_percent: 45.94634146341463\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072216123618392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.670797174344717\n",
      "    mean_inference_ms: 1.766467796306412\n",
      "    mean_raw_obs_processing_ms: 1.7436234821842862\n",
      "  time_since_restore: 4938.038321495056\n",
      "  time_this_iter_s: 28.88565444946289\n",
      "  time_total_s: 4938.038321495056\n",
      "  timers:\n",
      "    learn_throughput: 1451.753\n",
      "    learn_time_ms: 688.822\n",
      "    load_throughput: 96927.926\n",
      "    load_time_ms: 10.317\n",
      "    sample_throughput: 30.164\n",
      "    sample_time_ms: 33152.151\n",
      "    update_time_ms: 4.873\n",
      "  timestamp: 1635023233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         4938.04</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -2.8654</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            298.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 301.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8444999999999787\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 481\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.634857177734377e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.258009452290005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015637014326936496\n",
      "          policy_loss: -0.15188827796114815\n",
      "          total_loss: 0.25030273869633674\n",
      "          vf_explained_var: 0.5840619802474976\n",
      "          vf_loss: 0.4247703790664673\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.45074626865674\n",
      "    ram_util_percent: 45.797014925373134\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407257173599501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.70311970947059\n",
      "    mean_inference_ms: 1.7665135567482644\n",
      "    mean_raw_obs_processing_ms: 1.748118993326899\n",
      "  time_since_restore: 4985.124856710434\n",
      "  time_this_iter_s: 47.08653521537781\n",
      "  time_total_s: 4985.124856710434\n",
      "  timers:\n",
      "    learn_throughput: 1451.777\n",
      "    learn_time_ms: 688.811\n",
      "    load_throughput: 101615.802\n",
      "    load_time_ms: 9.841\n",
      "    sample_throughput: 30.449\n",
      "    sample_time_ms: 32841.734\n",
      "    update_time_ms: 4.755\n",
      "  timestamp: 1635023280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         4985.12</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\"> -2.8445</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            301.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 304.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.9675999999999774\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 484\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.634857177734377e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.163489744398329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025200680899626595\n",
      "          policy_loss: -0.03150860551330778\n",
      "          total_loss: 0.6687155942949984\n",
      "          vf_explained_var: 0.44684427976608276\n",
      "          vf_loss: 0.721857926580641\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.455\n",
      "    ram_util_percent: 45.62\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04072925251803032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.734302487212165\n",
      "    mean_inference_ms: 1.7665599801180087\n",
      "    mean_raw_obs_processing_ms: 1.7525388273814198\n",
      "  time_since_restore: 5013.275566101074\n",
      "  time_this_iter_s: 28.15070939064026\n",
      "  time_total_s: 5013.275566101074\n",
      "  timers:\n",
      "    learn_throughput: 1479.32\n",
      "    learn_time_ms: 675.986\n",
      "    load_throughput: 97222.701\n",
      "    load_time_ms: 10.286\n",
      "    sample_throughput: 30.794\n",
      "    sample_time_ms: 32473.388\n",
      "    update_time_ms: 4.976\n",
      "  timestamp: 1635023308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         5013.28</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\"> -2.9676</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">             304.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 307.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.9914999999999763\n",
      "  episode_reward_min: -12.94999999999995\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 486\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.95228576660156e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.227731606695387\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012040159775065017\n",
      "          policy_loss: 0.09870593970020612\n",
      "          total_loss: 0.4014350694086817\n",
      "          vf_explained_var: 0.6259739995002747\n",
      "          vf_loss: 0.3250056081347995\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.12500000000001\n",
      "    ram_util_percent: 45.63055555555555\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040731666930670575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.753887294040968\n",
      "    mean_inference_ms: 1.766591012710454\n",
      "    mean_raw_obs_processing_ms: 1.7554632979415918\n",
      "  time_since_restore: 5038.127066850662\n",
      "  time_this_iter_s: 24.851500749588013\n",
      "  time_total_s: 5038.127066850662\n",
      "  timers:\n",
      "    learn_throughput: 1495.455\n",
      "    learn_time_ms: 668.693\n",
      "    load_throughput: 102766.306\n",
      "    load_time_ms: 9.731\n",
      "    sample_throughput: 31.952\n",
      "    sample_time_ms: 31297.222\n",
      "    update_time_ms: 4.838\n",
      "  timestamp: 1635023333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         5038.13</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\"> -2.9915</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -12.95</td><td style=\"text-align: right;\">            307.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 310.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -3.142499999999976\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 489\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.95228576660156e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2069132698906793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02016214612311183\n",
      "          policy_loss: -0.14660294577479363\n",
      "          total_loss: 0.322786722994513\n",
      "          vf_explained_var: 0.5421581268310547\n",
      "          vf_loss: 0.4914574020438724\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.30263157894736\n",
      "    ram_util_percent: 45.618421052631575\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073577144874175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.782386640299606\n",
      "    mean_inference_ms: 1.7666380919939917\n",
      "    mean_raw_obs_processing_ms: 1.7597265865697787\n",
      "  time_since_restore: 5064.91952753067\n",
      "  time_this_iter_s: 26.792460680007935\n",
      "  time_total_s: 5064.91952753067\n",
      "  timers:\n",
      "    learn_throughput: 1507.03\n",
      "    learn_time_ms: 663.557\n",
      "    load_throughput: 112596.951\n",
      "    load_time_ms: 8.881\n",
      "    sample_throughput: 32.662\n",
      "    sample_time_ms: 30616.336\n",
      "    update_time_ms: 3.964\n",
      "  timestamp: 1635023360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         5064.92</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> -3.1425</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            310.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-09-46\n",
      "  done: false\n",
      "  episode_len_mean: 313.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -3.1248999999999763\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 491\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.340391770998637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013836798367225513\n",
      "          policy_loss: -0.008053399187823137\n",
      "          total_loss: 0.45841788889633284\n",
      "          vf_explained_var: 0.3683356046676636\n",
      "          vf_loss: 0.4898737594485283\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.58684210526314\n",
      "    ram_util_percent: 45.7\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04073858144782715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.799938689716072\n",
      "    mean_inference_ms: 1.7666728352905208\n",
      "    mean_raw_obs_processing_ms: 1.7601739960215286\n",
      "  time_since_restore: 5091.567432880402\n",
      "  time_this_iter_s: 26.647905349731445\n",
      "  time_total_s: 5091.567432880402\n",
      "  timers:\n",
      "    learn_throughput: 1499.125\n",
      "    learn_time_ms: 667.056\n",
      "    load_throughput: 113137.573\n",
      "    load_time_ms: 8.839\n",
      "    sample_throughput: 33.435\n",
      "    sample_time_ms: 29909.199\n",
      "    update_time_ms: 3.764\n",
      "  timestamp: 1635023386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         5091.57</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\"> -3.1249</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            313.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 316.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -3.039399999999975\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 493\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2673724518881904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012208108649702294\n",
      "          policy_loss: -0.1481888903511895\n",
      "          total_loss: -0.03018768255909284\n",
      "          vf_explained_var: 0.9031509160995483\n",
      "          vf_loss: 0.1406736571341753\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.77352941176471\n",
      "    ram_util_percent: 45.73529411764705\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407411704468018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.816619722076457\n",
      "    mean_inference_ms: 1.7667038616176072\n",
      "    mean_raw_obs_processing_ms: 1.7605418842568978\n",
      "  time_since_restore: 5115.539509057999\n",
      "  time_this_iter_s: 23.972076177597046\n",
      "  time_total_s: 5115.539509057999\n",
      "  timers:\n",
      "    learn_throughput: 1472.224\n",
      "    learn_time_ms: 679.245\n",
      "    load_throughput: 103392.273\n",
      "    load_time_ms: 9.672\n",
      "    sample_throughput: 34.633\n",
      "    sample_time_ms: 28873.934\n",
      "    update_time_ms: 3.009\n",
      "  timestamp: 1635023410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         5115.54</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\"> -3.0394</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            316.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-10-38\n",
      "  done: false\n",
      "  episode_len_mean: 319.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.979299999999973\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 496\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2454637580447727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01963621514898556\n",
      "          policy_loss: -0.08880255714886719\n",
      "          total_loss: 0.12360305823385716\n",
      "          vf_explained_var: 0.5949297547340393\n",
      "          vf_loss: 0.2348582059972816\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.51282051282051\n",
      "    ram_util_percent: 45.82051282051282\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04074482840454443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.840924575202465\n",
      "    mean_inference_ms: 1.7667490987290124\n",
      "    mean_raw_obs_processing_ms: 1.7609881073397256\n",
      "  time_since_restore: 5142.844585418701\n",
      "  time_this_iter_s: 27.305076360702515\n",
      "  time_total_s: 5142.844585418701\n",
      "  timers:\n",
      "    learn_throughput: 1483.624\n",
      "    learn_time_ms: 674.025\n",
      "    load_throughput: 99716.705\n",
      "    load_time_ms: 10.028\n",
      "    sample_throughput: 35.439\n",
      "    sample_time_ms: 28217.238\n",
      "    update_time_ms: 3.026\n",
      "  timestamp: 1635023438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         5142.84</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\"> -2.9793</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            319.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 322.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.9929999999999732\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 499\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.273485853936937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013754736601652004\n",
      "          policy_loss: -0.02939257820447286\n",
      "          total_loss: 0.4148647008670701\n",
      "          vf_explained_var: 0.7099465727806091\n",
      "          vf_loss: 0.4669907015230921\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.10000000000001\n",
      "    ram_util_percent: 46.0\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040748341867888234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.863606809620773\n",
      "    mean_inference_ms: 1.766787886842928\n",
      "    mean_raw_obs_processing_ms: 1.7614880749705109\n",
      "  time_since_restore: 5171.744878530502\n",
      "  time_this_iter_s: 28.900293111801147\n",
      "  time_total_s: 5171.744878530502\n",
      "  timers:\n",
      "    learn_throughput: 1480.03\n",
      "    learn_time_ms: 675.662\n",
      "    load_throughput: 99910.53\n",
      "    load_time_ms: 10.009\n",
      "    sample_throughput: 35.234\n",
      "    sample_time_ms: 28381.507\n",
      "    update_time_ms: 3.235\n",
      "  timestamp: 1635023467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         5171.74</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  -2.993</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            322.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 324.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8901999999999726\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 501\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2893944395913017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008556843554443697\n",
      "          policy_loss: -0.10940224470363723\n",
      "          total_loss: 0.06422823203934563\n",
      "          vf_explained_var: 0.8405069708824158\n",
      "          vf_loss: 0.19652353020177948\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.39473684210527\n",
      "    ram_util_percent: 46.01052631578947\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040750685228264694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.877922724401106\n",
      "    mean_inference_ms: 1.766813822866315\n",
      "    mean_raw_obs_processing_ms: 1.7618465412912996\n",
      "  time_since_restore: 5198.830148458481\n",
      "  time_this_iter_s: 27.085269927978516\n",
      "  time_total_s: 5198.830148458481\n",
      "  timers:\n",
      "    learn_throughput: 1466.958\n",
      "    learn_time_ms: 681.683\n",
      "    load_throughput: 91419.205\n",
      "    load_time_ms: 10.939\n",
      "    sample_throughput: 35.377\n",
      "    sample_time_ms: 28267.065\n",
      "    update_time_ms: 3.304\n",
      "  timestamp: 1635023494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         5198.83</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\"> -2.8902</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            324.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 328.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.8444999999999716\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 504\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.232221841812134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012551946859083252\n",
      "          policy_loss: 0.04302969210677677\n",
      "          total_loss: 0.1950057026412752\n",
      "          vf_explained_var: 0.668362021446228\n",
      "          vf_loss: 0.17429692470985983\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91578947368421\n",
      "    ram_util_percent: 46.107894736842105\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04075429443699795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.89862998143496\n",
      "    mean_inference_ms: 1.7668538008186243\n",
      "    mean_raw_obs_processing_ms: 1.7622306708617643\n",
      "  time_since_restore: 5225.372321605682\n",
      "  time_this_iter_s: 26.542173147201538\n",
      "  time_total_s: 5225.372321605682\n",
      "  timers:\n",
      "    learn_throughput: 1487.127\n",
      "    learn_time_ms: 672.437\n",
      "    load_throughput: 98599.027\n",
      "    load_time_ms: 10.142\n",
      "    sample_throughput: 35.66\n",
      "    sample_time_ms: 28042.884\n",
      "    update_time_ms: 2.981\n",
      "  timestamp: 1635023520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         5225.37</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\"> -2.8445</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            328.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 329.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.7651999999999703\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 506\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.135526998837789\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008853672497034035\n",
      "          policy_loss: -0.17549099483423763\n",
      "          total_loss: 0.2306461897989114\n",
      "          vf_explained_var: 0.8901785016059875\n",
      "          vf_loss: 0.42749152415328556\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.13095238095238\n",
      "    ram_util_percent: 46.166666666666664\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0407567412750841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.911867058186637\n",
      "    mean_inference_ms: 1.7668826422300896\n",
      "    mean_raw_obs_processing_ms: 1.7624496662553555\n",
      "  time_since_restore: 5254.612020730972\n",
      "  time_this_iter_s: 29.239699125289917\n",
      "  time_total_s: 5254.612020730972\n",
      "  timers:\n",
      "    learn_throughput: 1486.859\n",
      "    learn_time_ms: 672.559\n",
      "    load_throughput: 98325.816\n",
      "    load_time_ms: 10.17\n",
      "    sample_throughput: 38.084\n",
      "    sample_time_ms: 26257.601\n",
      "    update_time_ms: 3.364\n",
      "  timestamp: 1635023550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         5254.61</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\"> -2.7652</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            329.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 332.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.7278999999999685\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 509\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.158736801147461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008841052576846285\n",
      "          policy_loss: -0.0181614614609215\n",
      "          total_loss: 0.03943174526923233\n",
      "          vf_explained_var: 0.6441906690597534\n",
      "          vf_loss: 0.07917965137296253\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.33846153846153\n",
      "    ram_util_percent: 46.112820512820505\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04076071092377519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.930212713373972\n",
      "    mean_inference_ms: 1.7669226504982358\n",
      "    mean_raw_obs_processing_ms: 1.7628284244747676\n",
      "  time_since_restore: 5281.98871421814\n",
      "  time_this_iter_s: 27.37669348716736\n",
      "  time_total_s: 5281.98871421814\n",
      "  timers:\n",
      "    learn_throughput: 1486.249\n",
      "    learn_time_ms: 672.835\n",
      "    load_throughput: 103334.705\n",
      "    load_time_ms: 9.677\n",
      "    sample_throughput: 38.196\n",
      "    sample_time_ms: 26180.438\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1635023577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         5281.99</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -2.7279</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            332.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 335.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.587599999999967\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 512\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2710063616434732\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008259669278860847\n",
      "          policy_loss: -0.006067297524876065\n",
      "          total_loss: 0.1534433269666301\n",
      "          vf_explained_var: 0.8393402695655823\n",
      "          vf_loss: 0.1822198267922633\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.86031746031749\n",
      "    ram_util_percent: 46.05079365079366\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040765559597460914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.947384287870708\n",
      "    mean_inference_ms: 1.7669653044881628\n",
      "    mean_raw_obs_processing_ms: 1.766039272609158\n",
      "  time_since_restore: 5325.975408554077\n",
      "  time_this_iter_s: 43.9866943359375\n",
      "  time_total_s: 5325.975408554077\n",
      "  timers:\n",
      "    learn_throughput: 1488.519\n",
      "    learn_time_ms: 671.809\n",
      "    load_throughput: 104558.041\n",
      "    load_time_ms: 9.564\n",
      "    sample_throughput: 35.594\n",
      "    sample_time_ms: 28095.009\n",
      "    update_time_ms: 3.385\n",
      "  timestamp: 1635023621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         5325.98</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\"> -2.5876</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            335.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 337.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.466599999999966\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 514\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.286147379875183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009197345350410677\n",
      "          policy_loss: 0.028468621232443387\n",
      "          total_loss: 0.05458464783926805\n",
      "          vf_explained_var: 0.667973518371582\n",
      "          vf_loss: 0.04897653909607066\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.18108108108109\n",
      "    ram_util_percent: 45.94864864864866\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040768591750404176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.9583210509747\n",
      "    mean_inference_ms: 1.7669939240557844\n",
      "    mean_raw_obs_processing_ms: 1.7680672942846336\n",
      "  time_since_restore: 5352.052876472473\n",
      "  time_this_iter_s: 26.077467918395996\n",
      "  time_total_s: 5352.052876472473\n",
      "  timers:\n",
      "    learn_throughput: 1482.841\n",
      "    learn_time_ms: 674.381\n",
      "    load_throughput: 104770.117\n",
      "    load_time_ms: 9.545\n",
      "    sample_throughput: 35.688\n",
      "    sample_time_ms: 28020.742\n",
      "    update_time_ms: 3.459\n",
      "  timestamp: 1635023647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         5352.05</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\"> -2.4666</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            337.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-14-34\n",
      "  done: false\n",
      "  episode_len_mean: 340.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.3672999999999647\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 517\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00010428428649902343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1887316624323527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021629483294293077\n",
      "          policy_loss: 0.03237164095044136\n",
      "          total_loss: 0.37523557961814935\n",
      "          vf_explained_var: 0.4271693825721741\n",
      "          vf_loss: 0.36474899573044645\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.31794871794872\n",
      "    ram_util_percent: 45.91025641025642\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04077313145840778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.973164345964413\n",
      "    mean_inference_ms: 1.7670422584758443\n",
      "    mean_raw_obs_processing_ms: 1.771147402984943\n",
      "  time_since_restore: 5379.438344478607\n",
      "  time_this_iter_s: 27.385468006134033\n",
      "  time_total_s: 5379.438344478607\n",
      "  timers:\n",
      "    learn_throughput: 1470.799\n",
      "    learn_time_ms: 679.902\n",
      "    load_throughput: 103284.322\n",
      "    load_time_ms: 9.682\n",
      "    sample_throughput: 35.601\n",
      "    sample_time_ms: 28088.914\n",
      "    update_time_ms: 3.428\n",
      "  timestamp: 1635023674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         5379.44</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\"> -2.3673</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            340.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 343.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.2887999999999638\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 519\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001564264297485352\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.145202128092448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008527028899606595\n",
      "          policy_loss: -0.1645810615685251\n",
      "          total_loss: -0.011178689532809788\n",
      "          vf_explained_var: 0.9083503484725952\n",
      "          vf_loss: 0.17485305770403808\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.1846153846154\n",
      "    ram_util_percent: 45.887179487179495\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04077618789141216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.98162424607162\n",
      "    mean_inference_ms: 1.767071832028964\n",
      "    mean_raw_obs_processing_ms: 1.7709277849415952\n",
      "  time_since_restore: 5406.418810367584\n",
      "  time_this_iter_s: 26.98046588897705\n",
      "  time_total_s: 5406.418810367584\n",
      "  timers:\n",
      "    learn_throughput: 1492.286\n",
      "    learn_time_ms: 670.113\n",
      "    load_throughput: 112719.807\n",
      "    load_time_ms: 8.872\n",
      "    sample_throughput: 35.212\n",
      "    sample_time_ms: 28399.797\n",
      "    update_time_ms: 3.881\n",
      "  timestamp: 1635023701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         5406.42</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -2.2888</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">             343.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 347.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.2441999999999624\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 522\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001564264297485352\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2368374188741047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01334640211383918\n",
      "          policy_loss: 0.09934771872229047\n",
      "          total_loss: 0.29432862218883304\n",
      "          vf_explained_var: 0.8607624769210815\n",
      "          vf_loss: 0.21734719168808725\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.19736842105262\n",
      "    ram_util_percent: 45.91052631578948\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040780696979666055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.993757680019385\n",
      "    mean_inference_ms: 1.767118843939559\n",
      "    mean_raw_obs_processing_ms: 1.7704482968285913\n",
      "  time_since_restore: 5433.027577161789\n",
      "  time_this_iter_s: 26.608766794204712\n",
      "  time_total_s: 5433.027577161789\n",
      "  timers:\n",
      "    learn_throughput: 1480.39\n",
      "    learn_time_ms: 675.498\n",
      "    load_throughput: 117753.69\n",
      "    load_time_ms: 8.492\n",
      "    sample_throughput: 35.304\n",
      "    sample_time_ms: 28325.15\n",
      "    update_time_ms: 3.93\n",
      "  timestamp: 1635023728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         5433.03</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\"> -2.2442</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            347.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 349.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.1586999999999614\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 524\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001564264297485352\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3126681751675076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022209773416866932\n",
      "          policy_loss: -0.039041568256086776\n",
      "          total_loss: 0.08232333999541071\n",
      "          vf_explained_var: 0.8914930820465088\n",
      "          vf_loss: 0.14448811478084989\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.53714285714287\n",
      "    ram_util_percent: 45.96571428571429\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040783684718052396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.001047988759197\n",
      "    mean_inference_ms: 1.7671521710970637\n",
      "    mean_raw_obs_processing_ms: 1.770091437344189\n",
      "  time_since_restore: 5457.734260320663\n",
      "  time_this_iter_s: 24.70668315887451\n",
      "  time_total_s: 5457.734260320663\n",
      "  timers:\n",
      "    learn_throughput: 1486.062\n",
      "    learn_time_ms: 672.919\n",
      "    load_throughput: 118496.553\n",
      "    load_time_ms: 8.439\n",
      "    sample_throughput: 35.83\n",
      "    sample_time_ms: 27909.278\n",
      "    update_time_ms: 3.588\n",
      "  timestamp: 1635023753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         5457.73</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\"> -2.1587</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            349.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 352.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.1311999999999602\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 527\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2256024016274347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015309936172273374\n",
      "          policy_loss: -0.08574139144685533\n",
      "          total_loss: 0.12911696640981568\n",
      "          vf_explained_var: 0.7263476252555847\n",
      "          vf_loss: 0.23711078754729695\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88108108108108\n",
      "    ram_util_percent: 45.9054054054054\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040788061739673094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.011130586605656\n",
      "    mean_inference_ms: 1.767197951327187\n",
      "    mean_raw_obs_processing_ms: 1.7695151615474851\n",
      "  time_since_restore: 5483.696532487869\n",
      "  time_this_iter_s: 25.96227216720581\n",
      "  time_total_s: 5483.696532487869\n",
      "  timers:\n",
      "    learn_throughput: 1503.274\n",
      "    learn_time_ms: 665.215\n",
      "    load_throughput: 131124.04\n",
      "    load_time_ms: 7.626\n",
      "    sample_throughput: 35.965\n",
      "    sample_time_ms: 27805.097\n",
      "    update_time_ms: 3.814\n",
      "  timestamp: 1635023779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">          5483.7</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\"> -2.1312</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            352.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 355.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -2.036099999999959\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 529\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2650026109483505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009545636737370383\n",
      "          policy_loss: -0.16205463463233577\n",
      "          total_loss: -0.08102078032162455\n",
      "          vf_explained_var: 0.9300705194473267\n",
      "          vf_loss: 0.10368164305885633\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.38823529411764\n",
      "    ram_util_percent: 45.982352941176465\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04079092664825448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.016996275761045\n",
      "    mean_inference_ms: 1.7672283400098234\n",
      "    mean_raw_obs_processing_ms: 1.769131665255024\n",
      "  time_since_restore: 5507.7892689704895\n",
      "  time_this_iter_s: 24.09273648262024\n",
      "  time_total_s: 5507.7892689704895\n",
      "  timers:\n",
      "    learn_throughput: 1495.943\n",
      "    learn_time_ms: 668.475\n",
      "    load_throughput: 125947.889\n",
      "    load_time_ms: 7.94\n",
      "    sample_throughput: 36.29\n",
      "    sample_time_ms: 27555.834\n",
      "    update_time_ms: 4.634\n",
      "  timestamp: 1635023803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         5507.79</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -2.0361</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            355.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-17-10\n",
      "  done: false\n",
      "  episode_len_mean: 358.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.140000000000036\n",
      "  episode_reward_mean: -1.9054999999999571\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 532\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.065948634677463\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010275334894681245\n",
      "          policy_loss: 0.056790202442142695\n",
      "          total_loss: 0.07832690328359604\n",
      "          vf_explained_var: 0.963187575340271\n",
      "          vf_loss: 0.042193776244918504\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.66749999999999\n",
      "    ram_util_percent: 46.192499999999995\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04079566321648457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.02487191823933\n",
      "    mean_inference_ms: 1.7672756441630793\n",
      "    mean_raw_obs_processing_ms: 1.768567600034171\n",
      "  time_since_restore: 5535.295631885529\n",
      "  time_this_iter_s: 27.506362915039062\n",
      "  time_total_s: 5535.295631885529\n",
      "  timers:\n",
      "    learn_throughput: 1497.302\n",
      "    learn_time_ms: 667.868\n",
      "    load_throughput: 126283.868\n",
      "    load_time_ms: 7.919\n",
      "    sample_throughput: 36.519\n",
      "    sample_time_ms: 27382.862\n",
      "    update_time_ms: 4.933\n",
      "  timestamp: 1635023830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          5535.3</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\"> -1.9055</td><td style=\"text-align: right;\">                5.14</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            358.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 358.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: -1.6841999999999562\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 535\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1319031185574002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012016317514404937\n",
      "          policy_loss: -0.027953410893678664\n",
      "          total_loss: 0.2537827313774162\n",
      "          vf_explained_var: 0.4642668068408966\n",
      "          vf_loss: 0.3030523467395041\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.12345679012346\n",
      "    ram_util_percent: 46.12962962962963\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080031223309523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.031381753952022\n",
      "    mean_inference_ms: 1.7673177555099309\n",
      "    mean_raw_obs_processing_ms: 1.7730341033654924\n",
      "  time_since_restore: 5592.336467504501\n",
      "  time_this_iter_s: 57.04083561897278\n",
      "  time_total_s: 5592.336467504501\n",
      "  timers:\n",
      "    learn_throughput: 1501.048\n",
      "    learn_time_ms: 666.201\n",
      "    load_throughput: 126329.131\n",
      "    load_time_ms: 7.916\n",
      "    sample_throughput: 32.948\n",
      "    sample_time_ms: 30351.085\n",
      "    update_time_ms: 4.836\n",
      "  timestamp: 1635023888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         5592.34</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\"> -1.6842</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            358.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 361.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: -1.6231999999999547\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 537\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2585471921496922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01364848854363797\n",
      "          policy_loss: 0.028886329548226463\n",
      "          total_loss: 0.2897060186498695\n",
      "          vf_explained_var: 0.2186102569103241\n",
      "          vf_loss: 0.28340195847882166\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.953125\n",
      "    ram_util_percent: 46.19375\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080345413044314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.034685666710903\n",
      "    mean_inference_ms: 1.7673467981893083\n",
      "    mean_raw_obs_processing_ms: 1.7760268279626825\n",
      "  time_since_restore: 5614.8298761844635\n",
      "  time_this_iter_s: 22.493408679962158\n",
      "  time_total_s: 5614.8298761844635\n",
      "  timers:\n",
      "    learn_throughput: 1499.323\n",
      "    learn_time_ms: 666.968\n",
      "    load_throughput: 126427.755\n",
      "    load_time_ms: 7.91\n",
      "    sample_throughput: 35.462\n",
      "    sample_time_ms: 28198.87\n",
      "    update_time_ms: 5.025\n",
      "  timestamp: 1635023910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         5614.83</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\"> -1.6232</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">             361.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-19-36\n",
      "  done: false\n",
      "  episode_len_mean: 361.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: -1.5221999999999545\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 541\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1226214210192365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011821567892069713\n",
      "          policy_loss: 0.11560804636942015\n",
      "          total_loss: 0.5437354048921003\n",
      "          vf_explained_var: 0.5227752327919006\n",
      "          vf_loss: 0.4493507843050692\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.5578947368421\n",
      "    ram_util_percent: 46.18421052631579\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04080938521390932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.04033835342838\n",
      "    mean_inference_ms: 1.767393199917006\n",
      "    mean_raw_obs_processing_ms: 1.789877577938068\n",
      "  time_since_restore: 5681.1662838459015\n",
      "  time_this_iter_s: 66.33640766143799\n",
      "  time_total_s: 5681.1662838459015\n",
      "  timers:\n",
      "    learn_throughput: 1495.276\n",
      "    learn_time_ms: 668.773\n",
      "    load_throughput: 125967.18\n",
      "    load_time_ms: 7.939\n",
      "    sample_throughput: 31.034\n",
      "    sample_time_ms: 32222.322\n",
      "    update_time_ms: 5.802\n",
      "  timestamp: 1635023976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         5681.17</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -1.5222</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">             361.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 360.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: -1.439999999999954\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 544\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2463506089316474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015580387560224215\n",
      "          policy_loss: 0.19161078267627293\n",
      "          total_loss: 0.4339534211282929\n",
      "          vf_explained_var: 0.7491140365600586\n",
      "          vf_loss: 0.2648024895124965\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.1778947368421\n",
      "    ram_util_percent: 46.23999999999998\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04081383120601876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.043703751715768\n",
      "    mean_inference_ms: 1.767429434706916\n",
      "    mean_raw_obs_processing_ms: 1.8064591576216327\n",
      "  time_since_restore: 5747.771035432816\n",
      "  time_this_iter_s: 66.60475158691406\n",
      "  time_total_s: 5747.771035432816\n",
      "  timers:\n",
      "    learn_throughput: 1509.707\n",
      "    learn_time_ms: 662.38\n",
      "    load_throughput: 126430.423\n",
      "    load_time_ms: 7.909\n",
      "    sample_throughput: 27.663\n",
      "    sample_time_ms: 36149.982\n",
      "    update_time_ms: 5.869\n",
      "  timestamp: 1635024043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         5747.77</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">   -1.44</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            360.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-21-15\n",
      "  done: false\n",
      "  episode_len_mean: 361.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.830000000000002\n",
      "  episode_reward_mean: -1.377399999999953\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 547\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.100402002864414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012449790051811589\n",
      "          policy_loss: 0.07725811186763976\n",
      "          total_loss: 0.3238889319201311\n",
      "          vf_explained_var: 0.6816869378089905\n",
      "          vf_loss: 0.2676319206754367\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.44130434782609\n",
      "    ram_util_percent: 46.123913043478254\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04081853621040791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.047486186912483\n",
      "    mean_inference_ms: 1.7674686464457878\n",
      "    mean_raw_obs_processing_ms: 1.823005891986605\n",
      "  time_since_restore: 5779.866911888123\n",
      "  time_this_iter_s: 32.09587645530701\n",
      "  time_total_s: 5779.866911888123\n",
      "  timers:\n",
      "    learn_throughput: 1469.951\n",
      "    learn_time_ms: 680.295\n",
      "    load_throughput: 125975.882\n",
      "    load_time_ms: 7.938\n",
      "    sample_throughput: 27.29\n",
      "    sample_time_ms: 36643.917\n",
      "    update_time_ms: 5.515\n",
      "  timestamp: 1635024075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         5779.87</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\"> -1.3774</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            361.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 359.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: -1.1389999999999516\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 551\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.213673135969374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011114096513302465\n",
      "          policy_loss: -0.048851247959666784\n",
      "          total_loss: 0.42414777394798064\n",
      "          vf_explained_var: 0.7969878911972046\n",
      "          vf_loss: 0.495133144987954\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.7590909090909\n",
      "    ram_util_percent: 45.828787878787885\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04082484391258516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.051477228789846\n",
      "    mean_inference_ms: 1.767518670951692\n",
      "    mean_raw_obs_processing_ms: 1.844264645966161\n",
      "  time_since_restore: 5826.421845197678\n",
      "  time_this_iter_s: 46.554933309555054\n",
      "  time_total_s: 5826.421845197678\n",
      "  timers:\n",
      "    learn_throughput: 1478.088\n",
      "    learn_time_ms: 676.55\n",
      "    load_throughput: 120715.375\n",
      "    load_time_ms: 8.284\n",
      "    sample_throughput: 25.879\n",
      "    sample_time_ms: 38642.048\n",
      "    update_time_ms: 5.429\n",
      "  timestamp: 1635024122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         5826.42</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">  -1.139</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            359.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-22-32\n",
      "  done: false\n",
      "  episode_len_mean: 360.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: -1.076799999999951\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 554\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.145713872379727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012946597442134037\n",
      "          policy_loss: -0.07241265902088748\n",
      "          total_loss: 0.16084849884112676\n",
      "          vf_explained_var: 0.25786706805229187\n",
      "          vf_loss: 0.2547152601182461\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.12727272727274\n",
      "    ram_util_percent: 45.790909090909096\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04082878026885669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.054089997699826\n",
      "    mean_inference_ms: 1.767556651609197\n",
      "    mean_raw_obs_processing_ms: 1.8601861292778517\n",
      "  time_since_restore: 5856.816314697266\n",
      "  time_this_iter_s: 30.394469499588013\n",
      "  time_total_s: 5856.816314697266\n",
      "  timers:\n",
      "    learn_throughput: 1478.806\n",
      "    learn_time_ms: 676.221\n",
      "    load_throughput: 118786.169\n",
      "    load_time_ms: 8.418\n",
      "    sample_throughput: 25.503\n",
      "    sample_time_ms: 39210.81\n",
      "    update_time_ms: 5.523\n",
      "  timestamp: 1635024152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         5856.82</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -1.0768</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            360.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 348.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: -0.46139999999995124\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 561\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9868686172697279\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010770091599761104\n",
      "          policy_loss: -0.08007577508687973\n",
      "          total_loss: 0.6983074559105767\n",
      "          vf_explained_var: 0.6820271611213684\n",
      "          vf_loss: 0.7982493920458688\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.56478260869567\n",
      "    ram_util_percent: 46.20913043478261\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04083730229600041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.0578195592857\n",
      "    mean_inference_ms: 1.7676464678134198\n",
      "    mean_raw_obs_processing_ms: 1.9421287776986451\n",
      "  time_since_restore: 6018.149971246719\n",
      "  time_this_iter_s: 161.33365654945374\n",
      "  time_total_s: 6018.149971246719\n",
      "  timers:\n",
      "    learn_throughput: 1472.181\n",
      "    learn_time_ms: 679.264\n",
      "    load_throughput: 112749.501\n",
      "    load_time_ms: 8.869\n",
      "    sample_throughput: 18.959\n",
      "    sample_time_ms: 52745.059\n",
      "    update_time_ms: 5.332\n",
      "  timestamp: 1635024314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         6018.15</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\"> -0.4614</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            348.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 342.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 0.04570000000004902\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 567\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9933779875437418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011511207897614244\n",
      "          policy_loss: -0.04364959034654829\n",
      "          total_loss: 0.526906614502271\n",
      "          vf_explained_var: 0.9197143316268921\n",
      "          vf_loss: 0.5904872824748357\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.4\n",
      "    ram_util_percent: 46.25057471264368\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040844802055705766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.05917623052455\n",
      "    mean_inference_ms: 1.7677566094937487\n",
      "    mean_raw_obs_processing_ms: 2.0387311222624698\n",
      "  time_since_restore: 6140.294634819031\n",
      "  time_this_iter_s: 122.1446635723114\n",
      "  time_total_s: 6140.294634819031\n",
      "  timers:\n",
      "    learn_throughput: 1474.565\n",
      "    learn_time_ms: 678.166\n",
      "    load_throughput: 115787.347\n",
      "    load_time_ms: 8.637\n",
      "    sample_throughput: 15.987\n",
      "    sample_time_ms: 62552.515\n",
      "    update_time_ms: 4.517\n",
      "  timestamp: 1635024436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         6140.29</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">  0.0457</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            342.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 339.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 0.14230000000004836\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 570\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.923460070292155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012276125898168842\n",
      "          policy_loss: -0.012769325574239095\n",
      "          total_loss: 0.3508809619479709\n",
      "          vf_explained_var: 0.7758435606956482\n",
      "          vf_loss: 0.3828820081220733\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.0152380952381\n",
      "    ram_util_percent: 46.50380952380953\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04084829658196155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.059139115321805\n",
      "    mean_inference_ms: 1.7678122814841217\n",
      "    mean_raw_obs_processing_ms: 2.0928640302504387\n",
      "  time_since_restore: 6213.650527238846\n",
      "  time_this_iter_s: 73.35589241981506\n",
      "  time_total_s: 6213.650527238846\n",
      "  timers:\n",
      "    learn_throughput: 1473.002\n",
      "    learn_time_ms: 678.886\n",
      "    load_throughput: 115051.446\n",
      "    load_time_ms: 8.692\n",
      "    sample_throughput: 14.895\n",
      "    sample_time_ms: 67137.403\n",
      "    update_time_ms: 3.777\n",
      "  timestamp: 1635024509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         6213.65</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">  0.1423</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">             339.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-30-06\n",
      "  done: false\n",
      "  episode_len_mean: 329.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 0.7316000000000484\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 576\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1272336231337654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014285732910615481\n",
      "          policy_loss: -0.1234145758052667\n",
      "          total_loss: 0.21762380252281824\n",
      "          vf_explained_var: 0.28377020359039307\n",
      "          vf_loss: 0.36230736374855044\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.73237410071943\n",
      "    ram_util_percent: 46.608633093525185\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040855053190448125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.060104864538033\n",
      "    mean_inference_ms: 1.7679136851423616\n",
      "    mean_raw_obs_processing_ms: 2.2207497918597494\n",
      "  time_since_restore: 6310.812697649002\n",
      "  time_this_iter_s: 97.16217041015625\n",
      "  time_total_s: 6310.812697649002\n",
      "  timers:\n",
      "    learn_throughput: 1461.655\n",
      "    learn_time_ms: 684.156\n",
      "    load_throughput: 114324.528\n",
      "    load_time_ms: 8.747\n",
      "    sample_throughput: 14.056\n",
      "    sample_time_ms: 71143.757\n",
      "    update_time_ms: 4.088\n",
      "  timestamp: 1635024606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         6310.81</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">  0.7316</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">             329.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-32-00\n",
      "  done: false\n",
      "  episode_len_mean: 317.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 1.2607000000000468\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 582\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1007375836372377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009315729289143506\n",
      "          policy_loss: 0.041377445889843836\n",
      "          total_loss: 0.3825796961784363\n",
      "          vf_explained_var: 0.2841889560222626\n",
      "          vf_loss: 0.36220743540260525\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.52283950617283\n",
      "    ram_util_percent: 46.66790123456791\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04086079359677223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.061765429696116\n",
      "    mean_inference_ms: 1.7680086077516972\n",
      "    mean_raw_obs_processing_ms: 2.3688501689041743\n",
      "  time_since_restore: 6424.260666131973\n",
      "  time_this_iter_s: 113.44796848297119\n",
      "  time_total_s: 6424.260666131973\n",
      "  timers:\n",
      "    learn_throughput: 1452.017\n",
      "    learn_time_ms: 688.697\n",
      "    load_throughput: 113351.296\n",
      "    load_time_ms: 8.822\n",
      "    sample_throughput: 12.463\n",
      "    sample_time_ms: 80235.856\n",
      "    update_time_ms: 4.257\n",
      "  timestamp: 1635024720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         6424.26</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">  1.2607</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            317.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-33-20\n",
      "  done: false\n",
      "  episode_len_mean: 309.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 1.6536000000000461\n",
      "  episode_reward_min: -17.550000000000065\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 586\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.08216839366489\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012125372538746898\n",
      "          policy_loss: -0.05179716265863842\n",
      "          total_loss: 0.3165338936779234\n",
      "          vf_explained_var: 0.7679511904716492\n",
      "          vf_loss: 0.3891498984562026\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.03333333333333\n",
      "    ram_util_percent: 46.59561403508771\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040864690791592065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.06370150461909\n",
      "    mean_inference_ms: 1.768078051690776\n",
      "    mean_raw_obs_processing_ms: 2.475665123867946\n",
      "  time_since_restore: 6504.250232219696\n",
      "  time_this_iter_s: 79.98956608772278\n",
      "  time_total_s: 6504.250232219696\n",
      "  timers:\n",
      "    learn_throughput: 1448.376\n",
      "    learn_time_ms: 690.429\n",
      "    load_throughput: 102754.221\n",
      "    load_time_ms: 9.732\n",
      "    sample_throughput: 12.255\n",
      "    sample_time_ms: 81599.449\n",
      "    update_time_ms: 3.423\n",
      "  timestamp: 1635024800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         6504.25</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">  1.6536</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -17.55</td><td style=\"text-align: right;\">            309.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 265.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 3.1679000000000412\n",
      "  episode_reward_min: -11.529999999999928\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 600\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.403108505407969\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013214262961980723\n",
      "          policy_loss: -0.18217792842123243\n",
      "          total_loss: 0.4710430810848872\n",
      "          vf_explained_var: 0.9365185499191284\n",
      "          vf_loss: 0.6672489908006456\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.43503649635038\n",
      "    ram_util_percent: 48.786131386861314\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04088032353924929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.082810130991444\n",
      "    mean_inference_ms: 1.7683366039560264\n",
      "    mean_raw_obs_processing_ms: 3.0233087957700415\n",
      "  time_since_restore: 6792.332788467407\n",
      "  time_this_iter_s: 288.0825562477112\n",
      "  time_total_s: 6792.332788467407\n",
      "  timers:\n",
      "    learn_throughput: 1429.001\n",
      "    learn_time_ms: 699.79\n",
      "    load_throughput: 102782.675\n",
      "    load_time_ms: 9.729\n",
      "    sample_throughput: 9.64\n",
      "    sample_time_ms: 103737.397\n",
      "    update_time_ms: 3.786\n",
      "  timestamp: 1635025088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         6792.33</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">  3.1679</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -11.53</td><td style=\"text-align: right;\">             265.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 251.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 3.5590000000000397\n",
      "  episode_reward_min: -11.529999999999928\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 606\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9703303707970514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010482035541164001\n",
      "          policy_loss: 0.07012307213412391\n",
      "          total_loss: 0.4441275811029805\n",
      "          vf_explained_var: 0.6932211518287659\n",
      "          vf_loss: 0.39370535446537863\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.45612244897958\n",
      "    ram_util_percent: 50.03877551020408\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04088749923231728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.093765965784964\n",
      "    mean_inference_ms: 1.7684577931674477\n",
      "    mean_raw_obs_processing_ms: 3.288763685524931\n",
      "  time_since_restore: 6929.870703935623\n",
      "  time_this_iter_s: 137.53791546821594\n",
      "  time_total_s: 6929.870703935623\n",
      "  timers:\n",
      "    learn_throughput: 1463.641\n",
      "    learn_time_ms: 683.227\n",
      "    load_throughput: 101956.678\n",
      "    load_time_ms: 9.808\n",
      "    sample_throughput: 8.749\n",
      "    sample_time_ms: 114298.006\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1635025225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         6929.87</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   3.559</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -11.53</td><td style=\"text-align: right;\">            251.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 220.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 4.296100000000034\n",
      "  episode_reward_min: -10.259999999999923\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 617\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4449400617016686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00922859213542044\n",
      "          policy_loss: -0.08635116500986947\n",
      "          total_loss: 0.32329024577306376\n",
      "          vf_explained_var: 0.5456183552742004\n",
      "          vf_loss: 0.42408863951762515\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.63322784810126\n",
      "    ram_util_percent: 56.59651898734176\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040899063187353285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.12407487736055\n",
      "    mean_inference_ms: 1.7687987684887816\n",
      "    mean_raw_obs_processing_ms: 3.862061834683055\n",
      "  time_since_restore: 7151.489670991898\n",
      "  time_this_iter_s: 221.6189670562744\n",
      "  time_total_s: 7151.489670991898\n",
      "  timers:\n",
      "    learn_throughput: 1442.478\n",
      "    learn_time_ms: 693.251\n",
      "    load_throughput: 104890.378\n",
      "    load_time_ms: 9.534\n",
      "    sample_throughput: 7.588\n",
      "    sample_time_ms: 131793.434\n",
      "    update_time_ms: 4.589\n",
      "  timestamp: 1635025447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         7151.49</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">  4.2961</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">              -10.26</td><td style=\"text-align: right;\">            220.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 201.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 4.702600000000031\n",
      "  episode_reward_min: -10.259999999999923\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 624\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8479074557622275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008641582810016542\n",
      "          policy_loss: 0.05757935245831807\n",
      "          total_loss: 0.46248715179454947\n",
      "          vf_explained_var: 0.930167555809021\n",
      "          vf_loss: 0.4233848365644614\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.88798283261804\n",
      "    ram_util_percent: 60.34549356223175\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04091024272031473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.152313913074458\n",
      "    mean_inference_ms: 1.7690765262775907\n",
      "    mean_raw_obs_processing_ms: 4.26331332123714\n",
      "  time_since_restore: 7314.60281419754\n",
      "  time_this_iter_s: 163.1131432056427\n",
      "  time_total_s: 7314.60281419754\n",
      "  timers:\n",
      "    learn_throughput: 1423.204\n",
      "    learn_time_ms: 702.64\n",
      "    load_throughput: 104932.364\n",
      "    load_time_ms: 9.53\n",
      "    sample_throughput: 6.894\n",
      "    sample_time_ms: 145055.63\n",
      "    update_time_ms: 4.583\n",
      "  timestamp: 1635025610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">          7314.6</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">  4.7026</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -10.26</td><td style=\"text-align: right;\">            201.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 161.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 5.640300000000024\n",
      "  episode_reward_min: -10.259999999999923\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 637\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8753045373492772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015073288873997113\n",
      "          policy_loss: -0.18166353404521943\n",
      "          total_loss: 0.4177710900704066\n",
      "          vf_explained_var: 0.36157354712486267\n",
      "          vf_loss: 0.6181841308871905\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.36710182767624\n",
      "    ram_util_percent: 60.33472584856396\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04093932703404377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.22405886356245\n",
      "    mean_inference_ms: 1.769741772074732\n",
      "    mean_raw_obs_processing_ms: 5.137651402385868\n",
      "  time_since_restore: 7583.33000254631\n",
      "  time_this_iter_s: 268.72718834877014\n",
      "  time_total_s: 7583.33000254631\n",
      "  timers:\n",
      "    learn_throughput: 1416.292\n",
      "    learn_time_ms: 706.069\n",
      "    load_throughput: 102914.825\n",
      "    load_time_ms: 9.717\n",
      "    sample_throughput: 6.419\n",
      "    sample_time_ms: 155790.73\n",
      "    update_time_ms: 5.207\n",
      "  timestamp: 1635025879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         7583.33</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">  5.6403</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">              -10.26</td><td style=\"text-align: right;\">            161.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_21-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 116.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 7.199400000000018\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 656\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9260768161879644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01857156476964878\n",
      "          policy_loss: 0.1455887140499221\n",
      "          total_loss: 0.5220456113417943\n",
      "          vf_explained_var: 0.9548944234848022\n",
      "          vf_loss: 0.39571330580446457\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.0499105545617\n",
      "    ram_util_percent: 60.83237924865832\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04098659035104422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.34542916779282\n",
      "    mean_inference_ms: 1.7709034916546225\n",
      "    mean_raw_obs_processing_ms: 6.6260939806685295\n",
      "  time_since_restore: 7975.275609254837\n",
      "  time_this_iter_s: 391.9456067085266\n",
      "  time_total_s: 7975.275609254837\n",
      "  timers:\n",
      "    learn_throughput: 1399.973\n",
      "    learn_time_ms: 714.299\n",
      "    load_throughput: 100308.365\n",
      "    load_time_ms: 9.969\n",
      "    sample_throughput: 5.472\n",
      "    sample_time_ms: 182761.286\n",
      "    update_time_ms: 6.058\n",
      "  timestamp: 1635026271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         7975.28</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  7.1994</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">             116.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 106.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 7.412800000000015\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 669\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8666565312279595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012611360034385724\n",
      "          policy_loss: -0.1265413729680909\n",
      "          total_loss: 0.5195617288351059\n",
      "          vf_explained_var: 0.9451103806495667\n",
      "          vf_loss: 0.6647667007313834\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.33233695652174\n",
      "    ram_util_percent: 60.90217391304348\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04101906026321173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.4266008187424\n",
      "    mean_inference_ms: 1.7717451013929633\n",
      "    mean_raw_obs_processing_ms: 7.651825991901896\n",
      "  time_since_restore: 8232.845861196518\n",
      "  time_this_iter_s: 257.5702519416809\n",
      "  time_total_s: 8232.845861196518\n",
      "  timers:\n",
      "    learn_throughput: 1382.897\n",
      "    learn_time_ms: 723.12\n",
      "    load_throughput: 99014.974\n",
      "    load_time_ms: 10.099\n",
      "    sample_throughput: 4.971\n",
      "    sample_time_ms: 201173.365\n",
      "    update_time_ms: 6.4\n",
      "  timestamp: 1635026529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         8232.85</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">  7.4128</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">            106.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 90.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 7.8745000000000145\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 681\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1319461782773335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01234258680754553\n",
      "          policy_loss: 0.0057180222537782455\n",
      "          total_loss: 0.24485843856301573\n",
      "          vf_explained_var: 0.6254619359970093\n",
      "          vf_loss: 0.2604569810960028\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.94899135446687\n",
      "    ram_util_percent: 60.7250720461095\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041051024843261556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.501241654994747\n",
      "    mean_inference_ms: 1.7725911299854402\n",
      "    mean_raw_obs_processing_ms: 8.615648724102991\n",
      "  time_since_restore: 8475.95403122902\n",
      "  time_this_iter_s: 243.10817003250122\n",
      "  time_total_s: 8475.95403122902\n",
      "  timers:\n",
      "    learn_throughput: 1360.541\n",
      "    learn_time_ms: 735.002\n",
      "    load_throughput: 97797.364\n",
      "    load_time_ms: 10.225\n",
      "    sample_throughput: 4.635\n",
      "    sample_time_ms: 215756.287\n",
      "    update_time_ms: 6.188\n",
      "  timestamp: 1635026772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         8475.95</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">  7.8745</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">             90.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 86.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.93\n",
      "  episode_reward_mean: 7.914300000000012\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 695\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4584123465749952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01684644817605538\n",
      "          policy_loss: 0.03130647771888309\n",
      "          total_loss: 0.9256586124499638\n",
      "          vf_explained_var: 0.9142425656318665\n",
      "          vf_loss: 0.9089323023955027\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.05889145496536\n",
      "    ram_util_percent: 61.2039260969977\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041087399357632545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.59484187306929\n",
      "    mean_inference_ms: 1.7736368393190884\n",
      "    mean_raw_obs_processing_ms: 9.726653522290396\n",
      "  time_since_restore: 8779.488129854202\n",
      "  time_this_iter_s: 303.5340986251831\n",
      "  time_total_s: 8779.488129854202\n",
      "  timers:\n",
      "    learn_throughput: 1344.311\n",
      "    learn_time_ms: 743.876\n",
      "    load_throughput: 96258.801\n",
      "    load_time_ms: 10.389\n",
      "    sample_throughput: 4.26\n",
      "    sample_time_ms: 234756.038\n",
      "    update_time_ms: 6.177\n",
      "  timestamp: 1635027075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         8779.49</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">  7.9143</td><td style=\"text-align: right;\">                9.93</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">             86.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 86.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.929700000000013\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 701\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1599000334739684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015000025198013205\n",
      "          policy_loss: -0.028348036772674983\n",
      "          total_loss: 0.46036309632990097\n",
      "          vf_explained_var: 0.44487571716308594\n",
      "          vf_loss: 0.5103066146373749\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.00409356725146\n",
      "    ram_util_percent: 61.02865497076022\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04110480738960181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.636395708186715\n",
      "    mean_inference_ms: 1.7741333376924222\n",
      "    mean_raw_obs_processing_ms: 10.190223925183123\n",
      "  time_since_restore: 8899.071361780167\n",
      "  time_this_iter_s: 119.58323192596436\n",
      "  time_total_s: 8899.071361780167\n",
      "  timers:\n",
      "    learn_throughput: 1328.027\n",
      "    learn_time_ms: 752.997\n",
      "    load_throughput: 104904.02\n",
      "    load_time_ms: 9.533\n",
      "    sample_throughput: 4.189\n",
      "    sample_time_ms: 238705.909\n",
      "    update_time_ms: 7.188\n",
      "  timestamp: 1635027195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         8899.07</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  7.9297</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">             86.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-17-11\n",
      "  done: false\n",
      "  episode_len_mean: 82.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 7.928500000000011\n",
      "  episode_reward_min: -6.349999999999955\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 711\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8945344673262703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010815200475052677\n",
      "          policy_loss: -0.077765863471561\n",
      "          total_loss: 0.5967002282540004\n",
      "          vf_explained_var: 0.6744087338447571\n",
      "          vf_loss: 0.6934088902340995\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.66587537091989\n",
      "    ram_util_percent: 60.99970326409495\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04113668362087916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.703786329907643\n",
      "    mean_inference_ms: 1.774965128835181\n",
      "    mean_raw_obs_processing_ms: 10.967711148460172\n",
      "  time_since_restore: 9135.611208677292\n",
      "  time_this_iter_s: 236.53984689712524\n",
      "  time_total_s: 9135.611208677292\n",
      "  timers:\n",
      "    learn_throughput: 1317.211\n",
      "    learn_time_ms: 759.18\n",
      "    load_throughput: 103459.092\n",
      "    load_time_ms: 9.666\n",
      "    sample_throughput: 4.282\n",
      "    sample_time_ms: 233545.231\n",
      "    update_time_ms: 7.91\n",
      "  timestamp: 1635027431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         9135.61</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">  7.9285</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -6.35</td><td style=\"text-align: right;\">             82.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 68.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 8.225900000000008\n",
      "  episode_reward_min: -5.109999999999947\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 738\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.760629743999905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006603608725905226\n",
      "          policy_loss: -0.05232590263088544\n",
      "          total_loss: 0.9284400920901034\n",
      "          vf_explained_var: 0.9370135068893433\n",
      "          vf_loss: 0.9883707526657316\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.77146464646465\n",
      "    ram_util_percent: 61.152398989898984\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04119926529824111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.869904495853397\n",
      "    mean_inference_ms: 1.777060215255647\n",
      "    mean_raw_obs_processing_ms: 13.300215762263104\n",
      "  time_since_restore: 9690.83644747734\n",
      "  time_this_iter_s: 555.2252388000488\n",
      "  time_total_s: 9690.83644747734\n",
      "  timers:\n",
      "    learn_throughput: 1294.576\n",
      "    learn_time_ms: 772.454\n",
      "    load_throughput: 101751.878\n",
      "    load_time_ms: 9.828\n",
      "    sample_throughput: 3.632\n",
      "    sample_time_ms: 275300.335\n",
      "    update_time_ms: 8.151\n",
      "  timestamp: 1635027987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         9690.84</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">  8.2259</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -5.11</td><td style=\"text-align: right;\">             68.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-39-59\n",
      "  done: false\n",
      "  episode_len_mean: 50.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 8.752600000000006\n",
      "  episode_reward_min: -5.109999999999947\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 778\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1999010317855412\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008945223117372928\n",
      "          policy_loss: -0.0362746246986919\n",
      "          total_loss: 0.41656385784347855\n",
      "          vf_explained_var: 0.37114444375038147\n",
      "          vf_loss: 0.46483539508448707\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.63874029335634\n",
      "    ram_util_percent: 61.318205349439175\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041262200956300156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.062992099878702\n",
      "    mean_inference_ms: 1.7795731396451964\n",
      "    mean_raw_obs_processing_ms: 16.988887777657762\n",
      "  time_since_restore: 10503.446259260178\n",
      "  time_this_iter_s: 812.6098117828369\n",
      "  time_total_s: 10503.446259260178\n",
      "  timers:\n",
      "    learn_throughput: 1284.019\n",
      "    learn_time_ms: 778.805\n",
      "    load_throughput: 96226.559\n",
      "    load_time_ms: 10.392\n",
      "    sample_throughput: 2.99\n",
      "    sample_time_ms: 334393.204\n",
      "    update_time_ms: 7.73\n",
      "  timestamp: 1635028799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         10503.4</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">  8.7526</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -5.11</td><td style=\"text-align: right;\">             50.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 53.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 8.790600000000007\n",
      "  episode_reward_min: -5.109999999999947\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 792\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.108937242296007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015824962639999225\n",
      "          policy_loss: 0.10529365357425478\n",
      "          total_loss: 0.44902627252870136\n",
      "          vf_explained_var: 0.1266472488641739\n",
      "          vf_loss: 0.3648182738158438\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.96261682242991\n",
      "    ram_util_percent: 61.50911214953271\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041285643582720875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.112268603596796\n",
      "    mean_inference_ms: 1.780299819386717\n",
      "    mean_raw_obs_processing_ms: 18.152793892132934\n",
      "  time_since_restore: 10803.514515638351\n",
      "  time_this_iter_s: 300.0682563781738\n",
      "  time_total_s: 10803.514515638351\n",
      "  timers:\n",
      "    learn_throughput: 1278.058\n",
      "    learn_time_ms: 782.437\n",
      "    load_throughput: 95889.093\n",
      "    load_time_ms: 10.429\n",
      "    sample_throughput: 2.873\n",
      "    sample_time_ms: 348084.338\n",
      "    update_time_ms: 8.767\n",
      "  timestamp: 1635029099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         10803.5</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  8.7906</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -5.11</td><td style=\"text-align: right;\">             53.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_22-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 36.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.330200000000003\n",
      "  episode_reward_min: -0.33999999999991964\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 830\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4087916274865468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01009425514542072\n",
      "          policy_loss: 0.17532012230820126\n",
      "          total_loss: 0.6073270304335489\n",
      "          vf_explained_var: 0.9620267152786255\n",
      "          vf_loss: 0.44609245599971875\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.51717902350813\n",
      "    ram_util_percent: 61.63264014466546\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132163372352141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.21477299945108\n",
      "    mean_inference_ms: 1.7816934299057225\n",
      "    mean_raw_obs_processing_ms: 21.66361241629066\n",
      "  time_since_restore: 11578.467002391815\n",
      "  time_this_iter_s: 774.9524867534637\n",
      "  time_total_s: 11578.467002391815\n",
      "  timers:\n",
      "    learn_throughput: 1265.635\n",
      "    learn_time_ms: 790.117\n",
      "    load_throughput: 99753.465\n",
      "    load_time_ms: 10.025\n",
      "    sample_throughput: 2.508\n",
      "    sample_time_ms: 398699.89\n",
      "    update_time_ms: 8.348\n",
      "  timestamp: 1635029874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 28.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         11578.5</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">  9.3302</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -0.34</td><td style=\"text-align: right;\">             36.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_23-09-46\n",
      "  done: false\n",
      "  episode_len_mean: 32.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.395900000000001\n",
      "  episode_reward_min: -7.289999999999935\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 865\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.533068014515771\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011312260293765301\n",
      "          policy_loss: 0.10141057852241728\n",
      "          total_loss: 0.5495521164602704\n",
      "          vf_explained_var: 0.576428234577179\n",
      "          vf_loss: 0.46346956193447114\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.97645320197044\n",
      "    ram_util_percent: 61.75724137931035\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134808975991512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.286558806530756\n",
      "    mean_inference_ms: 1.7827189017892393\n",
      "    mean_raw_obs_processing_ms: 24.508183857301784\n",
      "  time_since_restore: 12289.802010774612\n",
      "  time_this_iter_s: 711.3350083827972\n",
      "  time_total_s: 12289.802010774612\n",
      "  timers:\n",
      "    learn_throughput: 1257.728\n",
      "    learn_time_ms: 795.084\n",
      "    load_throughput: 99501.202\n",
      "    load_time_ms: 10.05\n",
      "    sample_throughput: 2.322\n",
      "    sample_time_ms: 430634.183\n",
      "    update_time_ms: 7.827\n",
      "  timestamp: 1635030586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         12289.8</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">  9.3959</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">             32.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_23-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 26.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.498800000000001\n",
      "  episode_reward_min: -7.289999999999935\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 911\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.233613861931695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012114555202581463\n",
      "          policy_loss: 0.010555066499445173\n",
      "          total_loss: 0.6503975960943434\n",
      "          vf_explained_var: 0.9463294148445129\n",
      "          vf_loss: 0.6521758251720005\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.64698972099853\n",
      "    ram_util_percent: 61.99419970631425\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04138127294071353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.37592783664228\n",
      "    mean_inference_ms: 1.7841285111692802\n",
      "    mean_raw_obs_processing_ms: 28.745377286449887\n",
      "  time_since_restore: 13244.757168769836\n",
      "  time_this_iter_s: 954.955157995224\n",
      "  time_total_s: 13244.757168769836\n",
      "  timers:\n",
      "    learn_throughput: 1252.582\n",
      "    learn_time_ms: 798.351\n",
      "    load_throughput: 88473.8\n",
      "    load_time_ms: 11.303\n",
      "    sample_throughput: 1.999\n",
      "    sample_time_ms: 500366.779\n",
      "    update_time_ms: 8.557\n",
      "  timestamp: 1635031541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         13244.8</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">  9.4988</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.29</td><td style=\"text-align: right;\">             26.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_23-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 22.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.5184\n",
      "  episode_reward_min: -7.979999999999939\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 949\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5197084923585256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011469699794970723\n",
      "          policy_loss: -0.005524614618884193\n",
      "          total_loss: 0.6488356414768431\n",
      "          vf_explained_var: 0.5307630896568298\n",
      "          vf_loss: 0.6695546466443274\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.58948322756119\n",
      "    ram_util_percent: 62.08812330009066\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141401646176284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.4404526300379\n",
      "    mean_inference_ms: 1.7852492672268907\n",
      "    mean_raw_obs_processing_ms: 32.04313786993275\n",
      "  time_since_restore: 14018.188174724579\n",
      "  time_this_iter_s: 773.4310059547424\n",
      "  time_total_s: 14018.188174724579\n",
      "  timers:\n",
      "    learn_throughput: 1256.017\n",
      "    learn_time_ms: 796.167\n",
      "    load_throughput: 89661.538\n",
      "    load_time_ms: 11.153\n",
      "    sample_throughput: 1.807\n",
      "    sample_time_ms: 553400.838\n",
      "    update_time_ms: 9.174\n",
      "  timestamp: 1635032314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         14018.2</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  9.5184</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.98</td><td style=\"text-align: right;\">             22.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-23_23-48-53\n",
      "  done: false\n",
      "  episode_len_mean: 25.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.501700000000003\n",
      "  episode_reward_min: -7.979999999999939\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 979\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2802790416611565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008146499333868679\n",
      "          policy_loss: -0.05719970773077673\n",
      "          total_loss: 0.312090934606062\n",
      "          vf_explained_var: 0.6022082567214966\n",
      "          vf_loss: 0.3820915169186062\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.63759909399774\n",
      "    ram_util_percent: 62.18063420158551\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041435295011841976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.491120639475547\n",
      "    mean_inference_ms: 1.7860662653713526\n",
      "    mean_raw_obs_processing_ms: 34.37274477170025\n",
      "  time_since_restore: 14636.488700151443\n",
      "  time_this_iter_s: 618.3005254268646\n",
      "  time_total_s: 14636.488700151443\n",
      "  timers:\n",
      "    learn_throughput: 1264.587\n",
      "    learn_time_ms: 790.772\n",
      "    load_throughput: 91085.08\n",
      "    load_time_ms: 10.979\n",
      "    sample_throughput: 1.71\n",
      "    sample_time_ms: 584882.607\n",
      "    update_time_ms: 9.637\n",
      "  timestamp: 1635032933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         14636.5</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">  9.5017</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -7.98</td><td style=\"text-align: right;\">             25.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_00-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 20.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.715700000000002\n",
      "  episode_reward_min: 1.1000000000000691\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1047\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5557803577846951\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007399959243587754\n",
      "          policy_loss: -0.03369577133821117\n",
      "          total_loss: 0.0691984427264995\n",
      "          vf_explained_var: 0.9899912476539612\n",
      "          vf_loss: 0.10845028393798405\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.70045203415368\n",
      "    ram_util_percent: 62.288146659969854\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147986459237138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.616917764342887\n",
      "    mean_inference_ms: 1.7877987902083605\n",
      "    mean_raw_obs_processing_ms: 41.06048623908022\n",
      "  time_since_restore: 16032.30812573433\n",
      "  time_this_iter_s: 1395.8194255828857\n",
      "  time_total_s: 16032.30812573433\n",
      "  timers:\n",
      "    learn_throughput: 1270.972\n",
      "    learn_time_ms: 786.799\n",
      "    load_throughput: 90710.209\n",
      "    load_time_ms: 11.024\n",
      "    sample_throughput: 1.403\n",
      "    sample_time_ms: 712511.177\n",
      "    update_time_ms: 8.675\n",
      "  timestamp: 1635034328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         16032.3</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">  9.7157</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                 1.1</td><td style=\"text-align: right;\">             20.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_00-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 18.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.711500000000001\n",
      "  episode_reward_min: -5.4799999999999125\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1087\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1508510092894235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01367495933511166\n",
      "          policy_loss: 0.0438562440375487\n",
      "          total_loss: 0.3201247345242235\n",
      "          vf_explained_var: 0.6826922297477722\n",
      "          vf_loss: 0.28777379186617\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.81051282051281\n",
      "    ram_util_percent: 62.4242735042735\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415011551164298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.67161625956217\n",
      "    mean_inference_ms: 1.7886499590227234\n",
      "    mean_raw_obs_processing_ms: 44.31245081022044\n",
      "  time_since_restore: 16852.194934606552\n",
      "  time_this_iter_s: 819.8868088722229\n",
      "  time_total_s: 16852.194934606552\n",
      "  timers:\n",
      "    learn_throughput: 1280.558\n",
      "    learn_time_ms: 780.909\n",
      "    load_throughput: 92081.519\n",
      "    load_time_ms: 10.86\n",
      "    sample_throughput: 1.297\n",
      "    sample_time_ms: 770851.922\n",
      "    update_time_ms: 8.391\n",
      "  timestamp: 1635035148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         16852.2</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">  9.7115</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -5.48</td><td style=\"text-align: right;\">             18.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_00-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 22.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.5012\n",
      "  episode_reward_min: -9.799999999999953\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1132\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00023463964462280265\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6656813492377599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004552802631267876\n",
      "          policy_loss: -0.26878375858068465\n",
      "          total_loss: -0.05413750807444254\n",
      "          vf_explained_var: 0.9861242771148682\n",
      "          vf_loss: 0.22130198892619873\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.94618867924528\n",
      "    ram_util_percent: 62.556377358490565\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041528268534254995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.744173772000295\n",
      "    mean_inference_ms: 1.7897131963381872\n",
      "    mean_raw_obs_processing_ms: 47.46148967339037\n",
      "  time_since_restore: 17781.1925842762\n",
      "  time_this_iter_s: 928.9976496696472\n",
      "  time_total_s: 17781.1925842762\n",
      "  timers:\n",
      "    learn_throughput: 1276.554\n",
      "    learn_time_ms: 783.359\n",
      "    load_throughput: 78110.002\n",
      "    load_time_ms: 12.802\n",
      "    sample_throughput: 1.237\n",
      "    sample_time_ms: 808224.989\n",
      "    update_time_ms: 8.2\n",
      "  timestamp: 1635036077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         17781.2</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  9.5012</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">              22.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_00-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 20.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.6247\n",
      "  episode_reward_min: -9.799999999999953\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1181\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00011731982231140132\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8183363603221046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0037391605009223067\n",
      "          policy_loss: 0.038345351815223694\n",
      "          total_loss: 0.2250489234096474\n",
      "          vf_explained_var: 0.6929179430007935\n",
      "          vf_loss: 0.19488649807042546\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.19950738916256\n",
      "    ram_util_percent: 62.65826882477129\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155513112641323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.848057205425043\n",
      "    mean_inference_ms: 1.790889457366598\n",
      "    mean_raw_obs_processing_ms: 51.72230562103277\n",
      "  time_since_restore: 18777.51030611992\n",
      "  time_this_iter_s: 996.3177218437195\n",
      "  time_total_s: 18777.51030611992\n",
      "  timers:\n",
      "    learn_throughput: 1279.051\n",
      "    learn_time_ms: 781.83\n",
      "    load_throughput: 75950.471\n",
      "    load_time_ms: 13.166\n",
      "    sample_throughput: 1.21\n",
      "    sample_time_ms: 826596.948\n",
      "    update_time_ms: 8.165\n",
      "  timestamp: 1635037074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         18777.5</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">  9.6247</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                -9.8</td><td style=\"text-align: right;\">             20.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_01-18-21\n",
      "  done: false\n",
      "  episode_len_mean: 17.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.833\n",
      "  episode_reward_min: 4.850000000000065\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 1241\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.865991115570066e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.663498310579194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004869333510762017\n",
      "          policy_loss: -0.1452683062189155\n",
      "          total_loss: 0.0823093579047256\n",
      "          vf_explained_var: 0.9850636720657349\n",
      "          vf_loss: 0.23421236558092964\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.72134703196346\n",
      "    ram_util_percent: 62.93881278538813\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157674518297238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.9428605529641\n",
      "    mean_inference_ms: 1.7921165653290627\n",
      "    mean_raw_obs_processing_ms: 56.970682629748\n",
      "  time_since_restore: 20005.223987340927\n",
      "  time_this_iter_s: 1227.7136812210083\n",
      "  time_total_s: 20005.223987340927\n",
      "  timers:\n",
      "    learn_throughput: 1294.952\n",
      "    learn_time_ms: 772.23\n",
      "    load_throughput: 76218.638\n",
      "    load_time_ms: 13.12\n",
      "    sample_throughput: 1.088\n",
      "    sample_time_ms: 919371.705\n",
      "    update_time_ms: 7.538\n",
      "  timestamp: 1635038301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         20005.2</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">   9.833</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                4.85</td><td style=\"text-align: right;\">             17.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_01-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 24.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.681600000000001\n",
      "  episode_reward_min: -0.149999999999969\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1268\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.932995557785033e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2977517472373115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00845802754853179\n",
      "          policy_loss: 0.08649353647811545\n",
      "          total_loss: 0.41246618326339457\n",
      "          vf_explained_var: 0.26440396904945374\n",
      "          vf_loss: 0.33894991882973247\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.15524934383203\n",
      "    ram_util_percent: 63.08451443569554\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158703271783252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.98236062142778\n",
      "    mean_inference_ms: 1.7926816881536605\n",
      "    mean_raw_obs_processing_ms: 58.82824833266125\n",
      "  time_since_restore: 20539.91013789177\n",
      "  time_this_iter_s: 534.6861505508423\n",
      "  time_total_s: 20539.91013789177\n",
      "  timers:\n",
      "    learn_throughput: 1306.871\n",
      "    learn_time_ms: 765.186\n",
      "    load_throughput: 77697.764\n",
      "    load_time_ms: 12.87\n",
      "    sample_throughput: 1.117\n",
      "    sample_time_ms: 895352.557\n",
      "    update_time_ms: 7.32\n",
      "  timestamp: 1635038836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         20539.9</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">  9.6816</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -0.15</td><td style=\"text-align: right;\">             24.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_01-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 24.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.6751\n",
      "  episode_reward_min: -0.149999999999969\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 1321\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.932995557785033e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7366250177224477\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005789436100610492\n",
      "          policy_loss: 0.1260036653942532\n",
      "          total_loss: 0.26401676221026316\n",
      "          vf_explained_var: 0.9364825487136841\n",
      "          vf_loss: 0.14537917855713103\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.61134289439373\n",
      "    ram_util_percent: 63.248239895697516\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04164259692299945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.07669981510737\n",
      "    mean_inference_ms: 1.7938208260446171\n",
      "    mean_raw_obs_processing_ms: 62.72074543788183\n",
      "  time_since_restore: 21614.818387269974\n",
      "  time_this_iter_s: 1074.9082493782043\n",
      "  time_total_s: 21614.818387269974\n",
      "  timers:\n",
      "    learn_throughput: 1330.61\n",
      "    learn_time_ms: 751.535\n",
      "    load_throughput: 76387.431\n",
      "    load_time_ms: 13.091\n",
      "    sample_throughput: 1.073\n",
      "    sample_time_ms: 931723.193\n",
      "    update_time_ms: 7.424\n",
      "  timestamp: 1635039911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 29.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         21614.8</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  9.6751</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -0.15</td><td style=\"text-align: right;\">             24.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_02-09-57\n",
      "  done: false\n",
      "  episode_len_mean: 13.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.9049\n",
      "  episode_reward_min: 8.880000000000003\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 1394\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.932995557785033e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.47089790503184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00901407848947253\n",
      "          policy_loss: 0.026760050861371888\n",
      "          total_loss: 0.08769444218940205\n",
      "          vf_explained_var: 0.9941310286521912\n",
      "          vf_loss: 0.06564310849126842\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.75427088249175\n",
      "    ram_util_percent: 63.593723454459656\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04170770639154542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.186117183148678\n",
      "    mean_inference_ms: 1.7951664021799838\n",
      "    mean_raw_obs_processing_ms: 69.46541761379193\n",
      "  time_since_restore: 23100.408772468567\n",
      "  time_this_iter_s: 1485.5903851985931\n",
      "  time_total_s: 23100.408772468567\n",
      "  timers:\n",
      "    learn_throughput: 1340.555\n",
      "    learn_time_ms: 745.96\n",
      "    load_throughput: 79350.26\n",
      "    load_time_ms: 12.602\n",
      "    sample_throughput: 1.015\n",
      "    sample_time_ms: 984793.645\n",
      "    update_time_ms: 6.747\n",
      "  timestamp: 1635041397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         23100.4</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">  9.9049</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                8.88</td><td style=\"text-align: right;\">             13.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_02-35-24\n",
      "  done: false\n",
      "  episode_len_mean: 13.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.916099999999998\n",
      "  episode_reward_min: 9.840000000000002\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1469\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.932995557785033e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40498721831374696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009170128674534341\n",
      "          policy_loss: -0.023302103868789142\n",
      "          total_loss: 0.08554777020795477\n",
      "          vf_explained_var: 0.9896326065063477\n",
      "          vf_loss: 0.11289948082218568\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.12373737373737\n",
      "    ram_util_percent: 63.86014692378328\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04172401420878097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.26427313813981\n",
      "    mean_inference_ms: 1.7962349332726786\n",
      "    mean_raw_obs_processing_ms: 75.67541974787346\n",
      "  time_since_restore: 24627.251554965973\n",
      "  time_this_iter_s: 1526.842782497406\n",
      "  time_total_s: 24627.251554965973\n",
      "  timers:\n",
      "    learn_throughput: 1347.703\n",
      "    learn_time_ms: 742.003\n",
      "    load_throughput: 76883.105\n",
      "    load_time_ms: 13.007\n",
      "    sample_throughput: 0.943\n",
      "    sample_time_ms: 1060138.703\n",
      "    update_time_ms: 6.337\n",
      "  timestamp: 1635042924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         24627.3</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">  9.9161</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                9.84</td><td style=\"text-align: right;\">             13.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_03-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 13.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.9186\n",
      "  episode_reward_min: 9.830000000000002\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 1545\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.932995557785033e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3459425542089674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0037183995466616537\n",
      "          policy_loss: -0.03269985591371854\n",
      "          total_loss: -0.019662895198497506\n",
      "          vf_explained_var: 0.9984441995620728\n",
      "          vf_loss: 0.016496279204471245\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.07462277091908\n",
      "    ram_util_percent: 64.02876085962507\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04174985932472801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.344174914560966\n",
      "    mean_inference_ms: 1.7972910200267045\n",
      "    mean_raw_obs_processing_ms: 81.83445636411743\n",
      "  time_since_restore: 26159.92433667183\n",
      "  time_this_iter_s: 1532.6727817058563\n",
      "  time_total_s: 26159.92433667183\n",
      "  timers:\n",
      "    learn_throughput: 1358.472\n",
      "    learn_time_ms: 736.121\n",
      "    load_throughput: 71434.236\n",
      "    load_time_ms: 13.999\n",
      "    sample_throughput: 0.868\n",
      "    sample_time_ms: 1151581.744\n",
      "    update_time_ms: 5.594\n",
      "  timestamp: 1635044456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         26159.9</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">  9.9186</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">             13.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_03-26-46\n",
      "  done: false\n",
      "  episode_len_mean: 13.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.909500000000001\n",
      "  episode_reward_min: 8.820000000000004\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 1622\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4664977788925165e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3305304318666458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006821419198687482\n",
      "          policy_loss: 0.03406525839947992\n",
      "          total_loss: 0.059712142611129414\n",
      "          vf_explained_var: 0.9974610805511475\n",
      "          vf_loss: 0.028952090297308233\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.90809954751131\n",
      "    ram_util_percent: 64.28733031674209\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04177582846738366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.421165555056497\n",
      "    mean_inference_ms: 1.7982272681964933\n",
      "    mean_raw_obs_processing_ms: 88.00360550501145\n",
      "  time_since_restore: 27709.220218658447\n",
      "  time_this_iter_s: 1549.295881986618\n",
      "  time_total_s: 27709.220218658447\n",
      "  timers:\n",
      "    learn_throughput: 1364.749\n",
      "    learn_time_ms: 732.735\n",
      "    load_throughput: 67634.138\n",
      "    load_time_ms: 14.785\n",
      "    sample_throughput: 0.857\n",
      "    sample_time_ms: 1166931.306\n",
      "    update_time_ms: 6.341\n",
      "  timestamp: 1635046006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         27709.2</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  9.9095</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                8.82</td><td style=\"text-align: right;\">             13.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_03-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 12.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.921899999999999\n",
      "  episode_reward_min: 9.88\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1700\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4664977788925165e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3415527754359775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.001380184612124966\n",
      "          policy_loss: -0.014308996881461805\n",
      "          total_loss: 0.07000597163827883\n",
      "          vf_explained_var: 0.9917638301849365\n",
      "          vf_loss: 0.08773047753816678\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8054818744474\n",
      "    ram_util_percent: 64.6080017683466\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04180168801226463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.502181382651813\n",
      "    mean_inference_ms: 1.7992254162336747\n",
      "    mean_raw_obs_processing_ms: 94.25317055349457\n",
      "  time_since_restore: 29294.859347343445\n",
      "  time_this_iter_s: 1585.6391286849976\n",
      "  time_total_s: 29294.859347343445\n",
      "  timers:\n",
      "    learn_throughput: 1365.318\n",
      "    learn_time_ms: 732.43\n",
      "    load_throughput: 63166.182\n",
      "    load_time_ms: 15.831\n",
      "    sample_throughput: 0.804\n",
      "    sample_time_ms: 1243506.8\n",
      "    update_time_ms: 5.513\n",
      "  timestamp: 1635047591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         29294.9</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">  9.9219</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">             12.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_04-19-20\n",
      "  done: false\n",
      "  episode_len_mean: 13.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.909500000000001\n",
      "  episode_reward_min: 8.850000000000001\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 1777\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.332488894462583e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3431713306241565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00396151318383648\n",
      "          policy_loss: -0.009833649680432346\n",
      "          total_loss: 0.07594802129185863\n",
      "          vf_explained_var: 0.9923407435417175\n",
      "          vf_loss: 0.08921335662404696\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8146177916853\n",
      "    ram_util_percent: 64.87232901206973\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04182141153829076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.579967365963046\n",
      "    mean_inference_ms: 1.8002202067843165\n",
      "    mean_raw_obs_processing_ms: 100.30585477041066\n",
      "  time_since_restore: 30863.272018671036\n",
      "  time_this_iter_s: 1568.412671327591\n",
      "  time_total_s: 30863.272018671036\n",
      "  timers:\n",
      "    learn_throughput: 1393.248\n",
      "    learn_time_ms: 717.747\n",
      "    load_throughput: 68123.08\n",
      "    load_time_ms: 14.679\n",
      "    sample_throughput: 0.765\n",
      "    sample_time_ms: 1307464.175\n",
      "    update_time_ms: 5.489\n",
      "  timestamp: 1635049160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         30863.3</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">  9.9095</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                8.85</td><td style=\"text-align: right;\">             13.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_04-45-41\n",
      "  done: false\n",
      "  episode_len_mean: 13.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.91\n",
      "  episode_reward_min: 8.850000000000001\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1855\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6662444472312914e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.31232206688986885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009122718113462729\n",
      "          policy_loss: -0.13734322438637417\n",
      "          total_loss: -0.13409643570582072\n",
      "          vf_explained_var: 0.9993990659713745\n",
      "          vf_loss: 0.006369977661718925\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.38772163120566\n",
      "    ram_util_percent: 65.20443262411348\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04182971076710019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.651508562138698\n",
      "    mean_inference_ms: 1.801080629282822\n",
      "    mean_raw_obs_processing_ms: 106.46173583984564\n",
      "  time_since_restore: 32444.560145139694\n",
      "  time_this_iter_s: 1581.2881264686584\n",
      "  time_total_s: 32444.560145139694\n",
      "  timers:\n",
      "    learn_throughput: 1399.008\n",
      "    learn_time_ms: 714.792\n",
      "    load_throughput: 67989.794\n",
      "    load_time_ms: 14.708\n",
      "    sample_throughput: 0.732\n",
      "    sample_time_ms: 1365963.61\n",
      "    update_time_ms: 5.962\n",
      "  timestamp: 1635050741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 30.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         32444.6</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">    9.91</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                8.85</td><td style=\"text-align: right;\">             13.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_05-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 13.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.856799999999998\n",
      "  episode_reward_min: 7.910000000000001\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1930\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6662444472312914e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2889241811301973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038120317020504915\n",
      "          policy_loss: 0.04745578741033872\n",
      "          total_loss: 0.12193427032066716\n",
      "          vf_explained_var: 0.9932722449302673\n",
      "          vf_loss: 0.07736771301262908\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.10146923783287\n",
      "    ram_util_percent: 65.61515151515151\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04186576074536392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.722978951233337\n",
      "    mean_inference_ms: 1.8021901114348648\n",
      "    mean_raw_obs_processing_ms: 112.15765378496636\n",
      "  time_since_restore: 33970.93442153931\n",
      "  time_this_iter_s: 1526.3742763996124\n",
      "  time_total_s: 33970.93442153931\n",
      "  timers:\n",
      "    learn_throughput: 1387.316\n",
      "    learn_time_ms: 720.816\n",
      "    load_throughput: 63746.225\n",
      "    load_time_ms: 15.687\n",
      "    sample_throughput: 0.716\n",
      "    sample_time_ms: 1395822.226\n",
      "    update_time_ms: 6.24\n",
      "  timestamp: 1635052268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         33970.9</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  9.8568</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                7.91</td><td style=\"text-align: right;\">             13.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_05-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 12.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.8814\n",
      "  episode_reward_min: 7.910000000000001\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1964\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8331222236156457e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.32384351028336417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016790976428133614\n",
      "          policy_loss: -0.2042993684609731\n",
      "          total_loss: -0.06500066758857834\n",
      "          vf_explained_var: 0.8122290968894958\n",
      "          vf_loss: 0.1425371047626767\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15898709036743\n",
      "    ram_util_percent: 65.87696127110226\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041900399778232116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.7423153960789\n",
      "    mean_inference_ms: 1.80280218622815\n",
      "    mean_raw_obs_processing_ms: 114.41439802755903\n",
      "  time_since_restore: 34676.90128874779\n",
      "  time_this_iter_s: 705.9668672084808\n",
      "  time_total_s: 34676.90128874779\n",
      "  timers:\n",
      "    learn_throughput: 1381.779\n",
      "    learn_time_ms: 723.705\n",
      "    load_throughput: 63900.059\n",
      "    load_time_ms: 15.649\n",
      "    sample_throughput: 0.708\n",
      "    sample_time_ms: 1412947.567\n",
      "    update_time_ms: 6.077\n",
      "  timestamp: 1635052974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         34676.9</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">  9.8814</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                7.91</td><td style=\"text-align: right;\">             12.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_05-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 21.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.8139\n",
      "  episode_reward_min: -0.8899999999998761\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 2018\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8331222236156457e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.38682499279578525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.035703702594227456\n",
      "          policy_loss: 0.14559035615788565\n",
      "          total_loss: 0.20002881122959984\n",
      "          vf_explained_var: 0.6456944942474365\n",
      "          vf_loss: 0.05830664682305521\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.80430314707772\n",
      "    ram_util_percent: 66.2776493256262\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04194965682347208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.767701425674208\n",
      "    mean_inference_ms: 1.8037514593023178\n",
      "    mean_raw_obs_processing_ms: 117.81329830852914\n",
      "  time_since_restore: 35768.63994932175\n",
      "  time_this_iter_s: 1091.7386605739594\n",
      "  time_total_s: 35768.63994932175\n",
      "  timers:\n",
      "    learn_throughput: 1378.505\n",
      "    learn_time_ms: 725.424\n",
      "    load_throughput: 66356.645\n",
      "    load_time_ms: 15.07\n",
      "    sample_throughput: 0.707\n",
      "    sample_time_ms: 1414630.08\n",
      "    update_time_ms: 5.717\n",
      "  timestamp: 1635054065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         35768.6</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">  9.8139</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -0.89</td><td style=\"text-align: right;\">             21.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_05-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 12.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.9211\n",
      "  episode_reward_min: 9.800000000000004\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 2071\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7496833354234702e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3849554643034935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008085812231639226\n",
      "          policy_loss: -0.2638840701844957\n",
      "          total_loss: -0.2175902574426598\n",
      "          vf_explained_var: 0.9961357712745667\n",
      "          vf_loss: 0.05014335031931599\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.54896907216495\n",
      "    ram_util_percent: 66.42893041237112\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04197283668849847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.812133654186283\n",
      "    mean_inference_ms: 1.8046917455288487\n",
      "    mean_raw_obs_processing_ms: 121.9587292849823\n",
      "  time_since_restore: 36856.5335829258\n",
      "  time_this_iter_s: 1087.8936336040497\n",
      "  time_total_s: 36856.5335829258\n",
      "  timers:\n",
      "    learn_throughput: 1388.972\n",
      "    learn_time_ms: 719.957\n",
      "    load_throughput: 70956.164\n",
      "    load_time_ms: 14.093\n",
      "    sample_throughput: 0.727\n",
      "    sample_time_ms: 1374867.687\n",
      "    update_time_ms: 5.295\n",
      "  timestamp: 1635055153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         36856.5</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">  9.9211</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                 9.8</td><td style=\"text-align: right;\">             12.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_06-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 22.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.760200000000001\n",
      "  episode_reward_min: -0.6499999999999252\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 2102\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7496833354234702e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9892290532588959\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02493309127863144\n",
      "          policy_loss: 0.1490683061381181\n",
      "          total_loss: 0.41658329806394045\n",
      "          vf_explained_var: 0.9398901462554932\n",
      "          vf_loss: 0.2774072091612551\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.60864745011087\n",
      "    ram_util_percent: 66.58946784922395\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041979434895341367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.83944306867465\n",
      "    mean_inference_ms: 1.805178308310027\n",
      "    mean_raw_obs_processing_ms: 123.76417925395441\n",
      "  time_since_restore: 37488.74149918556\n",
      "  time_this_iter_s: 632.2079162597656\n",
      "  time_total_s: 37488.74149918556\n",
      "  timers:\n",
      "    learn_throughput: 1376.282\n",
      "    learn_time_ms: 726.595\n",
      "    load_throughput: 67618.873\n",
      "    load_time_ms: 14.789\n",
      "    sample_throughput: 0.778\n",
      "    sample_time_ms: 1285395.894\n",
      "    update_time_ms: 6.199\n",
      "  timestamp: 1635055785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         37488.7</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  9.7602</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">               -0.65</td><td style=\"text-align: right;\">             22.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_06-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 15.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.8705\n",
      "  episode_reward_min: 4.340000000000077\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 2173\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.124525003135205e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.165877293712563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0014062915058711785\n",
      "          policy_loss: -0.2633011824554867\n",
      "          total_loss: -0.2536825991339154\n",
      "          vf_explained_var: 0.998932421207428\n",
      "          vf_loss: 0.011277352728777461\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.23363899613899\n",
      "    ram_util_percent: 66.97606177606178\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0419953686034097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.93462808044058\n",
      "    mean_inference_ms: 1.8064221437841967\n",
      "    mean_raw_obs_processing_ms: 129.48646372100131\n",
      "  time_since_restore: 38940.799595594406\n",
      "  time_this_iter_s: 1452.058096408844\n",
      "  time_total_s: 38940.799595594406\n",
      "  timers:\n",
      "    learn_throughput: 1366.561\n",
      "    learn_time_ms: 731.764\n",
      "    load_throughput: 67884.155\n",
      "    load_time_ms: 14.731\n",
      "    sample_throughput: 0.783\n",
      "    sample_time_ms: 1277328.57\n",
      "    update_time_ms: 6.599\n",
      "  timestamp: 1635057238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         38940.8</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">  9.8705</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                4.34</td><td style=\"text-align: right;\">             15.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_06-58-37\n",
      "  done: false\n",
      "  episode_len_mean: 14.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.8124\n",
      "  episode_reward_min: 0.7400000000000462\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 2245\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0622625015676027e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.19991424481074016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006299904571207864\n",
      "          policy_loss: 0.06429068769017855\n",
      "          total_loss: 0.27508265111181474\n",
      "          vf_explained_var: 0.9853004217147827\n",
      "          vf_loss: 0.2127910885752903\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67511848341232\n",
      "    ram_util_percent: 67.16957345971564\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04200010985571057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.024463806873975\n",
      "    mean_inference_ms: 1.8073563890741098\n",
      "    mean_raw_obs_processing_ms: 134.8751205244133\n",
      "  time_since_restore: 40420.019975185394\n",
      "  time_this_iter_s: 1479.2203795909882\n",
      "  time_total_s: 40420.019975185394\n",
      "  timers:\n",
      "    learn_throughput: 1372.085\n",
      "    learn_time_ms: 728.818\n",
      "    load_throughput: 72780.503\n",
      "    load_time_ms: 13.74\n",
      "    sample_throughput: 0.787\n",
      "    sample_time_ms: 1270324.864\n",
      "    update_time_ms: 6.695\n",
      "  timestamp: 1635058717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">           40420</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">  9.8124</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                0.74</td><td style=\"text-align: right;\">             14.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_07-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 12.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.9163\n",
      "  episode_reward_min: 8.900000000000002\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2326\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0622625015676027e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.16564417249626584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0017003969687835684\n",
      "          policy_loss: -0.004265369764632649\n",
      "          total_loss: 0.02809107028361824\n",
      "          vf_explained_var: 0.996944010257721\n",
      "          vf_loss: 0.034012882018254864\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.86772688719253\n",
      "    ram_util_percent: 67.34614079728584\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04201574295414906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.113456308000742\n",
      "    mean_inference_ms: 1.8083880781564994\n",
      "    mean_raw_obs_processing_ms: 141.26124161225724\n",
      "  time_since_restore: 42072.68046307564\n",
      "  time_this_iter_s: 1652.6604878902435\n",
      "  time_total_s: 42072.68046307564\n",
      "  timers:\n",
      "    learn_throughput: 1360.367\n",
      "    learn_time_ms: 735.096\n",
      "    load_throughput: 79487.557\n",
      "    load_time_ms: 12.581\n",
      "    sample_throughput: 0.783\n",
      "    sample_time_ms: 1277021.25\n",
      "    update_time_ms: 7.018\n",
      "  timestamp: 1635060369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         42072.7</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">  9.9163</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                 8.9</td><td style=\"text-align: right;\">             12.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_07-44-22\n",
      "  done: false\n",
      "  episode_len_mean: 15.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.832200000000002\n",
      "  episode_reward_min: 0.43000000000005323\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 2379\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0311312507838013e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3514874868922763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005962281604859814\n",
      "          policy_loss: -0.13915971351994408\n",
      "          total_loss: -0.008072002728780111\n",
      "          vf_explained_var: 0.9892051815986633\n",
      "          vf_loss: 0.13460258150266277\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.76401539448362\n",
      "    ram_util_percent: 67.57081462475946\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04202032282505837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.1781083860957\n",
      "    mean_inference_ms: 1.808973132185654\n",
      "    mean_raw_obs_processing_ms: 144.35516695476798\n",
      "  time_since_restore: 43165.51201367378\n",
      "  time_this_iter_s: 1092.8315505981445\n",
      "  time_total_s: 43165.51201367378\n",
      "  timers:\n",
      "    learn_throughput: 1362.529\n",
      "    learn_time_ms: 733.929\n",
      "    load_throughput: 86611.931\n",
      "    load_time_ms: 11.546\n",
      "    sample_throughput: 0.813\n",
      "    sample_time_ms: 1229464.689\n",
      "    update_time_ms: 7.343\n",
      "  timestamp: 1635061462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         43165.5</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  9.8322</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                0.43</td><td style=\"text-align: right;\">             15.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_aa44d_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-24_07-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 15.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.940000000000001\n",
      "  episode_reward_mean: 9.821800000000001\n",
      "  episode_reward_min: 0.43000000000005323\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 2422\n",
      "  experiment_id: d089d148212147fca860827e989d529c\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0311312507838013e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4883825722667906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010021544252595464\n",
      "          policy_loss: -0.3225179077850448\n",
      "          total_loss: -0.15349496122863557\n",
      "          vf_explained_var: 0.9670743942260742\n",
      "          vf_loss: 0.17390676839277147\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.24721792890263\n",
      "    ram_util_percent: 67.75370942812982\n",
      "  pid: 343315\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04202723538809931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.24017953626315\n",
      "    mean_inference_ms: 1.8096487372015628\n",
      "    mean_raw_obs_processing_ms: 147.19502935060177\n",
      "  time_since_restore: 44072.836792230606\n",
      "  time_this_iter_s: 907.3247785568237\n",
      "  time_total_s: 44072.836792230606\n",
      "  timers:\n",
      "    learn_throughput: 1372.547\n",
      "    learn_time_ms: 728.572\n",
      "    load_throughput: 87230.497\n",
      "    load_time_ms: 11.464\n",
      "    sample_throughput: 0.861\n",
      "    sample_time_ms: 1162074.274\n",
      "    update_time_ms: 6.795\n",
      "  timestamp: 1635062370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: aa44d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.1 GiB heap, 0.0/11.55 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-23_19-44-41<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_aa44d_00000</td><td>RUNNING </td><td>192.168.3.5:343315</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         44072.8</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">  9.8218</td><td style=\"text-align: right;\">                9.94</td><td style=\"text-align: right;\">                0.43</td><td style=\"text-align: right;\">             15.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=343312)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-24 08:02:00,288\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-10-24 08:02:00,288\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "Process _WandbLoggingProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/integration/wandb.py\", line 200, in run\n",
      "    result = self.queue.get()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    p.join()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt\n",
      "Process wandb_internal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 152, in wandb_internal\n",
      "    thread.join()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "2021-10-24 08:02:01,263\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_343202/3896502348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         },\n\u001b[0;32m---> 29\u001b[0;31m         loggers=[WandbLogger])\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             trial = self.trial_executor.get_next_available_trial(\n\u001b[0;32m--> 675\u001b[0;31m                 timeout=timeout)  # blocking\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\":  \"PPO C32 pretrained (frozen AngelaCNN + MLP) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
