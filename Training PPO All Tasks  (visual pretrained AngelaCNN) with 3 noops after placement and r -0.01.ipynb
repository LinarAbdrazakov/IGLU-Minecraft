{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592760ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362368"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_features_dim = 512\n",
    "target_features_dim = 9 * 11 * 11\n",
    "policy_hidden_dim = 256 \n",
    "\n",
    "policy_network = nn.Sequential(\n",
    "    nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    #nn.ELU(),\n",
    ")\n",
    "\n",
    "sum(p.numel() for p in policy_network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tasks = []\n",
    "for i in range(1,156):\n",
    "    if ('C'+str(i)) == 'C38': continue\n",
    "    tasks.append('C'+str(i))\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        if abs(rew) == 1:\n",
    "            rew /= 10\n",
    "            \n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=250)\n",
    "    env.update_taskset(TaskSet(preset=tasks))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 18.6/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 11:02:20,138\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-11-07 11:02:20,186\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id a82ef_00000 but id 2d626_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO All Tasks pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/2d626_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/2d626_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211107_110220-2d626_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m 2021-11-07 11:02:25,484\tWARNING ppo.py:143 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1666.\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m 2021-11-07 11:02:25,484\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m 2021-11-07 11:02:25,484\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m 2021-11-07 11:02:36,843\tINFO trainable.py:109 -- Trainable.setup took 15.320 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=480810)\u001b[0m 2021-11-07 11:02:36,845\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 9996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.33673469387755\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.7100000000000017\n",
      "  episode_reward_mean: -0.8501020408163271\n",
      "  episode_reward_min: -1.5200000000000007\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 98\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8824299178571784\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.006863669508837896\n",
      "          policy_loss: -0.015661495382714476\n",
      "          total_loss: -0.013310577764979795\n",
      "          vf_explained_var: -0.3116755485534668\n",
      "          vf_loss: 0.02980248174081859\n",
      "    num_agent_steps_sampled: 9996\n",
      "    num_agent_steps_trained: 9996\n",
      "    num_steps_sampled: 9996\n",
      "    num_steps_trained: 9996\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45925925925927\n",
      "    ram_util_percent: 25.122222222222224\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07220444631698289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.765588813263484\n",
      "    mean_inference_ms: 4.398790568419349\n",
      "    mean_raw_obs_processing_ms: 0.7972040556361766\n",
      "  time_since_restore: 207.76151871681213\n",
      "  time_this_iter_s: 207.76151871681213\n",
      "  time_total_s: 207.76151871681213\n",
      "  timers:\n",
      "    learn_throughput: 867.533\n",
      "    learn_time_ms: 11522.322\n",
      "    load_throughput: 81692.892\n",
      "    load_time_ms: 122.361\n",
      "    sample_throughput: 50.975\n",
      "    sample_time_ms: 196095.071\n",
      "    update_time_ms: 6.702\n",
      "  timestamp: 1636283164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9996\n",
      "  training_iteration: 1\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         207.762</td><td style=\"text-align: right;\">9996</td><td style=\"text-align: right;\">-0.850102</td><td style=\"text-align: right;\">                2.71</td><td style=\"text-align: right;\">               -1.52</td><td style=\"text-align: right;\">           100.337</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 19992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 98.63725490196079\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.980000000000009\n",
      "  episode_reward_mean: -0.6757843137254905\n",
      "  episode_reward_min: -1.5299999999999994\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 200\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8692716274506007\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008616683900278067\n",
      "          policy_loss: -0.022889049405343514\n",
      "          total_loss: 0.0355423043983487\n",
      "          vf_explained_var: 0.005149574019014835\n",
      "          vf_loss: 0.08540073371607747\n",
      "    num_agent_steps_sampled: 19992\n",
      "    num_agent_steps_trained: 19992\n",
      "    num_steps_sampled: 19992\n",
      "    num_steps_trained: 19992\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.3841584158416\n",
      "    ram_util_percent: 28.167326732673263\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07103266875292824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.77327907308291\n",
      "    mean_inference_ms: 4.3132812339293745\n",
      "    mean_raw_obs_processing_ms: 0.7852096394049202\n",
      "  time_since_restore: 349.5984799861908\n",
      "  time_this_iter_s: 141.83696126937866\n",
      "  time_total_s: 349.5984799861908\n",
      "  timers:\n",
      "    learn_throughput: 901.791\n",
      "    learn_time_ms: 11084.606\n",
      "    load_throughput: 73690.075\n",
      "    load_time_ms: 135.649\n",
      "    sample_throughput: 61.117\n",
      "    sample_time_ms: 163556.343\n",
      "    update_time_ms: 7.392\n",
      "  timestamp: 1636283306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19992\n",
      "  training_iteration: 2\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.4/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         349.598</td><td style=\"text-align: right;\">19992</td><td style=\"text-align: right;\">-0.675784</td><td style=\"text-align: right;\">                4.98</td><td style=\"text-align: right;\">               -1.53</td><td style=\"text-align: right;\">           98.6373</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 29988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-10-46\n",
      "  done: false\n",
      "  episode_len_mean: 96.68932038834951\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.760000000000003\n",
      "  episode_reward_mean: 0.14650485436893218\n",
      "  episode_reward_min: -2.1599999999999997\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 303\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8413309853301087\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012549451784464913\n",
      "          policy_loss: -0.02715505621372125\n",
      "          total_loss: 0.1572444661297541\n",
      "          vf_explained_var: 0.2533511221408844\n",
      "          vf_loss: 0.21030294048225778\n",
      "    num_agent_steps_sampled: 29988\n",
      "    num_agent_steps_trained: 29988\n",
      "    num_steps_sampled: 29988\n",
      "    num_steps_trained: 29988\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.06030150753769\n",
      "    ram_util_percent: 28.23015075376884\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06978640949232881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.64717454207063\n",
      "    mean_inference_ms: 4.234904932956853\n",
      "    mean_raw_obs_processing_ms: 0.7937374495611215\n",
      "  time_since_restore: 489.2336859703064\n",
      "  time_this_iter_s: 139.6352059841156\n",
      "  time_total_s: 489.2336859703064\n",
      "  timers:\n",
      "    learn_throughput: 922.028\n",
      "    learn_time_ms: 10841.321\n",
      "    load_throughput: 66265.314\n",
      "    load_time_ms: 150.848\n",
      "    sample_throughput: 65.736\n",
      "    sample_time_ms: 152063.823\n",
      "    update_time_ms: 7.489\n",
      "  timestamp: 1636283446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29988\n",
      "  training_iteration: 3\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         489.234</td><td style=\"text-align: right;\">29988</td><td style=\"text-align: right;\">0.146505</td><td style=\"text-align: right;\">                4.76</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">           96.6893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 39984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-13-47\n",
      "  done: false\n",
      "  episode_len_mean: 94.99056603773585\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.950000000000011\n",
      "  episode_reward_mean: -0.020377358490565843\n",
      "  episode_reward_min: -1.8600000000000012\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 409\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8288764187413404\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01456120752207934\n",
      "          policy_loss: -0.03160574484266277\n",
      "          total_loss: 0.1447193642385686\n",
      "          vf_explained_var: 0.32688796520233154\n",
      "          vf_loss: 0.2017016316461576\n",
      "    num_agent_steps_sampled: 39984\n",
      "    num_agent_steps_trained: 39984\n",
      "    num_steps_sampled: 39984\n",
      "    num_steps_trained: 39984\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.63050193050192\n",
      "    ram_util_percent: 28.25057915057915\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06937348423469597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.62723290131213\n",
      "    mean_inference_ms: 4.212219271073564\n",
      "    mean_raw_obs_processing_ms: 2.1579282784763123\n",
      "  time_since_restore: 670.6501774787903\n",
      "  time_this_iter_s: 181.4164915084839\n",
      "  time_total_s: 670.6501774787903\n",
      "  timers:\n",
      "    learn_throughput: 920.369\n",
      "    learn_time_ms: 10860.861\n",
      "    load_throughput: 70309.98\n",
      "    load_time_ms: 142.17\n",
      "    sample_throughput: 63.816\n",
      "    sample_time_ms: 156637.256\n",
      "    update_time_ms: 7.541\n",
      "  timestamp: 1636283627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39984\n",
      "  training_iteration: 4\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          670.65</td><td style=\"text-align: right;\">39984</td><td style=\"text-align: right;\">-0.0203774</td><td style=\"text-align: right;\">                4.95</td><td style=\"text-align: right;\">               -1.86</td><td style=\"text-align: right;\">           94.9906</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 49980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 99.20792079207921\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.740000000000011\n",
      "  episode_reward_mean: 0.3672277227722781\n",
      "  episode_reward_min: -1.800000000000001\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 510\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7900016894707313\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017356929272214057\n",
      "          policy_loss: -0.034028121275015366\n",
      "          total_loss: 0.21365480529001127\n",
      "          vf_explained_var: 0.49137574434280396\n",
      "          vf_loss: 0.27211155756416483\n",
      "    num_agent_steps_sampled: 49980\n",
      "    num_agent_steps_trained: 49980\n",
      "    num_steps_sampled: 49980\n",
      "    num_steps_trained: 49980\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.33301435406698\n",
      "    ram_util_percent: 28.783732057416266\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06909172295729349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.91087980232715\n",
      "    mean_inference_ms: 4.196678031615066\n",
      "    mean_raw_obs_processing_ms: 1.8978262071377736\n",
      "  time_since_restore: 816.6428463459015\n",
      "  time_this_iter_s: 145.9926688671112\n",
      "  time_total_s: 816.6428463459015\n",
      "  timers:\n",
      "    learn_throughput: 919.557\n",
      "    learn_time_ms: 10870.453\n",
      "    load_throughput: 73311.966\n",
      "    load_time_ms: 136.349\n",
      "    sample_throughput: 65.635\n",
      "    sample_time_ms: 152297.194\n",
      "    update_time_ms: 8.014\n",
      "  timestamp: 1636283773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49980\n",
      "  training_iteration: 5\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         816.643</td><td style=\"text-align: right;\">49980</td><td style=\"text-align: right;\">0.367228</td><td style=\"text-align: right;\">                6.74</td><td style=\"text-align: right;\">                -1.8</td><td style=\"text-align: right;\">           99.2079</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 59976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 101.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.700000000000011\n",
      "  episode_reward_mean: 0.7994000000000016\n",
      "  episode_reward_min: -1.890000000000001\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 607\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7651500689677704\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0169888024268638\n",
      "          policy_loss: -0.02865576571983914\n",
      "          total_loss: 0.2669947608947181\n",
      "          vf_explained_var: 0.526110053062439\n",
      "          vf_loss: 0.3199042649032214\n",
      "    num_agent_steps_sampled: 59976\n",
      "    num_agent_steps_trained: 59976\n",
      "    num_steps_sampled: 59976\n",
      "    num_steps_trained: 59976\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.50616113744076\n",
      "    ram_util_percent: 28.844549763033175\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928097047746506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.4178066421514\n",
      "    mean_inference_ms: 4.206187814511859\n",
      "    mean_raw_obs_processing_ms: 1.7313219118628924\n",
      "  time_since_restore: 964.9111173152924\n",
      "  time_this_iter_s: 148.26827096939087\n",
      "  time_total_s: 964.9111173152924\n",
      "  timers:\n",
      "    learn_throughput: 915.704\n",
      "    learn_time_ms: 10916.193\n",
      "    load_throughput: 74756.392\n",
      "    load_time_ms: 133.714\n",
      "    sample_throughput: 66.754\n",
      "    sample_time_ms: 149744.473\n",
      "    update_time_ms: 8.058\n",
      "  timestamp: 1636283922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59976\n",
      "  training_iteration: 6\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         964.911</td><td style=\"text-align: right;\">59976</td><td style=\"text-align: right;\">  0.7994</td><td style=\"text-align: right;\">                 4.7</td><td style=\"text-align: right;\">               -1.89</td><td style=\"text-align: right;\">            101.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 69972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 99.87128712871286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.68000000000001\n",
      "  episode_reward_mean: 1.159405940594062\n",
      "  episode_reward_min: -1.980000000000001\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 708\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7489571300327267\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01725398939928771\n",
      "          policy_loss: -0.030773013384423703\n",
      "          total_loss: 0.2902104803273438\n",
      "          vf_explained_var: 0.4236692786216736\n",
      "          vf_loss: 0.3450222675807965\n",
      "    num_agent_steps_sampled: 69972\n",
      "    num_agent_steps_trained: 69972\n",
      "    num_steps_sampled: 69972\n",
      "    num_steps_trained: 69972\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45462184873949\n",
      "    ram_util_percent: 28.793697478991593\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929942484820809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.20792371033293\n",
      "    mean_inference_ms: 4.206742492965954\n",
      "    mean_raw_obs_processing_ms: 1.8907364785595957\n",
      "  time_since_restore: 1131.3642938137054\n",
      "  time_this_iter_s: 166.4531764984131\n",
      "  time_total_s: 1131.3642938137054\n",
      "  timers:\n",
      "    learn_throughput: 911.201\n",
      "    learn_time_ms: 10970.139\n",
      "    load_throughput: 76121.002\n",
      "    load_time_ms: 131.317\n",
      "    sample_throughput: 66.42\n",
      "    sample_time_ms: 150497.918\n",
      "    update_time_ms: 7.977\n",
      "  timestamp: 1636284088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69972\n",
      "  training_iteration: 7\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         1131.36</td><td style=\"text-align: right;\">69972</td><td style=\"text-align: right;\"> 1.15941</td><td style=\"text-align: right;\">                8.68</td><td style=\"text-align: right;\">               -1.98</td><td style=\"text-align: right;\">           99.8713</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 79968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.430000000000009\n",
      "  episode_reward_mean: 1.1632000000000022\n",
      "  episode_reward_min: -2.3299999999999983\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 807\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7295347162800976\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017289181047384604\n",
      "          policy_loss: -0.029841385960069477\n",
      "          total_loss: 0.32073425146329226\n",
      "          vf_explained_var: 0.3982108235359192\n",
      "          vf_loss: 0.3744131489728506\n",
      "    num_agent_steps_sampled: 79968\n",
      "    num_agent_steps_trained: 79968\n",
      "    num_steps_sampled: 79968\n",
      "    num_steps_trained: 79968\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.4715909090909\n",
      "    ram_util_percent: 28.706060606060603\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06985601368272337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.482030978609856\n",
      "    mean_inference_ms: 4.242832242829102\n",
      "    mean_raw_obs_processing_ms: 2.2112433733348924\n",
      "  time_since_restore: 1316.574904680252\n",
      "  time_this_iter_s: 185.21061086654663\n",
      "  time_total_s: 1316.574904680252\n",
      "  timers:\n",
      "    learn_throughput: 897.745\n",
      "    learn_time_ms: 11134.568\n",
      "    load_throughput: 73565.011\n",
      "    load_time_ms: 135.88\n",
      "    sample_throughput: 65.215\n",
      "    sample_time_ms: 153276.697\n",
      "    update_time_ms: 8.149\n",
      "  timestamp: 1636284273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79968\n",
      "  training_iteration: 8\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1316.57</td><td style=\"text-align: right;\">79968</td><td style=\"text-align: right;\">  1.1632</td><td style=\"text-align: right;\">                8.43</td><td style=\"text-align: right;\">               -2.33</td><td style=\"text-align: right;\">            100.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 89964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-27-19\n",
      "  done: false\n",
      "  episode_len_mean: 99.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.900000000000011\n",
      "  episode_reward_mean: 1.2344000000000026\n",
      "  episode_reward_min: -1.960000000000001\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 907\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.713480949605632\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0183344661856083\n",
      "          policy_loss: -0.03112746148298566\n",
      "          total_loss: 0.32692285677752436\n",
      "          vf_explained_var: 0.4642910361289978\n",
      "          vf_loss: 0.3815182348792879\n",
      "    num_agent_steps_sampled: 89964\n",
      "    num_agent_steps_trained: 89964\n",
      "    num_steps_sampled: 89964\n",
      "    num_steps_trained: 89964\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.36962025316456\n",
      "    ram_util_percent: 28.997468354430378\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0701916727014332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.80777387058688\n",
      "    mean_inference_ms: 4.274560974758\n",
      "    mean_raw_obs_processing_ms: 2.0803204163605367\n",
      "  time_since_restore: 1482.463607788086\n",
      "  time_this_iter_s: 165.88870310783386\n",
      "  time_total_s: 1482.463607788086\n",
      "  timers:\n",
      "    learn_throughput: 886.195\n",
      "    learn_time_ms: 11279.684\n",
      "    load_throughput: 73647.905\n",
      "    load_time_ms: 135.727\n",
      "    sample_throughput: 65.215\n",
      "    sample_time_ms: 153277.713\n",
      "    update_time_ms: 8.163\n",
      "  timestamp: 1636284439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89964\n",
      "  training_iteration: 9\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1482.46</td><td style=\"text-align: right;\">89964</td><td style=\"text-align: right;\">  1.2344</td><td style=\"text-align: right;\">                 8.9</td><td style=\"text-align: right;\">               -1.96</td><td style=\"text-align: right;\">             99.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 99960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-30-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.690000000000014\n",
      "  episode_reward_mean: 1.3341000000000034\n",
      "  episode_reward_min: -2.3399999999999985\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1006\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6809067689455475\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01793057939114074\n",
      "          policy_loss: -0.03519265549655399\n",
      "          total_loss: 0.2704306389658879\n",
      "          vf_explained_var: 0.592963457107544\n",
      "          vf_loss: 0.3288462467873708\n",
      "    num_agent_steps_sampled: 99960\n",
      "    num_agent_steps_trained: 99960\n",
      "    num_steps_sampled: 99960\n",
      "    num_steps_trained: 99960\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.58857142857143\n",
      "    ram_util_percent: 28.968571428571426\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07065022671643677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.089157784299054\n",
      "    mean_inference_ms: 4.3106338582575505\n",
      "    mean_raw_obs_processing_ms: 1.9719699102649582\n",
      "  time_since_restore: 1654.5358712673187\n",
      "  time_this_iter_s: 172.0722634792328\n",
      "  time_total_s: 1654.5358712673187\n",
      "  timers:\n",
      "    learn_throughput: 877.762\n",
      "    learn_time_ms: 11388.053\n",
      "    load_throughput: 73492.726\n",
      "    load_time_ms: 136.013\n",
      "    sample_throughput: 64.95\n",
      "    sample_time_ms: 153904.177\n",
      "    update_time_ms: 8.264\n",
      "  timestamp: 1636284611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99960\n",
      "  training_iteration: 10\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1654.54</td><td style=\"text-align: right;\">99960</td><td style=\"text-align: right;\">  1.3341</td><td style=\"text-align: right;\">                6.69</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">            100.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 109956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-33-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.240000000000016\n",
      "  episode_reward_mean: 1.614600000000004\n",
      "  episode_reward_min: -2.159999999999998\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 1106\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.662437510286641\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.021484141610822162\n",
      "          policy_loss: -0.03407769135844249\n",
      "          total_loss: 0.30724473188225276\n",
      "          vf_explained_var: 0.674784243106842\n",
      "          vf_loss: 0.36364996986001985\n",
      "    num_agent_steps_sampled: 109956\n",
      "    num_agent_steps_trained: 109956\n",
      "    num_steps_sampled: 109956\n",
      "    num_steps_trained: 109956\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.82441860465116\n",
      "    ram_util_percent: 28.78682170542636\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07076818601589388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.246086091828445\n",
      "    mean_inference_ms: 4.327700691841357\n",
      "    mean_raw_obs_processing_ms: 2.373049923255655\n",
      "  time_since_restore: 1835.0761678218842\n",
      "  time_this_iter_s: 180.54029655456543\n",
      "  time_total_s: 1835.0761678218842\n",
      "  timers:\n",
      "    learn_throughput: 877.208\n",
      "    learn_time_ms: 11395.247\n",
      "    load_throughput: 73635.174\n",
      "    load_time_ms: 135.75\n",
      "    sample_throughput: 66.123\n",
      "    sample_time_ms: 151173.778\n",
      "    update_time_ms: 8.991\n",
      "  timestamp: 1636284792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109956\n",
      "  training_iteration: 11\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         1835.08</td><td style=\"text-align: right;\">109956</td><td style=\"text-align: right;\">  1.6146</td><td style=\"text-align: right;\">               12.24</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">            100.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 119952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.240000000000016\n",
      "  episode_reward_mean: 1.202200000000004\n",
      "  episode_reward_min: -1.6700000000000008\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1205\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6180043240897675\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019916820623234393\n",
      "          policy_loss: -0.03613648228984103\n",
      "          total_loss: 0.3193798809001843\n",
      "          vf_explained_var: 0.5948522686958313\n",
      "          vf_loss: 0.3757213587562243\n",
      "    num_agent_steps_sampled: 119952\n",
      "    num_agent_steps_trained: 119952\n",
      "    num_steps_sampled: 119952\n",
      "    num_steps_trained: 119952\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.78108108108108\n",
      "    ram_util_percent: 28.93828828828828\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07068772963165061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.187266481004905\n",
      "    mean_inference_ms: 4.324302375423466\n",
      "    mean_raw_obs_processing_ms: 2.2541488291693055\n",
      "  time_since_restore: 1990.7732758522034\n",
      "  time_this_iter_s: 155.6971080303192\n",
      "  time_total_s: 1990.7732758522034\n",
      "  timers:\n",
      "    learn_throughput: 867.231\n",
      "    learn_time_ms: 11526.346\n",
      "    load_throughput: 73501.088\n",
      "    load_time_ms: 135.998\n",
      "    sample_throughput: 65.579\n",
      "    sample_time_ms: 152427.921\n",
      "    update_time_ms: 9.139\n",
      "  timestamp: 1636284948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119952\n",
      "  training_iteration: 12\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.3/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         1990.77</td><td style=\"text-align: right;\">119952</td><td style=\"text-align: right;\">  1.2022</td><td style=\"text-align: right;\">                6.24</td><td style=\"text-align: right;\">               -1.67</td><td style=\"text-align: right;\">            100.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 129948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 102.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.010000000000018\n",
      "  episode_reward_mean: 1.5771000000000046\n",
      "  episode_reward_min: -1.9700000000000006\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1304\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.614659170207814\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.021549868906797946\n",
      "          policy_loss: -0.034046648546225494\n",
      "          total_loss: 0.3504248603605307\n",
      "          vf_explained_var: 0.605085551738739\n",
      "          vf_loss: 0.40415314065340235\n",
      "    num_agent_steps_sampled: 129948\n",
      "    num_agent_steps_trained: 129948\n",
      "    num_steps_sampled: 129948\n",
      "    num_steps_trained: 129948\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.75967741935484\n",
      "    ram_util_percent: 29.033870967741937\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07069975663571543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.12809300706944\n",
      "    mean_inference_ms: 4.328322060876286\n",
      "    mean_raw_obs_processing_ms: 2.299783783279373\n",
      "  time_since_restore: 2164.8159737586975\n",
      "  time_this_iter_s: 174.04269790649414\n",
      "  time_total_s: 2164.8159737586975\n",
      "  timers:\n",
      "    learn_throughput: 854.116\n",
      "    learn_time_ms: 11703.332\n",
      "    load_throughput: 74673.296\n",
      "    load_time_ms: 133.863\n",
      "    sample_throughput: 64.204\n",
      "    sample_time_ms: 155692.355\n",
      "    update_time_ms: 9.715\n",
      "  timestamp: 1636285122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129948\n",
      "  training_iteration: 13\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         2164.82</td><td style=\"text-align: right;\">129948</td><td style=\"text-align: right;\">  1.5771</td><td style=\"text-align: right;\">                8.01</td><td style=\"text-align: right;\">               -1.97</td><td style=\"text-align: right;\">            102.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 139944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 102.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.35000000000001\n",
      "  episode_reward_mean: 1.7069000000000047\n",
      "  episode_reward_min: -2.219999999999996\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 1401\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5943965365744046\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01987462641473992\n",
      "          policy_loss: -0.03398648996988678\n",
      "          total_loss: 0.3431404800209989\n",
      "          vf_explained_var: 0.6039495468139648\n",
      "          vf_loss: 0.3941273544206578\n",
      "    num_agent_steps_sampled: 139944\n",
      "    num_agent_steps_trained: 139944\n",
      "    num_steps_sampled: 139944\n",
      "    num_steps_trained: 139944\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.58705357142857\n",
      "    ram_util_percent: 28.989732142857147\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0708151088422268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.0601736771645\n",
      "    mean_inference_ms: 4.3308965086976805\n",
      "    mean_raw_obs_processing_ms: 2.218355120791871\n",
      "  time_since_restore: 2321.3941671848297\n",
      "  time_this_iter_s: 156.5781934261322\n",
      "  time_total_s: 2321.3941671848297\n",
      "  timers:\n",
      "    learn_throughput: 846.405\n",
      "    learn_time_ms: 11809.948\n",
      "    load_throughput: 72999.131\n",
      "    load_time_ms: 136.933\n",
      "    sample_throughput: 65.291\n",
      "    sample_time_ms: 153098.665\n",
      "    update_time_ms: 9.845\n",
      "  timestamp: 1636285278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139944\n",
      "  training_iteration: 14\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         2321.39</td><td style=\"text-align: right;\">139944</td><td style=\"text-align: right;\">  1.7069</td><td style=\"text-align: right;\">                8.35</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">            102.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 149940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 101.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.870000000000019\n",
      "  episode_reward_mean: 2.2744000000000058\n",
      "  episode_reward_min: -2.3000000000000003\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1500\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5963516989324846\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.021240241890703844\n",
      "          policy_loss: -0.03041607791510148\n",
      "          total_loss: 0.37552797342133193\n",
      "          vf_explained_var: 0.6254737973213196\n",
      "          vf_loss: 0.42234946002817564\n",
      "    num_agent_steps_sampled: 149940\n",
      "    num_agent_steps_trained: 149940\n",
      "    num_steps_sampled: 149940\n",
      "    num_steps_trained: 149940\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.68129496402877\n",
      "    ram_util_percent: 28.941366906474816\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07076896355009743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.117583791176806\n",
      "    mean_inference_ms: 4.335777912322004\n",
      "    mean_raw_obs_processing_ms: 2.566133734020764\n",
      "  time_since_restore: 2516.319385766983\n",
      "  time_this_iter_s: 194.92521858215332\n",
      "  time_total_s: 2516.319385766983\n",
      "  timers:\n",
      "    learn_throughput: 838.288\n",
      "    learn_time_ms: 11924.301\n",
      "    load_throughput: 72407.393\n",
      "    load_time_ms: 138.052\n",
      "    sample_throughput: 63.315\n",
      "    sample_time_ms: 157876.694\n",
      "    update_time_ms: 10.264\n",
      "  timestamp: 1636285473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149940\n",
      "  training_iteration: 15\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.2/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         2516.32</td><td style=\"text-align: right;\">149940</td><td style=\"text-align: right;\">  2.2744</td><td style=\"text-align: right;\">               10.87</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">            101.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 159936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 102.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.360000000000015\n",
      "  episode_reward_mean: 1.8732000000000053\n",
      "  episode_reward_min: -2.05\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 1598\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.59061807139307\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01842378289118406\n",
      "          policy_loss: -0.03514813760566151\n",
      "          total_loss: 0.31956948150331393\n",
      "          vf_explained_var: 0.6440045833587646\n",
      "          vf_loss: 0.3681877443805719\n",
      "    num_agent_steps_sampled: 159936\n",
      "    num_agent_steps_trained: 159936\n",
      "    num_steps_sampled: 159936\n",
      "    num_steps_trained: 159936\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.18508771929824\n",
      "    ram_util_percent: 29.075877192982457\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07093145833745766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.15949987150283\n",
      "    mean_inference_ms: 4.339913578150999\n",
      "    mean_raw_obs_processing_ms: 2.4825408222694287\n",
      "  time_since_restore: 2676.0265853405\n",
      "  time_this_iter_s: 159.70719957351685\n",
      "  time_total_s: 2676.0265853405\n",
      "  timers:\n",
      "    learn_throughput: 831.099\n",
      "    learn_time_ms: 12027.445\n",
      "    load_throughput: 71952.083\n",
      "    load_time_ms: 138.926\n",
      "    sample_throughput: 62.901\n",
      "    sample_time_ms: 158915.743\n",
      "    update_time_ms: 10.545\n",
      "  timestamp: 1636285633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159936\n",
      "  training_iteration: 16\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         2676.03</td><td style=\"text-align: right;\">159936</td><td style=\"text-align: right;\">  1.8732</td><td style=\"text-align: right;\">                8.36</td><td style=\"text-align: right;\">               -2.05</td><td style=\"text-align: right;\">            102.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 169932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-49-53\n",
      "  done: false\n",
      "  episode_len_mean: 102.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14.260000000000007\n",
      "  episode_reward_mean: 2.000500000000006\n",
      "  episode_reward_min: -2.279999999999997\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 1695\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5483795161940095\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.022659175432540844\n",
      "          policy_loss: -0.031090029104588888\n",
      "          total_loss: 0.36387887579572\n",
      "          vf_explained_var: 0.6696630120277405\n",
      "          vf_loss: 0.4051577560762819\n",
      "    num_agent_steps_sampled: 169932\n",
      "    num_agent_steps_trained: 169932\n",
      "    num_steps_sampled: 169932\n",
      "    num_steps_trained: 169932\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.64649122807016\n",
      "    ram_util_percent: 28.856578947368423\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07090373311964175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.13583139749177\n",
      "    mean_inference_ms: 4.342762984067972\n",
      "    mean_raw_obs_processing_ms: 2.3933418062362164\n",
      "  time_since_restore: 2835.847177505493\n",
      "  time_this_iter_s: 159.8205921649933\n",
      "  time_total_s: 2835.847177505493\n",
      "  timers:\n",
      "    learn_throughput: 823.256\n",
      "    learn_time_ms: 12142.031\n",
      "    load_throughput: 70975.762\n",
      "    load_time_ms: 140.837\n",
      "    sample_throughput: 63.211\n",
      "    sample_time_ms: 158135.906\n",
      "    update_time_ms: 10.624\n",
      "  timestamp: 1636285793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169932\n",
      "  training_iteration: 17\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         2835.85</td><td style=\"text-align: right;\">169932</td><td style=\"text-align: right;\">  2.0005</td><td style=\"text-align: right;\">               14.26</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">            102.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 179928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 97.72549019607843\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.610000000000015\n",
      "  episode_reward_mean: 1.9702941176470643\n",
      "  episode_reward_min: -1.8300000000000007\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 1797\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5413418700552395\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.017422394847111736\n",
      "          policy_loss: -0.03603555550559973\n",
      "          total_loss: 0.35187148137225044\n",
      "          vf_explained_var: 0.6744968891143799\n",
      "          vf_loss: 0.39568027912551523\n",
      "    num_agent_steps_sampled: 179928\n",
      "    num_agent_steps_trained: 179928\n",
      "    num_steps_sampled: 179928\n",
      "    num_steps_trained: 179928\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.2468503937008\n",
      "    ram_util_percent: 28.746062992125985\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07086954532835746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.17436772498556\n",
      "    mean_inference_ms: 4.3401705021598\n",
      "    mean_raw_obs_processing_ms: 2.5833828899993434\n",
      "  time_since_restore: 3013.98228764534\n",
      "  time_this_iter_s: 178.1351101398468\n",
      "  time_total_s: 3013.98228764534\n",
      "  timers:\n",
      "    learn_throughput: 832.458\n",
      "    learn_time_ms: 12007.807\n",
      "    load_throughput: 73570.284\n",
      "    load_time_ms: 135.87\n",
      "    sample_throughput: 63.439\n",
      "    sample_time_ms: 157567.88\n",
      "    update_time_ms: 10.474\n",
      "  timestamp: 1636285971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179928\n",
      "  training_iteration: 18\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 31.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         3013.98</td><td style=\"text-align: right;\">179928</td><td style=\"text-align: right;\"> 1.97029</td><td style=\"text-align: right;\">               10.61</td><td style=\"text-align: right;\">               -1.83</td><td style=\"text-align: right;\">           97.7255</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 189924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 99.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.290000000000017\n",
      "  episode_reward_mean: 2.283800000000006\n",
      "  episode_reward_min: -1.890000000000001\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 1897\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5340015947309316\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01759126624453554\n",
      "          policy_loss: -0.03222607903373547\n",
      "          total_loss: 0.33446433681867316\n",
      "          vf_explained_var: 0.6947193741798401\n",
      "          vf_loss: 0.37421927476922673\n",
      "    num_agent_steps_sampled: 189924\n",
      "    num_agent_steps_trained: 189924\n",
      "    num_steps_sampled: 189924\n",
      "    num_steps_trained: 189924\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.55283842794759\n",
      "    ram_util_percent: 28.75764192139738\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07082356739892283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.105657095390086\n",
      "    mean_inference_ms: 4.339316764702838\n",
      "    mean_raw_obs_processing_ms: 2.4938490440902394\n",
      "  time_since_restore: 3174.495966911316\n",
      "  time_this_iter_s: 160.51367926597595\n",
      "  time_total_s: 3174.495966911316\n",
      "  timers:\n",
      "    learn_throughput: 840.565\n",
      "    learn_time_ms: 11891.995\n",
      "    load_throughput: 73577.424\n",
      "    load_time_ms: 135.857\n",
      "    sample_throughput: 63.61\n",
      "    sample_time_ms: 157146.13\n",
      "    update_time_ms: 10.641\n",
      "  timestamp: 1636286132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189924\n",
      "  training_iteration: 19\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">          3174.5</td><td style=\"text-align: right;\">189924</td><td style=\"text-align: right;\">  2.2838</td><td style=\"text-align: right;\">               12.29</td><td style=\"text-align: right;\">               -1.89</td><td style=\"text-align: right;\">             99.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 199920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_11-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 99.94059405940594\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.75000000000001\n",
      "  episode_reward_mean: 2.449504950495056\n",
      "  episode_reward_min: -2.08\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 1998\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.500853073087513\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018609703954739022\n",
      "          policy_loss: -0.02951853795088509\n",
      "          total_loss: 0.3500171518558238\n",
      "          vf_explained_var: 0.7009139060974121\n",
      "          vf_loss: 0.3857018941838262\n",
      "    num_agent_steps_sampled: 199920\n",
      "    num_agent_steps_trained: 199920\n",
      "    num_steps_sampled: 199920\n",
      "    num_steps_trained: 199920\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.23761061946901\n",
      "    ram_util_percent: 28.972566371681417\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07074165416528187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.04668298151989\n",
      "    mean_inference_ms: 4.335104079920056\n",
      "    mean_raw_obs_processing_ms: 2.413013693782442\n",
      "  time_since_restore: 3333.3567078113556\n",
      "  time_this_iter_s: 158.86074090003967\n",
      "  time_total_s: 3333.3567078113556\n",
      "  timers:\n",
      "    learn_throughput: 848.755\n",
      "    learn_time_ms: 11777.25\n",
      "    load_throughput: 75007.009\n",
      "    load_time_ms: 133.268\n",
      "    sample_throughput: 64.1\n",
      "    sample_time_ms: 155942.722\n",
      "    update_time_ms: 10.4\n",
      "  timestamp: 1636286291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199920\n",
      "  training_iteration: 20\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         3333.36</td><td style=\"text-align: right;\">199920</td><td style=\"text-align: right;\">  2.4495</td><td style=\"text-align: right;\">                8.75</td><td style=\"text-align: right;\">               -2.08</td><td style=\"text-align: right;\">           99.9406</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2d626_00000:\n",
      "  agent_timesteps_total: 209916\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_12-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 98.61386138613861\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.690000000000012\n",
      "  episode_reward_mean: 2.1023762376237687\n",
      "  episode_reward_min: -2.359999999999997\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 2099\n",
      "  experiment_id: df9a4fe7112046deb1a947ba7e3d3727\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.521676038676857\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015389998334360765\n",
      "          policy_loss: -0.03269229088233322\n",
      "          total_loss: 0.3262905108400135\n",
      "          vf_explained_var: 0.6767266988754272\n",
      "          vf_loss: 0.3686171882555016\n",
      "    num_agent_steps_sampled: 209916\n",
      "    num_agent_steps_trained: 209916\n",
      "    num_steps_sampled: 209916\n",
      "    num_steps_trained: 209916\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.01397379912663\n",
      "    ram_util_percent: 28.905676855895198\n",
      "  pid: 480810\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07065541384165885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.012548584726915\n",
      "    mean_inference_ms: 4.327652445848877\n",
      "    mean_raw_obs_processing_ms: 2.3478572342925093\n",
      "  time_since_restore: 3493.2860407829285\n",
      "  time_this_iter_s: 159.92933297157288\n",
      "  time_total_s: 3493.2860407829285\n",
      "  timers:\n",
      "    learn_throughput: 852.737\n",
      "    learn_time_ms: 11722.26\n",
      "    load_throughput: 74478.458\n",
      "    load_time_ms: 134.213\n",
      "    sample_throughput: 64.936\n",
      "    sample_time_ms: 153937.082\n",
      "    update_time_ms: 9.735\n",
      "  timestamp: 1636286451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209916\n",
      "  training_iteration: 21\n",
      "  trial_id: 2d626_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/57.7 GiB heap, 0.0/28.72 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_11-02-19<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2d626_00000</td><td>RUNNING </td><td>192.168.1.96:480810</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         3493.29</td><td style=\"text-align: right;\">209916</td><td style=\"text-align: right;\"> 2.10238</td><td style=\"text-align: right;\">                8.69</td><td style=\"text-align: right;\">               -2.36</td><td style=\"text-align: right;\">           98.6139</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480811)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480813)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=480807)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 3,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 5_000,\n",
    "             \"lr\": 1e-4,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO All Tasks pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        local_dir=\"/IGLU-Minecraft/checkpoints/all_tasks\",\n",
    "        keep_checkpoints_num=50,\n",
    "        checkpoint_freq=5,\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
