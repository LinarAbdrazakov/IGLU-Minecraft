{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1427c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.torch.recurrent_net import DeveloperAPI, RecurrentNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.rnn_sequencing import add_time_dimension\n",
    "\n",
    "class MyModelClass(RecurrentNetwork, nn.Module): # RecurrentNetwork\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.time_major = self.model_config.get(\"_time_major\", False)\n",
    "        self.features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.gru_hidden_dim = 64\n",
    "        self.gru = nn.GRU(self.features_dim, self.gru_hidden_dim, batch_first=not self.time_major)\n",
    "        \n",
    "        self.action_head = nn.Linear(self.gru_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(self.gru_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        \n",
    "        if isinstance(seq_lens, np.ndarray):\n",
    "            seq_lens = torch.Tensor(seq_lens).int()\n",
    "        max_seq_len = features.shape[0] // seq_lens.shape[0]    \n",
    "        inputs = add_time_dimension(\n",
    "            features,\n",
    "            max_seq_len=max_seq_len,\n",
    "            framework=\"torch\",\n",
    "            time_major=self.time_major,\n",
    "        )\n",
    "        #print('features:', features, 'shape:', features.shape)\n",
    "        #print('inputs:', inputs, 'shape:', inputs.shape)\n",
    "        #print('hidden state:', state[0], 'shape:', state[0].shape)\n",
    "        \n",
    "        h = state[0].permute(1, 0, 2)\n",
    "        output, new_h = self.gru(inputs, h)\n",
    "        new_state = [new_h.permute(1, 0, 2)]\n",
    "        \n",
    "        output = output.reshape(-1, self.gru_hidden_dim)\n",
    "        \n",
    "        action = self.action_head(output)\n",
    "        self.last_value = self.value_head(output).squeeze(1)\n",
    "        return action, new_state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def get_initial_state(self):\n",
    "        #print('i was here')\n",
    "        return [torch.zeros(1, self.gru_hidden_dim)]\n",
    "    \n",
    "    \"\"\"@override(RecurrentNetwork)\n",
    "    def forward_rnn(self, input_dict, state, seq_lens):\n",
    "        x = nn.functional.relu(self.fc1(input_dict[\"obs_flat\"].float()))\n",
    "        h_in = state[0].reshape(-1, self.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        self._cur_value = self.value_branch(h).squeeze(1)\n",
    "        return q, [h]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/6 CPUs, 0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 10:20:22,097\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-01 10:20:22,110\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained (AnnaCNN) + GRU</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/2f3c2_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/2f3c2_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211001_102022-2f3c2_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m 2021-10-01 10:20:26,161\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m 2021-10-01 10:20:26,161\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m 2021-10-01 10:20:32,889\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.703709946738349\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00965274962776467\n",
      "          policy_loss: -0.0923098218627274\n",
      "          total_loss: -0.11293265991824025\n",
      "          vf_explained_var: 0.03169650956988335\n",
      "          vf_loss: 0.0044837128991882\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.43894736842105\n",
      "    ram_util_percent: 82.58315789473684\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04813816402103756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 62.89344400792689\n",
      "    mean_inference_ms: 1.8444316132323488\n",
      "    mean_raw_obs_processing_ms: 0.17737961196518326\n",
      "  time_since_restore: 66.34647703170776\n",
      "  time_this_iter_s: 66.34647703170776\n",
      "  time_total_s: 66.34647703170776\n",
      "  timers:\n",
      "    learn_throughput: 963.417\n",
      "    learn_time_ms: 1037.972\n",
      "    load_throughput: 7353.668\n",
      "    load_time_ms: 135.987\n",
      "    sample_throughput: 15.345\n",
      "    sample_time_ms: 65165.764\n",
      "    update_time_ms: 2.274\n",
      "  timestamp: 1633083699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.3465</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.739139864179823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009254969133539218\n",
      "          policy_loss: 0.18727723095152113\n",
      "          total_loss: 0.16425911539958582\n",
      "          vf_explained_var: 0.4264393150806427\n",
      "          vf_loss: 0.002522287904866971\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.86111111111111\n",
      "    ram_util_percent: 89.0611111111111\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0481419712174656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.487769084011525\n",
      "    mean_inference_ms: 1.8624454479409893\n",
      "    mean_raw_obs_processing_ms: 0.17730496371668236\n",
      "  time_since_restore: 78.68399381637573\n",
      "  time_this_iter_s: 12.337516784667969\n",
      "  time_total_s: 78.68399381637573\n",
      "  timers:\n",
      "    learn_throughput: 1055.374\n",
      "    learn_time_ms: 947.532\n",
      "    load_throughput: 12728.971\n",
      "    load_time_ms: 78.561\n",
      "    sample_throughput: 26.103\n",
      "    sample_time_ms: 38309.3\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633083711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          78.684</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-22-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.73066815800137\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006969921565658385\n",
      "          policy_loss: 0.12619963967137868\n",
      "          total_loss: 0.13434874812761943\n",
      "          vf_explained_var: 0.4080866575241089\n",
      "          vf_loss: 0.034061807580292223\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 92.47777777777779\n",
      "    ram_util_percent: 89.14444444444445\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047923808567544766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.11231586275891\n",
      "    mean_inference_ms: 1.8701573650497247\n",
      "    mean_raw_obs_processing_ms: 0.1771381150283877\n",
      "  time_since_restore: 91.72292613983154\n",
      "  time_this_iter_s: 13.03893232345581\n",
      "  time_total_s: 91.72292613983154\n",
      "  timers:\n",
      "    learn_throughput: 1085.131\n",
      "    learn_time_ms: 921.548\n",
      "    load_throughput: 17444.044\n",
      "    load_time_ms: 57.326\n",
      "    sample_throughput: 33.797\n",
      "    sample_time_ms: 29588.651\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1633083724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         91.7229</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7435591565238107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008900370724115822\n",
      "          policy_loss: 0.19691290573941336\n",
      "          total_loss: 0.17288700342178345\n",
      "          vf_explained_var: 0.3730916678905487\n",
      "          vf_loss: 0.0016296139403291616\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.13529411764705\n",
      "    ram_util_percent: 89.0\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04774967101458336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.24013509219146\n",
      "    mean_inference_ms: 1.8729979624816284\n",
      "    mean_raw_obs_processing_ms: 0.1740093048604118\n",
      "  time_since_restore: 103.0831036567688\n",
      "  time_this_iter_s: 11.360177516937256\n",
      "  time_total_s: 103.0831036567688\n",
      "  timers:\n",
      "    learn_throughput: 1112.863\n",
      "    learn_time_ms: 898.583\n",
      "    load_throughput: 20899.729\n",
      "    load_time_ms: 47.848\n",
      "    sample_throughput: 40.293\n",
      "    sample_time_ms: 24817.932\n",
      "    update_time_ms: 1.925\n",
      "  timestamp: 1633083736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         103.083</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-22-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7383231427934436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008033587776009616\n",
      "          policy_loss: 0.03687968982590569\n",
      "          total_loss: 0.013218507046500842\n",
      "          vf_explained_var: -0.7381928563117981\n",
      "          vf_loss: 0.0021153326482615535\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.47058823529412\n",
      "    ram_util_percent: 89.0529411764706\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04756450013524709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.768180325275765\n",
      "    mean_inference_ms: 1.8725994334743874\n",
      "    mean_raw_obs_processing_ms: 0.17086427284663447\n",
      "  time_since_restore: 114.90638947486877\n",
      "  time_this_iter_s: 11.823285818099976\n",
      "  time_total_s: 114.90638947486877\n",
      "  timers:\n",
      "    learn_throughput: 1128.665\n",
      "    learn_time_ms: 886.002\n",
      "    load_throughput: 24574.771\n",
      "    load_time_ms: 40.692\n",
      "    sample_throughput: 45.355\n",
      "    sample_time_ms: 22048.217\n",
      "    update_time_ms: 1.902\n",
      "  timestamp: 1633083747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         114.906</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-22-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.744913864135742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076107483911666465\n",
      "          policy_loss: 0.19132733868641985\n",
      "          total_loss: 0.16749370172862352\n",
      "          vf_explained_var: -0.16877679526805878\n",
      "          vf_loss: 0.0020933512641931884\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.94375\n",
      "    ram_util_percent: 88.55\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047432747224671355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.13963331691649\n",
      "    mean_inference_ms: 1.8718881108781085\n",
      "    mean_raw_obs_processing_ms: 0.16805883272648625\n",
      "  time_since_restore: 126.42711877822876\n",
      "  time_this_iter_s: 11.520729303359985\n",
      "  time_total_s: 126.42711877822876\n",
      "  timers:\n",
      "    learn_throughput: 1140.056\n",
      "    learn_time_ms: 877.15\n",
      "    load_throughput: 27033.196\n",
      "    load_time_ms: 36.992\n",
      "    sample_throughput: 49.626\n",
      "    sample_time_ms: 20150.819\n",
      "    update_time_ms: 1.872\n",
      "  timestamp: 1633083759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         126.427</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-22-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6988585048251683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007876575393220444\n",
      "          policy_loss: 0.07629010923103326\n",
      "          total_loss: 0.052838297767771616\n",
      "          vf_explained_var: -0.9252479672431946\n",
      "          vf_loss: 0.001961457618098292\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.72777777777779\n",
      "    ram_util_percent: 88.68333333333332\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04732157987465267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.094792594925018\n",
      "    mean_inference_ms: 1.8702685371002818\n",
      "    mean_raw_obs_processing_ms: 0.16556837740009742\n",
      "  time_since_restore: 139.1099283695221\n",
      "  time_this_iter_s: 12.682809591293335\n",
      "  time_total_s: 139.1099283695221\n",
      "  timers:\n",
      "    learn_throughput: 1148.838\n",
      "    learn_time_ms: 870.445\n",
      "    load_throughput: 29838.772\n",
      "    load_time_ms: 33.513\n",
      "    sample_throughput: 52.735\n",
      "    sample_time_ms: 18962.811\n",
      "    update_time_ms: 1.845\n",
      "  timestamp: 1633083772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          139.11</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-23-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.375\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7230493836932714\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011894338149331891\n",
      "          policy_loss: 0.05437346007092856\n",
      "          total_loss: 0.03240594930248335\n",
      "          vf_explained_var: 0.1587696671485901\n",
      "          vf_loss: 0.002884113597166207\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.51764705882354\n",
      "    ram_util_percent: 88.31764705882354\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04722713241836132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.440310029616125\n",
      "    mean_inference_ms: 1.8681902341938146\n",
      "    mean_raw_obs_processing_ms: 0.16343690865437993\n",
      "  time_since_restore: 151.111145734787\n",
      "  time_this_iter_s: 12.001217365264893\n",
      "  time_total_s: 151.111145734787\n",
      "  timers:\n",
      "    learn_throughput: 1153.032\n",
      "    learn_time_ms: 867.279\n",
      "    load_throughput: 31592.03\n",
      "    load_time_ms: 31.654\n",
      "    sample_throughput: 55.605\n",
      "    sample_time_ms: 17983.886\n",
      "    update_time_ms: 1.819\n",
      "  timestamp: 1633083784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         151.111</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -0.375</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-23-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7217303117116294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010318207712395031\n",
      "          policy_loss: -0.07122858320362865\n",
      "          total_loss: -0.09359369141360124\n",
      "          vf_explained_var: -0.23562747240066528\n",
      "          vf_loss: 0.002788553085540318\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.34705882352942\n",
      "    ram_util_percent: 88.50588235294117\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04714550037777879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.067523922352965\n",
      "    mean_inference_ms: 1.8662943792791675\n",
      "    mean_raw_obs_processing_ms: 0.16155519976337712\n",
      "  time_since_restore: 162.90178847312927\n",
      "  time_this_iter_s: 11.790642738342285\n",
      "  time_total_s: 162.90178847312927\n",
      "  timers:\n",
      "    learn_throughput: 1159.109\n",
      "    learn_time_ms: 862.732\n",
      "    load_throughput: 33811.277\n",
      "    load_time_ms: 29.576\n",
      "    sample_throughput: 58.133\n",
      "    sample_time_ms: 17201.921\n",
      "    update_time_ms: 1.794\n",
      "  timestamp: 1633083796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         162.902</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-23-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.712425473001268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008882743794480829\n",
      "          policy_loss: 0.05272905917631255\n",
      "          total_loss: 0.029905850662746362\n",
      "          vf_explained_var: -0.6862989068031311\n",
      "          vf_loss: 0.002524496318720695\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.67058823529412\n",
      "    ram_util_percent: 88.21764705882353\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04706875073919072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.90602642965747\n",
      "    mean_inference_ms: 1.8646330215772249\n",
      "    mean_raw_obs_processing_ms: 0.15994321311223472\n",
      "  time_since_restore: 174.61206698417664\n",
      "  time_this_iter_s: 11.710278511047363\n",
      "  time_total_s: 174.61206698417664\n",
      "  timers:\n",
      "    learn_throughput: 1160.752\n",
      "    learn_time_ms: 861.51\n",
      "    load_throughput: 35107.856\n",
      "    load_time_ms: 28.484\n",
      "    sample_throughput: 60.367\n",
      "    sample_time_ms: 16565.24\n",
      "    update_time_ms: 1.78\n",
      "  timestamp: 1633083807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         174.612</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2727272727272727\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6983704725901285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00930188855091623\n",
      "          policy_loss: -0.041410812021543585\n",
      "          total_loss: -0.06410642845763101\n",
      "          vf_explained_var: -0.8949530720710754\n",
      "          vf_loss: 0.0024277069667328357\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.45294117647059\n",
      "    ram_util_percent: 88.51176470588234\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046999318989426615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.909353998977284\n",
      "    mean_inference_ms: 1.862852204589023\n",
      "    mean_raw_obs_processing_ms: 0.15855577681756594\n",
      "  time_since_restore: 186.36027359962463\n",
      "  time_this_iter_s: 11.748206615447998\n",
      "  time_total_s: 186.36027359962463\n",
      "  timers:\n",
      "    learn_throughput: 1186.75\n",
      "    learn_time_ms: 842.637\n",
      "    load_throughput: 61348.576\n",
      "    load_time_ms: 16.3\n",
      "    sample_throughput: 89.794\n",
      "    sample_time_ms: 11136.578\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1633083819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          186.36</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.272727</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7235150416692098\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010566616058651477\n",
      "          policy_loss: -0.035482479590508674\n",
      "          total_loss: -0.056249481646551025\n",
      "          vf_explained_var: -0.4826214611530304\n",
      "          vf_loss: 0.004354823634235395\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.35882352941177\n",
      "    ram_util_percent: 88.30588235294117\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046938308273933194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.046645423883035\n",
      "    mean_inference_ms: 1.861249847697211\n",
      "    mean_raw_obs_processing_ms: 0.1573991116622516\n",
      "  time_since_restore: 198.6023120880127\n",
      "  time_this_iter_s: 12.242038488388062\n",
      "  time_total_s: 198.6023120880127\n",
      "  timers:\n",
      "    learn_throughput: 1189.153\n",
      "    learn_time_ms: 840.934\n",
      "    load_throughput: 62317.774\n",
      "    load_time_ms: 16.047\n",
      "    sample_throughput: 89.855\n",
      "    sample_time_ms: 11129.091\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633083831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         198.602</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-24-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.23076923076923078\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.71121363374922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011089640761362244\n",
      "          policy_loss: -0.006153399538662698\n",
      "          total_loss: -0.02784951444508326\n",
      "          vf_explained_var: -0.6530073881149292\n",
      "          vf_loss: 0.0031980905862939025\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.13888888888889\n",
      "    ram_util_percent: 88.59999999999998\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046884197319450274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.29178749800284\n",
      "    mean_inference_ms: 1.8597196672843164\n",
      "    mean_raw_obs_processing_ms: 0.15633595579181925\n",
      "  time_since_restore: 210.8622009754181\n",
      "  time_this_iter_s: 12.259888887405396\n",
      "  time_total_s: 210.8622009754181\n",
      "  timers:\n",
      "    learn_throughput: 1193.42\n",
      "    learn_time_ms: 837.928\n",
      "    load_throughput: 63314.359\n",
      "    load_time_ms: 15.794\n",
      "    sample_throughput: 90.46\n",
      "    sample_time_ms: 11054.601\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1633083844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         210.862</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.230769</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.21428571428571427\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.723418500688341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005954186678036984\n",
      "          policy_loss: -0.05089437961578369\n",
      "          total_loss: -0.07478558679835665\n",
      "          vf_explained_var: -0.6406571865081787\n",
      "          vf_loss: 0.0021521408519119076\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.80588235294118\n",
      "    ram_util_percent: 88.38823529411764\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04683553334081559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.623434700355414\n",
      "    mean_inference_ms: 1.8583208994860212\n",
      "    mean_raw_obs_processing_ms: 0.15543092382500134\n",
      "  time_since_restore: 222.85501551628113\n",
      "  time_this_iter_s: 11.992814540863037\n",
      "  time_total_s: 222.85501551628113\n",
      "  timers:\n",
      "    learn_throughput: 1191.394\n",
      "    learn_time_ms: 839.353\n",
      "    load_throughput: 63375.586\n",
      "    load_time_ms: 15.779\n",
      "    sample_throughput: 89.957\n",
      "    sample_time_ms: 11116.379\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633083856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         222.855</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.214286</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7303375800450644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009546727111153747\n",
      "          policy_loss: 0.004513467084487072\n",
      "          total_loss: -0.018852394757171473\n",
      "          vf_explained_var: -0.4909204840660095\n",
      "          vf_loss: 0.002028166020560699\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.4125\n",
      "    ram_util_percent: 88.61875\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04679137436635337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.023542622195123\n",
      "    mean_inference_ms: 1.8570358612318845\n",
      "    mean_raw_obs_processing_ms: 0.15460288019314225\n",
      "  time_since_restore: 234.05603647232056\n",
      "  time_this_iter_s: 11.201020956039429\n",
      "  time_total_s: 234.05603647232056\n",
      "  timers:\n",
      "    learn_throughput: 1191.574\n",
      "    learn_time_ms: 839.226\n",
      "    load_throughput: 63289.519\n",
      "    load_time_ms: 15.8\n",
      "    sample_throughput: 90.462\n",
      "    sample_time_ms: 11054.351\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1633083867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         234.056</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1875\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.682371507750617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012422928624433647\n",
      "          policy_loss: -0.05420239522225327\n",
      "          total_loss: -0.07525481371105545\n",
      "          vf_explained_var: -0.21129010617733002\n",
      "          vf_loss: 0.0032867091248691494\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.88235294117646\n",
      "    ram_util_percent: 88.41764705882353\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04675072073176595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.484458192886017\n",
      "    mean_inference_ms: 1.8558033234991758\n",
      "    mean_raw_obs_processing_ms: 0.15385892278329094\n",
      "  time_since_restore: 245.95117497444153\n",
      "  time_this_iter_s: 11.895138502120972\n",
      "  time_total_s: 245.95117497444153\n",
      "  timers:\n",
      "    learn_throughput: 1191.932\n",
      "    learn_time_ms: 838.974\n",
      "    load_throughput: 62944.175\n",
      "    load_time_ms: 15.887\n",
      "    sample_throughput: 90.155\n",
      "    sample_time_ms: 11092.0\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633083879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         245.951</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -0.1875</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-24-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17647058823529413\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.632796944512261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008543929694583594\n",
      "          policy_loss: -0.11504525161451763\n",
      "          total_loss: -0.13681527227163315\n",
      "          vf_explained_var: -0.4962610900402069\n",
      "          vf_loss: 0.0028491648017532297\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.42222222222222\n",
      "    ram_util_percent: 88.7\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04671115911993958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.999509124212384\n",
      "    mean_inference_ms: 1.8546269570932008\n",
      "    mean_raw_obs_processing_ms: 0.153213424754568\n",
      "  time_since_restore: 258.57976245880127\n",
      "  time_this_iter_s: 12.628587484359741\n",
      "  time_total_s: 258.57976245880127\n",
      "  timers:\n",
      "    learn_throughput: 1191.871\n",
      "    learn_time_ms: 839.017\n",
      "    load_throughput: 63072.905\n",
      "    load_time_ms: 15.855\n",
      "    sample_throughput: 90.2\n",
      "    sample_time_ms: 11086.531\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633083891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          258.58</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.176471</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-25-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.667787048551771\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009053410413795709\n",
      "          policy_loss: 0.15276608020067214\n",
      "          total_loss: 0.13231454350882108\n",
      "          vf_explained_var: -0.4116354286670685\n",
      "          vf_loss: 0.00441565242751191\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.33333333333333\n",
      "    ram_util_percent: 88.49444444444444\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04667285654472288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.56097383212697\n",
      "    mean_inference_ms: 1.8534891509812814\n",
      "    mean_raw_obs_processing_ms: 0.15259915437560478\n",
      "  time_since_restore: 271.2994396686554\n",
      "  time_this_iter_s: 12.719677209854126\n",
      "  time_total_s: 271.2994396686554\n",
      "  timers:\n",
      "    learn_throughput: 1192.394\n",
      "    learn_time_ms: 838.649\n",
      "    load_throughput: 62932.748\n",
      "    load_time_ms: 15.89\n",
      "    sample_throughput: 89.616\n",
      "    sample_time_ms: 11158.724\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1633083904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         271.299</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-25-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15789473684210525\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7340723461574976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008657786306896299\n",
      "          policy_loss: -0.039854896831093355\n",
      "          total_loss: -0.0632725209929049\n",
      "          vf_explained_var: -0.5479024648666382\n",
      "          vf_loss: 0.0021915383426757114\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.20625000000001\n",
      "    ram_util_percent: 88.8\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046637305259543155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.158080714619597\n",
      "    mean_inference_ms: 1.8523885919118492\n",
      "    mean_raw_obs_processing_ms: 0.1520554026041816\n",
      "  time_since_restore: 282.5221915245056\n",
      "  time_this_iter_s: 11.22275185585022\n",
      "  time_total_s: 282.5221915245056\n",
      "  timers:\n",
      "    learn_throughput: 1189.728\n",
      "    learn_time_ms: 840.528\n",
      "    load_throughput: 62992.955\n",
      "    load_time_ms: 15.875\n",
      "    sample_throughput: 90.09\n",
      "    sample_time_ms: 11100.033\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1633083915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         282.522</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.157895</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6943329572677612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01019458976984706\n",
      "          policy_loss: -0.008706297725439072\n",
      "          total_loss: -0.031437198321024575\n",
      "          vf_explained_var: -0.581558346748352\n",
      "          vf_loss: 0.0021735133027606128\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.78888888888889\n",
      "    ram_util_percent: 88.6111111111111\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04660365157829648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.789179450782573\n",
      "    mean_inference_ms: 1.851288517970979\n",
      "    mean_raw_obs_processing_ms: 0.15153089100783082\n",
      "  time_since_restore: 294.7940442562103\n",
      "  time_this_iter_s: 12.271852731704712\n",
      "  time_total_s: 294.7940442562103\n",
      "  timers:\n",
      "    learn_throughput: 1190.726\n",
      "    learn_time_ms: 839.824\n",
      "    load_throughput: 62748.778\n",
      "    load_time_ms: 15.937\n",
      "    sample_throughput: 89.631\n",
      "    sample_time_ms: 11156.863\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633083928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         294.794</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7036946720547146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00753261841578989\n",
      "          policy_loss: -0.1357502227322483\n",
      "          total_loss: -0.15676699148542766\n",
      "          vf_explained_var: -0.4938938021659851\n",
      "          vf_loss: 0.004513651254819706\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.40000000000002\n",
      "    ram_util_percent: 88.85555555555555\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046571702849321786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.45073951225215\n",
      "    mean_inference_ms: 1.850232958664282\n",
      "    mean_raw_obs_processing_ms: 0.15105798911799406\n",
      "  time_since_restore: 307.4099736213684\n",
      "  time_this_iter_s: 12.615929365158081\n",
      "  time_total_s: 307.4099736213684\n",
      "  timers:\n",
      "    learn_throughput: 1193.401\n",
      "    learn_time_ms: 837.941\n",
      "    load_throughput: 62786.068\n",
      "    load_time_ms: 15.927\n",
      "    sample_throughput: 88.924\n",
      "    sample_time_ms: 11245.535\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1633083940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          307.41</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.045454545454545456\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.690987375047472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008689853599971513\n",
      "          policy_loss: -0.12039416581392288\n",
      "          total_loss: -0.10324848973088795\n",
      "          vf_explained_var: 0.3901374936103821\n",
      "          vf_loss: 0.04231757907998852\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.41176470588235\n",
      "    ram_util_percent: 88.63529411764705\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04655328305045883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.136976236896537\n",
      "    mean_inference_ms: 1.8492741588633843\n",
      "    mean_raw_obs_processing_ms: 0.15064814236860097\n",
      "  time_since_restore: 319.08894324302673\n",
      "  time_this_iter_s: 11.678969621658325\n",
      "  time_total_s: 319.08894324302673\n",
      "  timers:\n",
      "    learn_throughput: 1194.128\n",
      "    learn_time_ms: 837.431\n",
      "    load_throughput: 62832.626\n",
      "    load_time_ms: 15.915\n",
      "    sample_throughput: 89.368\n",
      "    sample_time_ms: 11189.721\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1633083952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         319.089</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.0454545</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.043478260869565216\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7255021810531614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010211754657593896\n",
      "          policy_loss: -0.1937797842754258\n",
      "          total_loss: -0.216247995197773\n",
      "          vf_explained_var: -0.28861263394355774\n",
      "          vf_loss: 0.0027444567932333385\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.23529411764706\n",
      "    ram_util_percent: 88.90588235294118\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04653560724522156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.845640863888995\n",
      "    mean_inference_ms: 1.8484321098616017\n",
      "    mean_raw_obs_processing_ms: 0.1502530218541478\n",
      "  time_since_restore: 330.98894572257996\n",
      "  time_this_iter_s: 11.900002479553223\n",
      "  time_total_s: 330.98894572257996\n",
      "  timers:\n",
      "    learn_throughput: 1194.722\n",
      "    learn_time_ms: 837.015\n",
      "    load_throughput: 62763.895\n",
      "    load_time_ms: 15.933\n",
      "    sample_throughput: 89.653\n",
      "    sample_time_ms: 11154.152\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633083964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         330.989</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.0434783</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.041666666666666664\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7357361131244238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006138144202830099\n",
      "          policy_loss: -0.15744254146185185\n",
      "          total_loss: -0.18149308090408642\n",
      "          vf_explained_var: 0.10192135721445084\n",
      "          vf_loss: 0.002079194350193979\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.08125\n",
      "    ram_util_percent: 88.66875\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04651896508372381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.573206490402526\n",
      "    mean_inference_ms: 1.847667864667051\n",
      "    mean_raw_obs_processing_ms: 0.1498689216769842\n",
      "  time_since_restore: 342.2539393901825\n",
      "  time_this_iter_s: 11.264993667602539\n",
      "  time_total_s: 342.2539393901825\n",
      "  timers:\n",
      "    learn_throughput: 1194.95\n",
      "    learn_time_ms: 836.855\n",
      "    load_throughput: 63035.273\n",
      "    load_time_ms: 15.864\n",
      "    sample_throughput: 90.239\n",
      "    sample_time_ms: 11081.672\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1633083975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         342.254</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.0416667</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.734258540471395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009176388143457625\n",
      "          policy_loss: -0.12917184498575\n",
      "          total_loss: -0.15330548588600423\n",
      "          vf_explained_var: -0.7588332295417786\n",
      "          vf_loss: 0.0013736655708473538\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.91875\n",
      "    ram_util_percent: 89.0\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04650294432111025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.317685620211673\n",
      "    mean_inference_ms: 1.846974522509827\n",
      "    mean_raw_obs_processing_ms: 0.14950056748512108\n",
      "  time_since_restore: 353.44491243362427\n",
      "  time_this_iter_s: 11.190973043441772\n",
      "  time_total_s: 353.44491243362427\n",
      "  timers:\n",
      "    learn_throughput: 1194.328\n",
      "    learn_time_ms: 837.291\n",
      "    load_throughput: 62940.869\n",
      "    load_time_ms: 15.888\n",
      "    sample_throughput: 90.251\n",
      "    sample_time_ms: 11080.199\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1633083987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         353.445</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.038461538461538464\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7031096537907917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011837801763319182\n",
      "          policy_loss: -0.0032949855939174693\n",
      "          total_loss: -0.025255027216755682\n",
      "          vf_explained_var: -0.8992470502853394\n",
      "          vf_loss: 0.002703494913940732\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.33749999999999\n",
      "    ram_util_percent: 88.7875\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04648792817241014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0773090691327\n",
      "    mean_inference_ms: 1.846319655914105\n",
      "    mean_raw_obs_processing_ms: 0.14914957008398153\n",
      "  time_since_restore: 364.5018525123596\n",
      "  time_this_iter_s: 11.056940078735352\n",
      "  time_total_s: 364.5018525123596\n",
      "  timers:\n",
      "    learn_throughput: 1192.705\n",
      "    learn_time_ms: 838.43\n",
      "    load_throughput: 63240.947\n",
      "    load_time_ms: 15.813\n",
      "    sample_throughput: 90.949\n",
      "    sample_time_ms: 10995.233\n",
      "    update_time_ms: 1.614\n",
      "  timestamp: 1633083998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         364.502</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.0384615</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-26-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.1111111111111111\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7134987115859985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010321515849314915\n",
      "          policy_loss: 0.0418708071940475\n",
      "          total_loss: 0.10698903934357482\n",
      "          vf_explained_var: -0.06141440197825432\n",
      "          vf_loss: 0.09018891727642363\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.15\n",
      "    ram_util_percent: 89.05625\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04647396679029798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.851506381655547\n",
      "    mean_inference_ms: 1.8457555706596391\n",
      "    mean_raw_obs_processing_ms: 0.14882826019097373\n",
      "  time_since_restore: 376.1518096923828\n",
      "  time_this_iter_s: 11.649957180023193\n",
      "  time_total_s: 376.1518096923828\n",
      "  timers:\n",
      "    learn_throughput: 1192.84\n",
      "    learn_time_ms: 838.335\n",
      "    load_throughput: 63330.419\n",
      "    load_time_ms: 15.79\n",
      "    sample_throughput: 91.765\n",
      "    sample_time_ms: 10897.412\n",
      "    update_time_ms: 1.619\n",
      "  timestamp: 1633084009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         376.152</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.111111</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-27-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.10714285714285714\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.722605013847351\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007869530889055218\n",
      "          policy_loss: -0.05946897849337095\n",
      "          total_loss: -0.08335346584661035\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017676554154604674\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.94999999999999\n",
      "    ram_util_percent: 88.81875\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046460253838570824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.63830512239391\n",
      "    mean_inference_ms: 1.8452124780353685\n",
      "    mean_raw_obs_processing_ms: 0.14852113836693545\n",
      "  time_since_restore: 387.2821145057678\n",
      "  time_this_iter_s: 11.13030481338501\n",
      "  time_total_s: 387.2821145057678\n",
      "  timers:\n",
      "    learn_throughput: 1192.207\n",
      "    learn_time_ms: 838.781\n",
      "    load_throughput: 63438.563\n",
      "    load_time_ms: 15.763\n",
      "    sample_throughput: 93.126\n",
      "    sample_time_ms: 10738.116\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1633084020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         387.282</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.107143</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.10344827586206896\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7282517035802205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006468207584613881\n",
      "          policy_loss: -0.018372396232249837\n",
      "          total_loss: -0.042629944789870124\n",
      "          vf_explained_var: -0.844727635383606\n",
      "          vf_loss: 0.001731329189432371\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.55882352941177\n",
      "    ram_util_percent: 79.39411764705882\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0464472239625746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.436954052748582\n",
      "    mean_inference_ms: 1.844686149996993\n",
      "    mean_raw_obs_processing_ms: 0.14823303793802686\n",
      "  time_since_restore: 398.6690573692322\n",
      "  time_this_iter_s: 11.386942863464355\n",
      "  time_total_s: 398.6690573692322\n",
      "  timers:\n",
      "    learn_throughput: 1193.794\n",
      "    learn_time_ms: 837.666\n",
      "    load_throughput: 63452.766\n",
      "    load_time_ms: 15.76\n",
      "    sample_throughput: 92.975\n",
      "    sample_time_ms: 10755.567\n",
      "    update_time_ms: 1.574\n",
      "  timestamp: 1633084032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         398.669</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.103448</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9364)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-27-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.0333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7072768105400935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00629696250512123\n",
      "          policy_loss: -0.08779191594156954\n",
      "          total_loss: -0.1101781387709909\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.003427151375217363\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.7309523809524\n",
      "    ram_util_percent: 75.80000000000001\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04643514472063374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.24724997756496\n",
      "    mean_inference_ms: 1.8441582669754923\n",
      "    mean_raw_obs_processing_ms: 0.16728059672498788\n",
      "  time_since_restore: 428.2395865917206\n",
      "  time_this_iter_s: 29.570529222488403\n",
      "  time_total_s: 428.2395865917206\n",
      "  timers:\n",
      "    learn_throughput: 1189.232\n",
      "    learn_time_ms: 840.879\n",
      "    load_throughput: 51557.288\n",
      "    load_time_ms: 19.396\n",
      "    sample_throughput: 80.137\n",
      "    sample_time_ms: 12478.653\n",
      "    update_time_ms: 1.578\n",
      "  timestamp: 1633084062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          428.24</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.033</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.1612903225806\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.0967741935483871\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.724403484662374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005917082343840439\n",
      "          policy_loss: -0.002041113707754347\n",
      "          total_loss: -0.027149839285347196\n",
      "          vf_explained_var: -0.9990139603614807\n",
      "          vf_loss: 0.0009518911441167196\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.905\n",
      "    ram_util_percent: 77.45000000000002\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04643381540661135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.070295458240516\n",
      "    mean_inference_ms: 1.8436692740299279\n",
      "    mean_raw_obs_processing_ms: 0.18449407235830967\n",
      "  time_since_restore: 442.42317056655884\n",
      "  time_this_iter_s: 14.183583974838257\n",
      "  time_total_s: 442.42317056655884\n",
      "  timers:\n",
      "    learn_throughput: 1186.802\n",
      "    learn_time_ms: 842.601\n",
      "    load_throughput: 52020.569\n",
      "    load_time_ms: 19.223\n",
      "    sample_throughput: 79.153\n",
      "    sample_time_ms: 12633.832\n",
      "    update_time_ms: 1.589\n",
      "  timestamp: 1633084076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         442.423</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.0967742</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.161</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.28125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.09375\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6959205812878078\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008702416074404577\n",
      "          policy_loss: -0.03678029063675139\n",
      "          total_loss: -0.029355961084365844\n",
      "          vf_explained_var: -0.1736154854297638\n",
      "          vf_loss: 0.032643050897038645\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.42105263157893\n",
      "    ram_util_percent: 77.19999999999999\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046432875507106876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.903787907694278\n",
      "    mean_inference_ms: 1.8432370914500322\n",
      "    mean_raw_obs_processing_ms: 0.2000796839533826\n",
      "  time_since_restore: 455.53762793540955\n",
      "  time_this_iter_s: 13.114457368850708\n",
      "  time_total_s: 455.53762793540955\n",
      "  timers:\n",
      "    learn_throughput: 1185.145\n",
      "    learn_time_ms: 843.779\n",
      "    load_throughput: 51770.41\n",
      "    load_time_ms: 19.316\n",
      "    sample_throughput: 78.273\n",
      "    sample_time_ms: 12775.834\n",
      "    update_time_ms: 1.598\n",
      "  timestamp: 1633084089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         455.538</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.09375</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.281</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.3939393939394\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.09090909090909091\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.695132573445638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009163244326590811\n",
      "          policy_loss: -0.06481612101197243\n",
      "          total_loss: -0.02368549071252346\n",
      "          vf_explained_var: -0.5608433485031128\n",
      "          vf_loss: 0.06624930936830221\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.19999999999999\n",
      "    ram_util_percent: 77.50526315789473\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046431499901146076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.746854696885755\n",
      "    mean_inference_ms: 1.8428749305075582\n",
      "    mean_raw_obs_processing_ms: 0.21424183446315084\n",
      "  time_since_restore: 468.72365522384644\n",
      "  time_this_iter_s: 13.18602728843689\n",
      "  time_total_s: 468.72365522384644\n",
      "  timers:\n",
      "    learn_throughput: 1183.759\n",
      "    learn_time_ms: 844.766\n",
      "    load_throughput: 51573.581\n",
      "    load_time_ms: 19.39\n",
      "    sample_throughput: 77.499\n",
      "    sample_time_ms: 12903.345\n",
      "    update_time_ms: 1.581\n",
      "  timestamp: 1633084102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         468.724</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.0909091</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.394</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.08823529411764706\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.689849093225267\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009978526611128196\n",
      "          policy_loss: -0.06044511381122801\n",
      "          total_loss: -0.08141781878140238\n",
      "          vf_explained_var: -0.20683643221855164\n",
      "          vf_loss: 0.003930079559278157\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.64999999999999\n",
      "    ram_util_percent: 77.2277777777778\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04642959697597332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.598157780596667\n",
      "    mean_inference_ms: 1.8425329212521457\n",
      "    mean_raw_obs_processing_ms: 0.22712377226724867\n",
      "  time_since_restore: 481.2308897972107\n",
      "  time_this_iter_s: 12.507234573364258\n",
      "  time_total_s: 481.2308897972107\n",
      "  timers:\n",
      "    learn_throughput: 1186.14\n",
      "    learn_time_ms: 843.071\n",
      "    load_throughput: 51562.105\n",
      "    load_time_ms: 19.394\n",
      "    sample_throughput: 76.751\n",
      "    sample_time_ms: 13029.209\n",
      "    update_time_ms: 1.583\n",
      "  timestamp: 1633084115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         481.231</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.0882353</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             996.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.08571428571428572\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7104939248826767\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00803047145813107\n",
      "          policy_loss: 0.024992129392921926\n",
      "          total_loss: 0.000952526581628869\n",
      "          vf_explained_var: -0.1752758026123047\n",
      "          vf_loss: 0.0014592402763406022\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.48333333333333\n",
      "    ram_util_percent: 77.38888888888889\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04642759932068824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.457342605351363\n",
      "    mean_inference_ms: 1.8422068188723568\n",
      "    mean_raw_obs_processing_ms: 0.23886683791761526\n",
      "  time_since_restore: 494.1163115501404\n",
      "  time_this_iter_s: 12.885421752929688\n",
      "  time_total_s: 494.1163115501404\n",
      "  timers:\n",
      "    learn_throughput: 1185.196\n",
      "    learn_time_ms: 843.742\n",
      "    load_throughput: 51283.386\n",
      "    load_time_ms: 19.499\n",
      "    sample_throughput: 75.77\n",
      "    sample_time_ms: 13197.887\n",
      "    update_time_ms: 1.571\n",
      "  timestamp: 1633084128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         494.116</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.0857143</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             996.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-29-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.6944444444445\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.08333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7201423433091905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006079422675809076\n",
      "          policy_loss: -0.00865871202485222\n",
      "          total_loss: -0.033352758538805774\n",
      "          vf_explained_var: -0.8657673597335815\n",
      "          vf_loss: 0.001291493815369904\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.80000000000001\n",
      "    ram_util_percent: 76.99444444444445\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04642515477197967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.32373121468834\n",
      "    mean_inference_ms: 1.8418824794152502\n",
      "    mean_raw_obs_processing_ms: 0.24956994430589738\n",
      "  time_since_restore: 506.8957643508911\n",
      "  time_this_iter_s: 12.779452800750732\n",
      "  time_total_s: 506.8957643508911\n",
      "  timers:\n",
      "    learn_throughput: 1185.344\n",
      "    learn_time_ms: 843.637\n",
      "    load_throughput: 51112.712\n",
      "    load_time_ms: 19.565\n",
      "    sample_throughput: 74.793\n",
      "    sample_time_ms: 13370.235\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1633084140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         506.896</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.0833333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.694</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.7837837837837\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.08108108108108109\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.692693249384562\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008193924307821001\n",
      "          policy_loss: -0.06418568429847558\n",
      "          total_loss: -0.08710422503451506\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0023696053385113677\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.73888888888888\n",
      "    ram_util_percent: 77.18333333333332\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046422661705759274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.196472144504849\n",
      "    mean_inference_ms: 1.8415494971965025\n",
      "    mean_raw_obs_processing_ms: 0.2593504122860055\n",
      "  time_since_restore: 519.2673420906067\n",
      "  time_this_iter_s: 12.371577739715576\n",
      "  time_total_s: 519.2673420906067\n",
      "  timers:\n",
      "    learn_throughput: 1182.662\n",
      "    learn_time_ms: 845.55\n",
      "    load_throughput: 50909.656\n",
      "    load_time_ms: 19.643\n",
      "    sample_throughput: 74.402\n",
      "    sample_time_ms: 13440.444\n",
      "    update_time_ms: 1.591\n",
      "  timestamp: 1633084153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         519.267</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.0810811</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.784</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-29-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.8684210526316\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.07894736842105263\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.704553238550822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0088088931476096\n",
      "          policy_loss: -0.021105708016289606\n",
      "          total_loss: 0.06386137321694857\n",
      "          vf_explained_var: 0.12875548005104065\n",
      "          vf_loss: 0.11025083062316601\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.35999999999999\n",
      "    ram_util_percent: 76.87999999999998\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04642025340685626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.076019899189916\n",
      "    mean_inference_ms: 1.8412404534190643\n",
      "    mean_raw_obs_processing_ms: 0.26829051698076145\n",
      "  time_since_restore: 532.9778118133545\n",
      "  time_this_iter_s: 13.710469722747803\n",
      "  time_total_s: 532.9778118133545\n",
      "  timers:\n",
      "    learn_throughput: 1183.979\n",
      "    learn_time_ms: 844.609\n",
      "    load_throughput: 50879.395\n",
      "    load_time_ms: 19.654\n",
      "    sample_throughput: 72.996\n",
      "    sample_time_ms: 13699.336\n",
      "    update_time_ms: 1.608\n",
      "  timestamp: 1633084167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         532.978</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.0789474</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.868</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-29-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.9487179487179\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.07692307692307693\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.705911625756158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00957933652233461\n",
      "          policy_loss: -0.029268877477281623\n",
      "          total_loss: -0.050722723236928384\n",
      "          vf_explained_var: 0.04848836734890938\n",
      "          vf_loss: 0.0036894004145223234\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.99999999999999\n",
      "    ram_util_percent: 77.1\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641838337723165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.960996955176746\n",
      "    mean_inference_ms: 1.8409384813068215\n",
      "    mean_raw_obs_processing_ms: 0.27646600226486767\n",
      "  time_since_restore: 545.3729975223541\n",
      "  time_this_iter_s: 12.395185708999634\n",
      "  time_total_s: 545.3729975223541\n",
      "  timers:\n",
      "    learn_throughput: 1181.854\n",
      "    learn_time_ms: 846.129\n",
      "    load_throughput: 50974.436\n",
      "    load_time_ms: 19.618\n",
      "    sample_throughput: 72.471\n",
      "    sample_time_ms: 13798.702\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1633084179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         545.373</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.0769231</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.949</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-29-52\n",
      "  done: false\n",
      "  episode_len_mean: 997.025\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.710772024260627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0084305051749667\n",
      "          policy_loss: -0.01610534629888005\n",
      "          total_loss: -0.010575517184204526\n",
      "          vf_explained_var: 0.1123543307185173\n",
      "          vf_loss: 0.030951444758102298\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.68888888888888\n",
      "    ram_util_percent: 76.81666666666666\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641689864242521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.85119891287371\n",
      "    mean_inference_ms: 1.8406506626062047\n",
      "    mean_raw_obs_processing_ms: 0.28395866388866964\n",
      "  time_since_restore: 558.075001001358\n",
      "  time_this_iter_s: 12.702003479003906\n",
      "  time_total_s: 558.075001001358\n",
      "  timers:\n",
      "    learn_throughput: 1185.472\n",
      "    learn_time_ms: 843.546\n",
      "    load_throughput: 62318.607\n",
      "    load_time_ms: 16.047\n",
      "    sample_throughput: 82.522\n",
      "    sample_time_ms: 12117.97\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1633084192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         558.075</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.025</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 997.0975609756098\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.0975609756097561\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7209715366363527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005702868451177167\n",
      "          policy_loss: -0.03095887965626187\n",
      "          total_loss: -0.05586780409018199\n",
      "          vf_explained_var: -0.3401646018028259\n",
      "          vf_loss: 0.0011602171832540383\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.7529411764706\n",
      "    ram_util_percent: 77.1\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641508386659416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.745765531703332\n",
      "    mean_inference_ms: 1.8403636757366608\n",
      "    mean_raw_obs_processing_ms: 0.29082212268707086\n",
      "  time_since_restore: 569.8767530918121\n",
      "  time_this_iter_s: 11.801752090454102\n",
      "  time_total_s: 569.8767530918121\n",
      "  timers:\n",
      "    learn_throughput: 1185.549\n",
      "    learn_time_ms: 843.491\n",
      "    load_throughput: 62141.796\n",
      "    load_time_ms: 16.092\n",
      "    sample_throughput: 84.176\n",
      "    sample_time_ms: 11879.847\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1633084203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         569.877</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.097561</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.098</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-30-15\n",
      "  done: false\n",
      "  episode_len_mean: 997.1666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.09523809523809523\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7336245351367525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065880413232153245\n",
      "          policy_loss: -0.12184563784135713\n",
      "          total_loss: -0.14717358267969555\n",
      "          vf_explained_var: -0.8765683770179749\n",
      "          vf_loss: 0.000690691123357586\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.61176470588235\n",
      "    ram_util_percent: 76.8470588235294\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641310067404667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.644425474630086\n",
      "    mean_inference_ms: 1.840081610761269\n",
      "    mean_raw_obs_processing_ms: 0.2971139726817486\n",
      "  time_since_restore: 581.685836315155\n",
      "  time_this_iter_s: 11.809083223342896\n",
      "  time_total_s: 581.685836315155\n",
      "  timers:\n",
      "    learn_throughput: 1186.24\n",
      "    learn_time_ms: 843.0\n",
      "    load_throughput: 61840.271\n",
      "    load_time_ms: 16.171\n",
      "    sample_throughput: 85.106\n",
      "    sample_time_ms: 11750.047\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1633084215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         581.686</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.0952381</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 997.2325581395348\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.06976744186046512\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.676725482940674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006663335742310809\n",
      "          policy_loss: -0.06016012549904796\n",
      "          total_loss: -0.00682949208550983\n",
      "          vf_explained_var: -0.2617712914943695\n",
      "          vf_loss: 0.07876522023838738\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.69999999999999\n",
      "    ram_util_percent: 77.25555555555556\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04641116464358248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.547462807056176\n",
      "    mean_inference_ms: 1.839800921608331\n",
      "    mean_raw_obs_processing_ms: 0.3028911737244891\n",
      "  time_since_restore: 594.4661798477173\n",
      "  time_this_iter_s: 12.780343532562256\n",
      "  time_total_s: 594.4661798477173\n",
      "  timers:\n",
      "    learn_throughput: 1186.821\n",
      "    learn_time_ms: 842.587\n",
      "    load_throughput: 61988.788\n",
      "    load_time_ms: 16.132\n",
      "    sample_throughput: 85.398\n",
      "    sample_time_ms: 11709.89\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633084228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         594.466</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.0697674</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 997.2954545454545\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.13636363636363635\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6847104999754166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009909645461298498\n",
      "          policy_loss: 0.035555445425496955\n",
      "          total_loss: 0.060869502360259904\n",
      "          vf_explained_var: 0.3641158938407898\n",
      "          vf_loss: 0.05017923130136397\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.55789473684209\n",
      "    ram_util_percent: 77.1157894736842\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046408990707542855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.454617849840947\n",
      "    mean_inference_ms: 1.8395374424648891\n",
      "    mean_raw_obs_processing_ms: 0.3082019593192205\n",
      "  time_since_restore: 607.3376679420471\n",
      "  time_this_iter_s: 12.871488094329834\n",
      "  time_total_s: 607.3376679420471\n",
      "  timers:\n",
      "    learn_throughput: 1184.054\n",
      "    learn_time_ms: 844.556\n",
      "    load_throughput: 61863.986\n",
      "    load_time_ms: 16.164\n",
      "    sample_throughput: 85.147\n",
      "    sample_time_ms: 11744.342\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1633084241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         607.338</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.136364</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.295</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 997.3555555555556\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.13333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.672575275103251\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006145960435465778\n",
      "          policy_loss: -0.06824239546226131\n",
      "          total_loss: -0.09308898529658714\n",
      "          vf_explained_var: -0.8498677015304565\n",
      "          vf_loss: 0.0006499725672054208\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.7125\n",
      "    ram_util_percent: 77.45\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046406913469180085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.364864110968657\n",
      "    mean_inference_ms: 1.8392868266661135\n",
      "    mean_raw_obs_processing_ms: 0.3130812028868618\n",
      "  time_since_restore: 618.6370630264282\n",
      "  time_this_iter_s: 11.299395084381104\n",
      "  time_total_s: 618.6370630264282\n",
      "  timers:\n",
      "    learn_throughput: 1184.291\n",
      "    learn_time_ms: 844.387\n",
      "    load_throughput: 62162.702\n",
      "    load_time_ms: 16.087\n",
      "    sample_throughput: 86.311\n",
      "    sample_time_ms: 11585.952\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633084252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         618.637</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">-0.133333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.356</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-31-05\n",
      "  done: false\n",
      "  episode_len_mean: 997.4130434782609\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.15217391304347827\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.676489814122518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008103108883968218\n",
      "          policy_loss: -0.008482361543509695\n",
      "          total_loss: 0.026142248345745935\n",
      "          vf_explained_var: 0.47413334250450134\n",
      "          vf_loss: 0.059768886774286836\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.80000000000001\n",
      "    ram_util_percent: 77.21666666666667\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046404982165960285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.278596969970492\n",
      "    mean_inference_ms: 1.8390504125326763\n",
      "    mean_raw_obs_processing_ms: 0.3175717111740893\n",
      "  time_since_restore: 631.1387946605682\n",
      "  time_this_iter_s: 12.501731634140015\n",
      "  time_total_s: 631.1387946605682\n",
      "  timers:\n",
      "    learn_throughput: 1183.635\n",
      "    learn_time_ms: 844.855\n",
      "    load_throughput: 62136.456\n",
      "    load_time_ms: 16.094\n",
      "    sample_throughput: 86.523\n",
      "    sample_time_ms: 11557.591\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1633084265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         631.139</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.152174</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.413</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.468085106383\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.19148936170212766\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6724461767408583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009982950807639071\n",
      "          policy_loss: -0.01870488367146916\n",
      "          total_loss: 0.02716317938433753\n",
      "          vf_explained_var: 0.21686363220214844\n",
      "          vf_loss: 0.07059593113760153\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.63157894736842\n",
      "    ram_util_percent: 77.57894736842104\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04640339778510926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.195930839924738\n",
      "    mean_inference_ms: 1.838835113684451\n",
      "    mean_raw_obs_processing_ms: 0.32169997346447476\n",
      "  time_since_restore: 644.34002161026\n",
      "  time_this_iter_s: 13.201226949691772\n",
      "  time_total_s: 644.34002161026\n",
      "  timers:\n",
      "    learn_throughput: 1184.352\n",
      "    learn_time_ms: 844.344\n",
      "    load_throughput: 62276.783\n",
      "    load_time_ms: 16.057\n",
      "    sample_throughput: 85.902\n",
      "    sample_time_ms: 11641.139\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1633084278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          644.34</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.191489</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.468</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_2f3c2_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-01_10-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 997.5208333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.1875\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: ba18be64619d4801b3ca85757cadc21e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.655480718612671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008668941282091749\n",
      "          policy_loss: 0.03999067787081003\n",
      "          total_loss: 0.01598631539899442\n",
      "          vf_explained_var: -0.8452224135398865\n",
      "          vf_loss: 0.0008166570783942007\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.28750000000001\n",
      "    ram_util_percent: 77.31875\n",
      "  pid: 9363\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04640201619030757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.115949355793099\n",
      "    mean_inference_ms: 1.8386286727117245\n",
      "    mean_raw_obs_processing_ms: 0.3254941473318469\n",
      "  time_since_restore: 655.9906837940216\n",
      "  time_this_iter_s: 11.650662183761597\n",
      "  time_total_s: 655.9906837940216\n",
      "  timers:\n",
      "    learn_throughput: 1173.659\n",
      "    learn_time_ms: 852.036\n",
      "    load_throughput: 62415.888\n",
      "    load_time_ms: 16.022\n",
      "    sample_throughput: 87.511\n",
      "    sample_time_ms: 11427.123\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633084290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 2f3c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.71 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-01_10-20-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_2f3c2_00000</td><td>RUNNING </td><td>192.168.1.100:9363</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         655.991</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -0.1875</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           997.521</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained (AnnaCNN) + GRU\",\n",
    "                      \"notes\": \"camera noop removed from actions\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516063b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b43199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
