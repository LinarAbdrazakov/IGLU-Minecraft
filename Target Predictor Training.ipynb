{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Target_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkgcJ4hazc-C"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install torchmetrics\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "nltk.download('punkt')\n",
        "rubert_sentence = SentenceTransformer('all-distilroberta-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeriK177Hagi"
      },
      "source": [
        "import numpy as np\n",
        "augmented_chats = np.load(\"data/augmented_chats.npy\")\n",
        "augmented_targets = np.load(\"data/augmented_targets.npy\")\n",
        "augmented_chats.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IhVRKtwNYxg"
      },
      "source": [
        "chat_embeddings = []\n",
        "for i in range(augmented_chats.shape[0]):\n",
        "  chat_embeddings.append(rubert_sentence.encode(augmented_chats[i]))\n",
        "  if(i%10000==0):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1RESJw53YQQ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "all_targets = np.asarray(augmented_targets[:len(chat_embeddings)])\n",
        "all_chat_embeddings = np.asarray(chat_embeddings)\n",
        "\n",
        "train_targets = np.asarray(augmented_targets[:2500])\n",
        "train_chat_embeddings = np.asarray(chat_embeddings[:2500])\n",
        "\n",
        "test_targets = np.asarray(augmented_targets[2500:])\n",
        "test_chat_embeddings = np.asarray(chat_embeddings[2500:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9YOXVDf08Uo"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "class TargetDataset(Dataset):\n",
        "    def __init__(self, target_list, chat_list):\n",
        "        self.target_list = target_list\n",
        "        self.chat_list = chat_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_list)\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        target = self.target_list[idx]\n",
        "        target_tensor_target = torch.tensor(target, dtype=torch.long)\n",
        "        chat_tensor = self.chat_list[idx]\n",
        "        return chat_tensor, target_tensor_target\n",
        "    \n",
        "training_dataset = TargetDataset(train_targets, train_chat_embeddings)\n",
        "test_dataset = TargetDataset(test_targets, test_chat_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_TMZBqmbUUx"
      },
      "source": [
        "## Training on split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUe8QZwQ0_VE"
      },
      "source": [
        "train_set, val_set = torch.utils.data.random_split(training_dataset, [2300, 200])\n",
        "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_set, batch_size=8, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdB40qKZ1Bau"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class TargetDecoder(nn.Module):\n",
        "    def __init__(self, features_dim=768): \n",
        "        super(TargetDecoder, self).__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(features_dim, 15680))\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(), \n",
        "            nn.ConvTranspose3d(32, 7, kernel_size=3),\n",
        "            # nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = x.reshape(x.shape[0], 64, 5, 7, 7)\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "target_decoder = TargetDecoder().to(device)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJaPP7f21jKW"
      },
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "accuracy = Accuracy()\n",
        "EPOCHS = 100\n",
        "optimizer = optim.Adam(target_decoder.parameters(), lr=1e-3)\n",
        "count = 0\n",
        "i = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  count_per_epoch = 0\n",
        "  target_decoder.train()\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  for target_tensor_input, target_tensor_target in train_dataloader:\n",
        "    i += 1\n",
        "    target_tensor_input = target_tensor_input.float().to(device)\n",
        "    target_tensor_target = target_tensor_target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predict = target_decoder(target_tensor_input)\n",
        "    loss = loss_function(predict, target_tensor_target)\n",
        "    _,pred_label = torch.max(predict, dim = 1)\n",
        "    for k in range(len(pred_label)):\n",
        "      if(np.all(pred_label[k].to(\"cpu\").numpy() == target_tensor_target[k].to(\"cpu\").numpy())):\n",
        "        count+=1\n",
        "        count_per_epoch += 1\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(accuracy(predict.to(\"cpu\"),target_tensor_target.to(\"cpu\")))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss = np.array(train_loss).mean()\n",
        "  train_acc = np.array(train_acc).mean()\n",
        "\n",
        "  target_decoder.eval()\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  for target_tensor_input, target_tensor_target in val_dataloader:\n",
        "    i += 1\n",
        "    target_tensor_input = target_tensor_input.float().to(device)\n",
        "    predict = target_decoder(target_tensor_input)\n",
        "    _,pred_label = torch.max(predict, dim = 1)\n",
        "    for k in range(len(pred_label)):\n",
        "      if(np.all(pred_label[k].to(\"cpu\").numpy() == target_tensor_target[k].to(\"cpu\").numpy())):\n",
        "        count+=1\n",
        "        count_per_epoch += 1\n",
        "    loss = loss_function(predict.to(device), target_tensor_target.to(device))\n",
        "    val_loss.append(loss.item())\n",
        "    val_acc.append(accuracy(predict.to(\"cpu\"), target_tensor_target.to(\"cpu\")))\n",
        "  \n",
        "  val_loss = np.array(val_loss).mean()\n",
        "  val_acc = np.array(val_acc).mean()\n",
        "  print(f\"epoch: {epoch} | loss: {train_loss} | val_loss: {val_loss}\")\n",
        "  print(f\"train_acc: {train_acc} | val_acc: {val_acc} | count: {count_per_epoch}\")\n",
        "i = i/EPOCHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA4Lw1lhFC-0"
      },
      "source": [
        "from torchmetrics import F1\n",
        "f1 = F1(num_classes=7,mdmc_average=\"global\", ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jrI3iBwOz2A"
      },
      "source": [
        "test_loss = []\n",
        "test_acc = []\n",
        "test_f1 = []\n",
        "target_decoder.eval()\n",
        "count = 0\n",
        "i = 0\n",
        "for target_tensor_input, target_tensor_target in test_dataloader:\n",
        "  i+=1\n",
        "  target_tensor_input = target_tensor_input.float().to(device)\n",
        "  predict = target_decoder(target_tensor_input)\n",
        "  loss = loss_function(predict.to(device), target_tensor_target.to(device))\n",
        "  _,pred_label = torch.max(predict, dim = 1)\n",
        "  if(np.all(pred_label.to(\"cpu\").numpy() == target_tensor_target.to(\"cpu\").numpy())):\n",
        "    count+=1\n",
        "  test_loss.append(loss.item())\n",
        "  test_acc.append(accuracy(predict.to(\"cpu\"), target_tensor_target.to(\"cpu\")))\n",
        "  test_f1.append(f1(predict.to(\"cpu\"), target_tensor_target.to(\"cpu\")))\n",
        "test_loss = np.array(test_loss).mean()\n",
        "test_acc = np.array(test_acc).mean()\n",
        "test_f1 = np.array(test_f1).mean()\n",
        "print(f\"test_loss: {test_loss} | test_acc: {test_acc} | f1: {test_f1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWzO5qDcgLc_"
      },
      "source": [
        "## Final Training on the whole Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzEiCxPgKXq"
      },
      "source": [
        "all_training_dataset = TargetDataset(all_targets, all_chat_embeddings)\n",
        "all_train_dataloader = DataLoader(all_training_dataset, batch_size=8, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUm56WsIgWah"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class TargetDecoder(nn.Module):\n",
        "    def __init__(self, features_dim=768): \n",
        "        super(TargetDecoder, self).__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(features_dim, 15680))\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(), \n",
        "            nn.ConvTranspose3d(32, 7, kernel_size=3),\n",
        "            # nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = x.reshape(x.shape[0], 64, 5, 7, 7)\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "target_decoder = TargetDecoder().to(device)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T0MuWhCgolL"
      },
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "optimizer = optim.Adam(target_decoder.parameters(), lr=1e-6)\n",
        "accuracy = Accuracy()\n",
        "EPOCHS = 50\n",
        "count = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  count_per_epoch = 0\n",
        "  target_decoder.eval()\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  for target_tensor_input, target_tensor_target in all_train_dataloader:\n",
        "    target_tensor_input = target_tensor_input.float().to(device)\n",
        "    target_tensor_target = target_tensor_target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predict = target_decoder(target_tensor_input)\n",
        "    loss = loss_function(predict, target_tensor_target)\n",
        "    _,pred_label = torch.max(predict, dim = 1)\n",
        "    for k in range(len(pred_label)):\n",
        "      if(np.all(pred_label[k].to(\"cpu\").numpy() == target_tensor_target[k].to(\"cpu\").numpy())):\n",
        "        count+=1\n",
        "        count_per_epoch += 1\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(accuracy(predict.to(\"cpu\"),target_tensor_target.to(\"cpu\")))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss = np.array(train_loss).mean()\n",
        "  train_acc = np.array(train_acc).mean()\n",
        "\n",
        "  print(f\"epoch: {epoch} | loss: {train_loss} | train_acc: {train_acc} | count: {count_per_epoch}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeymUe2GSER1"
      },
      "source": [
        "torch.save(target_decoder.state_dict(), 'fragm_target_decoder.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}