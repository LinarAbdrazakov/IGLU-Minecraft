{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=0),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 64\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AlinaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 11:04:56,824\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-18 11:04:56,836\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=58357)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=58357)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/421d3_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/421d3_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210918_110457-421d3_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=58357)\u001b[0m 2021-09-18 11:05:00,414\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=58357)\u001b[0m 2021-09-18 11:05:00,414\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=58357)\u001b[0m 2021-09-18 11:05:06,477\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2018264439370897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0011697852865390403\n",
      "          policy_loss: -0.19659400859640705\n",
      "          total_loss: -0.19176375745899146\n",
      "          vf_explained_var: 0.24490948021411896\n",
      "          vf_loss: 0.00661455605748213\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.82\n",
      "    ram_util_percent: 90.77741935483871\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03918401011220225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 104.89415812802005\n",
      "    mean_inference_ms: 1.882579062249396\n",
      "    mean_raw_obs_processing_ms: 0.15775569073565596\n",
      "  time_since_restore: 107.96803379058838\n",
      "  time_this_iter_s: 107.96803379058838\n",
      "  time_total_s: 107.96803379058838\n",
      "  timers:\n",
      "    learn_throughput: 1243.848\n",
      "    learn_time_ms: 803.957\n",
      "    load_throughput: 51261.323\n",
      "    load_time_ms: 19.508\n",
      "    sample_throughput: 9.334\n",
      "    sample_time_ms: 107138.561\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1631963214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         107.968</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17195624974038864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0013576971563139355\n",
      "          policy_loss: -0.19032121176520983\n",
      "          total_loss: -0.1885722594956557\n",
      "          vf_explained_var: 0.26884379982948303\n",
      "          vf_loss: 0.0033327439237230768\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.9923076923077\n",
      "    ram_util_percent: 53.27692307692307\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039058447143285074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 80.57362350644183\n",
      "    mean_inference_ms: 1.7438840348448228\n",
      "    mean_raw_obs_processing_ms: 0.1472306409205826\n",
      "  time_since_restore: 117.66638135910034\n",
      "  time_this_iter_s: 9.698347568511963\n",
      "  time_total_s: 117.66638135910034\n",
      "  timers:\n",
      "    learn_throughput: 1411.053\n",
      "    learn_time_ms: 708.69\n",
      "    load_throughput: 86598.34\n",
      "    load_time_ms: 11.548\n",
      "    sample_throughput: 17.209\n",
      "    sample_time_ms: 58108.12\n",
      "    update_time_ms: 1.497\n",
      "  timestamp: 1631963224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         117.666</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15569361878765953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0016002887828744348\n",
      "          policy_loss: -0.23744630076818996\n",
      "          total_loss: -0.23661195950375663\n",
      "          vf_explained_var: 0.2895536720752716\n",
      "          vf_loss: 0.0023112639066918445\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.021428571428565\n",
      "    ram_util_percent: 53.7357142857143\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03893888494599488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 67.06464227718851\n",
      "    mean_inference_ms: 1.6650016278139168\n",
      "    mean_raw_obs_processing_ms: 0.14104516501022715\n",
      "  time_since_restore: 127.39348125457764\n",
      "  time_this_iter_s: 9.727099895477295\n",
      "  time_total_s: 127.39348125457764\n",
      "  timers:\n",
      "    learn_throughput: 1476.388\n",
      "    learn_time_ms: 677.329\n",
      "    load_throughput: 113432.123\n",
      "    load_time_ms: 8.816\n",
      "    sample_throughput: 23.938\n",
      "    sample_time_ms: 41773.834\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1631963233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         127.393</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.12306880975763003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.000839934298924473\n",
      "          policy_loss: -0.22062717196014192\n",
      "          total_loss: -0.22025686071978676\n",
      "          vf_explained_var: 0.5524377822875977\n",
      "          vf_loss: 0.0015800038456088967\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.685714285714276\n",
      "    ram_util_percent: 53.4642857142857\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875076897674319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 58.269877054092674\n",
      "    mean_inference_ms: 1.6109911720849805\n",
      "    mean_raw_obs_processing_ms: 0.13693614228157333\n",
      "  time_since_restore: 136.85356283187866\n",
      "  time_this_iter_s: 9.460081577301025\n",
      "  time_total_s: 136.85356283187866\n",
      "  timers:\n",
      "    learn_throughput: 1513.999\n",
      "    learn_time_ms: 660.502\n",
      "    load_throughput: 134821.729\n",
      "    load_time_ms: 7.417\n",
      "    sample_throughput: 29.814\n",
      "    sample_time_ms: 33541.045\n",
      "    update_time_ms: 1.526\n",
      "  timestamp: 1631963243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         136.854</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.13940821637709935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00042468792188975847\n",
      "          policy_loss: -0.17516648636923896\n",
      "          total_loss: -0.17484141050113572\n",
      "          vf_explained_var: 0.42267173528671265\n",
      "          vf_loss: 0.001713849793628065\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.25384615384616\n",
      "    ram_util_percent: 53.13076923076923\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038564000041653945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.004906926323976\n",
      "    mean_inference_ms: 1.5714198420051346\n",
      "    mean_raw_obs_processing_ms: 0.1338542696577787\n",
      "  time_since_restore: 146.08443021774292\n",
      "  time_this_iter_s: 9.230867385864258\n",
      "  time_total_s: 146.08443021774292\n",
      "  timers:\n",
      "    learn_throughput: 1540.844\n",
      "    learn_time_ms: 648.995\n",
      "    load_throughput: 152628.928\n",
      "    load_time_ms: 6.552\n",
      "    sample_throughput: 35.018\n",
      "    sample_time_ms: 28557.007\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1631963252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         146.084</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-41\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.12240182004041142\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0012270989028972206\n",
      "          policy_loss: -0.24358405735757616\n",
      "          total_loss: -0.24378442449702156\n",
      "          vf_explained_var: 0.6282027959823608\n",
      "          vf_loss: 0.0010159841415265367\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.900000000000006\n",
      "    ram_util_percent: 52.95384615384614\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038394767037164206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.278857335543144\n",
      "    mean_inference_ms: 1.5410321658745436\n",
      "    mean_raw_obs_processing_ms: 0.13147640476433273\n",
      "  time_since_restore: 155.29788255691528\n",
      "  time_this_iter_s: 9.213452339172363\n",
      "  time_total_s: 155.29788255691528\n",
      "  timers:\n",
      "    learn_throughput: 1559.277\n",
      "    learn_time_ms: 641.323\n",
      "    load_throughput: 166570.631\n",
      "    load_time_ms: 6.003\n",
      "    sample_throughput: 39.633\n",
      "    sample_time_ms: 25231.39\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1631963261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         155.298</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-07-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.13986577557192909\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0003794179507494672\n",
      "          policy_loss: -0.18513635471463202\n",
      "          total_loss: -0.18561745170089933\n",
      "          vf_explained_var: 0.7448583841323853\n",
      "          vf_loss: 0.0009163742960986888\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3\n",
      "    ram_util_percent: 52.869230769230754\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03823987394840805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.56645593872593\n",
      "    mean_inference_ms: 1.516577872825932\n",
      "    mean_raw_obs_processing_ms: 0.12956085120440025\n",
      "  time_since_restore: 164.48815321922302\n",
      "  time_this_iter_s: 9.19027066230774\n",
      "  time_total_s: 164.48815321922302\n",
      "  timers:\n",
      "    learn_throughput: 1570.877\n",
      "    learn_time_ms: 636.587\n",
      "    load_throughput: 178849.593\n",
      "    load_time_ms: 5.591\n",
      "    sample_throughput: 43.76\n",
      "    sample_time_ms: 22851.915\n",
      "    update_time_ms: 1.521\n",
      "  timestamp: 1631963271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         164.488</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015624999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.14316424594985114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0004500684930109047\n",
      "          policy_loss: -0.25178282550639575\n",
      "          total_loss: -0.25204691597157053\n",
      "          vf_explained_var: 0.47237062454223633\n",
      "          vf_loss: 0.0011668493491015397\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03076923076923\n",
      "    ram_util_percent: 52.79230769230769\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03810706294095018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.56032831178405\n",
      "    mean_inference_ms: 1.4964959867028171\n",
      "    mean_raw_obs_processing_ms: 0.1279832941205673\n",
      "  time_since_restore: 173.637291431427\n",
      "  time_this_iter_s: 9.14913821220398\n",
      "  time_total_s: 173.637291431427\n",
      "  timers:\n",
      "    learn_throughput: 1576.114\n",
      "    learn_time_ms: 634.472\n",
      "    load_throughput: 188891.133\n",
      "    load_time_ms: 5.294\n",
      "    sample_throughput: 47.482\n",
      "    sample_time_ms: 21060.718\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1631963280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         173.637</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-09\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812499999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.16709488299157885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0004365125089325122\n",
      "          policy_loss: -0.26218835032648513\n",
      "          total_loss: -0.2633674282166693\n",
      "          vf_explained_var: 0.6881450414657593\n",
      "          vf_loss: 0.0004915261861848801\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0923076923077\n",
      "    ram_util_percent: 52.784615384615385\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03798914923501112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.06795277948025\n",
      "    mean_inference_ms: 1.4796353744122197\n",
      "    mean_raw_obs_processing_ms: 0.12661471342949196\n",
      "  time_since_restore: 182.69688177108765\n",
      "  time_this_iter_s: 9.059590339660645\n",
      "  time_total_s: 182.69688177108765\n",
      "  timers:\n",
      "    learn_throughput: 1582.833\n",
      "    learn_time_ms: 631.778\n",
      "    load_throughput: 197942.047\n",
      "    load_time_ms: 5.052\n",
      "    sample_throughput: 50.868\n",
      "    sample_time_ms: 19658.669\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1631963289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         182.697</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906249999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.20750636491510605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0013582561992599353\n",
      "          policy_loss: -0.30657355181045004\n",
      "          total_loss: -0.30610203229718735\n",
      "          vf_explained_var: 0.8595510721206665\n",
      "          vf_loss: 0.002546053606945659\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.099999999999994\n",
      "    ram_util_percent: 52.79999999999999\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03788306180228999\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.96388120394302\n",
      "    mean_inference_ms: 1.465259299719779\n",
      "    mean_raw_obs_processing_ms: 0.12543529109386126\n",
      "  time_since_restore: 191.85502672195435\n",
      "  time_this_iter_s: 9.1581449508667\n",
      "  time_total_s: 191.85502672195435\n",
      "  timers:\n",
      "    learn_throughput: 1586.794\n",
      "    learn_time_ms: 630.202\n",
      "    load_throughput: 205305.243\n",
      "    load_time_ms: 4.871\n",
      "    sample_throughput: 53.919\n",
      "    sample_time_ms: 18546.289\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1631963298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         191.855</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531249999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2162388735347324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0007738437878677118\n",
      "          policy_loss: -0.29779169923729365\n",
      "          total_loss: -0.29901888229780726\n",
      "          vf_explained_var: 0.8082315325737\n",
      "          vf_loss: 0.0009350520164136671\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.88461538461539\n",
      "    ram_util_percent: 52.79230769230768\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03778962647468656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.160087306880314\n",
      "    mean_inference_ms: 1.4528431078833541\n",
      "    mean_raw_obs_processing_ms: 0.12441653976383983\n",
      "  time_since_restore: 200.9593963623047\n",
      "  time_this_iter_s: 9.104369640350342\n",
      "  time_total_s: 200.9593963623047\n",
      "  timers:\n",
      "    learn_throughput: 1639.763\n",
      "    learn_time_ms: 609.844\n",
      "    load_throughput: 308513.593\n",
      "    load_time_ms: 3.241\n",
      "    sample_throughput: 115.18\n",
      "    sample_time_ms: 8682.1\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1631963307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         200.959</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3019250732329157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017863470547736505\n",
      "          policy_loss: -0.29298865778578653\n",
      "          total_loss: -0.29388851159148743\n",
      "          vf_explained_var: 0.8795185685157776\n",
      "          vf_loss: 0.002117653380668748\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.800000000000004\n",
      "    ram_util_percent: 52.814285714285695\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03770473057577754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.59431098415245\n",
      "    mean_inference_ms: 1.4419795507728967\n",
      "    mean_raw_obs_processing_ms: 0.12350968376671922\n",
      "  time_since_restore: 210.1140604019165\n",
      "  time_this_iter_s: 9.154664039611816\n",
      "  time_total_s: 210.1140604019165\n",
      "  timers:\n",
      "    learn_throughput: 1637.608\n",
      "    learn_time_ms: 610.647\n",
      "    load_throughput: 312185.362\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 115.916\n",
      "    sample_time_ms: 8626.923\n",
      "    update_time_ms: 1.522\n",
      "  timestamp: 1631963316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         210.114</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765624999999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41232521103488073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0028415517179936956\n",
      "          policy_loss: -0.29263091534376146\n",
      "          total_loss: -0.29511622670623994\n",
      "          vf_explained_var: 0.36268362402915955\n",
      "          vf_loss: 0.0016376667436917261\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96153846153847\n",
      "    ram_util_percent: 52.79999999999999\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03762907803340489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.220492028170288\n",
      "    mean_inference_ms: 1.4323982749079593\n",
      "    mean_raw_obs_processing_ms: 0.12270476255914332\n",
      "  time_since_restore: 219.26353240013123\n",
      "  time_this_iter_s: 9.149471998214722\n",
      "  time_total_s: 219.26353240013123\n",
      "  timers:\n",
      "    learn_throughput: 1637.086\n",
      "    learn_time_ms: 610.841\n",
      "    load_throughput: 314406.165\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 116.699\n",
      "    sample_time_ms: 8569.025\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1631963325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         219.264</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.882812499999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3610242345266872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030075919693449767\n",
      "          policy_loss: -0.3418657117419773\n",
      "          total_loss: -0.34502124674618245\n",
      "          vf_explained_var: 0.8625590801239014\n",
      "          vf_loss: 0.0004545595512253284\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.76923076923077\n",
      "    ram_util_percent: 52.88461538461537\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037561282078045974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.003895040766917\n",
      "    mean_inference_ms: 1.4239104799432145\n",
      "    mean_raw_obs_processing_ms: 0.12199159681164037\n",
      "  time_since_restore: 228.37966132164001\n",
      "  time_this_iter_s: 9.116128921508789\n",
      "  time_total_s: 228.37966132164001\n",
      "  timers:\n",
      "    learn_throughput: 1641.637\n",
      "    learn_time_ms: 609.148\n",
      "    load_throughput: 314866.413\n",
      "    load_time_ms: 3.176\n",
      "    sample_throughput: 117.146\n",
      "    sample_time_ms: 8536.344\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1631963335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          228.38</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4414062499999995e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6607209573189418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06460219104603085\n",
      "          policy_loss: -0.01888044277826945\n",
      "          total_loss: -0.02389566467867957\n",
      "          vf_explained_var: 0.663105845451355\n",
      "          vf_loss: 0.001590412508812733\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53846153846154\n",
      "    ram_util_percent: 52.86153846153846\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749999767082206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.91828209905416\n",
      "    mean_inference_ms: 1.416347304525013\n",
      "    mean_raw_obs_processing_ms: 0.12134889993092599\n",
      "  time_since_restore: 237.58284044265747\n",
      "  time_this_iter_s: 9.203179121017456\n",
      "  time_total_s: 237.58284044265747\n",
      "  timers:\n",
      "    learn_throughput: 1642.726\n",
      "    learn_time_ms: 608.744\n",
      "    load_throughput: 314786.068\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 117.179\n",
      "    sample_time_ms: 8533.966\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1631963344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         237.583</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: 0.1875\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109375e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8807664725515577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023539998221427678\n",
      "          policy_loss: 0.15372695823510488\n",
      "          total_loss: 0.18073118097252316\n",
      "          vf_explained_var: 0.7232553362846375\n",
      "          vf_loss: 0.03581102263156532\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.292857142857144\n",
      "    ram_util_percent: 52.89285714285713\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03744850384506829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.944850676969192\n",
      "    mean_inference_ms: 1.4096126937334894\n",
      "    mean_raw_obs_processing_ms: 0.12101252463627549\n",
      "  time_since_restore: 247.3980951309204\n",
      "  time_this_iter_s: 9.81525468826294\n",
      "  time_total_s: 247.3980951309204\n",
      "  timers:\n",
      "    learn_throughput: 1644.516\n",
      "    learn_time_ms: 608.082\n",
      "    load_throughput: 315586.622\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 116.349\n",
      "    sample_time_ms: 8594.811\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1631963354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         247.398</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  0.1875</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.11764705882352941\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.493164062500002e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7813320272498661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02645949265188765\n",
      "          policy_loss: 0.06311811101105479\n",
      "          total_loss: 0.1279387176864677\n",
      "          vf_explained_var: 0.5167554020881653\n",
      "          vf_loss: 0.072632474033162\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82142857142857\n",
      "    ram_util_percent: 53.13571428571429\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037402961949649365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.066391389352443\n",
      "    mean_inference_ms: 1.4035810823574397\n",
      "    mean_raw_obs_processing_ms: 0.12076108954993409\n",
      "  time_since_restore: 257.2140429019928\n",
      "  time_this_iter_s: 9.815947771072388\n",
      "  time_total_s: 257.2140429019928\n",
      "  timers:\n",
      "    learn_throughput: 1646.458\n",
      "    learn_time_ms: 607.364\n",
      "    load_throughput: 313225.149\n",
      "    load_time_ms: 3.193\n",
      "    sample_throughput: 115.499\n",
      "    sample_time_ms: 8658.053\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1631963364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         257.214</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.117647</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1111111111111111\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.239746093749998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4423211455345153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03675220778792191\n",
      "          policy_loss: 0.16689947607616584\n",
      "          total_loss: 0.17278852835297584\n",
      "          vf_explained_var: 0.679054319858551\n",
      "          vf_loss: 0.02030923782537381\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.38666666666666\n",
      "    ram_util_percent: 53.88000000000001\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03736685755744365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.272049502920346\n",
      "    mean_inference_ms: 1.398338877590084\n",
      "    mean_raw_obs_processing_ms: 0.12051974499736197\n",
      "  time_since_restore: 268.0118763446808\n",
      "  time_this_iter_s: 10.797833442687988\n",
      "  time_total_s: 268.0118763446808\n",
      "  timers:\n",
      "    learn_throughput: 1643.783\n",
      "    learn_time_ms: 608.353\n",
      "    load_throughput: 312208.6\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 113.354\n",
      "    sample_time_ms: 8821.895\n",
      "    update_time_ms: 1.482\n",
      "  timestamp: 1631963374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         268.012</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.111111</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.10526315789473684\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0058646029896208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01809309908322166\n",
      "          policy_loss: 0.007175225350591872\n",
      "          total_loss: -0.0009995124406284757\n",
      "          vf_explained_var: 0.7102770805358887\n",
      "          vf_loss: 0.011881674614010585\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.53571428571428\n",
      "    ram_util_percent: 54.164285714285725\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03733704361507213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.546689182204478\n",
      "    mean_inference_ms: 1.393630970226535\n",
      "    mean_raw_obs_processing_ms: 0.12029062556217265\n",
      "  time_since_restore: 277.6142613887787\n",
      "  time_this_iter_s: 9.6023850440979\n",
      "  time_total_s: 277.6142613887787\n",
      "  timers:\n",
      "    learn_throughput: 1639.513\n",
      "    learn_time_ms: 609.937\n",
      "    load_throughput: 311117.836\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 112.681\n",
      "    sample_time_ms: 8874.572\n",
      "    update_time_ms: 1.476\n",
      "  timestamp: 1631963384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         277.614</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.105263</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-09-53\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9537920488251581\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008438425565529744\n",
      "          policy_loss: -0.1206215523597267\n",
      "          total_loss: -0.13344770107004378\n",
      "          vf_explained_var: 0.12720167636871338\n",
      "          vf_loss: 0.0067107313894666735\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.276923076923076\n",
      "    ram_util_percent: 54.15384615384616\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03730880504334144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.879224038988966\n",
      "    mean_inference_ms: 1.3892864580459938\n",
      "    mean_raw_obs_processing_ms: 0.12005780681300819\n",
      "  time_since_restore: 286.28969645500183\n",
      "  time_this_iter_s: 8.675435066223145\n",
      "  time_total_s: 286.28969645500183\n",
      "  timers:\n",
      "    learn_throughput: 1643.304\n",
      "    learn_time_ms: 608.53\n",
      "    load_throughput: 312206.276\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 113.28\n",
      "    sample_time_ms: 8827.715\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1631963393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          286.29</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09523809523809523\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7380413797166612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01368374277540755\n",
      "          policy_loss: 0.0805840445889367\n",
      "          total_loss: 0.06372020807531145\n",
      "          vf_explained_var: 0.12812069058418274\n",
      "          vf_loss: 0.0005148870280309994\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.345454545454544\n",
      "    ram_util_percent: 54.10909090909092\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03728045114796436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.260746501326665\n",
      "    mean_inference_ms: 1.3851700800596323\n",
      "    mean_raw_obs_processing_ms: 0.12003913466971573\n",
      "  time_since_restore: 294.16168189048767\n",
      "  time_this_iter_s: 7.87198543548584\n",
      "  time_total_s: 294.16168189048767\n",
      "  timers:\n",
      "    learn_throughput: 1640.222\n",
      "    learn_time_ms: 609.674\n",
      "    load_throughput: 310742.126\n",
      "    load_time_ms: 3.218\n",
      "    sample_throughput: 114.899\n",
      "    sample_time_ms: 8703.315\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1631963401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         294.162</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.0952381</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.09090909090909091\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8593016352918413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006778134844624143\n",
      "          policy_loss: -0.10300603128141828\n",
      "          total_loss: -0.10640543558531337\n",
      "          vf_explained_var: 0.1125631332397461\n",
      "          vf_loss: 0.00519277036914395\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.621428571428574\n",
      "    ram_util_percent: 54.28571428571428\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03725663827150868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.689604304161524\n",
      "    mean_inference_ms: 1.3814170275544977\n",
      "    mean_raw_obs_processing_ms: 0.12000793272144804\n",
      "  time_since_restore: 303.8367528915405\n",
      "  time_this_iter_s: 9.675071001052856\n",
      "  time_total_s: 303.8367528915405\n",
      "  timers:\n",
      "    learn_throughput: 1646.603\n",
      "    learn_time_ms: 607.311\n",
      "    load_throughput: 312183.039\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 114.185\n",
      "    sample_time_ms: 8757.741\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1631963410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         303.837</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.0909091</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.08695652173913043\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8704562856091393\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006414486770255302\n",
      "          policy_loss: -0.1381760804189576\n",
      "          total_loss: -0.14335700041717953\n",
      "          vf_explained_var: 0.21256276965141296\n",
      "          vf_loss: 0.0035228481833150405\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.099999999999994\n",
      "    ram_util_percent: 54.316666666666656\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037232515053586386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.15847986085505\n",
      "    mean_inference_ms: 1.3779050328878601\n",
      "    mean_raw_obs_processing_ms: 0.11998805396004923\n",
      "  time_since_restore: 312.47301983833313\n",
      "  time_this_iter_s: 8.636266946792603\n",
      "  time_total_s: 312.47301983833313\n",
      "  timers:\n",
      "    learn_throughput: 1652.393\n",
      "    learn_time_ms: 605.183\n",
      "    load_throughput: 312327.168\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 114.83\n",
      "    sample_time_ms: 8708.499\n",
      "    update_time_ms: 1.519\n",
      "  timestamp: 1631963419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         312.473</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.0869565</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.08333333333333333\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6874538130230374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012950988931919996\n",
      "          policy_loss: 0.0292768367462688\n",
      "          total_loss: 0.020437346026301383\n",
      "          vf_explained_var: 0.024658210575580597\n",
      "          vf_loss: 0.008033448380107682\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.98571428571429\n",
      "    ram_util_percent: 54.39285714285713\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03720937954413937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.665616183592707\n",
      "    mean_inference_ms: 1.3746266720709137\n",
      "    mean_raw_obs_processing_ms: 0.12004707986013137\n",
      "  time_since_restore: 322.59866762161255\n",
      "  time_this_iter_s: 10.125647783279419\n",
      "  time_total_s: 322.59866762161255\n",
      "  timers:\n",
      "    learn_throughput: 1648.357\n",
      "    learn_time_ms: 606.665\n",
      "    load_throughput: 313562.345\n",
      "    load_time_ms: 3.189\n",
      "    sample_throughput: 113.534\n",
      "    sample_time_ms: 8807.956\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1631963429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         322.599</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.0833333</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7816539406776428\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011588512964183116\n",
      "          policy_loss: -0.0294151504834493\n",
      "          total_loss: -0.041229958997832404\n",
      "          vf_explained_var: -0.11195705085992813\n",
      "          vf_loss: 0.006000296042152008\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.78666666666667\n",
      "    ram_util_percent: 54.506666666666675\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03718702505199055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.207172175078746\n",
      "    mean_inference_ms: 1.3715697114744703\n",
      "    mean_raw_obs_processing_ms: 0.12018468147463941\n",
      "  time_since_restore: 332.9199414253235\n",
      "  time_this_iter_s: 10.321273803710938\n",
      "  time_total_s: 332.9199414253235\n",
      "  timers:\n",
      "    learn_throughput: 1646.463\n",
      "    learn_time_ms: 607.362\n",
      "    load_throughput: 312739.365\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 112.119\n",
      "    sample_time_ms: 8919.073\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1631963439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          332.92</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.07692307692307693\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.770128779941135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01599521642388134\n",
      "          policy_loss: -0.039872885371247925\n",
      "          total_loss: -0.052074534859922195\n",
      "          vf_explained_var: -0.4746800661087036\n",
      "          vf_loss: 0.005497662994700173\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.60666666666667\n",
      "    ram_util_percent: 54.68000000000002\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716706924639947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.779559856339464\n",
      "    mean_inference_ms: 1.3687545318031864\n",
      "    mean_raw_obs_processing_ms: 0.12035293523432616\n",
      "  time_since_restore: 343.26536440849304\n",
      "  time_this_iter_s: 10.345422983169556\n",
      "  time_total_s: 343.26536440849304\n",
      "  timers:\n",
      "    learn_throughput: 1642.071\n",
      "    learn_time_ms: 608.987\n",
      "    load_throughput: 313658.486\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 111.477\n",
      "    sample_time_ms: 8970.469\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1631963450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         343.265</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.0769231</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.07407407407407407\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9447512798839146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016517577385555822\n",
      "          policy_loss: -0.09943221130718788\n",
      "          total_loss: -0.11274852990690205\n",
      "          vf_explained_var: 0.2861216366291046\n",
      "          vf_loss: 0.00612915197852999\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29285714285715\n",
      "    ram_util_percent: 54.60000000000001\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714808229901161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.37904726855974\n",
      "    mean_inference_ms: 1.3661193831616685\n",
      "    mean_raw_obs_processing_ms: 0.12051952696245676\n",
      "  time_since_restore: 353.126953125\n",
      "  time_this_iter_s: 9.861588716506958\n",
      "  time_total_s: 353.126953125\n",
      "  timers:\n",
      "    learn_throughput: 1634.515\n",
      "    learn_time_ms: 611.802\n",
      "    load_throughput: 316252.893\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 111.455\n",
      "    sample_time_ms: 8972.233\n",
      "    update_time_ms: 1.552\n",
      "  timestamp: 1631963460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         353.127</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.0740741</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.07142857142857142\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7941524108250937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01644895938988322\n",
      "          policy_loss: -0.06326146556271448\n",
      "          total_loss: -0.07805497679445479\n",
      "          vf_explained_var: 0.6235350966453552\n",
      "          vf_loss: 0.0031459786023737657\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.05333333333333\n",
      "    ram_util_percent: 54.96\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713108992108977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.003851642360225\n",
      "    mean_inference_ms: 1.3636807149747414\n",
      "    mean_raw_obs_processing_ms: 0.12070085440543654\n",
      "  time_since_restore: 363.63806438446045\n",
      "  time_this_iter_s: 10.51111125946045\n",
      "  time_total_s: 363.63806438446045\n",
      "  timers:\n",
      "    learn_throughput: 1640.772\n",
      "    learn_time_ms: 609.469\n",
      "    load_throughput: 318449.928\n",
      "    load_time_ms: 3.14\n",
      "    sample_throughput: 111.783\n",
      "    sample_time_ms: 8945.932\n",
      "    update_time_ms: 1.546\n",
      "  timestamp: 1631963470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         363.638</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.0714286</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-11-20\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.06896551724137931\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.109642067220476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012402049227611714\n",
      "          policy_loss: -0.0008963064601023991\n",
      "          total_loss: -0.0175599814289146\n",
      "          vf_explained_var: -0.25409334897994995\n",
      "          vf_loss: 0.004431212176051405\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.857142857142854\n",
      "    ram_util_percent: 54.89999999999999\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711462258895591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.650718128061754\n",
      "    mean_inference_ms: 1.3613675807034968\n",
      "    mean_raw_obs_processing_ms: 0.12087777318358742\n",
      "  time_since_restore: 373.3800296783447\n",
      "  time_this_iter_s: 9.741965293884277\n",
      "  time_total_s: 373.3800296783447\n",
      "  timers:\n",
      "    learn_throughput: 1644.516\n",
      "    learn_time_ms: 608.082\n",
      "    load_throughput: 319699.989\n",
      "    load_time_ms: 3.128\n",
      "    sample_throughput: 111.591\n",
      "    sample_time_ms: 8961.298\n",
      "    update_time_ms: 1.548\n",
      "  timestamp: 1631963480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          373.38</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.0689655</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=58353)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.0666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.06666666666666667\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.095770502090454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010864856369254354\n",
      "          policy_loss: 0.02779309472276105\n",
      "          total_loss: 0.009935800400045183\n",
      "          vf_explained_var: 0.2577665150165558\n",
      "          vf_loss: 0.0030990660798528957\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.51842105263159\n",
      "    ram_util_percent: 53.573684210526324\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709867610455487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.317648808603657\n",
      "    mean_inference_ms: 1.3591760996658777\n",
      "    mean_raw_obs_processing_ms: 0.13968268945112985\n",
      "  time_since_restore: 399.910861492157\n",
      "  time_this_iter_s: 26.530831813812256\n",
      "  time_total_s: 399.910861492157\n",
      "  timers:\n",
      "    learn_throughput: 1632.878\n",
      "    learn_time_ms: 612.416\n",
      "    load_throughput: 218759.825\n",
      "    load_time_ms: 4.571\n",
      "    sample_throughput: 93.101\n",
      "    sample_time_ms: 10741.058\n",
      "    update_time_ms: 1.553\n",
      "  timestamp: 1631963506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         399.911</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-0.0666667</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.067</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-11-57\n",
      "  done: false\n",
      "  episode_len_mean: 996.1935483870968\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.06451612903225806\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0456457773844403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009132240737117206\n",
      "          policy_loss: 0.03887356294112073\n",
      "          total_loss: 0.021112927173574766\n",
      "          vf_explained_var: -0.3266350328922272\n",
      "          vf_loss: 0.00269469449413009\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.67333333333333\n",
      "    ram_util_percent: 55.220000000000006\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708563929700727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.003722465523914\n",
      "    mean_inference_ms: 1.3571413889854131\n",
      "    mean_raw_obs_processing_ms: 0.1567128693238694\n",
      "  time_since_restore: 410.5225360393524\n",
      "  time_this_iter_s: 10.611674547195435\n",
      "  time_total_s: 410.5225360393524\n",
      "  timers:\n",
      "    learn_throughput: 1615.955\n",
      "    learn_time_ms: 618.829\n",
      "    load_throughput: 219195.401\n",
      "    load_time_ms: 4.562\n",
      "    sample_throughput: 90.838\n",
      "    sample_time_ms: 11008.604\n",
      "    update_time_ms: 1.574\n",
      "  timestamp: 1631963517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         410.523</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.0645161</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.194</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.3125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.0625\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.172387300597297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012328516236619224\n",
      "          policy_loss: -0.08037130898899503\n",
      "          total_loss: -0.10112953268819386\n",
      "          vf_explained_var: -0.4840739071369171\n",
      "          vf_loss: 0.0009641265842623802\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.88666666666667\n",
      "    ram_util_percent: 55.080000000000005\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707486590872548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.70701675915466\n",
      "    mean_inference_ms: 1.3552506692801893\n",
      "    mean_raw_obs_processing_ms: 0.17215789506250337\n",
      "  time_since_restore: 420.8134255409241\n",
      "  time_this_iter_s: 10.290889501571655\n",
      "  time_total_s: 420.8134255409241\n",
      "  timers:\n",
      "    learn_throughput: 1604.519\n",
      "    learn_time_ms: 623.24\n",
      "    load_throughput: 213538.609\n",
      "    load_time_ms: 4.683\n",
      "    sample_throughput: 90.37\n",
      "    sample_time_ms: 11065.637\n",
      "    update_time_ms: 1.574\n",
      "  timestamp: 1631963527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         420.813</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -0.0625</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.4242424242424\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.06060606060606061\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1235780742433334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011599546108501179\n",
      "          policy_loss: -0.03696580570605066\n",
      "          total_loss: -0.05730803575780657\n",
      "          vf_explained_var: -0.3118656575679779\n",
      "          vf_loss: 0.0008921169142316406\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.52000000000001\n",
      "    ram_util_percent: 55.24666666666667\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037066129187851016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.426369107274727\n",
      "    mean_inference_ms: 1.3534917078880586\n",
      "    mean_raw_obs_processing_ms: 0.18618251454525303\n",
      "  time_since_restore: 431.35428261756897\n",
      "  time_this_iter_s: 10.540857076644897\n",
      "  time_total_s: 431.35428261756897\n",
      "  timers:\n",
      "    learn_throughput: 1599.946\n",
      "    learn_time_ms: 625.021\n",
      "    load_throughput: 213477.745\n",
      "    load_time_ms: 4.684\n",
      "    sample_throughput: 88.855\n",
      "    sample_time_ms: 11254.302\n",
      "    update_time_ms: 1.578\n",
      "  timestamp: 1631963538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         431.354</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.0606061</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.424</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.5294117647059\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.058823529411764705\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9142776052157084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009818458341828871\n",
      "          policy_loss: 0.07874921965930197\n",
      "          total_loss: 0.06015040386054251\n",
      "          vf_explained_var: -0.7098816633224487\n",
      "          vf_loss: 0.0005427467583407027\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.82000000000001\n",
      "    ram_util_percent: 55.13333333333333\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705916176658878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.160682250308504\n",
      "    mean_inference_ms: 1.3518508996286638\n",
      "    mean_raw_obs_processing_ms: 0.19896263503789702\n",
      "  time_since_restore: 442.20810747146606\n",
      "  time_this_iter_s: 10.853824853897095\n",
      "  time_total_s: 442.20810747146606\n",
      "  timers:\n",
      "    learn_throughput: 1587.545\n",
      "    learn_time_ms: 629.904\n",
      "    load_throughput: 194885.396\n",
      "    load_time_ms: 5.131\n",
      "    sample_throughput: 88.325\n",
      "    sample_time_ms: 11321.797\n",
      "    update_time_ms: 1.576\n",
      "  timestamp: 1631963549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         442.208</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.0588235</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.529</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.6285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.05714285714285714\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1527813381618923\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01133659979486487\n",
      "          policy_loss: 0.021502523703707588\n",
      "          total_loss: 0.0025051930712329017\n",
      "          vf_explained_var: -0.4942820966243744\n",
      "          vf_loss: 0.0025290776348103664\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.81333333333334\n",
      "    ram_util_percent: 54.74000000000001\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705255764335652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.90832433773789\n",
      "    mean_inference_ms: 1.3502998813549356\n",
      "    mean_raw_obs_processing_ms: 0.2106220014369468\n",
      "  time_since_restore: 452.4564208984375\n",
      "  time_this_iter_s: 10.248313426971436\n",
      "  time_total_s: 452.4564208984375\n",
      "  timers:\n",
      "    learn_throughput: 1584.059\n",
      "    learn_time_ms: 631.29\n",
      "    load_throughput: 193760.03\n",
      "    load_time_ms: 5.161\n",
      "    sample_throughput: 88.393\n",
      "    sample_time_ms: 11313.066\n",
      "    update_time_ms: 1.573\n",
      "  timestamp: 1631963559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         452.456</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.0571429</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.629</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.7222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.05555555555555555\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0370110630989076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014701381772580992\n",
      "          policy_loss: 0.044881261533333196\n",
      "          total_loss: 0.02819081179590689\n",
      "          vf_explained_var: -0.2142939269542694\n",
      "          vf_loss: 0.0036778447864991093\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.9\n",
      "    ram_util_percent: 54.592857142857156\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704672974473737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.668044259829003\n",
      "    mean_inference_ms: 1.3488342349232783\n",
      "    mean_raw_obs_processing_ms: 0.22125482688150724\n",
      "  time_since_restore: 462.3665187358856\n",
      "  time_this_iter_s: 9.91009783744812\n",
      "  time_total_s: 462.3665187358856\n",
      "  timers:\n",
      "    learn_throughput: 1583.654\n",
      "    learn_time_ms: 631.451\n",
      "    load_throughput: 193346.486\n",
      "    load_time_ms: 5.172\n",
      "    sample_throughput: 88.736\n",
      "    sample_time_ms: 11269.352\n",
      "    update_time_ms: 1.566\n",
      "  timestamp: 1631963569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         462.367</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.0555556</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.8108108108108\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.05405405405405406\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1688566896650525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0102637997664965\n",
      "          policy_loss: -0.07569030183884833\n",
      "          total_loss: -0.09046404154764282\n",
      "          vf_explained_var: 0.23527081310749054\n",
      "          vf_loss: 0.006913555351396402\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.273333333333326\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037041498520080665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.439001639240793\n",
      "    mean_inference_ms: 1.347439484047293\n",
      "    mean_raw_obs_processing_ms: 0.23097189108590727\n",
      "  time_since_restore: 472.3544979095459\n",
      "  time_this_iter_s: 9.987979173660278\n",
      "  time_total_s: 472.3544979095459\n",
      "  timers:\n",
      "    learn_throughput: 1580.685\n",
      "    learn_time_ms: 632.637\n",
      "    load_throughput: 192048.645\n",
      "    load_time_ms: 5.207\n",
      "    sample_throughput: 88.647\n",
      "    sample_time_ms: 11280.757\n",
      "    update_time_ms: 1.567\n",
      "  timestamp: 1631963579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         472.354</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.0540541</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.811</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.8947368421053\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.05263157894736842\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2625883473290336\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01029669067981271\n",
      "          policy_loss: -0.11208353485498164\n",
      "          total_loss: -0.1282469150920709\n",
      "          vf_explained_var: 0.20722801983356476\n",
      "          vf_loss: 0.0064612270746794015\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.39999999999999\n",
      "    ram_util_percent: 81.98285714285713\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703699477809377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.230448408673194\n",
      "    mean_inference_ms: 1.346121802080486\n",
      "    mean_raw_obs_processing_ms: 0.23985805046243064\n",
      "  time_since_restore: 496.86834192276\n",
      "  time_this_iter_s: 24.51384401321411\n",
      "  time_total_s: 496.86834192276\n",
      "  timers:\n",
      "    learn_throughput: 1575.555\n",
      "    learn_time_ms: 634.697\n",
      "    load_throughput: 139351.203\n",
      "    load_time_ms: 7.176\n",
      "    sample_throughput: 78.883\n",
      "    sample_time_ms: 12676.992\n",
      "    update_time_ms: 1.575\n",
      "  timestamp: 1631963604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         496.868</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.0526316</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.895</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-18 11:13:26,783\tWARNING util.py:164 -- The `on_step_begin` operation took 1.497 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_421d3_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-18_11-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.974358974359\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3.0\n",
      "  episode_reward_mean: -0.05128205128205128\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: e1746907352f4fe585fcb0799b3f64f3\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2796993997361925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01315724665048656\n",
      "          policy_loss: 0.05450283686319987\n",
      "          total_loss: 0.03510063687960307\n",
      "          vf_explained_var: 0.12637978792190552\n",
      "          vf_loss: 0.0033931700862012805\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.36666666666667\n",
      "    ram_util_percent: 61.21666666666666\n",
      "  pid: 58357\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703361334275541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.032473156199938\n",
      "    mean_inference_ms: 1.3448797442422462\n",
      "    mean_raw_obs_processing_ms: 0.24799454240308513\n",
      "  time_since_restore: 509.4033770561218\n",
      "  time_this_iter_s: 12.535035133361816\n",
      "  time_total_s: 509.4033770561218\n",
      "  timers:\n",
      "    learn_throughput: 1555.761\n",
      "    learn_time_ms: 642.772\n",
      "    load_throughput: 135855.823\n",
      "    load_time_ms: 7.361\n",
      "    sample_throughput: 77.232\n",
      "    sample_time_ms: 12947.978\n",
      "    update_time_ms: 1.59\n",
      "  timestamp: 1631963617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 421d3_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/3.22 GiB heap, 0.0/1.61 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-18_11-04-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_421d3_00000</td><td>RUNNING </td><td>192.168.3.5:58357</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         509.403</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.0512821</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">           996.974</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
