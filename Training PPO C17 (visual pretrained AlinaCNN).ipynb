{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 64\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AlinaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-20 08:00:03,145\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-20 08:00:03,164\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 15b2d_00000 but id c2935_00000 is set.\n",
      "\u001b[2m\u001b[36m(pid=491044)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491044)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained (AlinaCNN)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/c2935_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/c2935_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210920_080003-c2935_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491044)\u001b[0m 2021-09-20 08:00:07,163\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=491044)\u001b[0m 2021-09-20 08:00:07,163\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=491044)\u001b[0m 2021-09-20 08:00:13,714\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.022018798134781213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0034394232049184477\n",
      "          policy_loss: 0.1774702523317602\n",
      "          total_loss: 3.7794544246461657\n",
      "          vf_explained_var: -0.17326650023460388\n",
      "          vf_loss: 3.601516452100542\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.76301369863013\n",
      "    ram_util_percent: 64.17397260273974\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04460451962588193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.13158381116259\n",
      "    mean_inference_ms: 1.5404846046592569\n",
      "    mean_raw_obs_processing_ms: 0.14154751460392634\n",
      "  time_since_restore: 50.584660053253174\n",
      "  time_this_iter_s: 50.584660053253174\n",
      "  time_total_s: 50.584660053253174\n",
      "  timers:\n",
      "    learn_throughput: 1689.961\n",
      "    learn_time_ms: 591.73\n",
      "    load_throughput: 58953.476\n",
      "    load_time_ms: 16.963\n",
      "    sample_throughput: 20.011\n",
      "    sample_time_ms: 49971.364\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1632124864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         50.5847</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.31149898279044363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.06106668933769445\n",
      "          policy_loss: 0.1635347416003545\n",
      "          total_loss: 2.9910327434539794\n",
      "          vf_explained_var: -0.10677826404571533\n",
      "          vf_loss: 2.824506273534563\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.78666666666667\n",
      "    ram_util_percent: 70.47333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04420620718846585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.15843608041542\n",
      "    mean_inference_ms: 1.522244466969202\n",
      "    mean_raw_obs_processing_ms: 0.13631438128852336\n",
      "  time_since_restore: 61.06653451919556\n",
      "  time_this_iter_s: 10.481874465942383\n",
      "  time_total_s: 61.06653451919556\n",
      "  timers:\n",
      "    learn_throughput: 1701.498\n",
      "    learn_time_ms: 587.718\n",
      "    load_throughput: 99422.896\n",
      "    load_time_ms: 10.058\n",
      "    sample_throughput: 33.411\n",
      "    sample_time_ms: 29930.671\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1632124874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         61.0665</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8175328115622202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07668064301072693\n",
      "          policy_loss: 0.1629496521419949\n",
      "          total_loss: 2.154094625843896\n",
      "          vf_explained_var: -0.09021443128585815\n",
      "          vf_loss: 1.9878181808524662\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.185714285714276\n",
      "    ram_util_percent: 70.52142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04387612790265183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.588290402274495\n",
      "    mean_inference_ms: 1.5099941287992034\n",
      "    mean_raw_obs_processing_ms: 0.1333953463814136\n",
      "  time_since_restore: 71.2503912448883\n",
      "  time_this_iter_s: 10.183856725692749\n",
      "  time_total_s: 71.2503912448883\n",
      "  timers:\n",
      "    learn_throughput: 1719.051\n",
      "    learn_time_ms: 581.716\n",
      "    load_throughput: 128972.172\n",
      "    load_time_ms: 7.754\n",
      "    sample_throughput: 43.186\n",
      "    sample_time_ms: 23155.851\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1632124885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         71.2504</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0083332816759745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013710865566527512\n",
      "          policy_loss: 0.17209153390593\n",
      "          total_loss: 1.6450878812207117\n",
      "          vf_explained_var: -0.09566884487867355\n",
      "          vf_loss: 1.47999473909537\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.773333333333326\n",
      "    ram_util_percent: 70.40666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043584714598721694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.955929832907515\n",
      "    mean_inference_ms: 1.4989282526518126\n",
      "    mean_raw_obs_processing_ms: 0.13134819960141458\n",
      "  time_since_restore: 81.30970859527588\n",
      "  time_this_iter_s: 10.059317350387573\n",
      "  time_total_s: 81.30970859527588\n",
      "  timers:\n",
      "    learn_throughput: 1728.985\n",
      "    learn_time_ms: 578.374\n",
      "    load_throughput: 151263.285\n",
      "    load_time_ms: 6.611\n",
      "    sample_throughput: 50.665\n",
      "    sample_time_ms: 19737.677\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632124895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         81.3097</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1137384838528104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021606520524539975\n",
      "          policy_loss: 0.17307312968704436\n",
      "          total_loss: 1.2429332335789998\n",
      "          vf_explained_var: -0.02948453091084957\n",
      "          vf_loss: 1.0761360036002265\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.32857142857142\n",
      "    ram_util_percent: 70.07857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043342868041540784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.36792360200668\n",
      "    mean_inference_ms: 1.4896788819199336\n",
      "    mean_raw_obs_processing_ms: 0.12988742160869732\n",
      "  time_since_restore: 91.32584238052368\n",
      "  time_this_iter_s: 10.016133785247803\n",
      "  time_total_s: 91.32584238052368\n",
      "  timers:\n",
      "    learn_throughput: 1730.456\n",
      "    learn_time_ms: 577.882\n",
      "    load_throughput: 168604.391\n",
      "    load_time_ms: 5.931\n",
      "    sample_throughput: 56.572\n",
      "    sample_time_ms: 17676.589\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632124905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         91.3258</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2394364886813694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013457797090596952\n",
      "          policy_loss: 0.17195199694898394\n",
      "          total_loss: 0.9491680155197779\n",
      "          vf_explained_var: -0.016296137124300003\n",
      "          vf_loss: 0.7850683700707224\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.16428571428571\n",
      "    ram_util_percent: 69.86428571428569\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043153497354506014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.41682642664126\n",
      "    mean_inference_ms: 1.4822032965404588\n",
      "    mean_raw_obs_processing_ms: 0.12876960505071036\n",
      "  time_since_restore: 101.38836765289307\n",
      "  time_this_iter_s: 10.062525272369385\n",
      "  time_total_s: 101.38836765289307\n",
      "  timers:\n",
      "    learn_throughput: 1733.53\n",
      "    learn_time_ms: 576.858\n",
      "    load_throughput: 183433.732\n",
      "    load_time_ms: 5.452\n",
      "    sample_throughput: 61.308\n",
      "    sample_time_ms: 16311.058\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632124915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         101.388</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2699478957388135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011165038388378118\n",
      "          policy_loss: 0.17652304122845333\n",
      "          total_loss: 0.7251684010028839\n",
      "          vf_explained_var: -0.015548793599009514\n",
      "          vf_loss: 0.5575766452484661\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.526666666666664\n",
      "    ram_util_percent: 69.64666666666669\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04298858291702939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.883485448284848\n",
      "    mean_inference_ms: 1.4759641784589579\n",
      "    mean_raw_obs_processing_ms: 0.12792916248001238\n",
      "  time_since_restore: 111.42208671569824\n",
      "  time_this_iter_s: 10.033719062805176\n",
      "  time_total_s: 111.42208671569824\n",
      "  timers:\n",
      "    learn_throughput: 1731.631\n",
      "    learn_time_ms: 577.49\n",
      "    load_throughput: 195157.821\n",
      "    load_time_ms: 5.124\n",
      "    sample_throughput: 65.231\n",
      "    sample_time_ms: 15330.118\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632124925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         111.422</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2780763705571492\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013563860625639555\n",
      "          policy_loss: 0.1674688673681683\n",
      "          total_loss: 0.5912710481219822\n",
      "          vf_explained_var: -0.17794854938983917\n",
      "          vf_loss: 0.4320051388608085\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.99285714285714\n",
      "    ram_util_percent: 69.57142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042846138955286564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.6424481600113\n",
      "    mean_inference_ms: 1.4705535379509265\n",
      "    mean_raw_obs_processing_ms: 0.12720573836213034\n",
      "  time_since_restore: 121.43674206733704\n",
      "  time_this_iter_s: 10.014655351638794\n",
      "  time_total_s: 121.43674206733704\n",
      "  timers:\n",
      "    learn_throughput: 1733.735\n",
      "    learn_time_ms: 576.79\n",
      "    load_throughput: 205114.231\n",
      "    load_time_ms: 4.875\n",
      "    sample_throughput: 68.525\n",
      "    sample_time_ms: 14593.242\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632124935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         121.437</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0563972201612262\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007316494324094799\n",
      "          policy_loss: 0.15939194998807377\n",
      "          total_loss: 0.45961321973138386\n",
      "          vf_explained_var: -0.05796094611287117\n",
      "          vf_loss: 0.30831592025028337\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.87857142857143\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042719972743583134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.614834043758048\n",
      "    mean_inference_ms: 1.4658787861023155\n",
      "    mean_raw_obs_processing_ms: 0.1265965247991402\n",
      "  time_since_restore: 131.50434517860413\n",
      "  time_this_iter_s: 10.06760311126709\n",
      "  time_total_s: 131.50434517860413\n",
      "  timers:\n",
      "    learn_throughput: 1734.45\n",
      "    learn_time_ms: 576.552\n",
      "    load_throughput: 213753.962\n",
      "    load_time_ms: 4.678\n",
      "    sample_throughput: 71.298\n",
      "    sample_time_ms: 14025.693\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632124945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         131.504</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2102949804729886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0022786411755371895\n",
      "          policy_loss: 0.15560743146472508\n",
      "          total_loss: 0.3748005219631725\n",
      "          vf_explained_var: -0.14641399681568146\n",
      "          vf_loss: 0.23052700319223934\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.18666666666666\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042608149623623345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.747178056872425\n",
      "    mean_inference_ms: 1.4617824829004307\n",
      "    mean_raw_obs_processing_ms: 0.12607023721073385\n",
      "  time_since_restore: 141.5051348209381\n",
      "  time_this_iter_s: 10.000789642333984\n",
      "  time_total_s: 141.5051348209381\n",
      "  timers:\n",
      "    learn_throughput: 1734.535\n",
      "    learn_time_ms: 576.523\n",
      "    load_throughput: 220920.276\n",
      "    load_time_ms: 4.527\n",
      "    sample_throughput: 73.72\n",
      "    sample_time_ms: 13564.824\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632124955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         141.505</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1389986197153728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008861119162583364\n",
      "          policy_loss: 0.15565405189990997\n",
      "          total_loss: 0.31676055673095915\n",
      "          vf_explained_var: -0.4349863827228546\n",
      "          vf_loss: 0.17100118117200003\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.12142857142857\n",
      "    ram_util_percent: 69.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04251058112275482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.003648535675506\n",
      "    mean_inference_ms: 1.4582142990165998\n",
      "    mean_raw_obs_processing_ms: 0.12562104487453296\n",
      "  time_since_restore: 151.54170155525208\n",
      "  time_this_iter_s: 10.036566734313965\n",
      "  time_total_s: 151.54170155525208\n",
      "  timers:\n",
      "    learn_throughput: 1740.545\n",
      "    learn_time_ms: 574.533\n",
      "    load_throughput: 316718.568\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 105.115\n",
      "    sample_time_ms: 9513.381\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632124965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         151.542</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16874999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1988748490810395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021481492900117912\n",
      "          policy_loss: 0.1486877203815513\n",
      "          total_loss: 0.25909436262316177\n",
      "          vf_explained_var: -0.2776701748371124\n",
      "          vf_loss: 0.11877038938303788\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.6\n",
      "    ram_util_percent: 69.51428571428572\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0424246418508723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.358342346535096\n",
      "    mean_inference_ms: 1.4550641282276855\n",
      "    mean_raw_obs_processing_ms: 0.12522107126095114\n",
      "  time_since_restore: 161.57529973983765\n",
      "  time_this_iter_s: 10.033598184585571\n",
      "  time_total_s: 161.57529973983765\n",
      "  timers:\n",
      "    learn_throughput: 1743.611\n",
      "    learn_time_ms: 573.523\n",
      "    load_throughput: 317747.896\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 105.601\n",
      "    sample_time_ms: 9469.609\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1632124975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         161.575</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4747856338818868\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01153716016885293\n",
      "          policy_loss: 0.14169710493750043\n",
      "          total_loss: 0.22117765761084027\n",
      "          vf_explained_var: -0.9870458245277405\n",
      "          vf_loss: 0.09130806403441562\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.180000000000014\n",
      "    ram_util_percent: 69.59333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04234789400416527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.79219504262267\n",
      "    mean_inference_ms: 1.4522690987055509\n",
      "    mean_raw_obs_processing_ms: 0.12486633828190195\n",
      "  time_since_restore: 171.59907221794128\n",
      "  time_this_iter_s: 10.023772478103638\n",
      "  time_total_s: 171.59907221794128\n",
      "  timers:\n",
      "    learn_throughput: 1743.618\n",
      "    learn_time_ms: 573.52\n",
      "    load_throughput: 316919.59\n",
      "    load_time_ms: 3.155\n",
      "    sample_throughput: 105.78\n",
      "    sample_time_ms: 9453.599\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1632124985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         171.599</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2857142857142857\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3392078505622016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031394556067568975\n",
      "          policy_loss: 0.05454140686326557\n",
      "          total_loss: 0.13318162982662518\n",
      "          vf_explained_var: -0.4369364380836487\n",
      "          vf_loss: 0.09123762659728527\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.17142857142857\n",
      "    ram_util_percent: 69.70714285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042282940775691044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.292112125770906\n",
      "    mean_inference_ms: 1.4499392787886662\n",
      "    mean_raw_obs_processing_ms: 0.12461943810899125\n",
      "  time_since_restore: 181.91446185112\n",
      "  time_this_iter_s: 10.315389633178711\n",
      "  time_total_s: 181.91446185112\n",
      "  timers:\n",
      "    learn_throughput: 1740.278\n",
      "    learn_time_ms: 574.621\n",
      "    load_throughput: 318147.988\n",
      "    load_time_ms: 3.143\n",
      "    sample_throughput: 105.506\n",
      "    sample_time_ms: 9478.147\n",
      "    update_time_ms: 1.622\n",
      "  timestamp: 1632124995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         181.914</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.285714</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.26666666666666666\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.684667542245653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00625263448764914\n",
      "          policy_loss: 0.14114322737894125\n",
      "          total_loss: 0.18808545615110134\n",
      "          vf_explained_var: -0.8617891669273376\n",
      "          vf_loss: 0.06299755389077796\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.53333333333334\n",
      "    ram_util_percent: 69.83333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04222393668114373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.845576924515015\n",
      "    mean_inference_ms: 1.447796844724091\n",
      "    mean_raw_obs_processing_ms: 0.12440693039688884\n",
      "  time_since_restore: 191.91635489463806\n",
      "  time_this_iter_s: 10.001893043518066\n",
      "  time_total_s: 191.91635489463806\n",
      "  timers:\n",
      "    learn_throughput: 1740.63\n",
      "    learn_time_ms: 574.505\n",
      "    load_throughput: 318844.519\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 105.52\n",
      "    sample_time_ms: 9476.878\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632125005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         191.916</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-0.266667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3125\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.747495170434316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008934066337062556\n",
      "          policy_loss: 0.18364473092887137\n",
      "          total_loss: 0.23413848554094632\n",
      "          vf_explained_var: -0.20700956881046295\n",
      "          vf_loss: 0.0668379854824808\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.27142857142858\n",
      "    ram_util_percent: 69.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042169636961220114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.444095923467977\n",
      "    mean_inference_ms: 1.445847507194618\n",
      "    mean_raw_obs_processing_ms: 0.12424290347575091\n",
      "  time_since_restore: 201.9276783466339\n",
      "  time_this_iter_s: 10.01132345199585\n",
      "  time_total_s: 201.9276783466339\n",
      "  timers:\n",
      "    learn_throughput: 1741.569\n",
      "    learn_time_ms: 574.195\n",
      "    load_throughput: 318943.927\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 105.574\n",
      "    sample_time_ms: 9472.045\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632125015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         201.928</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -0.3125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-46\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7058823529411765\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1265625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.597353794839647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004588904007252747\n",
      "          policy_loss: 0.14699883957703908\n",
      "          total_loss: 0.39683054586251576\n",
      "          vf_explained_var: -0.17212046682834625\n",
      "          vf_loss: 0.2652244619197316\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.32000000000001\n",
      "    ram_util_percent: 69.93333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04212178233571334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.08167386065747\n",
      "    mean_inference_ms: 1.444096672363473\n",
      "    mean_raw_obs_processing_ms: 0.12413026208168845\n",
      "  time_since_restore: 212.20120668411255\n",
      "  time_this_iter_s: 10.273528337478638\n",
      "  time_total_s: 212.20120668411255\n",
      "  timers:\n",
      "    learn_throughput: 1742.08\n",
      "    learn_time_ms: 574.026\n",
      "    load_throughput: 318592.643\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 105.305\n",
      "    sample_time_ms: 9496.202\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632125026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         212.201</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.705882</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-03-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7777777777777778\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.898730927043491\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0053305900465183634\n",
      "          policy_loss: 0.16403131554317144\n",
      "          total_loss: 0.1954755205454098\n",
      "          vf_explained_var: -0.5987529158592224\n",
      "          vf_loss: 0.05009418798403607\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.73571428571429\n",
      "    ram_util_percent: 70.00714285714285\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04207958992324549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.752357102685314\n",
      "    mean_inference_ms: 1.4425463548004873\n",
      "    mean_raw_obs_processing_ms: 0.12404818543403866\n",
      "  time_since_restore: 222.37011575698853\n",
      "  time_this_iter_s: 10.168909072875977\n",
      "  time_total_s: 222.37011575698853\n",
      "  timers:\n",
      "    learn_throughput: 1738.796\n",
      "    learn_time_ms: 575.111\n",
      "    load_throughput: 319412.701\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 105.147\n",
      "    sample_time_ms: 9510.539\n",
      "    update_time_ms: 1.61\n",
      "  timestamp: 1632125036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">          222.37</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.777778</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7745839383867053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00522929099883659\n",
      "          policy_loss: 0.07279962305393484\n",
      "          total_loss: 0.1361388617919551\n",
      "          vf_explained_var: 0.05862506106495857\n",
      "          vf_loss: 0.08075415893561311\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.12666666666666\n",
      "    ram_util_percent: 70.05333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0420412311529642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.451626792934002\n",
      "    mean_inference_ms: 1.441126610921526\n",
      "    mean_raw_obs_processing_ms: 0.12401261373349327\n",
      "  time_since_restore: 232.5240502357483\n",
      "  time_this_iter_s: 10.153934478759766\n",
      "  time_total_s: 232.5240502357483\n",
      "  timers:\n",
      "    learn_throughput: 1738.554\n",
      "    learn_time_ms: 575.191\n",
      "    load_throughput: 317228.798\n",
      "    load_time_ms: 3.152\n",
      "    sample_throughput: 105.052\n",
      "    sample_time_ms: 9519.056\n",
      "    update_time_ms: 1.601\n",
      "  timestamp: 1632125046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         232.524</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.95\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1947498083114625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005469076925727853\n",
      "          policy_loss: 0.13133073101441065\n",
      "          total_loss: 0.13953162357211113\n",
      "          vf_explained_var: -0.03940334543585777\n",
      "          vf_loss: 0.02980230421655708\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.52142857142858\n",
      "    ram_util_percent: 70.12142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04200536024063297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.175468042741324\n",
      "    mean_inference_ms: 1.4398071852101624\n",
      "    mean_raw_obs_processing_ms: 0.12396620180797221\n",
      "  time_since_restore: 242.55873727798462\n",
      "  time_this_iter_s: 10.034687042236328\n",
      "  time_total_s: 242.55873727798462\n",
      "  timers:\n",
      "    learn_throughput: 1733.537\n",
      "    learn_time_ms: 576.855\n",
      "    load_throughput: 318670.101\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 105.034\n",
      "    sample_time_ms: 9520.75\n",
      "    update_time_ms: 1.61\n",
      "  timestamp: 1632125056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         242.559</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   -0.95</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9047619047619048\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5823529813024733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0114372518874762\n",
      "          policy_loss: 0.2059796596566836\n",
      "          total_loss: 0.25784134194254876\n",
      "          vf_explained_var: 0.21563485264778137\n",
      "          vf_loss: 0.06696144657002555\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.04666666666666\n",
      "    ram_util_percent: 70.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041972272754079755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.920854803436338\n",
      "    mean_inference_ms: 1.4385904640652067\n",
      "    mean_raw_obs_processing_ms: 0.12391425519243417\n",
      "  time_since_restore: 252.57165455818176\n",
      "  time_this_iter_s: 10.012917280197144\n",
      "  time_total_s: 252.57165455818176\n",
      "  timers:\n",
      "    learn_throughput: 1732.269\n",
      "    learn_time_ms: 577.278\n",
      "    load_throughput: 319179.356\n",
      "    load_time_ms: 3.133\n",
      "    sample_throughput: 105.065\n",
      "    sample_time_ms: 9517.935\n",
      "    update_time_ms: 1.613\n",
      "  timestamp: 1632125066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         252.572</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.904762</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9090909090909091\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6852154824468824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008009050204750956\n",
      "          policy_loss: -0.027700532641675737\n",
      "          total_loss: 0.02654635202553537\n",
      "          vf_explained_var: 0.23346683382987976\n",
      "          vf_loss: 0.07059221872025066\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.55714285714287\n",
      "    ram_util_percent: 70.27142857142856\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041942150912362614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.685679978265648\n",
      "    mean_inference_ms: 1.4374763944789009\n",
      "    mean_raw_obs_processing_ms: 0.12387118172539836\n",
      "  time_since_restore: 262.79141211509705\n",
      "  time_this_iter_s: 10.219757556915283\n",
      "  time_total_s: 262.79141211509705\n",
      "  timers:\n",
      "    learn_throughput: 1732.502\n",
      "    learn_time_ms: 577.2\n",
      "    load_throughput: 319016.703\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 104.859\n",
      "    sample_time_ms: 9536.626\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1632125076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         262.791</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.909091</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8695652173913043\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1461122737990483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0055941965921537905\n",
      "          policy_loss: 0.09923322192496724\n",
      "          total_loss: 0.11322268255882793\n",
      "          vf_explained_var: 0.0606837160885334\n",
      "          vf_loss: 0.03509657559916377\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03333333333334\n",
      "    ram_util_percent: 70.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041913097135899975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.467409630825589\n",
      "    mean_inference_ms: 1.436413010015137\n",
      "    mean_raw_obs_processing_ms: 0.12382475224756698\n",
      "  time_since_restore: 272.8196270465851\n",
      "  time_this_iter_s: 10.028214931488037\n",
      "  time_total_s: 272.8196270465851\n",
      "  timers:\n",
      "    learn_throughput: 1731.892\n",
      "    learn_time_ms: 577.403\n",
      "    load_throughput: 319641.515\n",
      "    load_time_ms: 3.129\n",
      "    sample_throughput: 104.856\n",
      "    sample_time_ms: 9536.912\n",
      "    update_time_ms: 1.607\n",
      "  timestamp: 1632125087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          272.82</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.869565</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-04-57\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8333333333333334\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2864906655417547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005853268741449942\n",
      "          policy_loss: 0.14158919639885426\n",
      "          total_loss: 0.13272004851864444\n",
      "          vf_explained_var: -0.3802328407764435\n",
      "          vf_loss: 0.013625355044172869\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.24285714285715\n",
      "    ram_util_percent: 70.30714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04188504260569442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.264074227537499\n",
      "    mean_inference_ms: 1.4353991256126186\n",
      "    mean_raw_obs_processing_ms: 0.12377112228640892\n",
      "  time_since_restore: 282.7697253227234\n",
      "  time_this_iter_s: 9.950098276138306\n",
      "  time_total_s: 282.7697253227234\n",
      "  timers:\n",
      "    learn_throughput: 1732.854\n",
      "    learn_time_ms: 577.083\n",
      "    load_throughput: 318353.245\n",
      "    load_time_ms: 3.141\n",
      "    sample_throughput: 105.255\n",
      "    sample_time_ms: 9500.712\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1632125097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">          282.77</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.833333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2630078342225817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0049790473694316046\n",
      "          policy_loss: 0.22371155354711744\n",
      "          total_loss: 0.22317075381676357\n",
      "          vf_explained_var: -0.13977614045143127\n",
      "          vf_loss: 0.021774196417795287\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14285714285715\n",
      "    ram_util_percent: 70.37142857142855\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04185831190836172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.074334684022535\n",
      "    mean_inference_ms: 1.4344402773034455\n",
      "    mean_raw_obs_processing_ms: 0.12371521442523015\n",
      "  time_since_restore: 292.85403084754944\n",
      "  time_this_iter_s: 10.08430552482605\n",
      "  time_total_s: 292.85403084754944\n",
      "  timers:\n",
      "    learn_throughput: 1732.71\n",
      "    learn_time_ms: 577.13\n",
      "    load_throughput: 317877.936\n",
      "    load_time_ms: 3.146\n",
      "    sample_throughput: 105.165\n",
      "    sample_time_ms: 9508.894\n",
      "    update_time_ms: 1.577\n",
      "  timestamp: 1632125107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         292.854</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">    -0.8</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7692307692307693\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.031640625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.410488504833645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0043584785499291455\n",
      "          policy_loss: 0.1916356878148185\n",
      "          total_loss: 0.17704727119869657\n",
      "          vf_explained_var: -0.010133414529263973\n",
      "          vf_loss: 0.009378563017687865\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26428571428571\n",
      "    ram_util_percent: 70.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04183272515514795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.896635458683825\n",
      "    mean_inference_ms: 1.433531876477957\n",
      "    mean_raw_obs_processing_ms: 0.12365492344136005\n",
      "  time_since_restore: 302.8080348968506\n",
      "  time_this_iter_s: 9.954004049301147\n",
      "  time_total_s: 302.8080348968506\n",
      "  timers:\n",
      "    learn_throughput: 1731.766\n",
      "    learn_time_ms: 577.445\n",
      "    load_throughput: 317723.826\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 105.232\n",
      "    sample_time_ms: 9502.838\n",
      "    update_time_ms: 1.577\n",
      "  timestamp: 1632125117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         302.808</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.769231</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7407407407407407\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.389121323161655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004506429238428369\n",
      "          policy_loss: 0.02354866506324874\n",
      "          total_loss: 0.02130779309405221\n",
      "          vf_explained_var: -0.34199732542037964\n",
      "          vf_loss: 0.02157904789265659\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96666666666666\n",
      "    ram_util_percent: 70.40666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04180823568174973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.729894235827105\n",
      "    mean_inference_ms: 1.4326662503397425\n",
      "    mean_raw_obs_processing_ms: 0.12359333136954957\n",
      "  time_since_restore: 312.812513589859\n",
      "  time_this_iter_s: 10.004478693008423\n",
      "  time_total_s: 312.812513589859\n",
      "  timers:\n",
      "    learn_throughput: 1734.659\n",
      "    learn_time_ms: 576.482\n",
      "    load_throughput: 318643.47\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 105.52\n",
      "    sample_time_ms: 9476.906\n",
      "    update_time_ms: 1.572\n",
      "  timestamp: 1632125127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         312.813</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.740741</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-05-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00791015625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4477743016348943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005096143594975228\n",
      "          policy_loss: 0.134293665488561\n",
      "          total_loss: 0.14215705030494266\n",
      "          vf_explained_var: 0.13454528152942657\n",
      "          vf_loss: 0.03230081972562605\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17857142857142\n",
      "    ram_util_percent: 70.43571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041784574407077175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.573075950457971\n",
      "    mean_inference_ms: 1.4318340727106373\n",
      "    mean_raw_obs_processing_ms: 0.1235440408477948\n",
      "  time_since_restore: 322.8153612613678\n",
      "  time_this_iter_s: 10.002847671508789\n",
      "  time_total_s: 322.8153612613678\n",
      "  timers:\n",
      "    learn_throughput: 1737.723\n",
      "    learn_time_ms: 575.466\n",
      "    load_throughput: 318943.927\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 105.693\n",
      "    sample_time_ms: 9461.325\n",
      "    update_time_ms: 1.6\n",
      "  timestamp: 1632125137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         322.815</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7241379310344828\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00791015625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4772468275494046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004933931139211486\n",
      "          policy_loss: 0.046571990392274325\n",
      "          total_loss: 0.03328599420686563\n",
      "          vf_explained_var: -0.045847538858652115\n",
      "          vf_loss: 0.011447444785800245\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35\n",
      "    ram_util_percent: 70.47857142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04176175468698632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.425112354622572\n",
      "    mean_inference_ms: 1.4310345808606248\n",
      "    mean_raw_obs_processing_ms: 0.12349371815959008\n",
      "  time_since_restore: 332.6688632965088\n",
      "  time_this_iter_s: 9.853502035140991\n",
      "  time_total_s: 332.6688632965088\n",
      "  timers:\n",
      "    learn_throughput: 1736.486\n",
      "    learn_time_ms: 575.876\n",
      "    load_throughput: 319631.772\n",
      "    load_time_ms: 3.129\n",
      "    sample_throughput: 106.035\n",
      "    sample_time_ms: 9430.879\n",
      "    update_time_ms: 1.607\n",
      "  timestamp: 1632125147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         332.669</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.724138</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7666666666666667\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.003955078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2495517280366686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004462200511805718\n",
      "          policy_loss: 0.053642657813098696\n",
      "          total_loss: 0.10693179807729192\n",
      "          vf_explained_var: 0.2112397700548172\n",
      "          vf_loss: 0.0757670065595044\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.79\n",
      "    ram_util_percent: 69.17250000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04174058249744967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.285708042551347\n",
      "    mean_inference_ms: 1.4302861716896103\n",
      "    mean_raw_obs_processing_ms: 0.14310430050225073\n",
      "  time_since_restore: 360.66976261138916\n",
      "  time_this_iter_s: 28.00089931488037\n",
      "  time_total_s: 360.66976261138916\n",
      "  timers:\n",
      "    learn_throughput: 1740.357\n",
      "    learn_time_ms: 574.595\n",
      "    load_throughput: 222813.279\n",
      "    load_time_ms: 4.488\n",
      "    sample_throughput: 89.067\n",
      "    sample_time_ms: 11227.453\n",
      "    update_time_ms: 1.597\n",
      "  timestamp: 1632125175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          360.67</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-0.766667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.2258064516129\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8064516129032258\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0019775390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.43305881023407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00321282089867903\n",
      "          policy_loss: 0.005224992914332284\n",
      "          total_loss: 0.009745739524563153\n",
      "          vf_explained_var: -0.18894201517105103\n",
      "          vf_loss: 0.02884498245289756\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.955555555555556\n",
      "    ram_util_percent: 70.82222222222224\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04172181712214893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.156631949621008\n",
      "    mean_inference_ms: 1.4296101521828941\n",
      "    mean_raw_obs_processing_ms: 0.16084523633979342\n",
      "  time_since_restore: 373.42138671875\n",
      "  time_this_iter_s: 12.75162410736084\n",
      "  time_total_s: 373.42138671875\n",
      "  timers:\n",
      "    learn_throughput: 1738.526\n",
      "    learn_time_ms: 575.2\n",
      "    load_throughput: 223310.333\n",
      "    load_time_ms: 4.478\n",
      "    sample_throughput: 86.951\n",
      "    sample_time_ms: 11500.74\n",
      "    update_time_ms: 1.6\n",
      "  timestamp: 1632125187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         373.421</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.806452</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.226</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.34375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8125\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00098876953125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4541259659661185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004801641937460113\n",
      "          policy_loss: 0.025960430171754624\n",
      "          total_loss: 0.0562961146235466\n",
      "          vf_explained_var: -0.16461525857448578\n",
      "          vf_loss: 0.054872197119726074\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.925\n",
      "    ram_util_percent: 71.58749999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04170560854222869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.034758789778042\n",
      "    mean_inference_ms: 1.4289859495272097\n",
      "    mean_raw_obs_processing_ms: 0.17692568821369659\n",
      "  time_since_restore: 384.0450794696808\n",
      "  time_this_iter_s: 10.623692750930786\n",
      "  time_total_s: 384.0450794696808\n",
      "  timers:\n",
      "    learn_throughput: 1736.433\n",
      "    learn_time_ms: 575.893\n",
      "    load_throughput: 222881.951\n",
      "    load_time_ms: 4.487\n",
      "    sample_throughput: 86.652\n",
      "    sample_time_ms: 11540.429\n",
      "    update_time_ms: 1.6\n",
      "  timestamp: 1632125198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         384.045</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -0.8125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.344</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-06-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7878787878787878\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.000494384765625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5942294041315717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0033577603559753182\n",
      "          policy_loss: 0.07259517659743626\n",
      "          total_loss: 0.05065483682685428\n",
      "          vf_explained_var: -0.8574663996696472\n",
      "          vf_loss: 0.004000297442285551\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.97857142857141\n",
      "    ram_util_percent: 71.44999999999997\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0416907491112038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.919212883681302\n",
      "    mean_inference_ms: 1.4284056460938026\n",
      "    mean_raw_obs_processing_ms: 0.19152327784380493\n",
      "  time_since_restore: 394.3480689525604\n",
      "  time_this_iter_s: 10.302989482879639\n",
      "  time_total_s: 394.3480689525604\n",
      "  timers:\n",
      "    learn_throughput: 1733.123\n",
      "    learn_time_ms: 576.993\n",
      "    load_throughput: 222974.371\n",
      "    load_time_ms: 4.485\n",
      "    sample_throughput: 86.454\n",
      "    sample_time_ms: 11566.79\n",
      "    update_time_ms: 1.603\n",
      "  timestamp: 1632125208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         394.348</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.787879</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-06-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.5588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7941176470588235\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002471923828125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5945683929655288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003944450102400212\n",
      "          policy_loss: 0.0626499234802193\n",
      "          total_loss: 0.057620112349589664\n",
      "          vf_explained_var: 0.1980086863040924\n",
      "          vf_loss: 0.020914899909661874\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26666666666666\n",
      "    ram_util_percent: 71.24666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04167666925011511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.809581399881477\n",
      "    mean_inference_ms: 1.4278479218968543\n",
      "    mean_raw_obs_processing_ms: 0.2048064923037089\n",
      "  time_since_restore: 404.73418045043945\n",
      "  time_this_iter_s: 10.386111497879028\n",
      "  time_total_s: 404.73418045043945\n",
      "  timers:\n",
      "    learn_throughput: 1732.663\n",
      "    learn_time_ms: 577.146\n",
      "    load_throughput: 223528.12\n",
      "    load_time_ms: 4.474\n",
      "    sample_throughput: 86.131\n",
      "    sample_time_ms: 11610.17\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632125219\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         404.734</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.794118</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.6571428571428\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7714285714285715\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00012359619140625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6227351241641577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003386319579118712\n",
      "          policy_loss: 0.06348902814918095\n",
      "          total_loss: 0.05361422664589352\n",
      "          vf_explained_var: -0.1789819896221161\n",
      "          vf_loss: 0.016352130687381658\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.38\n",
      "    ram_util_percent: 71.12666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04166351874436739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.70549603561465\n",
      "    mean_inference_ms: 1.427317221665138\n",
      "    mean_raw_obs_processing_ms: 0.21690513861244226\n",
      "  time_since_restore: 415.2322084903717\n",
      "  time_this_iter_s: 10.498028039932251\n",
      "  time_total_s: 415.2322084903717\n",
      "  timers:\n",
      "    learn_throughput: 1730.533\n",
      "    learn_time_ms: 577.857\n",
      "    load_throughput: 222764.76\n",
      "    load_time_ms: 4.489\n",
      "    sample_throughput: 85.831\n",
      "    sample_time_ms: 11650.806\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632125229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         415.232</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.771429</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.1798095703125e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6163765589396157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0030236638947436717\n",
      "          policy_loss: 0.021101816950572862\n",
      "          total_loss: 0.008784584080179533\n",
      "          vf_explained_var: -0.2404847890138626\n",
      "          vf_loss: 0.013846346327207154\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.879999999999995\n",
      "    ram_util_percent: 71.07333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04165098695300215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.60642507454244\n",
      "    mean_inference_ms: 1.4268030966639245\n",
      "    mean_raw_obs_processing_ms: 0.22794067546119468\n",
      "  time_since_restore: 425.5757167339325\n",
      "  time_this_iter_s: 10.343508243560791\n",
      "  time_total_s: 425.5757167339325\n",
      "  timers:\n",
      "    learn_throughput: 1728.197\n",
      "    learn_time_ms: 578.638\n",
      "    load_throughput: 222816.83\n",
      "    load_time_ms: 4.488\n",
      "    sample_throughput: 85.551\n",
      "    sample_time_ms: 11688.979\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632125240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         425.576</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 996.8378378378378\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7297297297297297\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.08990478515625e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6074061022864448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0036677406278750367\n",
      "          policy_loss: -0.04430919893913799\n",
      "          total_loss: -0.04836702677938673\n",
      "          vf_explained_var: -0.5797099471092224\n",
      "          vf_loss: 0.022016121302213934\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.78\n",
      "    ram_util_percent: 70.99333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041638850566594984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.512080508030843\n",
      "    mean_inference_ms: 1.426303998872104\n",
      "    mean_raw_obs_processing_ms: 0.23801980448060264\n",
      "  time_since_restore: 436.0245397090912\n",
      "  time_this_iter_s: 10.448822975158691\n",
      "  time_total_s: 436.0245397090912\n",
      "  timers:\n",
      "    learn_throughput: 1726.127\n",
      "    learn_time_ms: 579.332\n",
      "    load_throughput: 222762.393\n",
      "    load_time_ms: 4.489\n",
      "    sample_throughput: 85.232\n",
      "    sample_time_ms: 11732.716\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632125250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         436.025</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.72973</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.838</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-07-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.921052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7105263157894737\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.544952392578125e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.599455073144701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0026598222771177897\n",
      "          policy_loss: 0.10083367923895518\n",
      "          total_loss: 0.09142110173900922\n",
      "          vf_explained_var: -0.7548536658287048\n",
      "          vf_loss: 0.016581935016438364\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82666666666666\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162714119545726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.422095819022696\n",
      "    mean_inference_ms: 1.4258212228824327\n",
      "    mean_raw_obs_processing_ms: 0.24723612097632017\n",
      "  time_since_restore: 446.43853640556335\n",
      "  time_this_iter_s: 10.413996696472168\n",
      "  time_total_s: 446.43853640556335\n",
      "  timers:\n",
      "    learn_throughput: 1724.105\n",
      "    learn_time_ms: 580.011\n",
      "    load_throughput: 222399.771\n",
      "    load_time_ms: 4.496\n",
      "    sample_throughput: 84.939\n",
      "    sample_time_ms: 11773.11\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632125261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         446.439</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.710526</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.921</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-07-51\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.717948717948718\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.724761962890625e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.607686241467794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0036505270332861293\n",
      "          policy_loss: -0.003344152722921636\n",
      "          total_loss: -0.002876546937558386\n",
      "          vf_explained_var: 0.12802450358867645\n",
      "          vf_loss: 0.026544440960666785\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.333333333333336\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041616031408873135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.336172315656402\n",
      "    mean_inference_ms: 1.4253581434235667\n",
      "    mean_raw_obs_processing_ms: 0.2556791412975792\n",
      "  time_since_restore: 456.8810558319092\n",
      "  time_this_iter_s: 10.442519426345825\n",
      "  time_total_s: 456.8810558319092\n",
      "  timers:\n",
      "    learn_throughput: 1724.079\n",
      "    learn_time_ms: 580.02\n",
      "    load_throughput: 222814.462\n",
      "    load_time_ms: 4.488\n",
      "    sample_throughput: 84.516\n",
      "    sample_time_ms: 11832.03\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632125271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         456.881</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.717949</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-02\n",
      "  done: false\n",
      "  episode_len_mean: 997.075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.862380981445312e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.634295116530524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018090857303911948\n",
      "          policy_loss: 0.038843577065401605\n",
      "          total_loss: 0.0214884954608149\n",
      "          vf_explained_var: -0.4222838580608368\n",
      "          vf_loss: 0.008987864633349494\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.986666666666665\n",
      "    ram_util_percent: 71.00666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160537555234941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.254137638828638\n",
      "    mean_inference_ms: 1.4249089241498454\n",
      "    mean_raw_obs_processing_ms: 0.2634154152822078\n",
      "  time_since_restore: 467.47316789627075\n",
      "  time_this_iter_s: 10.592112064361572\n",
      "  time_total_s: 467.47316789627075\n",
      "  timers:\n",
      "    learn_throughput: 1723.911\n",
      "    learn_time_ms: 580.077\n",
      "    load_throughput: 318621.685\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 99.084\n",
      "    sample_time_ms: 10092.401\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1632125282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         467.473</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">    -0.7</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.075</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 997.1463414634146\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6829268292682927\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.931190490722656e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5879279242621527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00496474378313432\n",
      "          policy_loss: -0.041690555794371496\n",
      "          total_loss: -0.03265620097517967\n",
      "          vf_explained_var: -0.08191383630037308\n",
      "          vf_loss: 0.03491362768949734\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.56666666666667\n",
      "    ram_util_percent: 71.06666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159542322741117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.175729173736693\n",
      "    mean_inference_ms: 1.4244836529764804\n",
      "    mean_raw_obs_processing_ms: 0.2705112540496781\n",
      "  time_since_restore: 478.10227823257446\n",
      "  time_this_iter_s: 10.629110336303711\n",
      "  time_total_s: 478.10227823257446\n",
      "  timers:\n",
      "    learn_throughput: 1720.998\n",
      "    learn_time_ms: 581.058\n",
      "    load_throughput: 317113.673\n",
      "    load_time_ms: 3.153\n",
      "    sample_throughput: 101.224\n",
      "    sample_time_ms: 9879.11\n",
      "    update_time_ms: 1.668\n",
      "  timestamp: 1632125292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         478.102</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.682927</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-23\n",
      "  done: false\n",
      "  episode_len_mean: 997.2142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6666666666666666\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.65595245361328e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6238890727361044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005850440773731592\n",
      "          policy_loss: -0.017567849283417068\n",
      "          total_loss: -0.031578112062480714\n",
      "          vf_explained_var: -0.41609713435173035\n",
      "          vf_loss: 0.012228624954716199\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.720000000000006\n",
      "    ram_util_percent: 71.19333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158681141752646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.100674939037447\n",
      "    mean_inference_ms: 1.4241028360702266\n",
      "    mean_raw_obs_processing_ms: 0.2770258052674321\n",
      "  time_since_restore: 488.7279517650604\n",
      "  time_this_iter_s: 10.625673532485962\n",
      "  time_total_s: 488.7279517650604\n",
      "  timers:\n",
      "    learn_throughput: 1714.917\n",
      "    learn_time_ms: 583.119\n",
      "    load_throughput: 316474.814\n",
      "    load_time_ms: 3.16\n",
      "    sample_throughput: 101.243\n",
      "    sample_time_ms: 9877.257\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1632125303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         488.728</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.666667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.214</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-34\n",
      "  done: false\n",
      "  episode_len_mean: 997.2790697674419\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6511627906976745\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.65595245361328e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6103406614727445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032932108870362834\n",
      "          policy_loss: 0.02693237786491712\n",
      "          total_loss: 0.011776618328359393\n",
      "          vf_explained_var: 0.0001791881222743541\n",
      "          vf_loss: 0.010947646718058321\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.170588235294126\n",
      "    ram_util_percent: 71.8529411764706\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041580157025389954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.029148758019339\n",
      "    mean_inference_ms: 1.4237794473363714\n",
      "    mean_raw_obs_processing_ms: 0.28301328365861045\n",
      "  time_since_restore: 500.1424400806427\n",
      "  time_this_iter_s: 11.414488315582275\n",
      "  time_total_s: 500.1424400806427\n",
      "  timers:\n",
      "    learn_throughput: 1703.59\n",
      "    learn_time_ms: 586.996\n",
      "    load_throughput: 316226.665\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 100.156\n",
      "    sample_time_ms: 9984.463\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1632125314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         500.142</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.651163</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 997.3409090909091\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6363636363636364\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.82797622680664e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6526761452356973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0033307080046174825\n",
      "          policy_loss: 0.02468040208849642\n",
      "          total_loss: 0.001723406029244264\n",
      "          vf_explained_var: -0.6630625128746033\n",
      "          vf_loss: 0.0035697629739944304\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.70625\n",
      "    ram_util_percent: 72.00625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157599052969923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.960826920490426\n",
      "    mean_inference_ms: 1.4234991586366\n",
      "    mean_raw_obs_processing_ms: 0.2885165404173308\n",
      "  time_since_restore: 511.34663677215576\n",
      "  time_this_iter_s: 11.204196691513062\n",
      "  time_total_s: 511.34663677215576\n",
      "  timers:\n",
      "    learn_throughput: 1700.393\n",
      "    learn_time_ms: 588.099\n",
      "    load_throughput: 312269.035\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 99.353\n",
      "    sample_time_ms: 10065.167\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632125326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         511.347</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.636364</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-08-57\n",
      "  done: false\n",
      "  episode_len_mean: 997.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6444444444444445\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.41398811340332e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6408694081836277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005840725317216627\n",
      "          policy_loss: 0.02731829293900066\n",
      "          total_loss: 0.016376158677869374\n",
      "          vf_explained_var: -0.666019082069397\n",
      "          vf_loss: 0.015466557120412795\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.043749999999996\n",
      "    ram_util_percent: 72.04375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157315749298849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.895498855006633\n",
      "    mean_inference_ms: 1.4232554128727133\n",
      "    mean_raw_obs_processing_ms: 0.29358170784160875\n",
      "  time_since_restore: 522.6314992904663\n",
      "  time_this_iter_s: 11.284862518310547\n",
      "  time_total_s: 522.6314992904663\n",
      "  timers:\n",
      "    learn_throughput: 1676.838\n",
      "    learn_time_ms: 596.36\n",
      "    load_throughput: 310502.883\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 98.663\n",
      "    sample_time_ms: 10135.54\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632125337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         522.631</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">-0.644444</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             997.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-09-08\n",
      "  done: false\n",
      "  episode_len_mean: 997.4565217391304\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6304347826086957\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.41398811340332e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6430564906862046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003425667961617397\n",
      "          policy_loss: 0.09955087055762608\n",
      "          total_loss: 0.07599424752924178\n",
      "          vf_explained_var: -0.007962530478835106\n",
      "          vf_loss: 0.0028739433949037147\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.56666666666667\n",
      "    ram_util_percent: 72.06666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157061487882848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.832830078211645\n",
      "    mean_inference_ms: 1.4230224619552758\n",
      "    mean_raw_obs_processing_ms: 0.2982403230425888\n",
      "  time_since_restore: 533.4680740833282\n",
      "  time_this_iter_s: 10.836574792861938\n",
      "  time_total_s: 533.4680740833282\n",
      "  timers:\n",
      "    learn_throughput: 1675.966\n",
      "    learn_time_ms: 596.671\n",
      "    load_throughput: 309936.155\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 98.188\n",
      "    sample_time_ms: 10184.517\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1632125348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         533.468</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.630435</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.457</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.5106382978723\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6170212765957447\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.20699405670166e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.606099149915907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004383823368592596\n",
      "          policy_loss: -0.025231662930713758\n",
      "          total_loss: -0.04715385536352793\n",
      "          vf_explained_var: -0.42673152685165405\n",
      "          vf_loss: 0.00413879830028034\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.146666666666654\n",
      "    ram_util_percent: 71.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041568089057236635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.772547914993275\n",
      "    mean_inference_ms: 1.4227971640636894\n",
      "    mean_raw_obs_processing_ms: 0.3025258349441853\n",
      "  time_since_restore: 544.0506944656372\n",
      "  time_this_iter_s: 10.58262038230896\n",
      "  time_total_s: 544.0506944656372\n",
      "  timers:\n",
      "    learn_throughput: 1674.817\n",
      "    learn_time_ms: 597.08\n",
      "    load_throughput: 308062.666\n",
      "    load_time_ms: 3.246\n",
      "    sample_throughput: 98.064\n",
      "    sample_time_ms: 10197.465\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632125358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         544.051</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.617021</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-09-29\n",
      "  done: false\n",
      "  episode_len_mean: 997.5625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6041666666666666\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.0349702835083e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.563018568356832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0044880991155320914\n",
      "          policy_loss: -0.10218354115883509\n",
      "          total_loss: -0.12624327407942879\n",
      "          vf_explained_var: -0.3741062581539154\n",
      "          vf_loss: 0.00157045147045412\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.919999999999995\n",
      "    ram_util_percent: 71.94666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156563765463416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.714333782063212\n",
      "    mean_inference_ms: 1.422578155830305\n",
      "    mean_raw_obs_processing_ms: 0.30648510996737605\n",
      "  time_since_restore: 554.2578938007355\n",
      "  time_this_iter_s: 10.207199335098267\n",
      "  time_total_s: 554.2578938007355\n",
      "  timers:\n",
      "    learn_throughput: 1672.876\n",
      "    learn_time_ms: 597.773\n",
      "    load_throughput: 307728.156\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 98.27\n",
      "    sample_time_ms: 10176.086\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1632125369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         554.258</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-0.604167</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-09-39\n",
      "  done: false\n",
      "  episode_len_mean: 997.6122448979592\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5918367346938775\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.01748514175415e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4522846115960015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007566481462875036\n",
      "          policy_loss: -0.02095703726841344\n",
      "          total_loss: -0.043783159222867754\n",
      "          vf_explained_var: -0.4392518699169159\n",
      "          vf_loss: 0.0016967253785373437\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.01333333333333\n",
      "    ram_util_percent: 63.15999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041563214649161316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.658162290054769\n",
      "    mean_inference_ms: 1.4223662712833678\n",
      "    mean_raw_obs_processing_ms: 0.31012837169640445\n",
      "  time_since_restore: 564.6402349472046\n",
      "  time_this_iter_s: 10.382341146469116\n",
      "  time_total_s: 564.6402349472046\n",
      "  timers:\n",
      "    learn_throughput: 1672.845\n",
      "    learn_time_ms: 597.784\n",
      "    load_throughput: 307710.096\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 98.328\n",
      "    sample_time_ms: 10170.024\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1632125379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          564.64</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-0.591837</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.612</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.58\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.01748514175415e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.396558992067973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009520673262798042\n",
      "          policy_loss: -0.09874415770173073\n",
      "          total_loss: -0.12181288715865878\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008968593895487074\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.84666666666667\n",
      "    ram_util_percent: 55.46\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156081189555497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.603890727612328\n",
      "    mean_inference_ms: 1.4221595948432437\n",
      "    mean_raw_obs_processing_ms: 0.3134805718920993\n",
      "  time_since_restore: 574.9281759262085\n",
      "  time_this_iter_s: 10.287940979003906\n",
      "  time_total_s: 574.9281759262085\n",
      "  timers:\n",
      "    learn_throughput: 1670.616\n",
      "    learn_time_ms: 598.582\n",
      "    load_throughput: 307432.676\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 98.631\n",
      "    sample_time_ms: 10138.815\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1632125389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         574.928</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">   -0.58</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-00\n",
      "  done: false\n",
      "  episode_len_mean: 997.7058823529412\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5686274509803921\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.01748514175415e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.49154744942983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0049596122725674855\n",
      "          policy_loss: 0.004612130526867177\n",
      "          total_loss: -0.018336312452124224\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001967031121926589\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.207142857142856\n",
      "    ram_util_percent: 55.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155845122418589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.551413388478109\n",
      "    mean_inference_ms: 1.4219577058838544\n",
      "    mean_raw_obs_processing_ms: 0.3165647457203151\n",
      "  time_since_restore: 585.2103230953217\n",
      "  time_this_iter_s: 10.28214716911316\n",
      "  time_total_s: 585.2103230953217\n",
      "  timers:\n",
      "    learn_throughput: 1668.117\n",
      "    learn_time_ms: 599.478\n",
      "    load_throughput: 305838.808\n",
      "    load_time_ms: 3.27\n",
      "    sample_throughput: 98.978\n",
      "    sample_time_ms: 10103.253\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632125400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">          585.21</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">-0.568627</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.706</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 997.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5576923076923077\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.508742570877075e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4782655821906197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006350309090825289\n",
      "          policy_loss: 0.005345335437191857\n",
      "          total_loss: -0.017293677603205045\n",
      "          vf_explained_var: -0.8284940123558044\n",
      "          vf_loss: 0.0021436419389728044\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63333333333333\n",
      "    ram_util_percent: 55.746666666666655\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155608800054366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.5006505178372\n",
      "    mean_inference_ms: 1.4217577825601277\n",
      "    mean_raw_obs_processing_ms: 0.31940146258422647\n",
      "  time_since_restore: 595.5247960090637\n",
      "  time_this_iter_s: 10.314472913742065\n",
      "  time_total_s: 595.5247960090637\n",
      "  timers:\n",
      "    learn_throughput: 1665.168\n",
      "    learn_time_ms: 600.54\n",
      "    load_throughput: 307239.005\n",
      "    load_time_ms: 3.255\n",
      "    sample_throughput: 99.295\n",
      "    sample_time_ms: 10071.044\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1632125410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         595.525</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.557692</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-20\n",
      "  done: false\n",
      "  episode_len_mean: 997.7924528301887\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5471698113207547\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.508742570877075e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4893258306715222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007009206009377205\n",
      "          policy_loss: 0.017355420026514266\n",
      "          total_loss: -0.0045830267998907305\n",
      "          vf_explained_var: -0.5620169043540955\n",
      "          vf_loss: 0.00295481108672296\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.480000000000004\n",
      "    ram_util_percent: 55.87999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155389022812579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.451525076583513\n",
      "    mean_inference_ms: 1.4215596869342324\n",
      "    mean_raw_obs_processing_ms: 0.3220094926507197\n",
      "  time_since_restore: 605.8365302085876\n",
      "  time_this_iter_s: 10.311734199523926\n",
      "  time_total_s: 605.8365302085876\n",
      "  timers:\n",
      "    learn_throughput: 1675.537\n",
      "    learn_time_ms: 596.824\n",
      "    load_throughput: 306937.724\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 100.356\n",
      "    sample_time_ms: 9964.523\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632125420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         605.837</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-0.54717</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 997.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5370370370370371\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.508742570877075e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4940910074445934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006454489497543816\n",
      "          policy_loss: -0.0008093346738153033\n",
      "          total_loss: -0.025087586335009997\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006626592114722977\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.66\n",
      "    ram_util_percent: 55.89333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155181920689269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.403966314423124\n",
      "    mean_inference_ms: 1.4213666033942014\n",
      "    mean_raw_obs_processing_ms: 0.3244063388739414\n",
      "  time_since_restore: 616.187023639679\n",
      "  time_this_iter_s: 10.350493431091309\n",
      "  time_total_s: 616.187023639679\n",
      "  timers:\n",
      "    learn_throughput: 1678.067\n",
      "    learn_time_ms: 595.924\n",
      "    load_throughput: 310429.344\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 101.214\n",
      "    sample_time_ms: 9880.077\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632125431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         616.187</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-0.537037</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 997.8727272727273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5272727272727272\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.508742570877075e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4805932389365304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032320657087816012\n",
      "          policy_loss: 0.08360755985809697\n",
      "          total_loss: 0.05969323449664646\n",
      "          vf_explained_var: -0.9249483942985535\n",
      "          vf_loss: 0.0008916064023247195\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.02857142857142\n",
      "    ram_util_percent: 55.97142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154978955972602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.35788083636885\n",
      "    mean_inference_ms: 1.421177674308135\n",
      "    mean_raw_obs_processing_ms: 0.3266071952301936\n",
      "  time_since_restore: 626.4768414497375\n",
      "  time_this_iter_s: 10.289817810058594\n",
      "  time_total_s: 626.4768414497375\n",
      "  timers:\n",
      "    learn_throughput: 1702.91\n",
      "    learn_time_ms: 587.23\n",
      "    load_throughput: 314474.527\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 102.152\n",
      "    sample_time_ms: 9789.326\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632125441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         626.477</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-0.527273</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 997.9107142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5178571428571429\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.543712854385376e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5066241794162325\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006547137022594433\n",
      "          policy_loss: -0.022917093667719098\n",
      "          total_loss: -0.04716400156418483\n",
      "          vf_explained_var: -0.6797129511833191\n",
      "          vf_loss: 0.0008193339075660333\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.74\n",
      "    ram_util_percent: 55.99333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041547796307188016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.313210957215906\n",
      "    mean_inference_ms: 1.4209928153787854\n",
      "    mean_raw_obs_processing_ms: 0.32862611330008307\n",
      "  time_since_restore: 636.809672832489\n",
      "  time_this_iter_s: 10.332831382751465\n",
      "  time_total_s: 636.809672832489\n",
      "  timers:\n",
      "    learn_throughput: 1702.687\n",
      "    learn_time_ms: 587.307\n",
      "    load_throughput: 315003.567\n",
      "    load_time_ms: 3.175\n",
      "    sample_throughput: 102.681\n",
      "    sample_time_ms: 9738.877\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1632125451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          636.81</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.517857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-11-02\n",
      "  done: false\n",
      "  episode_len_mean: 997.9473684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5087719298245614\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.543712854385376e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4600851880179513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009322396776339737\n",
      "          policy_loss: 0.07000961775581042\n",
      "          total_loss: 0.04703489931093322\n",
      "          vf_explained_var: -0.2718396782875061\n",
      "          vf_loss: 0.0016261326149106027\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.78\n",
      "    ram_util_percent: 56.00666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415458207996359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.269899140302329\n",
      "    mean_inference_ms: 1.4208136344852849\n",
      "    mean_raw_obs_processing_ms: 0.3304762108370813\n",
      "  time_since_restore: 647.1793518066406\n",
      "  time_this_iter_s: 10.369678974151611\n",
      "  time_total_s: 647.1793518066406\n",
      "  timers:\n",
      "    learn_throughput: 1701.88\n",
      "    learn_time_ms: 587.585\n",
      "    load_throughput: 315672.128\n",
      "    load_time_ms: 3.168\n",
      "    sample_throughput: 102.909\n",
      "    sample_time_ms: 9717.307\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1632125462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         647.179</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-0.508772</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-11-12\n",
      "  done: false\n",
      "  episode_len_mean: 997.9827586206897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.543712854385376e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.407454193962945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006754086248624246\n",
      "          policy_loss: -0.04098386826614539\n",
      "          total_loss: -0.06415992106000583\n",
      "          vf_explained_var: -0.5033674836158752\n",
      "          vf_loss: 0.0008984869688801054\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.693333333333335\n",
      "    ram_util_percent: 56.053333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154394510762039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.227844657348536\n",
      "    mean_inference_ms: 1.4206390740361534\n",
      "    mean_raw_obs_processing_ms: 0.33217034455224664\n",
      "  time_since_restore: 657.4337983131409\n",
      "  time_this_iter_s: 10.254446506500244\n",
      "  time_total_s: 657.4337983131409\n",
      "  timers:\n",
      "    learn_throughput: 1699.52\n",
      "    learn_time_ms: 588.401\n",
      "    load_throughput: 315681.632\n",
      "    load_time_ms: 3.168\n",
      "    sample_throughput: 102.868\n",
      "    sample_time_ms: 9721.191\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1632125472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         657.434</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 998.0169491525423\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4915254237288136\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.543712854385376e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3660900433858236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069634049309932385\n",
      "          policy_loss: -0.011804090812802315\n",
      "          total_loss: -0.03491983099116219\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005451611912576481\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.98571428571427\n",
      "    ram_util_percent: 56.092857142857156\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415421678254372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.186982356127803\n",
      "    mean_inference_ms: 1.420468713836265\n",
      "    mean_raw_obs_processing_ms: 0.3337187359475721\n",
      "  time_since_restore: 667.662885427475\n",
      "  time_this_iter_s: 10.229087114334106\n",
      "  time_total_s: 667.662885427475\n",
      "  timers:\n",
      "    learn_throughput: 1692.537\n",
      "    learn_time_ms: 590.829\n",
      "    load_throughput: 315498.789\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 103.056\n",
      "    sample_time_ms: 9703.453\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1632125482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         667.663</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-0.491525</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           998.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.48333333333333334\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.543712854385376e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.341927337646484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0044936869117049895\n",
      "          policy_loss: -0.07028319984674454\n",
      "          total_loss: -0.09268478949864706\n",
      "          vf_explained_var: -0.37072455883026123\n",
      "          vf_loss: 0.0010176821269043204\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.36052631578948\n",
      "    ram_util_percent: 56.15526315789473\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041540423154956437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.147282023291286\n",
      "    mean_inference_ms: 1.4203014993435863\n",
      "    mean_raw_obs_processing_ms: 0.33971897728866\n",
      "  time_since_restore: 694.4690546989441\n",
      "  time_this_iter_s: 26.806169271469116\n",
      "  time_total_s: 694.4690546989441\n",
      "  timers:\n",
      "    learn_throughput: 1694.921\n",
      "    learn_time_ms: 589.998\n",
      "    load_throughput: 224136.118\n",
      "    load_time_ms: 4.462\n",
      "    sample_throughput: 88.068\n",
      "    sample_time_ms: 11354.822\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632125509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         694.469</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-0.483333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.9016393442623\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.47540983606557374\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.343045269118415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009203696403490813\n",
      "          policy_loss: 0.004568339222007328\n",
      "          total_loss: -0.017843621803654564\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010184908880748684\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.33888888888888\n",
      "    ram_util_percent: 56.116666666666674\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153878659770688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.109196903356255\n",
      "    mean_inference_ms: 1.4201395884212014\n",
      "    mean_raw_obs_processing_ms: 0.34536903032003285\n",
      "  time_since_restore: 706.6419744491577\n",
      "  time_this_iter_s: 12.172919750213623\n",
      "  time_total_s: 706.6419744491577\n",
      "  timers:\n",
      "    learn_throughput: 1698.699\n",
      "    learn_time_ms: 588.686\n",
      "    load_throughput: 225227.762\n",
      "    load_time_ms: 4.44\n",
      "    sample_throughput: 86.616\n",
      "    sample_time_ms: 11545.251\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1632125521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         706.642</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">-0.47541</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.902</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.9677419354839\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.46774193548387094\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.225768786006504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005976327489370201\n",
      "          policy_loss: 0.011694894482692083\n",
      "          total_loss: -0.009778943947619863\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007838500754183365\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.13333333333333\n",
      "    ram_util_percent: 56.27999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041537277728241964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.072199037438986\n",
      "    mean_inference_ms: 1.4199834087725398\n",
      "    mean_raw_obs_processing_ms: 0.3506906417835892\n",
      "  time_since_restore: 717.1284158229828\n",
      "  time_this_iter_s: 10.486441373825073\n",
      "  time_total_s: 717.1284158229828\n",
      "  timers:\n",
      "    learn_throughput: 1708.258\n",
      "    learn_time_ms: 585.392\n",
      "    load_throughput: 225421.439\n",
      "    load_time_ms: 4.436\n",
      "    sample_throughput: 86.462\n",
      "    sample_time_ms: 11565.762\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1632125532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         717.128</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-0.467742</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.968</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 996.031746031746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4603174603174603\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2695210960176255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008859352032280377\n",
      "          policy_loss: 0.0026343086113532386\n",
      "          total_loss: -0.018822953146364955\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0012379492109175771\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.92666666666667\n",
      "    ram_util_percent: 56.24666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153574744418089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.03621492247329\n",
      "    mean_inference_ms: 1.4198327701119577\n",
      "    mean_raw_obs_processing_ms: 0.35570391166817217\n",
      "  time_since_restore: 727.4991927146912\n",
      "  time_this_iter_s: 10.370776891708374\n",
      "  time_total_s: 727.4991927146912\n",
      "  timers:\n",
      "    learn_throughput: 1709.72\n",
      "    learn_time_ms: 584.891\n",
      "    load_throughput: 225955.771\n",
      "    load_time_ms: 4.426\n",
      "    sample_throughput: 86.414\n",
      "    sample_time_ms: 11572.181\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632125542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         727.499</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-0.460317</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.032</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.09375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.453125\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2620367871390448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00896426176533386\n",
      "          policy_loss: -0.025816174927684996\n",
      "          total_loss: -0.04783390805953079\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000602634072290837\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.66428571428571\n",
      "    ram_util_percent: 56.250000000000014\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153424474847478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.001192678018908\n",
      "    mean_inference_ms: 1.419685991177186\n",
      "    mean_raw_obs_processing_ms: 0.36042756214976834\n",
      "  time_since_restore: 737.8337051868439\n",
      "  time_this_iter_s: 10.33451247215271\n",
      "  time_total_s: 737.8337051868439\n",
      "  timers:\n",
      "    learn_throughput: 1710.354\n",
      "    learn_time_ms: 584.674\n",
      "    load_throughput: 226206.807\n",
      "    load_time_ms: 4.421\n",
      "    sample_throughput: 86.425\n",
      "    sample_time_ms: 11570.791\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1632125553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         737.834</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.453125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.094</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.1538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4461538461538462\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.140932310952081\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074745019041894045\n",
      "          policy_loss: -0.056425204707516566\n",
      "          total_loss: -0.07196475822064612\n",
      "          vf_explained_var: -0.19392672181129456\n",
      "          vf_loss: 0.005869763592878978\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.9\n",
      "    ram_util_percent: 56.13333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041532745901090275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.967082332495977\n",
      "    mean_inference_ms: 1.4195424761423263\n",
      "    mean_raw_obs_processing_ms: 0.36488251639517494\n",
      "  time_since_restore: 748.1307022571564\n",
      "  time_this_iter_s: 10.2969970703125\n",
      "  time_total_s: 748.1307022571564\n",
      "  timers:\n",
      "    learn_throughput: 1709.258\n",
      "    learn_time_ms: 585.049\n",
      "    load_throughput: 225793.99\n",
      "    load_time_ms: 4.429\n",
      "    sample_throughput: 86.422\n",
      "    sample_time_ms: 11571.139\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632125563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         748.131</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-0.446154</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.154</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-12-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.2121212121212\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4393939393939394\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2290892548031276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007955580341179847\n",
      "          policy_loss: 0.011386701050731871\n",
      "          total_loss: -0.010012870033582052\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000891325483745378\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.84666666666667\n",
      "    ram_util_percent: 56.019999999999996\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153120442953971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.933832726573954\n",
      "    mean_inference_ms: 1.4194018208212273\n",
      "    mean_raw_obs_processing_ms: 0.3690810849912793\n",
      "  time_since_restore: 758.3441412448883\n",
      "  time_this_iter_s: 10.213438987731934\n",
      "  time_total_s: 758.3441412448883\n",
      "  timers:\n",
      "    learn_throughput: 1710.755\n",
      "    learn_time_ms: 584.537\n",
      "    load_throughput: 225056.153\n",
      "    load_time_ms: 4.443\n",
      "    sample_throughput: 86.507\n",
      "    sample_time_ms: 11559.713\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632125573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         758.344</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-0.439394</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.212</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.2686567164179\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.43283582089552236\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1579929245842826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062775401408358845\n",
      "          policy_loss: -0.03607189647025532\n",
      "          total_loss: -0.05655425894591543\n",
      "          vf_explained_var: -0.8169688582420349\n",
      "          vf_loss: 0.0010975623493626092\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.93571428571429\n",
      "    ram_util_percent: 55.88571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041529676667555235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.901419588765725\n",
      "    mean_inference_ms: 1.4192640828167056\n",
      "    mean_raw_obs_processing_ms: 0.37303862104545665\n",
      "  time_since_restore: 768.6061742305756\n",
      "  time_this_iter_s: 10.262032985687256\n",
      "  time_total_s: 768.6061742305756\n",
      "  timers:\n",
      "    learn_throughput: 1711.039\n",
      "    learn_time_ms: 584.44\n",
      "    load_throughput: 225054.945\n",
      "    load_time_ms: 4.443\n",
      "    sample_throughput: 86.587\n",
      "    sample_time_ms: 11549.033\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632125583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         768.606</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-0.432836</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.269</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.3235294117648\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4264705882352941\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.270199267069499\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010006589327236606\n",
      "          policy_loss: 0.01935245560275184\n",
      "          total_loss: -0.0023657381534576416\n",
      "          vf_explained_var: -0.912720263004303\n",
      "          vf_loss: 0.000983799349827071\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.89333333333333\n",
      "    ram_util_percent: 55.806666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04152817248511471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.869802691455318\n",
      "    mean_inference_ms: 1.419129287494492\n",
      "    mean_raw_obs_processing_ms: 0.37676875445919566\n",
      "  time_since_restore: 778.8391737937927\n",
      "  time_this_iter_s: 10.232999563217163\n",
      "  time_total_s: 778.8391737937927\n",
      "  timers:\n",
      "    learn_throughput: 1711.668\n",
      "    learn_time_ms: 584.225\n",
      "    load_throughput: 223618.692\n",
      "    load_time_ms: 4.472\n",
      "    sample_throughput: 86.602\n",
      "    sample_time_ms: 11547.105\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632125594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         778.839</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.426471</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.324</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.3768115942029\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4492753623188406\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.32478064166175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012718389822530598\n",
      "          policy_loss: 0.037521656850973764\n",
      "          total_loss: 0.05767673833502664\n",
      "          vf_explained_var: -0.6081134080886841\n",
      "          vf_loss: 0.04340289104147814\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.466666666666676\n",
      "    ram_util_percent: 55.766666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04152666382468273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.8389566903675\n",
      "    mean_inference_ms: 1.4189973389749657\n",
      "    mean_raw_obs_processing_ms: 0.3802864163682246\n",
      "  time_since_restore: 789.1225395202637\n",
      "  time_this_iter_s: 10.283365726470947\n",
      "  time_total_s: 789.1225395202637\n",
      "  timers:\n",
      "    learn_throughput: 1710.135\n",
      "    learn_time_ms: 584.749\n",
      "    load_throughput: 224033.159\n",
      "    load_time_ms: 4.464\n",
      "    sample_throughput: 86.566\n",
      "    sample_time_ms: 11551.869\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632125604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         789.123</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">-0.449275</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.377</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.4285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.44285714285714284\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3296514881981745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008392229222181078\n",
      "          policy_loss: -0.05651889091564549\n",
      "          total_loss: -0.07780311575366392\n",
      "          vf_explained_var: -0.8348697423934937\n",
      "          vf_loss: 0.0020122889804446865\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.449999999999996\n",
      "    ram_util_percent: 55.79285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415251276608131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.808845210546641\n",
      "    mean_inference_ms: 1.4188668739228305\n",
      "    mean_raw_obs_processing_ms: 0.3836018300934122\n",
      "  time_since_restore: 799.3276300430298\n",
      "  time_this_iter_s: 10.205090522766113\n",
      "  time_total_s: 799.3276300430298\n",
      "  timers:\n",
      "    learn_throughput: 1709.793\n",
      "    learn_time_ms: 584.866\n",
      "    load_throughput: 315093.492\n",
      "    load_time_ms: 3.174\n",
      "    sample_throughput: 101.082\n",
      "    sample_time_ms: 9892.924\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1632125614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         799.328</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-0.442857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.4788732394367\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.43661971830985913\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4168527126312256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009188476875546078\n",
      "          policy_loss: 0.07025119724373023\n",
      "          total_loss: 0.048109274274773066\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002026602109738936\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63333333333333\n",
      "    ram_util_percent: 55.76666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041523593318093875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.77943459328019\n",
      "    mean_inference_ms: 1.4187389126050318\n",
      "    mean_raw_obs_processing_ms: 0.38672663207918845\n",
      "  time_since_restore: 809.5001957416534\n",
      "  time_this_iter_s: 10.172565698623657\n",
      "  time_total_s: 809.5001957416534\n",
      "  timers:\n",
      "    learn_throughput: 1712.061\n",
      "    learn_time_ms: 584.091\n",
      "    load_throughput: 316097.973\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 103.16\n",
      "    sample_time_ms: 9693.66\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632125624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">           809.5</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-0.43662</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.479</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 996.5277777777778\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4305555555555556\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.311740851402283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069783834039981426\n",
      "          policy_loss: 0.037489894446399476\n",
      "          total_loss: 0.016251401354869206\n",
      "          vf_explained_var: -0.7034655213356018\n",
      "          vf_loss: 0.0018789133846035433\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.78666666666667\n",
      "    ram_util_percent: 55.766666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041522078550642376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.750722348647294\n",
      "    mean_inference_ms: 1.4186138693186614\n",
      "    mean_raw_obs_processing_ms: 0.3896711524170571\n",
      "  time_since_restore: 819.7886052131653\n",
      "  time_this_iter_s: 10.28840947151184\n",
      "  time_total_s: 819.7886052131653\n",
      "  timers:\n",
      "    learn_throughput: 1712.496\n",
      "    learn_time_ms: 583.943\n",
      "    load_throughput: 315593.746\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 103.37\n",
      "    sample_time_ms: 9674.009\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1632125635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         819.789</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.430556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.5753424657535\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4246575342465753\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3526579750908745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009866462119097955\n",
      "          policy_loss: -0.040647859871387484\n",
      "          total_loss: -0.060583584590090646\n",
      "          vf_explained_var: -0.9168829321861267\n",
      "          vf_loss: 0.0035908573617537817\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.32142857142856\n",
      "    ram_util_percent: 55.76428571428573\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041520602325104725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.722683345354778\n",
      "    mean_inference_ms: 1.4184925008276876\n",
      "    mean_raw_obs_processing_ms: 0.39244531240670066\n",
      "  time_since_restore: 830.0948915481567\n",
      "  time_this_iter_s: 10.306286334991455\n",
      "  time_total_s: 830.0948915481567\n",
      "  timers:\n",
      "    learn_throughput: 1710.257\n",
      "    learn_time_ms: 584.708\n",
      "    load_throughput: 314956.259\n",
      "    load_time_ms: 3.175\n",
      "    sample_throughput: 103.447\n",
      "    sample_time_ms: 9666.766\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1632125645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         830.095</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-0.424658</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.575</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.6216216216217\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4189189189189189\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3259214798609418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006409192282577752\n",
      "          policy_loss: -0.053664098266098234\n",
      "          total_loss: -0.07474372660120328\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.002179587004421693\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.773333333333326\n",
      "    ram_util_percent: 55.793333333333315\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151914452348728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.695272573859144\n",
      "    mean_inference_ms: 1.418373209747957\n",
      "    mean_raw_obs_processing_ms: 0.3950585996012241\n",
      "  time_since_restore: 840.2735779285431\n",
      "  time_this_iter_s: 10.178686380386353\n",
      "  time_total_s: 840.2735779285431\n",
      "  timers:\n",
      "    learn_throughput: 1709.4\n",
      "    learn_time_ms: 585.001\n",
      "    load_throughput: 314750.634\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 103.617\n",
      "    sample_time_ms: 9650.883\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1632125655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         840.274</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">-0.418919</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.622</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.6666666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.41333333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3124622080061172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006905447221724619\n",
      "          policy_loss: -0.0025403141975402833\n",
      "          total_loss: -0.02465064637362957\n",
      "          vf_explained_var: -0.7017570734024048\n",
      "          vf_loss: 0.0010142901207372131\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.86000000000001\n",
      "    ram_util_percent: 55.82666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151769102918041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.66848798604804\n",
      "    mean_inference_ms: 1.4182558384133725\n",
      "    mean_raw_obs_processing_ms: 0.3975195642708099\n",
      "  time_since_restore: 850.5652108192444\n",
      "  time_this_iter_s: 10.291632890701294\n",
      "  time_total_s: 850.5652108192444\n",
      "  timers:\n",
      "    learn_throughput: 1709.18\n",
      "    learn_time_ms: 585.076\n",
      "    load_throughput: 314163.602\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 103.624\n",
      "    sample_time_ms: 9650.253\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1632125666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         850.565</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">-0.413333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.7105263157895\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.40789473684210525\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3317090935177274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005974546825499989\n",
      "          policy_loss: -0.01819885340001848\n",
      "          total_loss: -0.03952925900618235\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0019866858350319995\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.26428571428571\n",
      "    ram_util_percent: 55.842857142857135\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151624294133817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.64230941520152\n",
      "    mean_inference_ms: 1.4181410127343943\n",
      "    mean_raw_obs_processing_ms: 0.39983907823844334\n",
      "  time_since_restore: 860.885241985321\n",
      "  time_this_iter_s: 10.32003116607666\n",
      "  time_total_s: 860.885241985321\n",
      "  timers:\n",
      "    learn_throughput: 1707.962\n",
      "    learn_time_ms: 585.493\n",
      "    load_throughput: 315290.085\n",
      "    load_time_ms: 3.172\n",
      "    sample_throughput: 103.515\n",
      "    sample_time_ms: 9660.43\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1632125676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         860.885</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.407895</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.711</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.7532467532468\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4025974025974026\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.285791293780009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00896986479547035\n",
      "          policy_loss: -0.04017348363995552\n",
      "          total_loss: -0.061470022052526475\n",
      "          vf_explained_var: -0.9925647377967834\n",
      "          vf_loss: 0.0015613742851807424\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.186666666666675\n",
      "    ram_util_percent: 55.93333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151479160238929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.616708120104633\n",
      "    mean_inference_ms: 1.4180280626925061\n",
      "    mean_raw_obs_processing_ms: 0.40202232422238776\n",
      "  time_since_restore: 871.1416683197021\n",
      "  time_this_iter_s: 10.256426334381104\n",
      "  time_total_s: 871.1416683197021\n",
      "  timers:\n",
      "    learn_throughput: 1708.276\n",
      "    learn_time_ms: 585.385\n",
      "    load_throughput: 315741.042\n",
      "    load_time_ms: 3.167\n",
      "    sample_throughput: 103.52\n",
      "    sample_time_ms: 9659.979\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632125686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         871.142</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">-0.402597</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.753</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.7948717948718\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3974358974358974\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.311977055337694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007199188128424201\n",
      "          policy_loss: 0.015399049636390475\n",
      "          total_loss: -0.005941364955570963\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017793565768645042\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.58666666666667\n",
      "    ram_util_percent: 55.96\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151336446639616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.59165823245842\n",
      "    mean_inference_ms: 1.4179168785647882\n",
      "    mean_raw_obs_processing_ms: 0.4040762878239128\n",
      "  time_since_restore: 881.373081445694\n",
      "  time_this_iter_s: 10.231413125991821\n",
      "  time_total_s: 881.373081445694\n",
      "  timers:\n",
      "    learn_throughput: 1707.2\n",
      "    learn_time_ms: 585.755\n",
      "    load_throughput: 315760.058\n",
      "    load_time_ms: 3.167\n",
      "    sample_throughput: 103.526\n",
      "    sample_time_ms: 9659.454\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1632125696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         881.373</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">-0.397436</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.795</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.8354430379746\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3924050632911392\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2699741151597763\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005093623619080922\n",
      "          policy_loss: -0.021172884561949306\n",
      "          total_loss: -0.041256869170400834\n",
      "          vf_explained_var: -0.9520399570465088\n",
      "          vf_loss: 0.0026157576551971338\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.80714285714286\n",
      "    ram_util_percent: 56.01428571428572\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151193182993161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.567131925133666\n",
      "    mean_inference_ms: 1.4178067300951933\n",
      "    mean_raw_obs_processing_ms: 0.4060075439007101\n",
      "  time_since_restore: 891.5523784160614\n",
      "  time_this_iter_s: 10.179296970367432\n",
      "  time_total_s: 891.5523784160614\n",
      "  timers:\n",
      "    learn_throughput: 1707.354\n",
      "    learn_time_ms: 585.702\n",
      "    load_throughput: 314793.155\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 103.636\n",
      "    sample_time_ms: 9649.174\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632125707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         891.552</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">-0.392405</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.835</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3875\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3753440618515014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01164003873105253\n",
      "          policy_loss: -0.012967914839585622\n",
      "          total_loss: -0.03511683642864227\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001604520180909377\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.306666666666665\n",
      "    ram_util_percent: 56.08000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041510493585915596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.543114800878538\n",
      "    mean_inference_ms: 1.4176975265922462\n",
      "    mean_raw_obs_processing_ms: 0.4078223827679359\n",
      "  time_since_restore: 901.7216384410858\n",
      "  time_this_iter_s: 10.169260025024414\n",
      "  time_total_s: 901.7216384410858\n",
      "  timers:\n",
      "    learn_throughput: 1706.802\n",
      "    learn_time_ms: 585.891\n",
      "    load_throughput: 315558.13\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 103.677\n",
      "    sample_time_ms: 9645.376\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632125717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         901.722</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -0.3875</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.9135802469136\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.38271604938271603\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.423143574926588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008311323280612962\n",
      "          policy_loss: 0.05834260479443603\n",
      "          total_loss: 0.03505118313348955\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009400136261117748\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.46\n",
      "    ram_util_percent: 56.106666666666676\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150911814172774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.519618442212735\n",
      "    mean_inference_ms: 1.417592598850339\n",
      "    mean_raw_obs_processing_ms: 0.40952679515530394\n",
      "  time_since_restore: 912.0917494297028\n",
      "  time_this_iter_s: 10.370110988616943\n",
      "  time_total_s: 912.0917494297028\n",
      "  timers:\n",
      "    learn_throughput: 1707.1\n",
      "    learn_time_ms: 585.789\n",
      "    load_throughput: 315418.121\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 103.464\n",
      "    sample_time_ms: 9665.242\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632125727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         912.092</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">-0.382716</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.914</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.9512195121952\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3780487804878049\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.393906916512383\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005436133746480346\n",
      "          policy_loss: 0.023520473080376785\n",
      "          total_loss: 0.0014544651119245424\n",
      "          vf_explained_var: -0.9472568035125732\n",
      "          vf_loss: 0.0018730602110736071\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.28571428571429\n",
      "    ram_util_percent: 56.164285714285725\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041507797436517856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.496608931679503\n",
      "    mean_inference_ms: 1.4174896076397698\n",
      "    mean_raw_obs_processing_ms: 0.4111265382706304\n",
      "  time_since_restore: 922.3362855911255\n",
      "  time_this_iter_s: 10.24453616142273\n",
      "  time_total_s: 922.3362855911255\n",
      "  timers:\n",
      "    learn_throughput: 1706.68\n",
      "    learn_time_ms: 585.933\n",
      "    load_throughput: 315570.001\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 103.512\n",
      "    sample_time_ms: 9660.705\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632125737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         922.336</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">-0.378049</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.951</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.9879518072289\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.37349397590361444\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4079736603630915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007251651489575018\n",
      "          policy_loss: -0.05303229424688551\n",
      "          total_loss: -0.07557375319302082\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015382776386104525\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.526666666666664\n",
      "    ram_util_percent: 56.19333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150649787453917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.474064332295379\n",
      "    mean_inference_ms: 1.4173883257170201\n",
      "    mean_raw_obs_processing_ms: 0.4126267754496338\n",
      "  time_since_restore: 932.5377233028412\n",
      "  time_this_iter_s: 10.201437711715698\n",
      "  time_total_s: 932.5377233028412\n",
      "  timers:\n",
      "    learn_throughput: 1707.936\n",
      "    learn_time_ms: 585.502\n",
      "    load_throughput: 315256.907\n",
      "    load_time_ms: 3.172\n",
      "    sample_throughput: 103.62\n",
      "    sample_time_ms: 9650.671\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1632125748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         932.538</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">-0.373494</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.988</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 997.0238095238095\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36904761904761907\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4894649585088096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009242237107236997\n",
      "          policy_loss: -0.02370550466908349\n",
      "          total_loss: -0.04771795065866576\n",
      "          vf_explained_var: -0.9340959191322327\n",
      "          vf_loss: 0.0008822037883672036\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.77333333333334\n",
      "    ram_util_percent: 56.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150522459473128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.451978032638639\n",
      "    mean_inference_ms: 1.4172882686523998\n",
      "    mean_raw_obs_processing_ms: 0.4140325428437527\n",
      "  time_since_restore: 942.7944078445435\n",
      "  time_this_iter_s: 10.25668454170227\n",
      "  time_total_s: 942.7944078445435\n",
      "  timers:\n",
      "    learn_throughput: 1706.937\n",
      "    learn_time_ms: 585.845\n",
      "    load_throughput: 315560.504\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 103.54\n",
      "    sample_time_ms: 9658.123\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632125758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         942.794</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.369048</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.024</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 997.0588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36470588235294116\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4620691935221353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009961547303179389\n",
      "          policy_loss: -0.008217651603950394\n",
      "          total_loss: -0.031658163087235555\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001180177493693514\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.91428571428571\n",
      "    ram_util_percent: 56.257142857142846\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150398799740473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.430323640587362\n",
      "    mean_inference_ms: 1.4171900202620071\n",
      "    mean_raw_obs_processing_ms: 0.4153484576603582\n",
      "  time_since_restore: 952.9684145450592\n",
      "  time_this_iter_s: 10.174006700515747\n",
      "  time_total_s: 952.9684145450592\n",
      "  timers:\n",
      "    learn_throughput: 1706.475\n",
      "    learn_time_ms: 586.003\n",
      "    load_throughput: 316680.307\n",
      "    load_time_ms: 3.158\n",
      "    sample_throughput: 103.668\n",
      "    sample_time_ms: 9646.193\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632125768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         952.968</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">-0.364706</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.059</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.0930232558139\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36046511627906974\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4973319186104668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006549659840126528\n",
      "          policy_loss: -0.02188535432020823\n",
      "          total_loss: -0.04248883260620965\n",
      "          vf_explained_var: -0.5554831624031067\n",
      "          vf_loss: 0.004369839808593194\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.86666666666666\n",
      "    ram_util_percent: 56.293333333333315\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041502742968932194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.409083813680867\n",
      "    mean_inference_ms: 1.417093228169884\n",
      "    mean_raw_obs_processing_ms: 0.4165791628574261\n",
      "  time_since_restore: 963.1127452850342\n",
      "  time_this_iter_s: 10.144330739974976\n",
      "  time_total_s: 963.1127452850342\n",
      "  timers:\n",
      "    learn_throughput: 1705.282\n",
      "    learn_time_ms: 586.413\n",
      "    load_throughput: 312578.549\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 103.861\n",
      "    sample_time_ms: 9628.213\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632125778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         963.113</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">-0.360465</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.093</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 997.1264367816092\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3563218390804598\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4973976479636297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007758554512635197\n",
      "          policy_loss: 0.021103145678838094\n",
      "          total_loss: -0.0014205592374006907\n",
      "          vf_explained_var: -0.9979952573776245\n",
      "          vf_loss: 0.002450269859077202\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.6857142857143\n",
      "    ram_util_percent: 56.32142857142856\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150150589064119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.388250422612952\n",
      "    mean_inference_ms: 1.4169985873849387\n",
      "    mean_raw_obs_processing_ms: 0.4177287522557424\n",
      "  time_since_restore: 973.3102910518646\n",
      "  time_this_iter_s: 10.197545766830444\n",
      "  time_total_s: 973.3102910518646\n",
      "  timers:\n",
      "    learn_throughput: 1700.708\n",
      "    learn_time_ms: 587.991\n",
      "    load_throughput: 312900.348\n",
      "    load_time_ms: 3.196\n",
      "    sample_throughput: 103.942\n",
      "    sample_time_ms: 9620.743\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632125789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">          973.31</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">-0.356322</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-16-39\n",
      "  done: false\n",
      "  episode_len_mean: 997.1590909090909\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3522727272727273\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4578857925203113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008620168077159502\n",
      "          policy_loss: -0.011294388481312327\n",
      "          total_loss: -0.03446149014764362\n",
      "          vf_explained_var: -0.6831714510917664\n",
      "          vf_loss: 0.0014117568640762733\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.88\n",
      "    ram_util_percent: 56.37333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150026390219656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.367817826178324\n",
      "    mean_inference_ms: 1.4169050042143077\n",
      "    mean_raw_obs_processing_ms: 0.4188011829450206\n",
      "  time_since_restore: 983.5656118392944\n",
      "  time_this_iter_s: 10.25532078742981\n",
      "  time_total_s: 983.5656118392944\n",
      "  timers:\n",
      "    learn_throughput: 1696.496\n",
      "    learn_time_ms: 589.45\n",
      "    load_throughput: 312825.669\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.932\n",
      "    sample_time_ms: 9621.68\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632125799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         983.566</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.352273</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.159</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.1910112359551\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.34831460674157305\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3884135431713527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010777655559550285\n",
      "          policy_loss: -0.03544772697819604\n",
      "          total_loss: -0.057188831683662206\n",
      "          vf_explained_var: -0.3412262499332428\n",
      "          vf_loss: 0.0021430302010331716\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.3\n",
      "    ram_util_percent: 56.446666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149905630951104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.347781589049646\n",
      "    mean_inference_ms: 1.4168134244857753\n",
      "    mean_raw_obs_processing_ms: 0.41980027478257204\n",
      "  time_since_restore: 993.88170170784\n",
      "  time_this_iter_s: 10.316089868545532\n",
      "  time_total_s: 993.88170170784\n",
      "  timers:\n",
      "    learn_throughput: 1694.881\n",
      "    learn_time_ms: 590.012\n",
      "    load_throughput: 312746.361\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 103.791\n",
      "    sample_time_ms: 9634.793\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632125809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         993.882</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">-0.348315</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.191</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-17-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.6888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.34444444444444444\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3614973889456854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00947310225095436\n",
      "          policy_loss: -0.019171862511171235\n",
      "          total_loss: -0.04046649742457602\n",
      "          vf_explained_var: -0.3211243152618408\n",
      "          vf_loss: 0.002320340954853843\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.44999999999999\n",
      "    ram_util_percent: 58.83095238095238\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149841360879135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.32828896846875\n",
      "    mean_inference_ms: 1.416736955850229\n",
      "    mean_raw_obs_processing_ms: 0.42293567251709835\n",
      "  time_since_restore: 1023.5295526981354\n",
      "  time_this_iter_s: 29.64785099029541\n",
      "  time_total_s: 1023.5295526981354\n",
      "  timers:\n",
      "    learn_throughput: 1680.975\n",
      "    learn_time_ms: 594.893\n",
      "    load_throughput: 234191.751\n",
      "    load_time_ms: 4.27\n",
      "    sample_throughput: 86.381\n",
      "    sample_time_ms: 11576.619\n",
      "    update_time_ms: 1.708\n",
      "  timestamp: 1632125839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1023.53</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">-0.344444</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.689</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.7362637362637\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.34065934065934067\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.336166485150655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009497544788770303\n",
      "          policy_loss: -0.005156326790650686\n",
      "          total_loss: -0.027258215182357364\n",
      "          vf_explained_var: -0.5011658072471619\n",
      "          vf_loss: 0.0012597763910889626\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.66\n",
      "    ram_util_percent: 60.88666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149789230624988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.309190539623772\n",
      "    mean_inference_ms: 1.4166650376665166\n",
      "    mean_raw_obs_processing_ms: 0.4259323761670186\n",
      "  time_since_restore: 1034.0864098072052\n",
      "  time_this_iter_s: 10.556857109069824\n",
      "  time_total_s: 1034.0864098072052\n",
      "  timers:\n",
      "    learn_throughput: 1673.728\n",
      "    learn_time_ms: 597.469\n",
      "    load_throughput: 233920.08\n",
      "    load_time_ms: 4.275\n",
      "    sample_throughput: 86.262\n",
      "    sample_time_ms: 11592.553\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1632125849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1034.09</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">-0.340659</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.736</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.7826086956521\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.33695652173913043\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3213805701997545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013437086693134306\n",
      "          policy_loss: -0.020886100269854067\n",
      "          total_loss: -0.04268429506984022\n",
      "          vf_explained_var: -0.15170928835868835\n",
      "          vf_loss: 0.001415611332696345\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.60666666666666\n",
      "    ram_util_percent: 60.686666666666675\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041497512220394904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.290482098266159\n",
      "    mean_inference_ms: 1.416598266185742\n",
      "    mean_raw_obs_processing_ms: 0.4287973436432373\n",
      "  time_since_restore: 1044.716251373291\n",
      "  time_this_iter_s: 10.629841566085815\n",
      "  time_total_s: 1044.716251373291\n",
      "  timers:\n",
      "    learn_throughput: 1668.074\n",
      "    learn_time_ms: 599.494\n",
      "    load_throughput: 231761.514\n",
      "    load_time_ms: 4.315\n",
      "    sample_throughput: 85.992\n",
      "    sample_time_ms: 11628.966\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1632125860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1044.72</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.336957</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.783</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.8279569892474\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.310570987065633\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005549898433668692\n",
      "          policy_loss: -0.01713656120830112\n",
      "          total_loss: -0.03518726598057482\n",
      "          vf_explained_var: -0.4510866701602936\n",
      "          vf_loss: 0.005055005504982546\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.73125\n",
      "    ram_util_percent: 60.831250000000004\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149724726044537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.272151956288077\n",
      "    mean_inference_ms: 1.4165361244351165\n",
      "    mean_raw_obs_processing_ms: 0.4315357070801274\n",
      "  time_since_restore: 1055.4031641483307\n",
      "  time_this_iter_s: 10.686912775039673\n",
      "  time_total_s: 1055.4031641483307\n",
      "  timers:\n",
      "    learn_throughput: 1659.764\n",
      "    learn_time_ms: 602.495\n",
      "    load_throughput: 229725.434\n",
      "    load_time_ms: 4.353\n",
      "    sample_throughput: 85.658\n",
      "    sample_time_ms: 11674.368\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1632125871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          1055.4</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.828</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.8723404255319\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.32978723404255317\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.298720587624444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009597911752083712\n",
      "          policy_loss: -0.05044111348688603\n",
      "          total_loss: -0.0700981990330749\n",
      "          vf_explained_var: -0.9952139258384705\n",
      "          vf_loss: 0.003330121065179507\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.98124999999999\n",
      "    ram_util_percent: 61.60625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041497455735567484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.254274378100856\n",
      "    mean_inference_ms: 1.4164849729727342\n",
      "    mean_raw_obs_processing_ms: 0.4341533723966082\n",
      "  time_since_restore: 1066.9696638584137\n",
      "  time_this_iter_s: 11.566499710083008\n",
      "  time_total_s: 1066.9696638584137\n",
      "  timers:\n",
      "    learn_throughput: 1628.452\n",
      "    learn_time_ms: 614.08\n",
      "    load_throughput: 223069.24\n",
      "    load_time_ms: 4.483\n",
      "    sample_throughput: 84.795\n",
      "    sample_time_ms: 11793.205\n",
      "    update_time_ms: 1.767\n",
      "  timestamp: 1632125882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1066.97</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">-0.329787</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.872</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.9157894736842\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3263157894736842\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.390142363972134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008053898306118414\n",
      "          policy_loss: -0.07536174398329523\n",
      "          total_loss: -0.09714563803540335\n",
      "          vf_explained_var: -0.10428398102521896\n",
      "          vf_loss: 0.0021175319264228974\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.81764705882354\n",
      "    ram_util_percent: 63.51764705882353\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414980369216093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.236828634665722\n",
      "    mean_inference_ms: 1.4164453153843402\n",
      "    mean_raw_obs_processing_ms: 0.4366568706106933\n",
      "  time_since_restore: 1078.4476120471954\n",
      "  time_this_iter_s: 11.477948188781738\n",
      "  time_total_s: 1078.4476120471954\n",
      "  timers:\n",
      "    learn_throughput: 1616.475\n",
      "    learn_time_ms: 618.63\n",
      "    load_throughput: 222198.301\n",
      "    load_time_ms: 4.5\n",
      "    sample_throughput: 83.899\n",
      "    sample_time_ms: 11919.03\n",
      "    update_time_ms: 1.775\n",
      "  timestamp: 1632125894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1078.45</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">-0.326316</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.916</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.9583333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3229166666666667\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4035850445429485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062040300603096385\n",
      "          policy_loss: -0.05837814536773496\n",
      "          total_loss: -0.0775292608473036\n",
      "          vf_explained_var: -0.4916500151157379\n",
      "          vf_loss: 0.004884733422659338\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.28666666666667\n",
      "    ram_util_percent: 63.220000000000006\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149883358858871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.21976340391358\n",
      "    mean_inference_ms: 1.4164130401364374\n",
      "    mean_raw_obs_processing_ms: 0.43904895831363633\n",
      "  time_since_restore: 1089.5421450138092\n",
      "  time_this_iter_s: 11.09453296661377\n",
      "  time_total_s: 1089.5421450138092\n",
      "  timers:\n",
      "    learn_throughput: 1599.129\n",
      "    learn_time_ms: 625.34\n",
      "    load_throughput: 222166.523\n",
      "    load_time_ms: 4.501\n",
      "    sample_throughput: 83.283\n",
      "    sample_time_ms: 12007.24\n",
      "    update_time_ms: 1.782\n",
      "  timestamp: 1632125905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1089.54</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-0.322917</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31958762886597936\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4282044993506537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007944600790457043\n",
      "          policy_loss: -0.05792387790150112\n",
      "          total_loss: -0.07877527756823434\n",
      "          vf_explained_var: 0.04478144273161888\n",
      "          vf_loss: 0.003430644024370445\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.5625\n",
      "    ram_util_percent: 63.025\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414997165502604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.203042080849727\n",
      "    mean_inference_ms: 1.4163843055784282\n",
      "    mean_raw_obs_processing_ms: 0.44133600955816854\n",
      "  time_since_restore: 1100.3298811912537\n",
      "  time_this_iter_s: 10.787736177444458\n",
      "  time_total_s: 1100.3298811912537\n",
      "  timers:\n",
      "    learn_throughput: 1600.295\n",
      "    learn_time_ms: 624.885\n",
      "    load_throughput: 222299.579\n",
      "    load_time_ms: 4.498\n",
      "    sample_throughput: 82.873\n",
      "    sample_time_ms: 12066.713\n",
      "    update_time_ms: 1.793\n",
      "  timestamp: 1632125916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1100.33</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">-0.319588</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               996</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.0408163265306\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3163265306122449\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4130851533677844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01086137503349489\n",
      "          policy_loss: -0.06538792457431555\n",
      "          total_loss: -0.08698751351071729\n",
      "          vf_explained_var: 0.04233058914542198\n",
      "          vf_loss: 0.002531261320432855\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.4625\n",
      "    ram_util_percent: 62.725\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150074963093771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.186718272502588\n",
      "    mean_inference_ms: 1.416361221491832\n",
      "    mean_raw_obs_processing_ms: 0.44352080891555085\n",
      "  time_since_restore: 1111.7359237670898\n",
      "  time_this_iter_s: 11.406042575836182\n",
      "  time_total_s: 1111.7359237670898\n",
      "  timers:\n",
      "    learn_throughput: 1605.363\n",
      "    learn_time_ms: 622.912\n",
      "    load_throughput: 223168.94\n",
      "    load_time_ms: 4.481\n",
      "    sample_throughput: 82.077\n",
      "    sample_time_ms: 12183.739\n",
      "    update_time_ms: 1.8\n",
      "  timestamp: 1632125927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1111.74</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">-0.316327</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.041</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.0808080808081\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31313131313131315\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.771856427192688e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.459831131829156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00492285153252995\n",
      "          policy_loss: -0.03830772216121356\n",
      "          total_loss: -0.058446320394674935\n",
      "          vf_explained_var: 0.2543487250804901\n",
      "          vf_loss: 0.004459710992200093\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.037499999999994\n",
      "    ram_util_percent: 62.76875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150184663906658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.170747751578952\n",
      "    mean_inference_ms: 1.4163426317495649\n",
      "    mean_raw_obs_processing_ms: 0.4456073632553623\n",
      "  time_since_restore: 1122.8198063373566\n",
      "  time_this_iter_s: 11.083882570266724\n",
      "  time_total_s: 1122.8198063373566\n",
      "  timers:\n",
      "    learn_throughput: 1611.597\n",
      "    learn_time_ms: 620.502\n",
      "    load_throughput: 222908.011\n",
      "    load_time_ms: 4.486\n",
      "    sample_throughput: 81.546\n",
      "    sample_time_ms: 12262.955\n",
      "    update_time_ms: 1.806\n",
      "  timestamp: 1632125938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1122.82</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">-0.313131</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.081</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-19-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.407797100808885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009938065341927625\n",
      "          policy_loss: 0.021529551388488874\n",
      "          total_loss: 0.0004727210021681256\n",
      "          vf_explained_var: -0.9533372521400452\n",
      "          vf_loss: 0.003021141794872367\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.224999999999994\n",
      "    ram_util_percent: 63.03125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041503105924609394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.155143780570365\n",
      "    mean_inference_ms: 1.4163294922004952\n",
      "    mean_raw_obs_processing_ms: 0.44760015568376965\n",
      "  time_since_restore: 1134.2081744670868\n",
      "  time_this_iter_s: 11.388368129730225\n",
      "  time_total_s: 1134.2081744670868\n",
      "  timers:\n",
      "    learn_throughput: 1614.314\n",
      "    learn_time_ms: 619.458\n",
      "    load_throughput: 292551.022\n",
      "    load_time_ms: 3.418\n",
      "    sample_throughput: 95.793\n",
      "    sample_time_ms: 10439.138\n",
      "    update_time_ms: 1.754\n",
      "  timestamp: 1632125950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1134.21</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.166332573360867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01071262888117409\n",
      "          policy_loss: -0.059672012304266296\n",
      "          total_loss: -0.07673635184764863\n",
      "          vf_explained_var: -0.3182019591331482\n",
      "          vf_loss: 0.004598987041713877\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.547058823529426\n",
      "    ram_util_percent: 63.111764705882344\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147353786366015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.75997247333494\n",
      "    mean_inference_ms: 1.415081943411032\n",
      "    mean_raw_obs_processing_ms: 0.4525823443204667\n",
      "  time_since_restore: 1145.5463044643402\n",
      "  time_this_iter_s: 11.338129997253418\n",
      "  time_total_s: 1145.5463044643402\n",
      "  timers:\n",
      "    learn_throughput: 1611.554\n",
      "    learn_time_ms: 620.519\n",
      "    load_throughput: 285598.802\n",
      "    load_time_ms: 3.501\n",
      "    sample_throughput: 95.091\n",
      "    sample_time_ms: 10516.221\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1632125961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1145.55</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4912734005186294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007961392025452657\n",
      "          policy_loss: -0.004550357825226254\n",
      "          total_loss: -0.027579569588932727\n",
      "          vf_explained_var: -0.8101011514663696\n",
      "          vf_loss: 0.001883521488505519\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.50625\n",
      "    ram_util_percent: 63.099999999999994\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145203555117132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.564310942297439\n",
      "    mean_inference_ms: 1.4142035766410934\n",
      "    mean_raw_obs_processing_ms: 0.45761944082597616\n",
      "  time_since_restore: 1156.897102355957\n",
      "  time_this_iter_s: 11.350797891616821\n",
      "  time_total_s: 1156.897102355957\n",
      "  timers:\n",
      "    learn_throughput: 1615.254\n",
      "    learn_time_ms: 619.098\n",
      "    load_throughput: 287837.055\n",
      "    load_time_ms: 3.474\n",
      "    sample_throughput: 94.43\n",
      "    sample_time_ms: 10589.817\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1632125973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">          1156.9</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5974363062116836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005813186456988846\n",
      "          policy_loss: 0.04096648229493035\n",
      "          total_loss: 0.016844936501648693\n",
      "          vf_explained_var: -0.8718859553337097\n",
      "          vf_loss: 0.0018528192148854336\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.46000000000001\n",
      "    ram_util_percent: 63.02666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143650371986571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.436014200031627\n",
      "    mean_inference_ms: 1.4135113625172213\n",
      "    mean_raw_obs_processing_ms: 0.4626423221282402\n",
      "  time_since_restore: 1167.6679112911224\n",
      "  time_this_iter_s: 10.770808935165405\n",
      "  time_total_s: 1167.6679112911224\n",
      "  timers:\n",
      "    learn_throughput: 1617.885\n",
      "    learn_time_ms: 618.091\n",
      "    load_throughput: 291824.361\n",
      "    load_time_ms: 3.427\n",
      "    sample_throughput: 94.346\n",
      "    sample_time_ms: 10599.325\n",
      "    update_time_ms: 1.735\n",
      "  timestamp: 1632125983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1167.67</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3487610896428426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007678505016726649\n",
      "          policy_loss: -0.08207566307650672\n",
      "          total_loss: -0.10298087443742487\n",
      "          vf_explained_var: -0.7661811709403992\n",
      "          vf_loss: 0.002582395786885172\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.79375\n",
      "    ram_util_percent: 63.03125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142606527066801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.341604552556545\n",
      "    mean_inference_ms: 1.4130183375120178\n",
      "    mean_raw_obs_processing_ms: 0.4676403345783924\n",
      "  time_since_restore: 1178.4883291721344\n",
      "  time_this_iter_s: 10.820417881011963\n",
      "  time_total_s: 1178.4883291721344\n",
      "  timers:\n",
      "    learn_throughput: 1642.259\n",
      "    learn_time_ms: 608.917\n",
      "    load_throughput: 297428.29\n",
      "    load_time_ms: 3.362\n",
      "    sample_throughput: 94.927\n",
      "    sample_time_ms: 10534.398\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1632125994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1178.49</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2409796635309855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011619050387981547\n",
      "          policy_loss: -0.019252650688091913\n",
      "          total_loss: -0.039628353011276984\n",
      "          vf_explained_var: -0.5301523208618164\n",
      "          vf_loss: 0.0020340911423166593\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.02\n",
      "    ram_util_percent: 63.08000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041419019857880476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.267613938593522\n",
      "    mean_inference_ms: 1.412662975137527\n",
      "    mean_raw_obs_processing_ms: 0.4726023515780028\n",
      "  time_since_restore: 1189.3028507232666\n",
      "  time_this_iter_s: 10.814521551132202\n",
      "  time_total_s: 1189.3028507232666\n",
      "  timers:\n",
      "    learn_throughput: 1647.896\n",
      "    learn_time_ms: 606.835\n",
      "    load_throughput: 298669.401\n",
      "    load_time_ms: 3.348\n",
      "    sample_throughput: 95.51\n",
      "    sample_time_ms: 10470.129\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1632126005\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">          1189.3</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-20-16\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2928194284439085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009670750261164922\n",
      "          policy_loss: -0.013915000690354241\n",
      "          total_loss: -0.03567327401704258\n",
      "          vf_explained_var: -0.3911290168762207\n",
      "          vf_loss: 0.0011699161421145415\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.31333333333334\n",
      "    ram_util_percent: 63.03999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141370813229452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.20715964627419\n",
      "    mean_inference_ms: 1.4123867992228392\n",
      "    mean_raw_obs_processing_ms: 0.4775262870999498\n",
      "  time_since_restore: 1200.0797576904297\n",
      "  time_this_iter_s: 10.776906967163086\n",
      "  time_total_s: 1200.0797576904297\n",
      "  timers:\n",
      "    learn_throughput: 1655.047\n",
      "    learn_time_ms: 604.212\n",
      "    load_throughput: 292033.643\n",
      "    load_time_ms: 3.424\n",
      "    sample_throughput: 95.776\n",
      "    sample_time_ms: 10441.008\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1632126016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1200.08</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4707800971137153\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009796295360820098\n",
      "          policy_loss: -0.01419802059729894\n",
      "          total_loss: -0.03721649216281043\n",
      "          vf_explained_var: -0.9703497886657715\n",
      "          vf_loss: 0.0016893265096263753\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.347058823529416\n",
      "    ram_util_percent: 63.417647058823526\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141066610103438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.156542525489535\n",
      "    mean_inference_ms: 1.4121794826582967\n",
      "    mean_raw_obs_processing_ms: 0.4824095099031476\n",
      "  time_since_restore: 1211.684066772461\n",
      "  time_this_iter_s: 11.60430908203125\n",
      "  time_total_s: 1211.684066772461\n",
      "  timers:\n",
      "    learn_throughput: 1644.417\n",
      "    learn_time_ms: 608.118\n",
      "    load_throughput: 288808.219\n",
      "    load_time_ms: 3.463\n",
      "    sample_throughput: 95.069\n",
      "    sample_time_ms: 10518.708\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632126027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1211.68</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.460733411047194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008936316653784833\n",
      "          policy_loss: -0.014189276264773475\n",
      "          total_loss: -0.03538565817806456\n",
      "          vf_explained_var: -0.5804853439331055\n",
      "          vf_loss: 0.003410951983338843\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.449999999999996\n",
      "    ram_util_percent: 63.9875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041409195272788156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.113235744261509\n",
      "    mean_inference_ms: 1.4120322176150168\n",
      "    mean_raw_obs_processing_ms: 0.48725605308627273\n",
      "  time_since_restore: 1222.8654582500458\n",
      "  time_this_iter_s: 11.181391477584839\n",
      "  time_total_s: 1222.8654582500458\n",
      "  timers:\n",
      "    learn_throughput: 1640.053\n",
      "    learn_time_ms: 609.736\n",
      "    load_throughput: 289132.734\n",
      "    load_time_ms: 3.459\n",
      "    sample_throughput: 95.287\n",
      "    sample_time_ms: 10494.657\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632126039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1222.87</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5318707492616443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007896944803522644\n",
      "          policy_loss: -0.05707492006735669\n",
      "          total_loss: -0.07614329257566067\n",
      "          vf_explained_var: -0.4392918348312378\n",
      "          vf_loss: 0.006250337845024963\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.39333333333334\n",
      "    ram_util_percent: 63.97333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140909096673883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.075524975276236\n",
      "    mean_inference_ms: 1.4119266272536821\n",
      "    mean_raw_obs_processing_ms: 0.4920632896345963\n",
      "  time_since_restore: 1233.546317577362\n",
      "  time_this_iter_s: 10.680859327316284\n",
      "  time_total_s: 1233.546317577362\n",
      "  timers:\n",
      "    learn_throughput: 1634.402\n",
      "    learn_time_ms: 611.845\n",
      "    load_throughput: 289033.112\n",
      "    load_time_ms: 3.46\n",
      "    sample_throughput: 95.673\n",
      "    sample_time_ms: 10452.253\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1632126049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         1233.55</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5249687088860404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005431909518442287\n",
      "          policy_loss: -0.021516631957557465\n",
      "          total_loss: -0.04512176480558183\n",
      "          vf_explained_var: -0.27421262860298157\n",
      "          vf_loss: 0.0016445547704481416\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.55\n",
      "    ram_util_percent: 63.881249999999994\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041410104927972126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.042369287792678\n",
      "    mean_inference_ms: 1.4118589220926758\n",
      "    mean_raw_obs_processing_ms: 0.49683222173582403\n",
      "  time_since_restore: 1244.414921283722\n",
      "  time_this_iter_s: 10.868603706359863\n",
      "  time_total_s: 1244.414921283722\n",
      "  timers:\n",
      "    learn_throughput: 1640.372\n",
      "    learn_time_ms: 609.618\n",
      "    load_throughput: 289250.376\n",
      "    load_time_ms: 3.457\n",
      "    sample_throughput: 96.13\n",
      "    sample_time_ms: 10402.529\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632126060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1244.41</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.563526082038879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008941194322795158\n",
      "          policy_loss: -0.06758573887248834\n",
      "          total_loss: -0.08726409073505137\n",
      "          vf_explained_var: -0.7841947674751282\n",
      "          vf_loss: 0.005956912353738315\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.2\n",
      "    ram_util_percent: 63.893333333333324\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041411809367230365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.01291305016162\n",
      "    mean_inference_ms: 1.411815417606711\n",
      "    mean_raw_obs_processing_ms: 0.5015606987336926\n",
      "  time_since_restore: 1255.279590845108\n",
      "  time_this_iter_s: 10.864669561386108\n",
      "  time_total_s: 1255.279590845108\n",
      "  timers:\n",
      "    learn_throughput: 1642.43\n",
      "    learn_time_ms: 608.854\n",
      "    load_throughput: 294850.266\n",
      "    load_time_ms: 3.392\n",
      "    sample_throughput: 96.563\n",
      "    sample_time_ms: 10355.979\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1632126071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1255.28</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3846345716052584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010708488648727057\n",
      "          policy_loss: -0.05965133814348115\n",
      "          total_loss: -0.08130778479907248\n",
      "          vf_explained_var: 0.29149097204208374\n",
      "          vf_loss: 0.0021898989927851492\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.17058823529412\n",
      "    ram_util_percent: 64.14117647058823\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041414127189666756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.986580982507986\n",
      "    mean_inference_ms: 1.4117960740500297\n",
      "    mean_raw_obs_processing_ms: 0.5062516222647636\n",
      "  time_since_restore: 1266.6236417293549\n",
      "  time_this_iter_s: 11.344050884246826\n",
      "  time_total_s: 1266.6236417293549\n",
      "  timers:\n",
      "    learn_throughput: 1639.547\n",
      "    learn_time_ms: 609.924\n",
      "    load_throughput: 295977.983\n",
      "    load_time_ms: 3.379\n",
      "    sample_throughput: 96.579\n",
      "    sample_time_ms: 10354.229\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1632126083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1266.62</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5292024930318195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005243174652156673\n",
      "          policy_loss: -0.10319898492760128\n",
      "          total_loss: -0.12613017608722052\n",
      "          vf_explained_var: -0.7759581804275513\n",
      "          vf_loss: 0.0023608340467843746\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.225\n",
      "    ram_util_percent: 64.40625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141703001439067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.962896403386535\n",
      "    mean_inference_ms: 1.4117955257813575\n",
      "    mean_raw_obs_processing_ms: 0.5109042031821212\n",
      "  time_since_restore: 1277.8863711357117\n",
      "  time_this_iter_s: 11.262729406356812\n",
      "  time_total_s: 1277.8863711357117\n",
      "  timers:\n",
      "    learn_throughput: 1637.185\n",
      "    learn_time_ms: 610.805\n",
      "    load_throughput: 292920.825\n",
      "    load_time_ms: 3.414\n",
      "    sample_throughput: 96.132\n",
      "    sample_time_ms: 10402.408\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1632126094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1277.89</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">   -0.31</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.27\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.521559739112854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009452891556116493\n",
      "          policy_loss: -0.09557699664599366\n",
      "          total_loss: -0.1180275296792388\n",
      "          vf_explained_var: 0.2908668518066406\n",
      "          vf_loss: 0.002765064244158566\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.10666666666667\n",
      "    ram_util_percent: 64.13333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414198257174771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.94128020168803\n",
      "    mean_inference_ms: 1.411786371282933\n",
      "    mean_raw_obs_processing_ms: 0.515508928215944\n",
      "  time_since_restore: 1288.6942880153656\n",
      "  time_this_iter_s: 10.80791687965393\n",
      "  time_total_s: 1288.6942880153656\n",
      "  timers:\n",
      "    learn_throughput: 1638.356\n",
      "    learn_time_ms: 610.368\n",
      "    load_throughput: 296700.315\n",
      "    load_time_ms: 3.37\n",
      "    sample_throughput: 96.139\n",
      "    sample_time_ms: 10401.642\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1632126105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1288.69</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">   -0.27</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-21-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.27\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.451542928483751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010848186253285045\n",
      "          policy_loss: -0.0478959514035119\n",
      "          total_loss: -0.07100737326674991\n",
      "          vf_explained_var: 0.0582311637699604\n",
      "          vf_loss: 0.0014040043823317521\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.875\n",
      "    ram_util_percent: 64.125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041423106969253484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.921648545964063\n",
      "    mean_inference_ms: 1.411797665756392\n",
      "    mean_raw_obs_processing_ms: 0.5200751274137914\n",
      "  time_since_restore: 1299.7755672931671\n",
      "  time_this_iter_s: 11.081279277801514\n",
      "  time_total_s: 1299.7755672931671\n",
      "  timers:\n",
      "    learn_throughput: 1636.547\n",
      "    learn_time_ms: 611.042\n",
      "    load_throughput: 296759.094\n",
      "    load_time_ms: 3.37\n",
      "    sample_throughput: 95.899\n",
      "    sample_time_ms: 10427.654\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1632126116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1299.78</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">   -0.27</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.26\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.321795916557312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010181037822126548\n",
      "          policy_loss: -0.10169545950161087\n",
      "          total_loss: -0.12291962603727977\n",
      "          vf_explained_var: 0.05604342743754387\n",
      "          vf_loss: 0.0019937909583354163\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.70666666666667\n",
      "    ram_util_percent: 64.20666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142681861303639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.903734119369295\n",
      "    mean_inference_ms: 1.4118207200384927\n",
      "    mean_raw_obs_processing_ms: 0.5245991602971526\n",
      "  time_since_restore: 1310.5545547008514\n",
      "  time_this_iter_s: 10.778987407684326\n",
      "  time_total_s: 1310.5545547008514\n",
      "  timers:\n",
      "    learn_throughput: 1649.871\n",
      "    learn_time_ms: 606.108\n",
      "    load_throughput: 307176.002\n",
      "    load_time_ms: 3.255\n",
      "    sample_throughput: 95.85\n",
      "    sample_time_ms: 10432.933\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1632126127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1310.55</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">   -0.26</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.376545940505134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013237263341726329\n",
      "          policy_loss: -0.007206623173422284\n",
      "          total_loss: -0.0281035249431928\n",
      "          vf_explained_var: -0.47286251187324524\n",
      "          vf_loss: 0.002868556986666388\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.56875\n",
      "    ram_util_percent: 64.24375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143056812969974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.887231014638413\n",
      "    mean_inference_ms: 1.4118516172476614\n",
      "    mean_raw_obs_processing_ms: 0.5290799649756042\n",
      "  time_since_restore: 1321.7079539299011\n",
      "  time_this_iter_s: 11.153399229049683\n",
      "  time_total_s: 1321.7079539299011\n",
      "  timers:\n",
      "    learn_throughput: 1657.592\n",
      "    learn_time_ms: 603.285\n",
      "    load_throughput: 311080.917\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 96.24\n",
      "    sample_time_ms: 10390.717\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1632126138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1321.71</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4730664094289145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007256819351579555\n",
      "          policy_loss: -0.09269964901937379\n",
      "          total_loss: -0.11517847213480208\n",
      "          vf_explained_var: 0.10354287922382355\n",
      "          vf_loss: 0.002251840627286583\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.311764705882354\n",
      "    ram_util_percent: 64.36470588235295\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041434338001300544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.872040244635446\n",
      "    mean_inference_ms: 1.4118838641126523\n",
      "    mean_raw_obs_processing_ms: 0.5335203520191214\n",
      "  time_since_restore: 1332.9627304077148\n",
      "  time_this_iter_s: 11.25477647781372\n",
      "  time_total_s: 1332.9627304077148\n",
      "  timers:\n",
      "    learn_throughput: 1637.873\n",
      "    learn_time_ms: 610.548\n",
      "    load_throughput: 305509.109\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 96.243\n",
      "    sample_time_ms: 10390.412\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1632126149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1332.96</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 996.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5283557494481403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0091202894687537\n",
      "          policy_loss: 0.029273948156171375\n",
      "          total_loss: 0.0067429322335455155\n",
      "          vf_explained_var: -0.2846772074699402\n",
      "          vf_loss: 0.002752542248668356\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.05625\n",
      "    ram_util_percent: 64.81875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143836575121534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.858061499192964\n",
      "    mean_inference_ms: 1.4119277158030392\n",
      "    mean_raw_obs_processing_ms: 0.5379172932592655\n",
      "  time_since_restore: 1344.6369631290436\n",
      "  time_this_iter_s: 11.674232721328735\n",
      "  time_total_s: 1344.6369631290436\n",
      "  timers:\n",
      "    learn_throughput: 1625.913\n",
      "    learn_time_ms: 615.039\n",
      "    load_throughput: 306652.727\n",
      "    load_time_ms: 3.261\n",
      "    sample_throughput: 95.372\n",
      "    sample_time_ms: 10485.225\n",
      "    update_time_ms: 1.741\n",
      "  timestamp: 1632126161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1344.64</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            996.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.53196083439721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007216053159211766\n",
      "          policy_loss: 0.017658105492591857\n",
      "          total_loss: -0.005680787563323975\n",
      "          vf_explained_var: -0.3531745374202728\n",
      "          vf_loss: 0.001980716921389103\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.792682926829265\n",
      "    ram_util_percent: 64.33902439024389\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041442777305012043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.84523045436915\n",
      "    mean_inference_ms: 1.4119850222352115\n",
      "    mean_raw_obs_processing_ms: 0.5437221161322868\n",
      "  time_since_restore: 1373.451235294342\n",
      "  time_this_iter_s: 28.814272165298462\n",
      "  time_total_s: 1373.451235294342\n",
      "  timers:\n",
      "    learn_throughput: 1626.39\n",
      "    learn_time_ms: 614.859\n",
      "    load_throughput: 204084.508\n",
      "    load_time_ms: 4.9\n",
      "    sample_throughput: 81.444\n",
      "    sample_time_ms: 12278.412\n",
      "    update_time_ms: 1.74\n",
      "  timestamp: 1632126190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1373.45</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.481657531526354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008435878821530057\n",
      "          policy_loss: -0.09491962186164327\n",
      "          total_loss: -0.11757861582769288\n",
      "          vf_explained_var: -0.054260995239019394\n",
      "          vf_loss: 0.0021575798047706483\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.43157894736842\n",
      "    ram_util_percent: 63.98947368421052\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04144729675397518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.833556309531346\n",
      "    mean_inference_ms: 1.412047805465018\n",
      "    mean_raw_obs_processing_ms: 0.5494814937993342\n",
      "  time_since_restore: 1386.2352185249329\n",
      "  time_this_iter_s: 12.78398323059082\n",
      "  time_total_s: 1386.2352185249329\n",
      "  timers:\n",
      "    learn_throughput: 1631.587\n",
      "    learn_time_ms: 612.9\n",
      "    load_throughput: 204640.125\n",
      "    load_time_ms: 4.887\n",
      "    sample_throughput: 80.177\n",
      "    sample_time_ms: 12472.38\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1632126202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1386.24</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4536408556832208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007120768245653656\n",
      "          policy_loss: -0.14534527775314118\n",
      "          total_loss: -0.1679513629939821\n",
      "          vf_explained_var: -0.8324922323226929\n",
      "          vf_loss: 0.0019303242236168847\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.8125\n",
      "    ram_util_percent: 63.99375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145180656476954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.822722931656253\n",
      "    mean_inference_ms: 1.4121131167987218\n",
      "    mean_raw_obs_processing_ms: 0.5551942107181266\n",
      "  time_since_restore: 1397.4352433681488\n",
      "  time_this_iter_s: 11.200024843215942\n",
      "  time_total_s: 1397.4352433681488\n",
      "  timers:\n",
      "    learn_throughput: 1634.259\n",
      "    learn_time_ms: 611.898\n",
      "    load_throughput: 204173.92\n",
      "    load_time_ms: 4.898\n",
      "    sample_throughput: 80.263\n",
      "    sample_time_ms: 12458.964\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1632126214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1397.44</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4102812502119275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007508642071127407\n",
      "          policy_loss: -0.07172692451212141\n",
      "          total_loss: -0.09430113616916869\n",
      "          vf_explained_var: -0.0654960349202156\n",
      "          vf_loss: 0.0015286014874517503\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.013333333333335\n",
      "    ram_util_percent: 63.96666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145664046640354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.812692524014393\n",
      "    mean_inference_ms: 1.4121884788762329\n",
      "    mean_raw_obs_processing_ms: 0.5608620447854923\n",
      "  time_since_restore: 1408.1450979709625\n",
      "  time_this_iter_s: 10.70985460281372\n",
      "  time_total_s: 1408.1450979709625\n",
      "  timers:\n",
      "    learn_throughput: 1641.445\n",
      "    learn_time_ms: 609.219\n",
      "    load_throughput: 204977.153\n",
      "    load_time_ms: 4.879\n",
      "    sample_throughput: 80.603\n",
      "    sample_time_ms: 12406.515\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1632126224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1408.15</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4685685820049708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010960985495339848\n",
      "          policy_loss: -0.03624789483017392\n",
      "          total_loss: -0.05943827960226271\n",
      "          vf_explained_var: 0.029352815821766853\n",
      "          vf_loss: 0.0014953026910208994\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8125\n",
      "    ram_util_percent: 63.962500000000006\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041461793420412174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.803445104431649\n",
      "    mean_inference_ms: 1.4122733376364491\n",
      "    mean_raw_obs_processing_ms: 0.5664868220813463\n",
      "  time_since_restore: 1419.0411808490753\n",
      "  time_this_iter_s: 10.896082878112793\n",
      "  time_total_s: 1419.0411808490753\n",
      "  timers:\n",
      "    learn_throughput: 1646.273\n",
      "    learn_time_ms: 607.433\n",
      "    load_throughput: 205773.607\n",
      "    load_time_ms: 4.86\n",
      "    sample_throughput: 80.534\n",
      "    sample_time_ms: 12417.104\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1632126235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1419.04</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.36182119846344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00909957723469981\n",
      "          policy_loss: 0.011405403249793583\n",
      "          total_loss: -0.008481340772575802\n",
      "          vf_explained_var: -0.28132882714271545\n",
      "          vf_loss: 0.003731466928083036\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.88\n",
      "    ram_util_percent: 63.71333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04146714525486229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.794862493685828\n",
      "    mean_inference_ms: 1.412364290923239\n",
      "    mean_raw_obs_processing_ms: 0.5720694921127768\n",
      "  time_since_restore: 1429.8790855407715\n",
      "  time_this_iter_s: 10.837904691696167\n",
      "  time_total_s: 1429.8790855407715\n",
      "  timers:\n",
      "    learn_throughput: 1656.294\n",
      "    learn_time_ms: 603.758\n",
      "    load_throughput: 206039.456\n",
      "    load_time_ms: 4.853\n",
      "    sample_throughput: 80.668\n",
      "    sample_time_ms: 12396.477\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1632126246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1429.88</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3566361559761897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010086306778129245\n",
      "          policy_loss: -0.043920949432584976\n",
      "          total_loss: -0.06572769482930502\n",
      "          vf_explained_var: 0.4685540497303009\n",
      "          vf_loss: 0.0017596160152202678\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.89333333333334\n",
      "    ram_util_percent: 63.14000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147266543199834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.786921898905279\n",
      "    mean_inference_ms: 1.4124594846262404\n",
      "    mean_raw_obs_processing_ms: 0.5776111103324646\n",
      "  time_since_restore: 1440.447405576706\n",
      "  time_this_iter_s: 10.568320035934448\n",
      "  time_total_s: 1440.447405576706\n",
      "  timers:\n",
      "    learn_throughput: 1656.763\n",
      "    learn_time_ms: 603.587\n",
      "    load_throughput: 206098.177\n",
      "    load_time_ms: 4.852\n",
      "    sample_throughput: 80.804\n",
      "    sample_time_ms: 12375.567\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1632126257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1440.45</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.11\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.256294083595276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007650280629792622\n",
      "          policy_loss: -0.0760901118732161\n",
      "          total_loss: -0.0946580182760954\n",
      "          vf_explained_var: -0.057296041399240494\n",
      "          vf_loss: 0.003995035354617155\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.76666666666667\n",
      "    ram_util_percent: 63.013333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147832344245431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.779552961939435\n",
      "    mean_inference_ms: 1.4125594824369911\n",
      "    mean_raw_obs_processing_ms: 0.5831134830027171\n",
      "  time_since_restore: 1451.0219223499298\n",
      "  time_this_iter_s: 10.574516773223877\n",
      "  time_total_s: 1451.0219223499298\n",
      "  timers:\n",
      "    learn_throughput: 1663.107\n",
      "    learn_time_ms: 601.284\n",
      "    load_throughput: 206035.408\n",
      "    load_time_ms: 4.854\n",
      "    sample_throughput: 81.169\n",
      "    sample_time_ms: 12320.002\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1632126267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1451.02</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">   -0.11</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3842038684421114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01338531042120326\n",
      "          policy_loss: 0.08450147766206\n",
      "          total_loss: 0.06240774289601379\n",
      "          vf_explained_var: 0.14106975495815277\n",
      "          vf_loss: 0.001748302799468446\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88125\n",
      "    ram_util_percent: 62.89375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041484161489371375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.772727360723874\n",
      "    mean_inference_ms: 1.4126656272399498\n",
      "    mean_raw_obs_processing_ms: 0.5885711489581132\n",
      "  time_since_restore: 1461.697502374649\n",
      "  time_this_iter_s: 10.675580024719238\n",
      "  time_total_s: 1461.697502374649\n",
      "  timers:\n",
      "    learn_throughput: 1691.054\n",
      "    learn_time_ms: 591.347\n",
      "    load_throughput: 207903.323\n",
      "    load_time_ms: 4.81\n",
      "    sample_throughput: 81.484\n",
      "    sample_time_ms: 12272.367\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632126278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">          1461.7</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4835367812050713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010977404613042117\n",
      "          policy_loss: 0.03789378661248419\n",
      "          total_loss: 0.014125591930415895\n",
      "          vf_explained_var: -0.10052572935819626\n",
      "          vf_loss: 0.0010671721087419429\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.699999999999996\n",
      "    ram_util_percent: 62.859999999999985\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149015158014126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.766449814419298\n",
      "    mean_inference_ms: 1.4127771498919683\n",
      "    mean_raw_obs_processing_ms: 0.5939888061577123\n",
      "  time_since_restore: 1472.2865238189697\n",
      "  time_this_iter_s: 10.589021444320679\n",
      "  time_total_s: 1472.2865238189697\n",
      "  timers:\n",
      "    learn_throughput: 1712.131\n",
      "    learn_time_ms: 584.067\n",
      "    load_throughput: 207676.852\n",
      "    load_time_ms: 4.815\n",
      "    sample_throughput: 82.162\n",
      "    sample_time_ms: 12171.148\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632126289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1472.29</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">    -0.1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-24-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3506083541446263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015150318932573542\n",
      "          policy_loss: 0.12736796364188194\n",
      "          total_loss: 0.10672159675094817\n",
      "          vf_explained_var: 0.5085309743881226\n",
      "          vf_loss: 0.00285971449338831\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.58666666666666\n",
      "    ram_util_percent: 62.83999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041496019278950635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.760544049552061\n",
      "    mean_inference_ms: 1.4128872207974263\n",
      "    mean_raw_obs_processing_ms: 0.5934699541646409\n",
      "  time_since_restore: 1482.8902463912964\n",
      "  time_this_iter_s: 10.60372257232666\n",
      "  time_total_s: 1482.8902463912964\n",
      "  timers:\n",
      "    learn_throughput: 1716.753\n",
      "    learn_time_ms: 582.495\n",
      "    load_throughput: 315441.842\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 96.588\n",
      "    sample_time_ms: 10353.294\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632126299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1482.89</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.06\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3464336580700342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008941185776151765\n",
      "          policy_loss: -0.16854503854281372\n",
      "          total_loss: -0.1897607916345199\n",
      "          vf_explained_var: 0.3250294029712677\n",
      "          vf_loss: 0.0022485808689250712\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.973333333333336\n",
      "    ram_util_percent: 62.85333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041501495796023136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.754211482148458\n",
      "    mean_inference_ms: 1.412988147834696\n",
      "    mean_raw_obs_processing_ms: 0.593098025162301\n",
      "  time_since_restore: 1493.557451248169\n",
      "  time_this_iter_s: 10.667204856872559\n",
      "  time_total_s: 1493.557451248169\n",
      "  timers:\n",
      "    learn_throughput: 1719.111\n",
      "    learn_time_ms: 581.696\n",
      "    load_throughput: 316661.18\n",
      "    load_time_ms: 3.158\n",
      "    sample_throughput: 98.596\n",
      "    sample_time_ms: 10142.416\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632126310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1493.56</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">   -0.06</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.143596926000383\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00987171896115421\n",
      "          policy_loss: -0.14958832123213345\n",
      "          total_loss: -0.1693976913889249\n",
      "          vf_explained_var: 0.06044469401240349\n",
      "          vf_loss: 0.001626598578877747\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.633333333333326\n",
      "    ram_util_percent: 62.87999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041506448027266184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.74814581723705\n",
      "    mean_inference_ms: 1.4130840131645757\n",
      "    mean_raw_obs_processing_ms: 0.5928621072068846\n",
      "  time_since_restore: 1504.252679824829\n",
      "  time_this_iter_s: 10.695228576660156\n",
      "  time_total_s: 1504.252679824829\n",
      "  timers:\n",
      "    learn_throughput: 1720.726\n",
      "    learn_time_ms: 581.15\n",
      "    load_throughput: 317591.507\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 99.083\n",
      "    sample_time_ms: 10092.526\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1632126321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1504.25</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1600682020187376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011992740191504146\n",
      "          policy_loss: -0.0203172889434629\n",
      "          total_loss: -0.025629706722166804\n",
      "          vf_explained_var: 0.14428938925266266\n",
      "          vf_loss: 0.01628826366375304\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.06875\n",
      "    ram_util_percent: 62.8625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151121269408693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.742414217298812\n",
      "    mean_inference_ms: 1.4131764811135001\n",
      "    mean_raw_obs_processing_ms: 0.59275525406487\n",
      "  time_since_restore: 1514.8989293575287\n",
      "  time_this_iter_s: 10.646249532699585\n",
      "  time_total_s: 1514.8989293575287\n",
      "  timers:\n",
      "    learn_throughput: 1720.705\n",
      "    learn_time_ms: 581.157\n",
      "    load_throughput: 319308.14\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 99.146\n",
      "    sample_time_ms: 10086.137\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632126331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">          1514.9</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-25-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2506763484742907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013779953460893602\n",
      "          policy_loss: 0.018953884310192532\n",
      "          total_loss: -0.0006148088309499952\n",
      "          vf_explained_var: 0.021052565425634384\n",
      "          vf_loss: 0.0029380670787456137\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.09999999999999\n",
      "    ram_util_percent: 62.78571428571427\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151594116046338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.73695009839865\n",
      "    mean_inference_ms: 1.413270927205109\n",
      "    mean_raw_obs_processing_ms: 0.5927640296627874\n",
      "  time_since_restore: 1525.287044286728\n",
      "  time_this_iter_s: 10.388114929199219\n",
      "  time_total_s: 1525.287044286728\n",
      "  timers:\n",
      "    learn_throughput: 1721.49\n",
      "    learn_time_ms: 580.892\n",
      "    load_throughput: 319446.759\n",
      "    load_time_ms: 3.13\n",
      "    sample_throughput: 99.645\n",
      "    sample_time_ms: 10035.62\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632126342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1525.29</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-25-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.206509155697293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011449207505250393\n",
      "          policy_loss: 0.0554154252840413\n",
      "          total_loss: 0.044023107985655466\n",
      "          vf_explained_var: -0.612646758556366\n",
      "          vf_loss: 0.010672773576031129\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.69333333333334\n",
      "    ram_util_percent: 62.78666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415205544942148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.731709504375049\n",
      "    mean_inference_ms: 1.4133654006151333\n",
      "    mean_raw_obs_processing_ms: 0.5928834573930875\n",
      "  time_since_restore: 1535.7393901348114\n",
      "  time_this_iter_s: 10.452345848083496\n",
      "  time_total_s: 1535.7393901348114\n",
      "  timers:\n",
      "    learn_throughput: 1720.46\n",
      "    learn_time_ms: 581.24\n",
      "    load_throughput: 319259.53\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 100.033\n",
      "    sample_time_ms: 9996.667\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632126352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1535.74</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2448644320170086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012273749478963009\n",
      "          policy_loss: -0.1088518454796738\n",
      "          total_loss: -0.12942870598700312\n",
      "          vf_explained_var: 0.3179677724838257\n",
      "          vf_loss: 0.0018717825930151674\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.65333333333333\n",
      "    ram_util_percent: 62.78666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041525139764850706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.72671539231783\n",
      "    mean_inference_ms: 1.413462857706562\n",
      "    mean_raw_obs_processing_ms: 0.5931054859566616\n",
      "  time_since_restore: 1546.1508898735046\n",
      "  time_this_iter_s: 10.411499738693237\n",
      "  time_total_s: 1546.1508898735046\n",
      "  timers:\n",
      "    learn_throughput: 1718.829\n",
      "    learn_time_ms: 581.791\n",
      "    load_throughput: 319741.42\n",
      "    load_time_ms: 3.128\n",
      "    sample_throughput: 100.196\n",
      "    sample_time_ms: 9980.432\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632126363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1546.15</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.290715585814582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009298926886234524\n",
      "          policy_loss: -0.0019533632530106437\n",
      "          total_loss: -0.02397676259279251\n",
      "          vf_explained_var: 0.11931979656219482\n",
      "          vf_loss: 0.000883755457147749\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.22000000000001\n",
      "    ram_util_percent: 62.81333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041529777155962705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.721921831464197\n",
      "    mean_inference_ms: 1.4135636281953199\n",
      "    mean_raw_obs_processing_ms: 0.5934231295291972\n",
      "  time_since_restore: 1556.5337054729462\n",
      "  time_this_iter_s: 10.382815599441528\n",
      "  time_total_s: 1556.5337054729462\n",
      "  timers:\n",
      "    learn_throughput: 1718.026\n",
      "    learn_time_ms: 582.063\n",
      "    load_throughput: 320151.439\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 100.392\n",
      "    sample_time_ms: 9961.0\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632126373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1556.53</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3273967848883736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006901245951107882\n",
      "          policy_loss: -0.11992142908275127\n",
      "          total_loss: -0.14213847004705005\n",
      "          vf_explained_var: -0.14774833619594574\n",
      "          vf_loss: 0.0010569248164150242\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.62666666666666\n",
      "    ram_util_percent: 62.86666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153442898606501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.717333788016736\n",
      "    mean_inference_ms: 1.4136665979531784\n",
      "    mean_raw_obs_processing_ms: 0.5938300513129182\n",
      "  time_since_restore: 1567.007889032364\n",
      "  time_this_iter_s: 10.474183559417725\n",
      "  time_total_s: 1567.007889032364\n",
      "  timers:\n",
      "    learn_throughput: 1717.732\n",
      "    learn_time_ms: 582.163\n",
      "    load_throughput: 323163.288\n",
      "    load_time_ms: 3.094\n",
      "    sample_throughput: 100.595\n",
      "    sample_time_ms: 9940.82\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632126384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1567.01</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2496753613154095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009769380593781563\n",
      "          policy_loss: -0.08147991672158242\n",
      "          total_loss: -0.10285244294338756\n",
      "          vf_explained_var: -0.24912908673286438\n",
      "          vf_loss: 0.001124224121061464\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75333333333333\n",
      "    ram_util_percent: 62.919999999999995\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041539025903753454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.712940646960988\n",
      "    mean_inference_ms: 1.4137699755167055\n",
      "    mean_raw_obs_processing_ms: 0.5943177312010242\n",
      "  time_since_restore: 1577.5337181091309\n",
      "  time_this_iter_s: 10.525829076766968\n",
      "  time_total_s: 1577.5337181091309\n",
      "  timers:\n",
      "    learn_throughput: 1716.683\n",
      "    learn_time_ms: 582.519\n",
      "    load_throughput: 323602.108\n",
      "    load_time_ms: 3.09\n",
      "    sample_throughput: 100.663\n",
      "    sample_time_ms: 9934.179\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632126394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1577.53</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.209455225202772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013067836652178795\n",
      "          policy_loss: -0.1071362187465032\n",
      "          total_loss: -0.1260456054781874\n",
      "          vf_explained_var: -0.24088501930236816\n",
      "          vf_loss: 0.0031851650965917444\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07333333333334\n",
      "    ram_util_percent: 63.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154361505236852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.708681647956599\n",
      "    mean_inference_ms: 1.4138757695236996\n",
      "    mean_raw_obs_processing_ms: 0.5948833518205956\n",
      "  time_since_restore: 1587.9465372562408\n",
      "  time_this_iter_s: 10.412819147109985\n",
      "  time_total_s: 1587.9465372562408\n",
      "  timers:\n",
      "    learn_throughput: 1716.377\n",
      "    learn_time_ms: 582.623\n",
      "    load_throughput: 322205.032\n",
      "    load_time_ms: 3.104\n",
      "    sample_throughput: 100.857\n",
      "    sample_time_ms: 9914.994\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632126405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1587.95</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.179179220729404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010212867164202758\n",
      "          policy_loss: -0.0896868345224195\n",
      "          total_loss: -0.10928364404373699\n",
      "          vf_explained_var: -0.6411134600639343\n",
      "          vf_loss: 0.0021949810709985386\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.593333333333334\n",
      "    ram_util_percent: 63.03333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154806449152751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.704556681588503\n",
      "    mean_inference_ms: 1.4139792234639124\n",
      "    mean_raw_obs_processing_ms: 0.5955213909528663\n",
      "  time_since_restore: 1598.5095963478088\n",
      "  time_this_iter_s: 10.563059091567993\n",
      "  time_total_s: 1598.5095963478088\n",
      "  timers:\n",
      "    learn_throughput: 1716.279\n",
      "    learn_time_ms: 582.656\n",
      "    load_throughput: 321752.712\n",
      "    load_time_ms: 3.108\n",
      "    sample_throughput: 100.964\n",
      "    sample_time_ms: 9904.553\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632126415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1598.51</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1212486346562702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010917696674493547\n",
      "          policy_loss: -0.001044074652923478\n",
      "          total_loss: -0.02071539101501306\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015411694294824782\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.77999999999999\n",
      "    ram_util_percent: 63.12000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155210114656364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.700573883282498\n",
      "    mean_inference_ms: 1.4140710218534218\n",
      "    mean_raw_obs_processing_ms: 0.5962267482574873\n",
      "  time_since_restore: 1609.068460226059\n",
      "  time_this_iter_s: 10.558863878250122\n",
      "  time_total_s: 1609.068460226059\n",
      "  timers:\n",
      "    learn_throughput: 1717.397\n",
      "    learn_time_ms: 582.277\n",
      "    load_throughput: 321131.919\n",
      "    load_time_ms: 3.114\n",
      "    sample_throughput: 101.101\n",
      "    sample_time_ms: 9891.093\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1632126426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1609.07</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 143\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1395360284381444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011996070296755857\n",
      "          policy_loss: -0.06037859258552392\n",
      "          total_loss: -0.08054419234395027\n",
      "          vf_explained_var: -0.6089682579040527\n",
      "          vf_loss: 0.0012297586647845391\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.63999999999999\n",
      "    ram_util_percent: 63.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155549055790452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.696553043598882\n",
      "    mean_inference_ms: 1.41414434235077\n",
      "    mean_raw_obs_processing_ms: 0.5969940087262446\n",
      "  time_since_restore: 1619.560078382492\n",
      "  time_this_iter_s: 10.491618156433105\n",
      "  time_total_s: 1619.560078382492\n",
      "  timers:\n",
      "    learn_throughput: 1718.513\n",
      "    learn_time_ms: 581.898\n",
      "    load_throughput: 320753.724\n",
      "    load_time_ms: 3.118\n",
      "    sample_throughput: 101.255\n",
      "    sample_time_ms: 9876.017\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1632126436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         1619.56</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 144\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.144549420144823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012865754111114965\n",
      "          policy_loss: -0.03366266820165846\n",
      "          total_loss: -0.05403324001365238\n",
      "          vf_explained_var: -0.7257843017578125\n",
      "          vf_loss: 0.0010749237003943159\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.11999999999999\n",
      "    ram_util_percent: 63.13333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155786869262038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.692531484039945\n",
      "    mean_inference_ms: 1.4142037392806983\n",
      "    mean_raw_obs_processing_ms: 0.5978205737942821\n",
      "  time_since_restore: 1630.0505607128143\n",
      "  time_this_iter_s: 10.490482330322266\n",
      "  time_total_s: 1630.0505607128143\n",
      "  timers:\n",
      "    learn_throughput: 1720.024\n",
      "    learn_time_ms: 581.387\n",
      "    load_throughput: 319590.369\n",
      "    load_time_ms: 3.129\n",
      "    sample_throughput: 101.146\n",
      "    sample_time_ms: 9886.732\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632126447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         1630.05</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 145\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0048095451460943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00807733539850659\n",
      "          policy_loss: 0.10506142543421851\n",
      "          total_loss: 0.08626478910446167\n",
      "          vf_explained_var: -0.627723753452301\n",
      "          vf_loss: 0.0012514573159731097\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.993333333333325\n",
      "    ram_util_percent: 63.13333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041559694036337164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.688506513118334\n",
      "    mean_inference_ms: 1.4142511105345092\n",
      "    mean_raw_obs_processing_ms: 0.5987006680751894\n",
      "  time_since_restore: 1640.5271117687225\n",
      "  time_this_iter_s: 10.476551055908203\n",
      "  time_total_s: 1640.5271117687225\n",
      "  timers:\n",
      "    learn_throughput: 1719.843\n",
      "    learn_time_ms: 581.448\n",
      "    load_throughput: 319419.998\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 101.121\n",
      "    sample_time_ms: 9889.106\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632126457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         1640.53</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 146\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8840147654215496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011834085839901299\n",
      "          policy_loss: -0.006128891474670834\n",
      "          total_loss: -0.022628596052527428\n",
      "          vf_explained_var: -0.9893885254859924\n",
      "          vf_loss: 0.0023404425566291645\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.58\n",
      "    ram_util_percent: 63.16666666666669\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041561378723276456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.684541669033212\n",
      "    mean_inference_ms: 1.4142967355512681\n",
      "    mean_raw_obs_processing_ms: 0.5996333483969408\n",
      "  time_since_restore: 1650.9850516319275\n",
      "  time_this_iter_s: 10.457939863204956\n",
      "  time_total_s: 1650.9850516319275\n",
      "  timers:\n",
      "    learn_throughput: 1718.813\n",
      "    learn_time_ms: 581.797\n",
      "    load_throughput: 319512.463\n",
      "    load_time_ms: 3.13\n",
      "    sample_throughput: 101.078\n",
      "    sample_time_ms: 9893.396\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632126468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         1650.99</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 147\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0320396264394125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012759966311192155\n",
      "          policy_loss: 0.038691465204788576\n",
      "          total_loss: 0.01979020072354211\n",
      "          vf_explained_var: -0.8683523535728455\n",
      "          vf_loss: 0.0014191312728346222\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.599999999999994\n",
      "    ram_util_percent: 63.233333333333356\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041563051104246546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.680687561529648\n",
      "    mean_inference_ms: 1.4143420372133955\n",
      "    mean_raw_obs_processing_ms: 0.6006156245798994\n",
      "  time_since_restore: 1661.4747068881989\n",
      "  time_this_iter_s: 10.489655256271362\n",
      "  time_total_s: 1661.4747068881989\n",
      "  timers:\n",
      "    learn_throughput: 1720.285\n",
      "    learn_time_ms: 581.299\n",
      "    load_throughput: 319332.45\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 100.963\n",
      "    sample_time_ms: 9904.573\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632126478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1661.47</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 148\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.022431939178043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009262098908400832\n",
      "          policy_loss: 0.04817235602272881\n",
      "          total_loss: 0.03044785120420986\n",
      "          vf_explained_var: -0.5675338506698608\n",
      "          vf_loss: 0.002499809752528866\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.91333333333333\n",
      "    ram_util_percent: 63.293333333333315\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156468609745947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.67702462449175\n",
      "    mean_inference_ms: 1.414387448066149\n",
      "    mean_raw_obs_processing_ms: 0.6016366183971098\n",
      "  time_since_restore: 1671.9489750862122\n",
      "  time_this_iter_s: 10.474268198013306\n",
      "  time_total_s: 1671.9489750862122\n",
      "  timers:\n",
      "    learn_throughput: 1718.75\n",
      "    learn_time_ms: 581.818\n",
      "    load_throughput: 319322.726\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 100.969\n",
      "    sample_time_ms: 9904.057\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632126489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         1671.95</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 149\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8411128520965576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012473689916192437\n",
      "          policy_loss: 0.03725215097268422\n",
      "          total_loss: 0.020581975496477552\n",
      "          vf_explained_var: 0.20229876041412354\n",
      "          vf_loss: 0.001740952266845852\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75333333333333\n",
      "    ram_util_percent: 63.43333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156629064049328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.673503148312477\n",
      "    mean_inference_ms: 1.4144323935751641\n",
      "    mean_raw_obs_processing_ms: 0.6027015359996213\n",
      "  time_since_restore: 1682.4114775657654\n",
      "  time_this_iter_s: 10.462502479553223\n",
      "  time_total_s: 1682.4114775657654\n",
      "  timers:\n",
      "    learn_throughput: 1720.876\n",
      "    learn_time_ms: 581.099\n",
      "    load_throughput: 319987.793\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 101.026\n",
      "    sample_time_ms: 9898.429\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632126499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1682.41</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-28-47\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 150\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.697166657447815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010253536281738937\n",
      "          policy_loss: 0.045269113034009933\n",
      "          total_loss: 0.030438541372617086\n",
      "          vf_explained_var: -0.471422016620636\n",
      "          vf_loss: 0.0021410984782657275\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.93658536585366\n",
      "    ram_util_percent: 62.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156788621678745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.67010638014495\n",
      "    mean_inference_ms: 1.4144778326742873\n",
      "    mean_raw_obs_processing_ms: 0.6050169728141745\n",
      "  time_since_restore: 1710.6222035884857\n",
      "  time_this_iter_s: 28.210726022720337\n",
      "  time_total_s: 1710.6222035884857\n",
      "  timers:\n",
      "    learn_throughput: 1721.306\n",
      "    learn_time_ms: 580.954\n",
      "    load_throughput: 223732.01\n",
      "    load_time_ms: 4.47\n",
      "    sample_throughput: 85.638\n",
      "    sample_time_ms: 11677.009\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632126527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1710.62</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-28-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 151\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7025994724697537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006432487398938767\n",
      "          policy_loss: 0.0036093667149543762\n",
      "          total_loss: -0.011025957928763496\n",
      "          vf_explained_var: -0.7287224531173706\n",
      "          vf_loss: 0.002390671381726861\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.971428571428575\n",
      "    ram_util_percent: 63.19285714285716\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415694652169985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.666834058567318\n",
      "    mean_inference_ms: 1.414523902717341\n",
      "    mean_raw_obs_processing_ms: 0.6073630846253152\n",
      "  time_since_restore: 1720.717901468277\n",
      "  time_this_iter_s: 10.09569787979126\n",
      "  time_total_s: 1720.717901468277\n",
      "  timers:\n",
      "    learn_throughput: 1719.905\n",
      "    learn_time_ms: 581.427\n",
      "    load_throughput: 223963.775\n",
      "    load_time_ms: 4.465\n",
      "    sample_throughput: 85.986\n",
      "    sample_time_ms: 11629.803\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632126538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1720.72</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 152\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8783858007854886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013936477993359\n",
      "          policy_loss: 0.10666710974441634\n",
      "          total_loss: 0.08989093229174613\n",
      "          vf_explained_var: -0.22379854321479797\n",
      "          vf_loss: 0.002007682747595633\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.77333333333333\n",
      "    ram_util_percent: 63.31333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415711748863583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.66369200239954\n",
      "    mean_inference_ms: 1.4145755188466194\n",
      "    mean_raw_obs_processing_ms: 0.6097379440142677\n",
      "  time_since_restore: 1731.2072131633759\n",
      "  time_this_iter_s: 10.489311695098877\n",
      "  time_total_s: 1731.2072131633759\n",
      "  timers:\n",
      "    learn_throughput: 1701.033\n",
      "    learn_time_ms: 587.878\n",
      "    load_throughput: 223123.826\n",
      "    load_time_ms: 4.482\n",
      "    sample_throughput: 86.088\n",
      "    sample_time_ms: 11615.973\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1632126548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1731.21</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 153\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.928761891523997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008961156287108293\n",
      "          policy_loss: 0.04052633022268613\n",
      "          total_loss: 0.023842195007536145\n",
      "          vf_explained_var: -0.5684307813644409\n",
      "          vf_loss: 0.0026034868645688726\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.50625\n",
      "    ram_util_percent: 63.68125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415731489396076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.660708905435804\n",
      "    mean_inference_ms: 1.414637016879712\n",
      "    mean_raw_obs_processing_ms: 0.6121397743759047\n",
      "  time_since_restore: 1742.4055054187775\n",
      "  time_this_iter_s: 11.198292255401611\n",
      "  time_total_s: 1742.4055054187775\n",
      "  timers:\n",
      "    learn_throughput: 1678.277\n",
      "    learn_time_ms: 595.849\n",
      "    load_throughput: 218451.058\n",
      "    load_time_ms: 4.578\n",
      "    sample_throughput: 85.627\n",
      "    sample_time_ms: 11678.573\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632126559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1742.41</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 154\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9069038391113282\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008835459813039521\n",
      "          policy_loss: -0.1281103394097752\n",
      "          total_loss: -0.14622064580519994\n",
      "          vf_explained_var: -0.5542846322059631\n",
      "          vf_loss: 0.0009587336984825217\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.37857142857143\n",
      "    ram_util_percent: 63.44285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041575078604389235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.657811072164739\n",
      "    mean_inference_ms: 1.41469880750882\n",
      "    mean_raw_obs_processing_ms: 0.6145659652923081\n",
      "  time_since_restore: 1752.409030675888\n",
      "  time_this_iter_s: 10.003525257110596\n",
      "  time_total_s: 1752.409030675888\n",
      "  timers:\n",
      "    learn_throughput: 1676.95\n",
      "    learn_time_ms: 596.321\n",
      "    load_throughput: 217966.315\n",
      "    load_time_ms: 4.588\n",
      "    sample_throughput: 85.996\n",
      "    sample_time_ms: 11628.411\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632126569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1752.41</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 155\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0331263767348395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009951206664902538\n",
      "          policy_loss: -0.008941985956496663\n",
      "          total_loss: -0.028210800306664573\n",
      "          vf_explained_var: -0.2712998390197754\n",
      "          vf_loss: 0.0010624469267592454\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.535714285714285\n",
      "    ram_util_percent: 63.171428571428585\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157697625197246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.654994769705194\n",
      "    mean_inference_ms: 1.4147606713182501\n",
      "    mean_raw_obs_processing_ms: 0.6170150070114299\n",
      "  time_since_restore: 1762.2583701610565\n",
      "  time_this_iter_s: 9.849339485168457\n",
      "  time_total_s: 1762.2583701610565\n",
      "  timers:\n",
      "    learn_throughput: 1676.953\n",
      "    learn_time_ms: 596.32\n",
      "    load_throughput: 217729.835\n",
      "    load_time_ms: 4.593\n",
      "    sample_throughput: 86.463\n",
      "    sample_time_ms: 11565.693\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1632126579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1762.26</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 156\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.854465937614441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010563008416927536\n",
      "          policy_loss: -0.05971614900562498\n",
      "          total_loss: -0.07668500070770581\n",
      "          vf_explained_var: -0.19832418859004974\n",
      "          vf_loss: 0.0015758118369073296\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.61333333333333\n",
      "    ram_util_percent: 63.113333333333344\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157884890885765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.652268769826529\n",
      "    mean_inference_ms: 1.4148227770045931\n",
      "    mean_raw_obs_processing_ms: 0.6194853707101892\n",
      "  time_since_restore: 1772.4228355884552\n",
      "  time_this_iter_s: 10.164465427398682\n",
      "  time_total_s: 1772.4228355884552\n",
      "  timers:\n",
      "    learn_throughput: 1677.077\n",
      "    learn_time_ms: 596.276\n",
      "    load_throughput: 217217.906\n",
      "    load_time_ms: 4.604\n",
      "    sample_throughput: 86.682\n",
      "    sample_time_ms: 11536.412\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632126589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1772.42</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 157\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.931015510029263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010701728258007828\n",
      "          policy_loss: -0.1580250684171915\n",
      "          total_loss: -0.1763134772164954\n",
      "          vf_explained_var: -0.3803372383117676\n",
      "          vf_loss: 0.0010217454963518926\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.97857142857142\n",
      "    ram_util_percent: 63.064285714285724\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158070333326676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.649614402721946\n",
      "    mean_inference_ms: 1.4148837945230994\n",
      "    mean_raw_obs_processing_ms: 0.6219757083003251\n",
      "  time_since_restore: 1782.4344110488892\n",
      "  time_this_iter_s: 10.01157546043396\n",
      "  time_total_s: 1782.4344110488892\n",
      "  timers:\n",
      "    learn_throughput: 1676.085\n",
      "    learn_time_ms: 596.628\n",
      "    load_throughput: 217114.461\n",
      "    load_time_ms: 4.606\n",
      "    sample_throughput: 87.046\n",
      "    sample_time_ms: 11488.23\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632126599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1782.43</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 158\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7057416399319967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007635152878485791\n",
      "          policy_loss: -0.01528987805876467\n",
      "          total_loss: -0.030321963710917367\n",
      "          vf_explained_var: -0.5940144658088684\n",
      "          vf_loss: 0.002025331723658989\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.980000000000004\n",
      "    ram_util_percent: 63.013333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041582502033947775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.647059384669488\n",
      "    mean_inference_ms: 1.4149452324530682\n",
      "    mean_raw_obs_processing_ms: 0.6244839271339434\n",
      "  time_since_restore: 1792.6160254478455\n",
      "  time_this_iter_s: 10.181614398956299\n",
      "  time_total_s: 1792.6160254478455\n",
      "  timers:\n",
      "    learn_throughput: 1675.98\n",
      "    learn_time_ms: 596.666\n",
      "    load_throughput: 216843.945\n",
      "    load_time_ms: 4.612\n",
      "    sample_throughput: 87.269\n",
      "    sample_time_ms: 11458.885\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1632126610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         1792.62</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 159\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9323585642708672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00679419075021287\n",
      "          policy_loss: -0.014811297009388606\n",
      "          total_loss: -0.032825601514842775\n",
      "          vf_explained_var: -0.3977850079536438\n",
      "          vf_loss: 0.0013092828093148354\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.864285714285714\n",
      "    ram_util_percent: 63.064285714285724\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041584239581792165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.644577002000506\n",
      "    mean_inference_ms: 1.4150061186193605\n",
      "    mean_raw_obs_processing_ms: 0.627009152665989\n",
      "  time_since_restore: 1802.3267333507538\n",
      "  time_this_iter_s: 9.710707902908325\n",
      "  time_total_s: 1802.3267333507538\n",
      "  timers:\n",
      "    learn_throughput: 1676.32\n",
      "    learn_time_ms: 596.545\n",
      "    load_throughput: 216879.825\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 87.844\n",
      "    sample_time_ms: 11383.862\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1632126619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         1802.33</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 160\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.709266726175944\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011849946770960271\n",
      "          policy_loss: 0.03635709020826552\n",
      "          total_loss: 0.02265868985818492\n",
      "          vf_explained_var: -0.5302795171737671\n",
      "          vf_loss: 0.0033942697783155987\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96428571428571\n",
      "    ram_util_percent: 63.042857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158595646798848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.642178248753716\n",
      "    mean_inference_ms: 1.415067266607409\n",
      "    mean_raw_obs_processing_ms: 0.6267981145095763\n",
      "  time_since_restore: 1812.4947860240936\n",
      "  time_this_iter_s: 10.168052673339844\n",
      "  time_total_s: 1812.4947860240936\n",
      "  timers:\n",
      "    learn_throughput: 1677.379\n",
      "    learn_time_ms: 596.168\n",
      "    load_throughput: 307027.597\n",
      "    load_time_ms: 3.257\n",
      "    sample_throughput: 104.37\n",
      "    sample_time_ms: 9581.312\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632126630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         1812.49</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 161\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.031524529722002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016136161397814694\n",
      "          policy_loss: 0.05047281897730298\n",
      "          total_loss: 0.03388754435711437\n",
      "          vf_explained_var: -0.21702606976032257\n",
      "          vf_loss: 0.003729969645953841\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35714285714285\n",
      "    ram_util_percent: 63.00714285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158759987902392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.639534921694388\n",
      "    mean_inference_ms: 1.4151272158633545\n",
      "    mean_raw_obs_processing_ms: 0.6266464591894608\n",
      "  time_since_restore: 1822.395412683487\n",
      "  time_this_iter_s: 9.90062665939331\n",
      "  time_total_s: 1822.395412683487\n",
      "  timers:\n",
      "    learn_throughput: 1677.2\n",
      "    learn_time_ms: 596.232\n",
      "    load_throughput: 305431.243\n",
      "    load_time_ms: 3.274\n",
      "    sample_throughput: 104.584\n",
      "    sample_time_ms: 9561.709\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632126639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">          1822.4</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 162\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8225414130422803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007339275940083469\n",
      "          policy_loss: 0.03949136982361476\n",
      "          total_loss: 0.022887613707118563\n",
      "          vf_explained_var: -0.9830220341682434\n",
      "          vf_loss: 0.0016216558119696047\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.766666666666666\n",
      "    ram_util_percent: 63.06000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041589156092189786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.636925876849821\n",
      "    mean_inference_ms: 1.4151857496901914\n",
      "    mean_raw_obs_processing_ms: 0.6265516347266948\n",
      "  time_since_restore: 1832.2726409435272\n",
      "  time_this_iter_s: 9.877228260040283\n",
      "  time_total_s: 1832.2726409435272\n",
      "  timers:\n",
      "    learn_throughput: 1693.617\n",
      "    learn_time_ms: 590.452\n",
      "    load_throughput: 307768.801\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 105.187\n",
      "    sample_time_ms: 9506.872\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1632126649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1832.27</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 163\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1643920527564156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006886587468833532\n",
      "          policy_loss: 0.044815319610966575\n",
      "          total_loss: 0.024910272740655475\n",
      "          vf_explained_var: -0.3528602719306946\n",
      "          vf_loss: 0.0017388750514429477\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.67857142857144\n",
      "    ram_util_percent: 63.092857142857156\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159072255685421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.634376649824127\n",
      "    mean_inference_ms: 1.4152428836927806\n",
      "    mean_raw_obs_processing_ms: 0.6265112917843955\n",
      "  time_since_restore: 1842.3275883197784\n",
      "  time_this_iter_s: 10.05494737625122\n",
      "  time_total_s: 1842.3275883197784\n",
      "  timers:\n",
      "    learn_throughput: 1718.057\n",
      "    learn_time_ms: 582.053\n",
      "    load_throughput: 316608.593\n",
      "    load_time_ms: 3.158\n",
      "    sample_throughput: 106.371\n",
      "    sample_time_ms: 9401.043\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632126659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         1842.33</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 164\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0015248974164326\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00900628923263447\n",
      "          policy_loss: 0.06433816916412777\n",
      "          total_loss: 0.04586701513164573\n",
      "          vf_explained_var: -0.05829077959060669\n",
      "          vf_loss: 0.0015440976570567323\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86428571428571\n",
      "    ram_util_percent: 63.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159227340960396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.631893774151344\n",
      "    mean_inference_ms: 1.4152995161820972\n",
      "    mean_raw_obs_processing_ms: 0.6265230885757503\n",
      "  time_since_restore: 1852.4454638957977\n",
      "  time_this_iter_s: 10.117875576019287\n",
      "  time_total_s: 1852.4454638957977\n",
      "  timers:\n",
      "    learn_throughput: 1718.023\n",
      "    learn_time_ms: 582.065\n",
      "    load_throughput: 318541.831\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 106.231\n",
      "    sample_time_ms: 9413.481\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632126670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1852.45</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-31-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 165\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0489945398436653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009430453485419873\n",
      "          policy_loss: 0.028449697047472\n",
      "          total_loss: 0.010367792430851194\n",
      "          vf_explained_var: 0.06774420291185379\n",
      "          vf_loss: 0.002408039236130814\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.83333333333334\n",
      "    ram_util_percent: 63.15333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159381274841051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.629491383939493\n",
      "    mean_inference_ms: 1.4153558230236924\n",
      "    mean_raw_obs_processing_ms: 0.6265824796386957\n",
      "  time_since_restore: 1862.7172696590424\n",
      "  time_this_iter_s: 10.271805763244629\n",
      "  time_total_s: 1862.7172696590424\n",
      "  timers:\n",
      "    learn_throughput: 1719.62\n",
      "    learn_time_ms: 581.524\n",
      "    load_throughput: 319385.946\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 105.75\n",
      "    sample_time_ms: 9456.28\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1632126680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1862.72</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 166\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0162402987480164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010650142933445681\n",
      "          policy_loss: -0.06921524935298495\n",
      "          total_loss: -0.08724224724703365\n",
      "          vf_explained_var: -0.32626304030418396\n",
      "          vf_loss: 0.002135405090585765\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.97142857142857\n",
      "    ram_util_percent: 63.17142857142859\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041595365898439374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.627161019155611\n",
      "    mean_inference_ms: 1.41541180831205\n",
      "    mean_raw_obs_processing_ms: 0.6266898034892592\n",
      "  time_since_restore: 1872.7389342784882\n",
      "  time_this_iter_s: 10.0216646194458\n",
      "  time_total_s: 1872.7389342784882\n",
      "  timers:\n",
      "    learn_throughput: 1720.937\n",
      "    learn_time_ms: 581.079\n",
      "    load_throughput: 320398.445\n",
      "    load_time_ms: 3.121\n",
      "    sample_throughput: 105.905\n",
      "    sample_time_ms: 9442.429\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632126690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1872.74</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 167\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0637549665239123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005865713156252106\n",
      "          policy_loss: -0.09443898503151205\n",
      "          total_loss: -0.11279116057687336\n",
      "          vf_explained_var: -0.7848948240280151\n",
      "          vf_loss: 0.0022853735187608336\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.726666666666674\n",
      "    ram_util_percent: 63.200000000000024\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159689776763499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.624902926611366\n",
      "    mean_inference_ms: 1.4154673679379761\n",
      "    mean_raw_obs_processing_ms: 0.6268428605415101\n",
      "  time_since_restore: 1882.9138014316559\n",
      "  time_this_iter_s: 10.174867153167725\n",
      "  time_total_s: 1882.9138014316559\n",
      "  timers:\n",
      "    learn_throughput: 1721.888\n",
      "    learn_time_ms: 580.758\n",
      "    load_throughput: 319235.231\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 105.718\n",
      "    sample_time_ms: 9459.096\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632126700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1882.91</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 168\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.009261128637526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00854285635255615\n",
      "          policy_loss: -0.038546573867400485\n",
      "          total_loss: -0.057449499600463444\n",
      "          vf_explained_var: 0.27705129981040955\n",
      "          vf_loss: 0.0011896852804865275\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.473333333333336\n",
      "    ram_util_percent: 63.24\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041598410741542066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.622724035330569\n",
      "    mean_inference_ms: 1.4155227174387437\n",
      "    mean_raw_obs_processing_ms: 0.6270401092621\n",
      "  time_since_restore: 1893.1684093475342\n",
      "  time_this_iter_s: 10.254607915878296\n",
      "  time_total_s: 1893.1684093475342\n",
      "  timers:\n",
      "    learn_throughput: 1723.796\n",
      "    learn_time_ms: 580.115\n",
      "    load_throughput: 318553.928\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 105.63\n",
      "    sample_time_ms: 9467.037\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1632126710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1893.17</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 169\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2485204935073853\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009245189221056115\n",
      "          policy_loss: 0.06970644891262054\n",
      "          total_loss: 0.04893759406275219\n",
      "          vf_explained_var: -0.4373665153980255\n",
      "          vf_loss: 0.0017163488819884757\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.964285714285715\n",
      "    ram_util_percent: 63.378571428571426\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159991754776107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.62061745209107\n",
      "    mean_inference_ms: 1.415577800290264\n",
      "    mean_raw_obs_processing_ms: 0.6272787557488484\n",
      "  time_since_restore: 1903.4138357639313\n",
      "  time_this_iter_s: 10.245426416397095\n",
      "  time_total_s: 1903.4138357639313\n",
      "  timers:\n",
      "    learn_throughput: 1723.052\n",
      "    learn_time_ms: 580.365\n",
      "    load_throughput: 318391.911\n",
      "    load_time_ms: 3.141\n",
      "    sample_throughput: 105.039\n",
      "    sample_time_ms: 9520.257\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632126721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1903.41</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 170\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0617015110121835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005143946470730719\n",
      "          policy_loss: -0.07652996364567015\n",
      "          total_loss: -0.09479184974398878\n",
      "          vf_explained_var: -0.35004207491874695\n",
      "          vf_loss: 0.002355129480646509\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.826666666666675\n",
      "    ram_util_percent: 63.49333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041601420512100605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.618576684527223\n",
      "    mean_inference_ms: 1.4156332888047636\n",
      "    mean_raw_obs_processing_ms: 0.6275578955038162\n",
      "  time_since_restore: 1913.4923400878906\n",
      "  time_this_iter_s: 10.07850432395935\n",
      "  time_total_s: 1913.4923400878906\n",
      "  timers:\n",
      "    learn_throughput: 1722.128\n",
      "    learn_time_ms: 580.677\n",
      "    load_throughput: 314897.144\n",
      "    load_time_ms: 3.176\n",
      "    sample_throughput: 105.142\n",
      "    sample_time_ms: 9510.97\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632126731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1913.49</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 171\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9662905004289415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010018537482064991\n",
      "          policy_loss: -0.06077228126426538\n",
      "          total_loss: -0.07819289486441347\n",
      "          vf_explained_var: -0.7911914587020874\n",
      "          vf_loss: 0.0022422926707400216\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.114285714285714\n",
      "    ram_util_percent: 63.57142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160291092824622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.61659794620828\n",
      "    mean_inference_ms: 1.4156884973999941\n",
      "    mean_raw_obs_processing_ms: 0.6278766576171645\n",
      "  time_since_restore: 1923.4809045791626\n",
      "  time_this_iter_s: 9.988564491271973\n",
      "  time_total_s: 1923.4809045791626\n",
      "  timers:\n",
      "    learn_throughput: 1724.773\n",
      "    learn_time_ms: 579.787\n",
      "    load_throughput: 315129.003\n",
      "    load_time_ms: 3.173\n",
      "    sample_throughput: 105.035\n",
      "    sample_time_ms: 9520.658\n",
      "    update_time_ms: 1.625\n",
      "  timestamp: 1632126741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         1923.48</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 172\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9080730398495993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006925906335424041\n",
      "          policy_loss: -0.039150238119893606\n",
      "          total_loss: -0.05638300933771663\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0018479586080906705\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.67857142857142\n",
      "    ram_util_percent: 63.578571428571436\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160438089624784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.61466952415505\n",
      "    mean_inference_ms: 1.4157431239242084\n",
      "    mean_raw_obs_processing_ms: 0.6282326881503905\n",
      "  time_since_restore: 1933.576608657837\n",
      "  time_this_iter_s: 10.095704078674316\n",
      "  time_total_s: 1933.576608657837\n",
      "  timers:\n",
      "    learn_throughput: 1725.623\n",
      "    learn_time_ms: 579.501\n",
      "    load_throughput: 312821.002\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 104.791\n",
      "    sample_time_ms: 9542.773\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1632126751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1933.58</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 173\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8249986688296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007627164796434233\n",
      "          policy_loss: -0.04270695013304551\n",
      "          total_loss: -0.05973047816918956\n",
      "          vf_explained_var: -0.6725946068763733\n",
      "          vf_loss: 0.0012264596945088771\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14\n",
      "    ram_util_percent: 63.59333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041605822971329046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.612793899339504\n",
      "    mean_inference_ms: 1.4157967651440049\n",
      "    mean_raw_obs_processing_ms: 0.6286245466953893\n",
      "  time_since_restore: 1943.7592332363129\n",
      "  time_this_iter_s: 10.182624578475952\n",
      "  time_total_s: 1943.7592332363129\n",
      "  timers:\n",
      "    learn_throughput: 1725.014\n",
      "    learn_time_ms: 579.706\n",
      "    load_throughput: 313255.56\n",
      "    load_time_ms: 3.192\n",
      "    sample_throughput: 104.655\n",
      "    sample_time_ms: 9555.247\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632126761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1943.76</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 174\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8058018591668872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005699818053959997\n",
      "          policy_loss: -0.05038702653514014\n",
      "          total_loss: -0.06702649494012197\n",
      "          vf_explained_var: -0.9999088048934937\n",
      "          vf_loss: 0.001418552723609739\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84285714285714\n",
      "    ram_util_percent: 63.65714285714288\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041607241213715555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.6109816337023\n",
      "    mean_inference_ms: 1.4158502045742365\n",
      "    mean_raw_obs_processing_ms: 0.6290506775383007\n",
      "  time_since_restore: 1953.884105205536\n",
      "  time_this_iter_s: 10.124871969223022\n",
      "  time_total_s: 1953.884105205536\n",
      "  timers:\n",
      "    learn_throughput: 1726.577\n",
      "    learn_time_ms: 579.181\n",
      "    load_throughput: 312874.673\n",
      "    load_time_ms: 3.196\n",
      "    sample_throughput: 104.641\n",
      "    sample_time_ms: 9556.485\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632126771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1953.88</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 175\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9335892743534513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008668825360535573\n",
      "          policy_loss: -0.03736035231914785\n",
      "          total_loss: -0.054838732671406536\n",
      "          vf_explained_var: -0.8436933159828186\n",
      "          vf_loss: 0.0018575120264560812\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.6\n",
      "    ram_util_percent: 63.69333333333336\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160864966794977\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.6092103218148\n",
      "    mean_inference_ms: 1.4159035007978504\n",
      "    mean_raw_obs_processing_ms: 0.6295098075842113\n",
      "  time_since_restore: 1963.9092438220978\n",
      "  time_this_iter_s: 10.02513861656189\n",
      "  time_total_s: 1963.9092438220978\n",
      "  timers:\n",
      "    learn_throughput: 1727.193\n",
      "    learn_time_ms: 578.974\n",
      "    load_throughput: 312108.702\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 104.909\n",
      "    sample_time_ms: 9532.033\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632126781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1963.91</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 176\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7783190184169346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010047696986673849\n",
      "          policy_loss: -0.007073184475302696\n",
      "          total_loss: -0.02311445830596818\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017419155687093736\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.828571428571415\n",
      "    ram_util_percent: 63.70714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041610045499489746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.607476575006618\n",
      "    mean_inference_ms: 1.4159561793993243\n",
      "    mean_raw_obs_processing_ms: 0.6299986010835754\n",
      "  time_since_restore: 1973.933079957962\n",
      "  time_this_iter_s: 10.023836135864258\n",
      "  time_total_s: 1973.933079957962\n",
      "  timers:\n",
      "    learn_throughput: 1728.066\n",
      "    learn_time_ms: 578.682\n",
      "    load_throughput: 312190.01\n",
      "    load_time_ms: 3.203\n",
      "    sample_throughput: 104.904\n",
      "    sample_time_ms: 9532.569\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632126791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1973.93</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-33-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 177\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7329726762241788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007181941080705675\n",
      "          policy_loss: 0.024278182577755717\n",
      "          total_loss: 0.008006243407726288\n",
      "          vf_explained_var: -0.7575583457946777\n",
      "          vf_loss: 0.0010577907604682776\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72666666666667\n",
      "    ram_util_percent: 63.83333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041611437024542924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.605788072611114\n",
      "    mean_inference_ms: 1.4160087266815895\n",
      "    mean_raw_obs_processing_ms: 0.6305176034173802\n",
      "  time_since_restore: 1984.0217506885529\n",
      "  time_this_iter_s: 10.08867073059082\n",
      "  time_total_s: 1984.0217506885529\n",
      "  timers:\n",
      "    learn_throughput: 1726.418\n",
      "    learn_time_ms: 579.234\n",
      "    load_throughput: 312818.669\n",
      "    load_time_ms: 3.197\n",
      "    sample_throughput: 105.005\n",
      "    sample_time_ms: 9523.358\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632126801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1984.02</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 178\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5324526720576817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0056427485046082635\n",
      "          policy_loss: -0.06298537010120021\n",
      "          total_loss: -0.07747171603971058\n",
      "          vf_explained_var: -0.5808159112930298\n",
      "          vf_loss: 0.0008381795157523205\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79285714285713\n",
      "    ram_util_percent: 63.79285714285712\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041612802739053635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.604154267663112\n",
      "    mean_inference_ms: 1.4160611108160646\n",
      "    mean_raw_obs_processing_ms: 0.6310657627619232\n",
      "  time_since_restore: 1994.2224354743958\n",
      "  time_this_iter_s: 10.200684785842896\n",
      "  time_total_s: 1994.2224354743958\n",
      "  timers:\n",
      "    learn_throughput: 1724.982\n",
      "    learn_time_ms: 579.716\n",
      "    load_throughput: 313944.91\n",
      "    load_time_ms: 3.185\n",
      "    sample_throughput: 105.07\n",
      "    sample_time_ms: 9517.471\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632126812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         1994.22</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 179\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.885928213596344e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7091769364145066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004875399914943457\n",
      "          policy_loss: -0.022072459591759575\n",
      "          total_loss: -0.03760491121146414\n",
      "          vf_explained_var: -0.4207025170326233\n",
      "          vf_loss: 0.0015593189552115898\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.85333333333334\n",
      "    ram_util_percent: 64.07333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041614235591386814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.602589367676579\n",
      "    mean_inference_ms: 1.416116021493728\n",
      "    mean_raw_obs_processing_ms: 0.6316422188210755\n",
      "  time_since_restore: 2004.6249372959137\n",
      "  time_this_iter_s: 10.402501821517944\n",
      "  time_total_s: 2004.6249372959137\n",
      "  timers:\n",
      "    learn_throughput: 1721.999\n",
      "    learn_time_ms: 580.72\n",
      "    load_throughput: 312462.118\n",
      "    load_time_ms: 3.2\n",
      "    sample_throughput: 104.908\n",
      "    sample_time_ms: 9532.139\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632126822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         2004.62</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 180\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.66976248688168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00750395816692316\n",
      "          policy_loss: -0.058999758544895385\n",
      "          total_loss: -0.07428325861692428\n",
      "          vf_explained_var: -0.3374709486961365\n",
      "          vf_loss: 0.001414123553937922\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.031578947368416\n",
      "    ram_util_percent: 64.56842105263158\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161568870864288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.601047921175853\n",
      "    mean_inference_ms: 1.4161719967601518\n",
      "    mean_raw_obs_processing_ms: 0.6331906508617928\n",
      "  time_since_restore: 2031.2605640888214\n",
      "  time_this_iter_s: 26.635626792907715\n",
      "  time_total_s: 2031.2605640888214\n",
      "  timers:\n",
      "    learn_throughput: 1720.828\n",
      "    learn_time_ms: 581.116\n",
      "    load_throughput: 221727.275\n",
      "    load_time_ms: 4.51\n",
      "    sample_throughput: 89.396\n",
      "    sample_time_ms: 11186.139\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632126849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         2031.26</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 181\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6754596577750311\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076887043196217225\n",
      "          policy_loss: -0.012537481304672029\n",
      "          total_loss: -0.027832083900769553\n",
      "          vf_explained_var: -0.5396286249160767\n",
      "          vf_loss: 0.00145999182584799\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.0625\n",
      "    ram_util_percent: 64.4625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161711056099537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.59959804301483\n",
      "    mean_inference_ms: 1.4162255898784264\n",
      "    mean_raw_obs_processing_ms: 0.634759913107523\n",
      "  time_since_restore: 2042.5274369716644\n",
      "  time_this_iter_s: 11.266872882843018\n",
      "  time_total_s: 2042.5274369716644\n",
      "  timers:\n",
      "    learn_throughput: 1719.093\n",
      "    learn_time_ms: 581.702\n",
      "    load_throughput: 222110.051\n",
      "    load_time_ms: 4.502\n",
      "    sample_throughput: 88.391\n",
      "    sample_time_ms: 11313.413\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632126860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         2042.53</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 182\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7885844786961873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006613154015048275\n",
      "          policy_loss: 0.07794097264607748\n",
      "          total_loss: 0.06150087784561846\n",
      "          vf_explained_var: -0.39964836835861206\n",
      "          vf_loss: 0.001445751596798396\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67142857142857\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161848701954364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.598171359498325\n",
      "    mean_inference_ms: 1.4162787204606107\n",
      "    mean_raw_obs_processing_ms: 0.6363489127647389\n",
      "  time_since_restore: 2052.336853981018\n",
      "  time_this_iter_s: 9.809417009353638\n",
      "  time_total_s: 2052.336853981018\n",
      "  timers:\n",
      "    learn_throughput: 1717.834\n",
      "    learn_time_ms: 582.128\n",
      "    load_throughput: 222877.214\n",
      "    load_time_ms: 4.487\n",
      "    sample_throughput: 88.618\n",
      "    sample_time_ms: 11284.375\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632126870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         2052.34</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 183\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.725207633442349\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007045241866112168\n",
      "          policy_loss: -0.0452356970972485\n",
      "          total_loss: -0.06109626938899358\n",
      "          vf_explained_var: -0.3046603202819824\n",
      "          vf_loss: 0.0013915043945113817\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25000000000001\n",
      "    ram_util_percent: 64.41428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161985441700832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.596762200038631\n",
      "    mean_inference_ms: 1.4163313331007306\n",
      "    mean_raw_obs_processing_ms: 0.6379569000398468\n",
      "  time_since_restore: 2061.959714412689\n",
      "  time_this_iter_s: 9.622860431671143\n",
      "  time_total_s: 2061.959714412689\n",
      "  timers:\n",
      "    learn_throughput: 1717.809\n",
      "    learn_time_ms: 582.137\n",
      "    load_throughput: 223145.193\n",
      "    load_time_ms: 4.481\n",
      "    sample_throughput: 89.059\n",
      "    sample_time_ms: 11228.484\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632126880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         2061.96</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 184\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6762632091840108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006744297719715359\n",
      "          policy_loss: 0.02709362900091542\n",
      "          total_loss: 0.010954050429993206\n",
      "          vf_explained_var: -0.6021944880485535\n",
      "          vf_loss: 0.0006230485923070875\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.949999999999996\n",
      "    ram_util_percent: 64.35714285714285\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041621198927010584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.595361453671824\n",
      "    mean_inference_ms: 1.416383683322517\n",
      "    mean_raw_obs_processing_ms: 0.6395829245032684\n",
      "  time_since_restore: 2071.556263923645\n",
      "  time_this_iter_s: 9.59654951095581\n",
      "  time_total_s: 2071.556263923645\n",
      "  timers:\n",
      "    learn_throughput: 1717.353\n",
      "    learn_time_ms: 582.292\n",
      "    load_throughput: 220352.728\n",
      "    load_time_ms: 4.538\n",
      "    sample_throughput: 89.482\n",
      "    sample_time_ms: 11175.453\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632126889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         2071.56</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-34-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 185\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8285709685749478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009952311876827243\n",
      "          policy_loss: -0.008421013462874624\n",
      "          total_loss: -0.026055426109168264\n",
      "          vf_explained_var: -0.7576141953468323\n",
      "          vf_loss: 0.0006512951765519877\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.107692307692304\n",
      "    ram_util_percent: 64.22307692307695\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162251193226323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.5939689093538\n",
      "    mean_inference_ms: 1.4164350607962641\n",
      "    mean_raw_obs_processing_ms: 0.6412262715481779\n",
      "  time_since_restore: 2080.9746718406677\n",
      "  time_this_iter_s: 9.418407917022705\n",
      "  time_total_s: 2080.9746718406677\n",
      "  timers:\n",
      "    learn_throughput: 1716.249\n",
      "    learn_time_ms: 582.666\n",
      "    load_throughput: 220329.577\n",
      "    load_time_ms: 4.539\n",
      "    sample_throughput: 89.974\n",
      "    sample_time_ms: 11114.369\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1632126899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         2080.97</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 186\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6682938920127022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00981139065730261\n",
      "          policy_loss: -0.08806632061799367\n",
      "          total_loss: -0.10409031994640827\n",
      "          vf_explained_var: -0.29615840315818787\n",
      "          vf_loss: 0.0006589406819936509\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35384615384615\n",
      "    ram_util_percent: 64.0923076923077\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162383726521157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592575552324442\n",
      "    mean_inference_ms: 1.4164858216317844\n",
      "    mean_raw_obs_processing_ms: 0.6428859656667336\n",
      "  time_since_restore: 2090.1710641384125\n",
      "  time_this_iter_s: 9.196392297744751\n",
      "  time_total_s: 2090.1710641384125\n",
      "  timers:\n",
      "    learn_throughput: 1717.055\n",
      "    learn_time_ms: 582.392\n",
      "    load_throughput: 220431.476\n",
      "    load_time_ms: 4.537\n",
      "    sample_throughput: 90.646\n",
      "    sample_time_ms: 11031.888\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1632126908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         2090.17</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-17\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 187\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6842811465263368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011735283864466487\n",
      "          policy_loss: -0.034306193060345117\n",
      "          total_loss: -0.0503106243080563\n",
      "          vf_explained_var: -0.0404602512717247\n",
      "          vf_loss: 0.0008383807043881259\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03846153846154\n",
      "    ram_util_percent: 64.07692307692308\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162515347474554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.591173431074402\n",
      "    mean_inference_ms: 1.416535165596595\n",
      "    mean_raw_obs_processing_ms: 0.6445616971109193\n",
      "  time_since_restore: 2099.310129404068\n",
      "  time_this_iter_s: 9.139065265655518\n",
      "  time_total_s: 2099.310129404068\n",
      "  timers:\n",
      "    learn_throughput: 1719.27\n",
      "    learn_time_ms: 581.642\n",
      "    load_throughput: 220443.062\n",
      "    load_time_ms: 4.536\n",
      "    sample_throughput: 91.427\n",
      "    sample_time_ms: 10937.708\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1632126917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         2099.31</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 188\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8335965924792819\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010174872020174518\n",
      "          policy_loss: -0.04677462660604053\n",
      "          total_loss: -0.06451612164576849\n",
      "          vf_explained_var: -0.33553895354270935\n",
      "          vf_loss: 0.0005944727420380028\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.11538461538461\n",
      "    ram_util_percent: 63.96923076923076\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041626474578319216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.589742167410389\n",
      "    mean_inference_ms: 1.4165839313445563\n",
      "    mean_raw_obs_processing_ms: 0.6462523620405995\n",
      "  time_since_restore: 2108.1823575496674\n",
      "  time_this_iter_s: 8.872228145599365\n",
      "  time_total_s: 2108.1823575496674\n",
      "  timers:\n",
      "    learn_throughput: 1720.577\n",
      "    learn_time_ms: 581.2\n",
      "    load_throughput: 220331.892\n",
      "    load_time_ms: 4.539\n",
      "    sample_throughput: 92.547\n",
      "    sample_time_ms: 10805.339\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1632126926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         2108.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9490810341305203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007056207957380699\n",
      "          policy_loss: 0.011537795265515646\n",
      "          total_loss: -0.007423992620574103\n",
      "          vf_explained_var: 0.3393772840499878\n",
      "          vf_loss: 0.0005290242477510927\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.16923076923077\n",
      "    ram_util_percent: 63.907692307692294\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162776071117651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.588269561918885\n",
      "    mean_inference_ms: 1.416631187927471\n",
      "    mean_raw_obs_processing_ms: 0.6479571575115114\n",
      "  time_since_restore: 2116.9520359039307\n",
      "  time_this_iter_s: 8.769678354263306\n",
      "  time_total_s: 2116.9520359039307\n",
      "  timers:\n",
      "    learn_throughput: 1724.144\n",
      "    learn_time_ms: 579.998\n",
      "    load_throughput: 221120.601\n",
      "    load_time_ms: 4.522\n",
      "    sample_throughput: 93.956\n",
      "    sample_time_ms: 10643.276\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1632126935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         2116.95</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 190\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6935953352186415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006623536253318305\n",
      "          policy_loss: -0.03833904792037275\n",
      "          total_loss: -0.05402970069812404\n",
      "          vf_explained_var: -0.9962718486785889\n",
      "          vf_loss: 0.001245298679148416\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.05384615384615\n",
      "    ram_util_percent: 63.94615384615384\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162856366750026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.58663069217705\n",
      "    mean_inference_ms: 1.4166660251529115\n",
      "    mean_raw_obs_processing_ms: 0.647689974089185\n",
      "  time_since_restore: 2126.0925619602203\n",
      "  time_this_iter_s: 9.140526056289673\n",
      "  time_total_s: 2126.0925619602203\n",
      "  timers:\n",
      "    learn_throughput: 1725.931\n",
      "    learn_time_ms: 579.397\n",
      "    load_throughput: 314540.56\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 112.414\n",
      "    sample_time_ms: 8895.715\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632126944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         2126.09</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 191\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8303142348925272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016685979339269134\n",
      "          policy_loss: -0.03619607144759761\n",
      "          total_loss: -0.052929166621632044\n",
      "          vf_explained_var: -0.34126976132392883\n",
      "          vf_loss: 0.0015700453520467918\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95384615384615\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041629235242222264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.584943011408804\n",
      "    mean_inference_ms: 1.4166967317088024\n",
      "    mean_raw_obs_processing_ms: 0.6474572714986866\n",
      "  time_since_restore: 2135.155714035034\n",
      "  time_this_iter_s: 9.063152074813843\n",
      "  time_total_s: 2135.155714035034\n",
      "  timers:\n",
      "    learn_throughput: 1727.799\n",
      "    learn_time_ms: 578.771\n",
      "    load_throughput: 314672.709\n",
      "    load_time_ms: 3.178\n",
      "    sample_throughput: 115.261\n",
      "    sample_time_ms: 8675.973\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632126953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         2135.16</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 192\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.42964106798172e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.502307724290424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0048305306444069475\n",
      "          policy_loss: -0.021690463026364643\n",
      "          total_loss: -0.03598155313067966\n",
      "          vf_explained_var: -0.5106896758079529\n",
      "          vf_loss: 0.000731987455381184\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.023076923076914\n",
      "    ram_util_percent: 63.95384615384615\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162978765060528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583208966217008\n",
      "    mean_inference_ms: 1.4167236195538744\n",
      "    mean_raw_obs_processing_ms: 0.6472571847684192\n",
      "  time_since_restore: 2144.4307057857513\n",
      "  time_this_iter_s: 9.274991750717163\n",
      "  time_total_s: 2144.4307057857513\n",
      "  timers:\n",
      "    learn_throughput: 1727.745\n",
      "    learn_time_ms: 578.789\n",
      "    load_throughput: 315486.923\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 115.975\n",
      "    sample_time_ms: 8622.534\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632126962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         2144.43</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 193\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7452350152863396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006355911212879937\n",
      "          policy_loss: -0.018942816721068487\n",
      "          total_loss: -0.03483642182416386\n",
      "          vf_explained_var: -0.4916817843914032\n",
      "          vf_loss: 0.001558745646616444\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.207692307692305\n",
      "    ram_util_percent: 63.93846153846153\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04163022201674459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.581414579302288\n",
      "    mean_inference_ms: 1.4167461832296138\n",
      "    mean_raw_obs_processing_ms: 0.6470890755204587\n",
      "  time_since_restore: 2153.4196271896362\n",
      "  time_this_iter_s: 8.988921403884888\n",
      "  time_total_s: 2153.4196271896362\n",
      "  timers:\n",
      "    learn_throughput: 1726.267\n",
      "    learn_time_ms: 579.285\n",
      "    load_throughput: 314462.738\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 116.841\n",
      "    sample_time_ms: 8558.63\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632126971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         2153.42</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 194\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7418097416559855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009173458863244516\n",
      "          policy_loss: -0.0694458967488673\n",
      "          total_loss: -0.08629562871323691\n",
      "          vf_explained_var: -0.3612527549266815\n",
      "          vf_loss: 0.000568366937457338\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.438461538461524\n",
      "    ram_util_percent: 64.00769230769232\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04163019046389307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.579486240731674\n",
      "    mean_inference_ms: 1.4167583608465106\n",
      "    mean_raw_obs_processing_ms: 0.6469515319385728\n",
      "  time_since_restore: 2162.5362186431885\n",
      "  time_this_iter_s: 9.116591453552246\n",
      "  time_total_s: 2162.5362186431885\n",
      "  timers:\n",
      "    learn_throughput: 1725.625\n",
      "    learn_time_ms: 579.5\n",
      "    load_throughput: 320239.437\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 117.502\n",
      "    sample_time_ms: 8510.473\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632126980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         2162.54</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 195\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4631439679198794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006025515311936916\n",
      "          policy_loss: 0.06606821376416418\n",
      "          total_loss: 0.052142609324720174\n",
      "          vf_explained_var: -0.047484006732702255\n",
      "          vf_loss: 0.0007058390328893438\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03846153846155\n",
      "    ram_util_percent: 64.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041629780761002246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.577432426153113\n",
      "    mean_inference_ms: 1.4167592670649842\n",
      "    mean_raw_obs_processing_ms: 0.6468422762790886\n",
      "  time_since_restore: 2171.685305118561\n",
      "  time_this_iter_s: 9.149086475372314\n",
      "  time_total_s: 2171.685305118561\n",
      "  timers:\n",
      "    learn_throughput: 1724.245\n",
      "    learn_time_ms: 579.964\n",
      "    load_throughput: 320913.243\n",
      "    load_time_ms: 3.116\n",
      "    sample_throughput: 117.881\n",
      "    sample_time_ms: 8483.115\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632126990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         2171.69</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 196\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.750146762530009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008246017619722067\n",
      "          policy_loss: -0.027149413567450313\n",
      "          total_loss: -0.043045986029836864\n",
      "          vf_explained_var: 0.10662201046943665\n",
      "          vf_loss: 0.0016048958812866154\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67692307692308\n",
      "    ram_util_percent: 64.03846153846155\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0416291217404419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.575289024951743\n",
      "    mean_inference_ms: 1.416752732871304\n",
      "    mean_raw_obs_processing_ms: 0.646762297120217\n",
      "  time_since_restore: 2180.81640958786\n",
      "  time_this_iter_s: 9.131104469299316\n",
      "  time_total_s: 2180.81640958786\n",
      "  timers:\n",
      "    learn_throughput: 1721.293\n",
      "    learn_time_ms: 580.959\n",
      "    load_throughput: 319048.249\n",
      "    load_time_ms: 3.134\n",
      "    sample_throughput: 117.986\n",
      "    sample_time_ms: 8475.579\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632126999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         2180.82</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 197\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.032422669728597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009014231226482237\n",
      "          policy_loss: -0.004732698533270094\n",
      "          total_loss: -0.023569897934794425\n",
      "          vf_explained_var: -0.9626822471618652\n",
      "          vf_loss: 0.0014870273349515627\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.60000000000001\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0416283592230552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.573068440058499\n",
      "    mean_inference_ms: 1.4167420795637469\n",
      "    mean_raw_obs_processing_ms: 0.6467090609472952\n",
      "  time_since_restore: 2189.705441236496\n",
      "  time_this_iter_s: 8.889031648635864\n",
      "  time_total_s: 2189.705441236496\n",
      "  timers:\n",
      "    learn_throughput: 1721.27\n",
      "    learn_time_ms: 580.967\n",
      "    load_throughput: 319135.642\n",
      "    load_time_ms: 3.133\n",
      "    sample_throughput: 118.335\n",
      "    sample_time_ms: 8450.568\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632127008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         2189.71</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-36-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 198\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9331730842590331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01895885131639723\n",
      "          policy_loss: -0.07065070025208924\n",
      "          total_loss: -0.08799697315941254\n",
      "          vf_explained_var: -0.5155646204948425\n",
      "          vf_loss: 0.001985458651809798\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.292307692307695\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0416274173341931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.570723867788033\n",
      "    mean_inference_ms: 1.4167251947801036\n",
      "    mean_raw_obs_processing_ms: 0.6466832281077266\n",
      "  time_since_restore: 2198.8795189857483\n",
      "  time_this_iter_s: 9.17407774925232\n",
      "  time_total_s: 2198.8795189857483\n",
      "  timers:\n",
      "    learn_throughput: 1723.094\n",
      "    learn_time_ms: 580.351\n",
      "    load_throughput: 319738.983\n",
      "    load_time_ms: 3.128\n",
      "    sample_throughput: 117.905\n",
      "    sample_time_ms: 8481.373\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632127017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         2198.88</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 199\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094277434878879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011805029237363998\n",
      "          policy_loss: -0.032470125953356424\n",
      "          total_loss: -0.0522883541468117\n",
      "          vf_explained_var: -0.8474217653274536\n",
      "          vf_loss: 0.0011245468755886475\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98461538461539\n",
      "    ram_util_percent: 64.13076923076925\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162637358416944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.568284267809899\n",
      "    mean_inference_ms: 1.4167030515504941\n",
      "    mean_raw_obs_processing_ms: 0.6466843601337318\n",
      "  time_since_restore: 2208.0285687446594\n",
      "  time_this_iter_s: 9.149049758911133\n",
      "  time_total_s: 2208.0285687446594\n",
      "  timers:\n",
      "    learn_throughput: 1722.831\n",
      "    learn_time_ms: 580.44\n",
      "    load_throughput: 319799.93\n",
      "    load_time_ms: 3.127\n",
      "    sample_throughput: 117.382\n",
      "    sample_time_ms: 8519.228\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632127026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         2208.03</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 200\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.672680393854777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006272767425976748\n",
      "          policy_loss: -0.07125072880751557\n",
      "          total_loss: -0.08595100550187959\n",
      "          vf_explained_var: -0.273054301738739\n",
      "          vf_loss: 0.0020265270131252086\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95714285714286\n",
      "    ram_util_percent: 64.14285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162511938473127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.565771251067\n",
      "    mean_inference_ms: 1.4166747896614458\n",
      "    mean_raw_obs_processing_ms: 0.6467110597441064\n",
      "  time_since_restore: 2218.0369913578033\n",
      "  time_this_iter_s: 10.008422613143921\n",
      "  time_total_s: 2218.0369913578033\n",
      "  timers:\n",
      "    learn_throughput: 1722.821\n",
      "    learn_time_ms: 580.443\n",
      "    load_throughput: 319087.085\n",
      "    load_time_ms: 3.134\n",
      "    sample_throughput: 116.198\n",
      "    sample_time_ms: 8606.02\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632127036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         2218.04</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 201\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4719526727994283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011739085157912202\n",
      "          policy_loss: -0.06932069431576464\n",
      "          total_loss: -0.08204991999599669\n",
      "          vf_explained_var: -0.6370185613632202\n",
      "          vf_loss: 0.001990299829049036\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.61333333333334\n",
      "    ram_util_percent: 64.24000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041623635786298435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.563199016720521\n",
      "    mean_inference_ms: 1.416638402966538\n",
      "    mean_raw_obs_processing_ms: 0.6467627279726983\n",
      "  time_since_restore: 2228.2010004520416\n",
      "  time_this_iter_s: 10.164009094238281\n",
      "  time_total_s: 2228.2010004520416\n",
      "  timers:\n",
      "    learn_throughput: 1722.105\n",
      "    learn_time_ms: 580.685\n",
      "    load_throughput: 317620.367\n",
      "    load_time_ms: 3.148\n",
      "    sample_throughput: 114.734\n",
      "    sample_time_ms: 8715.828\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632127046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">          2228.2</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 202\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.71482053399086e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3153857469558716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004299972275234904\n",
      "          policy_loss: 0.008980590291321278\n",
      "          total_loss: -0.0035310647967788907\n",
      "          vf_explained_var: -0.9119674563407898\n",
      "          vf_loss: 0.0006422012963513326\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.55999999999999\n",
      "    ram_util_percent: 64.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041622025082426434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.560570155536054\n",
      "    mean_inference_ms: 1.4165967220004676\n",
      "    mean_raw_obs_processing_ms: 0.6468384779547373\n",
      "  time_since_restore: 2238.541499376297\n",
      "  time_this_iter_s: 10.340498924255371\n",
      "  time_total_s: 2238.541499376297\n",
      "  timers:\n",
      "    learn_throughput: 1722.719\n",
      "    learn_time_ms: 580.478\n",
      "    load_throughput: 317661.262\n",
      "    load_time_ms: 3.148\n",
      "    sample_throughput: 113.347\n",
      "    sample_time_ms: 8822.501\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1632127057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         2238.54</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 203\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7039101084073385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007959033012303211\n",
      "          policy_loss: -0.016591800914870367\n",
      "          total_loss: -0.030524822655651304\n",
      "          vf_explained_var: -0.12489587813615799\n",
      "          vf_loss: 0.003106077051618033\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.092307692307685\n",
      "    ram_util_percent: 64.25384615384615\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04162033426752035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.557907972005419\n",
      "    mean_inference_ms: 1.4165529199398998\n",
      "    mean_raw_obs_processing_ms: 0.6469379756459955\n",
      "  time_since_restore: 2248.2405376434326\n",
      "  time_this_iter_s: 9.69903826713562\n",
      "  time_total_s: 2248.2405376434326\n",
      "  timers:\n",
      "    learn_throughput: 1723.217\n",
      "    learn_time_ms: 580.31\n",
      "    load_throughput: 317644.422\n",
      "    load_time_ms: 3.148\n",
      "    sample_throughput: 112.456\n",
      "    sample_time_ms: 8892.369\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632127066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         2248.24</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-37-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 204\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0990859296586777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009437312824042938\n",
      "          policy_loss: -0.01578670262048642\n",
      "          total_loss: -0.03540248307916853\n",
      "          vf_explained_var: -0.2151176929473877\n",
      "          vf_loss: 0.00137507768983293\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.864285714285714\n",
      "    ram_util_percent: 64.29285714285713\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041618575761592516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55518784014083\n",
      "    mean_inference_ms: 1.4165063705476155\n",
      "    mean_raw_obs_processing_ms: 0.6470603665196216\n",
      "  time_since_restore: 2257.4878313541412\n",
      "  time_this_iter_s: 9.247293710708618\n",
      "  time_total_s: 2257.4878313541412\n",
      "  timers:\n",
      "    learn_throughput: 1722.983\n",
      "    learn_time_ms: 580.389\n",
      "    load_throughput: 317651.639\n",
      "    load_time_ms: 3.148\n",
      "    sample_throughput: 112.292\n",
      "    sample_time_ms: 8905.353\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632127076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         2257.49</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 205\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9966341349813672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00860298653897419\n",
      "          policy_loss: 0.012926070474916035\n",
      "          total_loss: -0.005290281689829297\n",
      "          vf_explained_var: -0.8727582097053528\n",
      "          vf_loss: 0.0017499904442552683\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.099999999999994\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161673888616385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.552417602822409\n",
      "    mean_inference_ms: 1.416451433546388\n",
      "    mean_raw_obs_processing_ms: 0.6472051481250599\n",
      "  time_since_restore: 2266.755819797516\n",
      "  time_this_iter_s: 9.267988443374634\n",
      "  time_total_s: 2266.755819797516\n",
      "  timers:\n",
      "    learn_throughput: 1724.658\n",
      "    learn_time_ms: 579.825\n",
      "    load_throughput: 314469.811\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 112.135\n",
      "    sample_time_ms: 8917.794\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632127085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         2266.76</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 206\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0438517491022745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013057337814518307\n",
      "          policy_loss: -0.041487752232286665\n",
      "          total_loss: -0.06105372909870413\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008725404577691936\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.169230769230765\n",
      "    ram_util_percent: 64.33846153846153\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041614831967804786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.549592544441941\n",
      "    mean_inference_ms: 1.4163947250450364\n",
      "    mean_raw_obs_processing_ms: 0.6473717192984096\n",
      "  time_since_restore: 2275.945423603058\n",
      "  time_this_iter_s: 9.189603805541992\n",
      "  time_total_s: 2275.945423603058\n",
      "  timers:\n",
      "    learn_throughput: 1725.237\n",
      "    learn_time_ms: 579.63\n",
      "    load_throughput: 315475.059\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 112.059\n",
      "    sample_time_ms: 8923.855\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632127094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         2275.95</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 207\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7757176107830472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007606227185448431\n",
      "          policy_loss: -0.10941694660319222\n",
      "          total_loss: -0.12440366836057769\n",
      "          vf_explained_var: -0.8725439310073853\n",
      "          vf_loss: 0.0027704540433155164\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92857142857142\n",
      "    ram_util_percent: 64.42142857142856\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041612710735489486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.546664906328825\n",
      "    mean_inference_ms: 1.4163309534390347\n",
      "    mean_raw_obs_processing_ms: 0.6475572470655028\n",
      "  time_since_restore: 2285.5218589305878\n",
      "  time_this_iter_s: 9.576435327529907\n",
      "  time_total_s: 2285.5218589305878\n",
      "  timers:\n",
      "    learn_throughput: 1725.528\n",
      "    learn_time_ms: 579.533\n",
      "    load_throughput: 315567.627\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 111.202\n",
      "    sample_time_ms: 8992.681\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632127104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         2285.52</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 208\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.739012316862742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006077049121660928\n",
      "          policy_loss: -0.006134260528617435\n",
      "          total_loss: -0.01852975876794921\n",
      "          vf_explained_var: -0.5349135994911194\n",
      "          vf_loss: 0.004994624974723492\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04161048899567372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.543685456904283\n",
      "    mean_inference_ms: 1.4162645160571887\n",
      "    mean_raw_obs_processing_ms: 0.6477624022536524\n",
      "  time_since_restore: 2295.5187730789185\n",
      "  time_this_iter_s: 9.996914148330688\n",
      "  time_total_s: 2295.5187730789185\n",
      "  timers:\n",
      "    learn_throughput: 1724.025\n",
      "    learn_time_ms: 580.038\n",
      "    load_throughput: 314370.817\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 110.2\n",
      "    sample_time_ms: 9074.448\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632127114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         2295.52</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-38-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 209\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7524069918526544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008529864938479431\n",
      "          policy_loss: 0.005798413811458482\n",
      "          total_loss: -0.00826257032652696\n",
      "          vf_explained_var: -0.6780812740325928\n",
      "          vf_loss: 0.003463084936245448\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.42857142857142\n",
      "    ram_util_percent: 64.45714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160826456341627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.540700208974956\n",
      "    mean_inference_ms: 1.4161985642775696\n",
      "    mean_raw_obs_processing_ms: 0.6479868447299246\n",
      "  time_since_restore: 2305.571202278137\n",
      "  time_this_iter_s: 10.05242919921875\n",
      "  time_total_s: 2305.571202278137\n",
      "  timers:\n",
      "    learn_throughput: 1723.887\n",
      "    learn_time_ms: 580.084\n",
      "    load_throughput: 314107.136\n",
      "    load_time_ms: 3.184\n",
      "    sample_throughput: 109.114\n",
      "    sample_time_ms: 9164.741\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632127124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         2305.57</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-39-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 210\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7199339350064595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009458057210793076\n",
      "          policy_loss: -0.07043784126225446\n",
      "          total_loss: -0.0833537155141433\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.004283464141190052\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.80487804878049\n",
      "    ram_util_percent: 64.22439024390243\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041605982514374605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.537770755914934\n",
      "    mean_inference_ms: 1.4161299382638433\n",
      "    mean_raw_obs_processing_ms: 0.6490208273195018\n",
      "  time_since_restore: 2333.856413125992\n",
      "  time_this_iter_s: 28.285210847854614\n",
      "  time_total_s: 2333.856413125992\n",
      "  timers:\n",
      "    learn_throughput: 1721.807\n",
      "    learn_time_ms: 580.785\n",
      "    load_throughput: 220489.415\n",
      "    load_time_ms: 4.535\n",
      "    sample_throughput: 90.989\n",
      "    sample_time_ms: 10990.363\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632127152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         2333.86</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 211\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.86865410539839\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0095192220125447\n",
      "          policy_loss: -0.028390164093838797\n",
      "          total_loss: -0.04595774999923176\n",
      "          vf_explained_var: -0.7889145016670227\n",
      "          vf_loss: 0.0011189541333199789\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.292857142857144\n",
      "    ram_util_percent: 64.2785714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160365897864857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.53481619991686\n",
      "    mean_inference_ms: 1.4160608113234303\n",
      "    mean_raw_obs_processing_ms: 0.6500703885605453\n",
      "  time_since_restore: 2343.866833925247\n",
      "  time_this_iter_s: 10.010420799255371\n",
      "  time_total_s: 2343.866833925247\n",
      "  timers:\n",
      "    learn_throughput: 1720.722\n",
      "    learn_time_ms: 581.151\n",
      "    load_throughput: 215542.388\n",
      "    load_time_ms: 4.639\n",
      "    sample_throughput: 91.12\n",
      "    sample_time_ms: 10974.547\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632127162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         2343.87</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-39-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 212\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6827954702907137\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008147266709732016\n",
      "          policy_loss: 0.11255771422551739\n",
      "          total_loss: 0.09724707872503334\n",
      "          vf_explained_var: -0.28156396746635437\n",
      "          vf_loss: 0.001517318419387771\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87333333333333\n",
      "    ram_util_percent: 64.28666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04160125165910471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.531813253326742\n",
      "    mean_inference_ms: 1.4159881664312368\n",
      "    mean_raw_obs_processing_ms: 0.6511333878366908\n",
      "  time_since_restore: 2354.206191778183\n",
      "  time_this_iter_s: 10.339357852935791\n",
      "  time_total_s: 2354.206191778183\n",
      "  timers:\n",
      "    learn_throughput: 1720.605\n",
      "    learn_time_ms: 581.191\n",
      "    load_throughput: 215319.979\n",
      "    load_time_ms: 4.644\n",
      "    sample_throughput: 91.121\n",
      "    sample_time_ms: 10974.463\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632127172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         2354.21</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 213\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6208734936184352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00829815805707123\n",
      "          policy_loss: -0.009500197548833158\n",
      "          total_loss: -0.023068164185517364\n",
      "          vf_explained_var: -0.5166606903076172\n",
      "          vf_loss: 0.002640765393152833\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71333333333333\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159875795961925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.528773547545217\n",
      "    mean_inference_ms: 1.4159129952450404\n",
      "    mean_raw_obs_processing_ms: 0.6522099477482206\n",
      "  time_since_restore: 2364.6150345802307\n",
      "  time_this_iter_s: 10.40884280204773\n",
      "  time_total_s: 2364.6150345802307\n",
      "  timers:\n",
      "    learn_throughput: 1720.532\n",
      "    learn_time_ms: 581.216\n",
      "    load_throughput: 215339.878\n",
      "    load_time_ms: 4.644\n",
      "    sample_throughput: 90.525\n",
      "    sample_time_ms: 11046.721\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632127183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         2364.62</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 214\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4898710674709743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006202156311016359\n",
      "          policy_loss: -0.03298087923063172\n",
      "          total_loss: -0.045982972201373845\n",
      "          vf_explained_var: -0.8081682920455933\n",
      "          vf_loss: 0.0018966192605956976\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72\n",
      "    ram_util_percent: 64.26666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159623034024018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.525734323224766\n",
      "    mean_inference_ms: 1.4158365054042543\n",
      "    mean_raw_obs_processing_ms: 0.6532996712213738\n",
      "  time_since_restore: 2375.006193637848\n",
      "  time_this_iter_s: 10.391159057617188\n",
      "  time_total_s: 2375.006193637848\n",
      "  timers:\n",
      "    learn_throughput: 1722.333\n",
      "    learn_time_ms: 580.608\n",
      "    load_throughput: 215841.872\n",
      "    load_time_ms: 4.633\n",
      "    sample_throughput: 89.592\n",
      "    sample_time_ms: 11161.723\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632127193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         2375.01</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 215\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4176749295658535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008150875840293163\n",
      "          policy_loss: 0.047531225035587946\n",
      "          total_loss: 0.03428490294350518\n",
      "          vf_explained_var: -0.5455264449119568\n",
      "          vf_loss: 0.0009304258778380851\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03999999999999\n",
      "    ram_util_percent: 64.19333333333336\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159358979720025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.52268034118455\n",
      "    mean_inference_ms: 1.4157573645150439\n",
      "    mean_raw_obs_processing_ms: 0.6544012166164666\n",
      "  time_since_restore: 2385.5186319351196\n",
      "  time_this_iter_s: 10.512438297271729\n",
      "  time_total_s: 2385.5186319351196\n",
      "  timers:\n",
      "    learn_throughput: 1720.753\n",
      "    learn_time_ms: 581.141\n",
      "    load_throughput: 217275.294\n",
      "    load_time_ms: 4.602\n",
      "    sample_throughput: 88.608\n",
      "    sample_time_ms: 11285.64\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1632127204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         2385.52</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 216\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8524711079067655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01284171719509912\n",
      "          policy_loss: 0.007113679581218295\n",
      "          total_loss: -0.008810105754269494\n",
      "          vf_explained_var: -0.6507301330566406\n",
      "          vf_loss: 0.002600926709257894\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88571428571429\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04159092010622381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.519622336628299\n",
      "    mean_inference_ms: 1.4156778728321868\n",
      "    mean_raw_obs_processing_ms: 0.655515199148545\n",
      "  time_since_restore: 2395.8294723033905\n",
      "  time_this_iter_s: 10.310840368270874\n",
      "  time_total_s: 2395.8294723033905\n",
      "  timers:\n",
      "    learn_throughput: 1722.512\n",
      "    learn_time_ms: 580.547\n",
      "    load_throughput: 216921.326\n",
      "    load_time_ms: 4.61\n",
      "    sample_throughput: 87.732\n",
      "    sample_time_ms: 11398.34\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1632127214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         2395.83</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 217\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6527560008896722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008064838340319763\n",
      "          policy_loss: 0.011986394433511629\n",
      "          total_loss: -0.0027845049897829693\n",
      "          vf_explained_var: -0.6979973316192627\n",
      "          vf_loss: 0.0017566605923800833\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86\n",
      "    ram_util_percent: 64.02666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158818220353723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.516534070685147\n",
      "    mean_inference_ms: 1.4155950691101569\n",
      "    mean_raw_obs_processing_ms: 0.6566410054391385\n",
      "  time_since_restore: 2406.15154838562\n",
      "  time_this_iter_s: 10.322076082229614\n",
      "  time_total_s: 2406.15154838562\n",
      "  timers:\n",
      "    learn_throughput: 1719.663\n",
      "    learn_time_ms: 581.509\n",
      "    load_throughput: 216870.854\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 87.169\n",
      "    sample_time_ms: 11471.944\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1632127225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         2406.15</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-35\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 218\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.551818323135376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011265581409404115\n",
      "          policy_loss: 0.036117742334802944\n",
      "          total_loss: 0.021969865759213766\n",
      "          vf_explained_var: -0.7589706182479858\n",
      "          vf_loss: 0.0013703084492590278\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71333333333333\n",
      "    ram_util_percent: 63.99333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158532762615837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.513415518794897\n",
      "    mean_inference_ms: 1.415509136309046\n",
      "    mean_raw_obs_processing_ms: 0.657778436872675\n",
      "  time_since_restore: 2416.493914604187\n",
      "  time_this_iter_s: 10.342366218566895\n",
      "  time_total_s: 2416.493914604187\n",
      "  timers:\n",
      "    learn_throughput: 1719.092\n",
      "    learn_time_ms: 581.702\n",
      "    load_throughput: 217418.331\n",
      "    load_time_ms: 4.599\n",
      "    sample_throughput: 86.909\n",
      "    sample_time_ms: 11506.313\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632127235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         2416.49</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 994.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 219\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5283514936765035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007213799559780851\n",
      "          policy_loss: 0.028621351718902587\n",
      "          total_loss: 0.014414660301473406\n",
      "          vf_explained_var: -0.621243953704834\n",
      "          vf_loss: 0.0010768229458739775\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.20666666666666\n",
      "    ram_util_percent: 63.980000000000004\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04158231509100338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.510233258224652\n",
      "    mean_inference_ms: 1.4154172817065733\n",
      "    mean_raw_obs_processing_ms: 0.6589266222680688\n",
      "  time_since_restore: 2426.8562870025635\n",
      "  time_this_iter_s: 10.362372398376465\n",
      "  time_total_s: 2426.8562870025635\n",
      "  timers:\n",
      "    learn_throughput: 1718.266\n",
      "    learn_time_ms: 581.982\n",
      "    load_throughput: 217428.475\n",
      "    load_time_ms: 4.599\n",
      "    sample_throughput: 86.677\n",
      "    sample_time_ms: 11537.053\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1632127245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         2426.86</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            994.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 220\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.35741026699543e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7901013082928128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.043336969621260595\n",
      "          policy_loss: -0.08089988314443164\n",
      "          total_loss: -0.09497737818294101\n",
      "          vf_explained_var: 0.05714292451739311\n",
      "          vf_loss: 0.0038235192575181523\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.83333333333334\n",
      "    ram_util_percent: 63.986666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415791625684836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.507001343049557\n",
      "    mean_inference_ms: 1.4153196545239082\n",
      "    mean_raw_obs_processing_ms: 0.658644537994049\n",
      "  time_since_restore: 2437.3085837364197\n",
      "  time_this_iter_s: 10.452296733856201\n",
      "  time_total_s: 2437.3085837364197\n",
      "  timers:\n",
      "    learn_throughput: 1719.903\n",
      "    learn_time_ms: 581.428\n",
      "    load_throughput: 308852.087\n",
      "    load_time_ms: 3.238\n",
      "    sample_throughput: 102.505\n",
      "    sample_time_ms: 9755.639\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632127256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         2437.31</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 221\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.276676160759396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01016851611061769\n",
      "          policy_loss: 0.04005490725653039\n",
      "          total_loss: 0.018179547062350643\n",
      "          vf_explained_var: -0.639802873134613\n",
      "          vf_loss: 0.0008913997039295888\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.23076923076923\n",
      "    ram_util_percent: 63.907692307692294\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041576021243965394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.503553846239178\n",
      "    mean_inference_ms: 1.415220517133692\n",
      "    mean_raw_obs_processing_ms: 0.6583851761346214\n",
      "  time_since_restore: 2446.568681001663\n",
      "  time_this_iter_s: 9.26009726524353\n",
      "  time_total_s: 2446.568681001663\n",
      "  timers:\n",
      "    learn_throughput: 1713.125\n",
      "    learn_time_ms: 583.728\n",
      "    load_throughput: 316945.933\n",
      "    load_time_ms: 3.155\n",
      "    sample_throughput: 103.323\n",
      "    sample_time_ms: 9678.381\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632127265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         2446.57</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 222\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0157647172609967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010288753515158927\n",
      "          policy_loss: -0.00022315233945846558\n",
      "          total_loss: -0.01811146347059144\n",
      "          vf_explained_var: -0.8558865189552307\n",
      "          vf_loss: 0.0022693363690955773\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86428571428571\n",
      "    ram_util_percent: 63.91428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04157287594819918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.500051830887601\n",
      "    mean_inference_ms: 1.4151197211039368\n",
      "    mean_raw_obs_processing_ms: 0.658146731407118\n",
      "  time_since_restore: 2456.396857738495\n",
      "  time_this_iter_s: 9.828176736831665\n",
      "  time_total_s: 2456.396857738495\n",
      "  timers:\n",
      "    learn_throughput: 1714.5\n",
      "    learn_time_ms: 583.26\n",
      "    load_throughput: 316656.399\n",
      "    load_time_ms: 3.158\n",
      "    sample_throughput: 103.867\n",
      "    sample_time_ms: 9627.733\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632127275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">          2456.4</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 223\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2530080344941883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0108097874689506\n",
      "          policy_loss: 0.03349294281668133\n",
      "          total_loss: 0.01131325935324033\n",
      "          vf_explained_var: -0.24042575061321259\n",
      "          vf_loss: 0.00035039603119205116\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17692307692308\n",
      "    ram_util_percent: 63.93846153846153\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041569746971651826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.496495156604565\n",
      "    mean_inference_ms: 1.4150186110708993\n",
      "    mean_raw_obs_processing_ms: 0.6579302042616871\n",
      "  time_since_restore: 2465.374090909958\n",
      "  time_this_iter_s: 8.977233171463013\n",
      "  time_total_s: 2465.374090909958\n",
      "  timers:\n",
      "    learn_throughput: 1717.762\n",
      "    learn_time_ms: 582.153\n",
      "    load_throughput: 317596.317\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 105.422\n",
      "    sample_time_ms: 9485.71\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632127284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         2465.37</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 224\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.043202969763014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012194136847496\n",
      "          policy_loss: -0.021356350431839626\n",
      "          total_loss: -0.04028815080722173\n",
      "          vf_explained_var: -0.9671543836593628\n",
      "          vf_loss: 0.0015002321251409334\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.90714285714285\n",
      "    ram_util_percent: 63.964285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156661505419637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.49290562043414\n",
      "    mean_inference_ms: 1.4149164680566133\n",
      "    mean_raw_obs_processing_ms: 0.657735083573998\n",
      "  time_since_restore: 2475.161892414093\n",
      "  time_this_iter_s: 9.787801504135132\n",
      "  time_total_s: 2475.161892414093\n",
      "  timers:\n",
      "    learn_throughput: 1716.52\n",
      "    learn_time_ms: 582.574\n",
      "    load_throughput: 317022.592\n",
      "    load_time_ms: 3.154\n",
      "    sample_throughput: 106.101\n",
      "    sample_time_ms: 9424.965\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632127294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         2475.16</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 225\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2773196591271296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00992689444259543\n",
      "          policy_loss: -0.03281853770216306\n",
      "          total_loss: -0.0550518607099851\n",
      "          vf_explained_var: -0.36772027611732483\n",
      "          vf_loss: 0.0005398740374958935\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.169230769230765\n",
      "    ram_util_percent: 63.98461538461538\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041563486235352186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.489246810394558\n",
      "    mean_inference_ms: 1.414813819306086\n",
      "    mean_raw_obs_processing_ms: 0.6575595453973168\n",
      "  time_since_restore: 2484.009192943573\n",
      "  time_this_iter_s: 8.84730052947998\n",
      "  time_total_s: 2484.009192943573\n",
      "  timers:\n",
      "    learn_throughput: 1718.034\n",
      "    learn_time_ms: 582.061\n",
      "    load_throughput: 316424.675\n",
      "    load_time_ms: 3.16\n",
      "    sample_throughput: 108.004\n",
      "    sample_time_ms: 9258.94\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632127303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         2484.01</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 226\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9376076009538439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010310395248565543\n",
      "          policy_loss: -0.007476647881170114\n",
      "          total_loss: -0.025856616492900584\n",
      "          vf_explained_var: -0.5551285743713379\n",
      "          vf_loss: 0.0009961051982827485\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.12307692307692\n",
      "    ram_util_percent: 64.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04156040499371083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.485567958333563\n",
      "    mean_inference_ms: 1.4147120658862027\n",
      "    mean_raw_obs_processing_ms: 0.6574033659485323\n",
      "  time_since_restore: 2493.5167922973633\n",
      "  time_this_iter_s: 9.507599353790283\n",
      "  time_total_s: 2493.5167922973633\n",
      "  timers:\n",
      "    learn_throughput: 1717.418\n",
      "    learn_time_ms: 582.27\n",
      "    load_throughput: 317283.992\n",
      "    load_time_ms: 3.152\n",
      "    sample_throughput: 108.951\n",
      "    sample_time_ms: 9178.432\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632127312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         2493.52</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 227\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1817901876237658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010636047074657482\n",
      "          policy_loss: -0.01523798317131069\n",
      "          total_loss: -0.03594363973372512\n",
      "          vf_explained_var: -0.4592134654521942\n",
      "          vf_loss: 0.0011122453097211998\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.035714285714285\n",
      "    ram_util_percent: 64.04285714285716\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155737346962768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.481861179380228\n",
      "    mean_inference_ms: 1.4146110895953345\n",
      "    mean_raw_obs_processing_ms: 0.6572646430544546\n",
      "  time_since_restore: 2502.8163526058197\n",
      "  time_this_iter_s: 9.299560308456421\n",
      "  time_total_s: 2502.8163526058197\n",
      "  timers:\n",
      "    learn_throughput: 1718.299\n",
      "    learn_time_ms: 581.971\n",
      "    load_throughput: 317435.273\n",
      "    load_time_ms: 3.15\n",
      "    sample_throughput: 110.175\n",
      "    sample_time_ms: 9076.496\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632127321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         2502.82</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 228\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9410293089018928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013691920207669522\n",
      "          policy_loss: -0.04506187149220043\n",
      "          total_loss: -0.0634595814678404\n",
      "          vf_explained_var: -0.37869957089424133\n",
      "          vf_loss: 0.0010125837440783571\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.28461538461538\n",
      "    ram_util_percent: 64.0923076923077\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155439575771901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.478126116665893\n",
      "    mean_inference_ms: 1.414511142513402\n",
      "    mean_raw_obs_processing_ms: 0.6571448486958444\n",
      "  time_since_restore: 2512.333220243454\n",
      "  time_this_iter_s: 9.516867637634277\n",
      "  time_total_s: 2512.333220243454\n",
      "  timers:\n",
      "    learn_throughput: 1718.67\n",
      "    learn_time_ms: 581.845\n",
      "    load_throughput: 315774.321\n",
      "    load_time_ms: 3.167\n",
      "    sample_throughput: 111.185\n",
      "    sample_time_ms: 8994.044\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632127331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         2512.33</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 229\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.272096844514211\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012375246534125889\n",
      "          policy_loss: -0.025429291878309515\n",
      "          total_loss: -0.04783301163050863\n",
      "          vf_explained_var: -0.6569929122924805\n",
      "          vf_loss: 0.0003172502536067946\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26153846153845\n",
      "    ram_util_percent: 64.18461538461541\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04155147299682204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.474348841823458\n",
      "    mean_inference_ms: 1.4144121912132954\n",
      "    mean_raw_obs_processing_ms: 0.6570430835012802\n",
      "  time_since_restore: 2521.347395658493\n",
      "  time_this_iter_s: 9.014175415039062\n",
      "  time_total_s: 2521.347395658493\n",
      "  timers:\n",
      "    learn_throughput: 1721.673\n",
      "    learn_time_ms: 580.831\n",
      "    load_throughput: 315418.121\n",
      "    load_time_ms: 3.17\n",
      "    sample_throughput: 112.864\n",
      "    sample_time_ms: 8860.217\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632127340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         2521.35</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 230\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1718305985132855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008042288894903047\n",
      "          policy_loss: -0.01310912279619111\n",
      "          total_loss: -0.033598194354110295\n",
      "          vf_explained_var: -0.8047248721122742\n",
      "          vf_loss: 0.0012292357980590572\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.13076923076922\n",
      "    ram_util_percent: 64.23846153846154\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154860873345554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.470542404930875\n",
      "    mean_inference_ms: 1.4143144999744244\n",
      "    mean_raw_obs_processing_ms: 0.6569576040579014\n",
      "  time_since_restore: 2530.6700863838196\n",
      "  time_this_iter_s: 9.322690725326538\n",
      "  time_total_s: 2530.6700863838196\n",
      "  timers:\n",
      "    learn_throughput: 1722.798\n",
      "    learn_time_ms: 580.451\n",
      "    load_throughput: 315943.204\n",
      "    load_time_ms: 3.165\n",
      "    sample_throughput: 114.316\n",
      "    sample_time_ms: 8747.668\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632127349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         2530.67</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 231\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.248565453953213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01145060140454343\n",
      "          policy_loss: -0.04052614242666298\n",
      "          total_loss: -0.061933975997898315\n",
      "          vf_explained_var: -0.0205323938280344\n",
      "          vf_loss: 0.001077820095674219\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.535714285714285\n",
      "    ram_util_percent: 64.3142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154579073968363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.466698438859158\n",
      "    mean_inference_ms: 1.414217466759923\n",
      "    mean_raw_obs_processing_ms: 0.6568896343091584\n",
      "  time_since_restore: 2539.938009262085\n",
      "  time_this_iter_s: 9.26792287826538\n",
      "  time_total_s: 2539.938009262085\n",
      "  timers:\n",
      "    learn_throughput: 1730.458\n",
      "    learn_time_ms: 577.882\n",
      "    load_throughput: 319758.483\n",
      "    load_time_ms: 3.127\n",
      "    sample_throughput: 114.272\n",
      "    sample_time_ms: 8751.062\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632127359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         2539.94</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 232\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0988155047098798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012978029247075046\n",
      "          policy_loss: -0.0028877574536535474\n",
      "          total_loss: -0.022363094343907302\n",
      "          vf_explained_var: -0.30409228801727295\n",
      "          vf_loss: 0.0015128192155518467\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.56666666666668\n",
      "    ram_util_percent: 64.33999999999997\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04154303771371629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.462864893232505\n",
      "    mean_inference_ms: 1.4141218178608022\n",
      "    mean_raw_obs_processing_ms: 0.6568394939661394\n",
      "  time_since_restore: 2550.39835190773\n",
      "  time_this_iter_s: 10.460342645645142\n",
      "  time_total_s: 2550.39835190773\n",
      "  timers:\n",
      "    learn_throughput: 1729.51\n",
      "    learn_time_ms: 578.198\n",
      "    load_throughput: 320249.217\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 113.456\n",
      "    sample_time_ms: 8813.966\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632127369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">          2550.4</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 233\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0568087153964574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018956224026935774\n",
      "          policy_loss: -0.013946632461415396\n",
      "          total_loss: -0.03235093884997898\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0021637799554607936\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84\n",
      "    ram_util_percent: 64.42\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041540335611110785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.459050341837353\n",
      "    mean_inference_ms: 1.4140270653771532\n",
      "    mean_raw_obs_processing_ms: 0.6568055666171954\n",
      "  time_since_restore: 2560.9420158863068\n",
      "  time_this_iter_s: 10.54366397857666\n",
      "  time_total_s: 2560.9420158863068\n",
      "  timers:\n",
      "    learn_throughput: 1724.947\n",
      "    learn_time_ms: 579.728\n",
      "    load_throughput: 319909.693\n",
      "    load_time_ms: 3.126\n",
      "    sample_throughput: 111.495\n",
      "    sample_time_ms: 8969.049\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632127380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         2560.94</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 234\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.107780109511481\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011992258345314453\n",
      "          policy_loss: -0.008814310199684566\n",
      "          total_loss: -0.028243041535218557\n",
      "          vf_explained_var: -0.9847910404205322\n",
      "          vf_loss: 0.0016490675625391304\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87142857142857\n",
      "    ram_util_percent: 64.57142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041537686088071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.455252804325136\n",
      "    mean_inference_ms: 1.4139336335774062\n",
      "    mean_raw_obs_processing_ms: 0.6567886187687827\n",
      "  time_since_restore: 2571.0360016822815\n",
      "  time_this_iter_s: 10.093985795974731\n",
      "  time_total_s: 2571.0360016822815\n",
      "  timers:\n",
      "    learn_throughput: 1720.808\n",
      "    learn_time_ms: 581.123\n",
      "    load_throughput: 319768.234\n",
      "    load_time_ms: 3.127\n",
      "    sample_throughput: 111.133\n",
      "    sample_time_ms: 8998.265\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632127390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         2571.04</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 235\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9817204488648308\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013437970463033563\n",
      "          policy_loss: -0.15156706124544145\n",
      "          total_loss: -0.16940622842974132\n",
      "          vf_explained_var: -0.6857007145881653\n",
      "          vf_loss: 0.0019780359864752326\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.446666666666665\n",
      "    ram_util_percent: 64.51333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04153509756290158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.451484525308377\n",
      "    mean_inference_ms: 1.413841590298694\n",
      "    mean_raw_obs_processing_ms: 0.656787863044107\n",
      "  time_since_restore: 2581.5146429538727\n",
      "  time_this_iter_s: 10.478641271591187\n",
      "  time_total_s: 2581.5146429538727\n",
      "  timers:\n",
      "    learn_throughput: 1720.851\n",
      "    learn_time_ms: 581.108\n",
      "    load_throughput: 320802.79\n",
      "    load_time_ms: 3.117\n",
      "    sample_throughput: 109.153\n",
      "    sample_time_ms: 9161.45\n",
      "    update_time_ms: 1.625\n",
      "  timestamp: 1632127400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         2581.51</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 236\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6909074223703808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005450054605507611\n",
      "          policy_loss: -0.11543304233087434\n",
      "          total_loss: -0.12039515901770857\n",
      "          vf_explained_var: 0.07684057205915451\n",
      "          vf_loss: 0.001946956453482724\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.593333333333334\n",
      "    ram_util_percent: 64.55333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041532553194766626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.44775785223627\n",
      "    mean_inference_ms: 1.4137506359124332\n",
      "    mean_raw_obs_processing_ms: 0.6568033300927126\n",
      "  time_since_restore: 2592.22766661644\n",
      "  time_this_iter_s: 10.713023662567139\n",
      "  time_total_s: 2592.22766661644\n",
      "  timers:\n",
      "    learn_throughput: 1721.132\n",
      "    learn_time_ms: 581.013\n",
      "    load_throughput: 321181.101\n",
      "    load_time_ms: 3.114\n",
      "    sample_throughput: 107.734\n",
      "    sample_time_ms: 9282.097\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632127411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         2592.23</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 237\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9331088052855598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01408040941560093\n",
      "          policy_loss: -0.08577337997655074\n",
      "          total_loss: -0.10251738876104355\n",
      "          vf_explained_var: -0.25969669222831726\n",
      "          vf_loss: 0.0025870780269744704\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85000000000001\n",
      "    ram_util_percent: 64.58571428571429\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041530038425246385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.444031044510785\n",
      "    mean_inference_ms: 1.4136602432284702\n",
      "    mean_raw_obs_processing_ms: 0.6568345786686208\n",
      "  time_since_restore: 2601.8985896110535\n",
      "  time_this_iter_s: 9.670922994613647\n",
      "  time_total_s: 2601.8985896110535\n",
      "  timers:\n",
      "    learn_throughput: 1722.741\n",
      "    learn_time_ms: 580.47\n",
      "    load_throughput: 321629.348\n",
      "    load_time_ms: 3.109\n",
      "    sample_throughput: 107.299\n",
      "    sample_time_ms: 9319.787\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1632127421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          2601.9</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 238\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9474683178795709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010175479326364763\n",
      "          policy_loss: -0.004886252101924685\n",
      "          total_loss: -0.02222894210782316\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0021319950891969105\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75333333333334\n",
      "    ram_util_percent: 64.60000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041527575806800394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.440310728053742\n",
      "    mean_inference_ms: 1.4135707923770884\n",
      "    mean_raw_obs_processing_ms: 0.6568812331878804\n",
      "  time_since_restore: 2611.906147003174\n",
      "  time_this_iter_s: 10.007557392120361\n",
      "  time_total_s: 2611.906147003174\n",
      "  timers:\n",
      "    learn_throughput: 1722.606\n",
      "    learn_time_ms: 580.516\n",
      "    load_throughput: 323257.933\n",
      "    load_time_ms: 3.094\n",
      "    sample_throughput: 106.737\n",
      "    sample_time_ms: 9368.819\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632127431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         2611.91</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 240\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5361154004931466e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2247473471694523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02604151575871367\n",
      "          policy_loss: -0.10044373737441169\n",
      "          total_loss: -0.10419283757607142\n",
      "          vf_explained_var: 0.22659869492053986\n",
      "          vf_loss: 0.008498371808996631\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.87435897435898\n",
      "    ram_util_percent: 64.61282051282049\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041522791072628025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.4329502480505\n",
      "    mean_inference_ms: 1.4133953301226791\n",
      "    mean_raw_obs_processing_ms: 0.6584670050054183\n",
      "  time_since_restore: 2639.530372619629\n",
      "  time_this_iter_s: 27.624225616455078\n",
      "  time_total_s: 2639.530372619629\n",
      "  timers:\n",
      "    learn_throughput: 1718.101\n",
      "    learn_time_ms: 582.038\n",
      "    load_throughput: 217071.762\n",
      "    load_time_ms: 4.607\n",
      "    sample_throughput: 89.073\n",
      "    sample_time_ms: 11226.759\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632127458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         2639.53</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 241\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.304173100739716e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2603327406777276\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011750379562353983\n",
      "          policy_loss: -0.04903743742033839\n",
      "          total_loss: -0.07083116628022658\n",
      "          vf_explained_var: -0.5758861899375916\n",
      "          vf_loss: 0.00080959889920046\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.400000000000006\n",
      "    ram_util_percent: 64.40666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0415204727264011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.429319445949803\n",
      "    mean_inference_ms: 1.413308920355319\n",
      "    mean_raw_obs_processing_ms: 0.6592895865309102\n",
      "  time_since_restore: 2650.344045639038\n",
      "  time_this_iter_s: 10.81367301940918\n",
      "  time_total_s: 2650.344045639038\n",
      "  timers:\n",
      "    learn_throughput: 1715.182\n",
      "    learn_time_ms: 583.028\n",
      "    load_throughput: 216905.621\n",
      "    load_time_ms: 4.61\n",
      "    sample_throughput: 87.913\n",
      "    sample_time_ms: 11374.884\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632127469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         2650.34</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 242\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.304173100739716e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9019451459248861\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007055099535345417\n",
      "          policy_loss: 0.0689423634774155\n",
      "          total_loss: 0.06352142608828015\n",
      "          vf_explained_var: 0.11960338801145554\n",
      "          vf_loss: 0.003598514256171054\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.559999999999995\n",
      "    ram_util_percent: 64.39333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041518187475016635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.425695738139053\n",
      "    mean_inference_ms: 1.413223603694206\n",
      "    mean_raw_obs_processing_ms: 0.6601231250074071\n",
      "  time_since_restore: 2660.5490865707397\n",
      "  time_this_iter_s: 10.20504093170166\n",
      "  time_total_s: 2660.5490865707397\n",
      "  timers:\n",
      "    learn_throughput: 1713.887\n",
      "    learn_time_ms: 583.469\n",
      "    load_throughput: 216948.255\n",
      "    load_time_ms: 4.609\n",
      "    sample_throughput: 87.198\n",
      "    sample_time_ms: 11468.156\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632127480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         2660.55</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.02\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 243\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.304173100739716e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0265649603472815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01404587911385325\n",
      "          policy_loss: -0.022399708752830823\n",
      "          total_loss: 0.0035401361684004465\n",
      "          vf_explained_var: 0.029405275359749794\n",
      "          vf_loss: 0.036205493989917965\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.133333333333326\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041515866010515656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.422088043184397\n",
      "    mean_inference_ms: 1.413139133546365\n",
      "    mean_raw_obs_processing_ms: 0.6609676250448357\n",
      "  time_since_restore: 2670.845272064209\n",
      "  time_this_iter_s: 10.296185493469238\n",
      "  time_total_s: 2670.845272064209\n",
      "  timers:\n",
      "    learn_throughput: 1715.177\n",
      "    learn_time_ms: 583.03\n",
      "    load_throughput: 217107.718\n",
      "    load_time_ms: 4.606\n",
      "    sample_throughput: 87.32\n",
      "    sample_time_ms: 11452.156\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632127490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         2670.85</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">    0.02</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-45-23\n",
      "  done: false\n",
      "  episode_len_mean: 993.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 244\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.304173100739716e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3769606590270995\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02853776524557509\n",
      "          policy_loss: 0.14594898207320106\n",
      "          total_loss: 0.20952470401922862\n",
      "          vf_explained_var: -0.3451897203922272\n",
      "          vf_loss: 0.07734532931095196\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.643750000000004\n",
      "    ram_util_percent: 63.958333333333336\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151358148604736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.418489647433866\n",
      "    mean_inference_ms: 1.4130555186026559\n",
      "    mean_raw_obs_processing_ms: 0.662785237397945\n",
      "  time_since_restore: 2704.3979313373566\n",
      "  time_this_iter_s: 33.55265927314758\n",
      "  time_total_s: 2704.3979313373566\n",
      "  timers:\n",
      "    learn_throughput: 1717.496\n",
      "    learn_time_ms: 582.243\n",
      "    load_throughput: 164718.93\n",
      "    load_time_ms: 6.071\n",
      "    sample_throughput: 72.715\n",
      "    sample_time_ms: 13752.364\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1632127523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">          2704.4</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            993.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 993.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 245\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.956259651109576e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4335781123903062\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01767907067493061\n",
      "          policy_loss: 0.015790208594666587\n",
      "          total_loss: 0.026520576866136656\n",
      "          vf_explained_var: -0.29252174496650696\n",
      "          vf_loss: 0.025066150036743947\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.22941176470588\n",
      "    ram_util_percent: 64.30588235294117\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04151132273532222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.414978853659232\n",
      "    mean_inference_ms: 1.4129726765670785\n",
      "    mean_raw_obs_processing_ms: 0.6646093618543045\n",
      "  time_since_restore: 2716.44983792305\n",
      "  time_this_iter_s: 12.05190658569336\n",
      "  time_total_s: 2716.44983792305\n",
      "  timers:\n",
      "    learn_throughput: 1722.139\n",
      "    learn_time_ms: 580.673\n",
      "    load_throughput: 164194.686\n",
      "    load_time_ms: 6.09\n",
      "    sample_throughput: 71.686\n",
      "    sample_time_ms: 13949.709\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632127535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         2716.45</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            993.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 988.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 246\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.956259651109576e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6375966919793024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013193082553850388\n",
      "          policy_loss: -0.13412715399430858\n",
      "          total_loss: 0.06866510692569945\n",
      "          vf_explained_var: 0.2012467086315155\n",
      "          vf_loss: 0.21916822493076324\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.34375\n",
      "    ram_util_percent: 64.3078125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150911826015526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.411501496368048\n",
      "    mean_inference_ms: 1.4128913184111258\n",
      "    mean_raw_obs_processing_ms: 0.6678433636279482\n",
      "  time_since_restore: 2761.571828842163\n",
      "  time_this_iter_s: 45.12199091911316\n",
      "  time_total_s: 2761.571828842163\n",
      "  timers:\n",
      "    learn_throughput: 1719.739\n",
      "    learn_time_ms: 581.484\n",
      "    load_throughput: 131605.413\n",
      "    load_time_ms: 7.598\n",
      "    sample_throughput: 57.433\n",
      "    sample_time_ms: 17411.719\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1632127581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         2761.57</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 988.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 247\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.956259651109576e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9773715747727287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02108768091977274\n",
      "          policy_loss: 0.12334434621863895\n",
      "          total_loss: 0.10906514740652508\n",
      "          vf_explained_var: -0.2981175482273102\n",
      "          vf_loss: 0.005494525282928306\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.88823529411765\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150695556095936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.408098933485077\n",
      "    mean_inference_ms: 1.412810948447739\n",
      "    mean_raw_obs_processing_ms: 0.6710776624329692\n",
      "  time_since_restore: 2773.3567457199097\n",
      "  time_this_iter_s: 11.784916877746582\n",
      "  time_total_s: 2773.3567457199097\n",
      "  timers:\n",
      "    learn_throughput: 1718.069\n",
      "    learn_time_ms: 582.049\n",
      "    load_throughput: 131489.479\n",
      "    load_time_ms: 7.605\n",
      "    sample_throughput: 57.083\n",
      "    sample_time_ms: 17518.283\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1632127592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         2773.36</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-46-43\n",
      "  done: false\n",
      "  episode_len_mean: 988.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 248\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1934389476664367e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.663491419951121\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012805698994560435\n",
      "          policy_loss: -0.10184545053376091\n",
      "          total_loss: -0.05627917477654086\n",
      "          vf_explained_var: 0.2690364718437195\n",
      "          vf_loss: 0.06220119203854766\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.266666666666666\n",
      "    ram_util_percent: 64.1866666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150484332455401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.404715495065734\n",
      "    mean_inference_ms: 1.412731736247826\n",
      "    mean_raw_obs_processing_ms: 0.6743120305634513\n",
      "  time_since_restore: 2783.757529735565\n",
      "  time_this_iter_s: 10.400784015655518\n",
      "  time_total_s: 2783.757529735565\n",
      "  timers:\n",
      "    learn_throughput: 1716.709\n",
      "    learn_time_ms: 582.51\n",
      "    load_throughput: 131364.286\n",
      "    load_time_ms: 7.612\n",
      "    sample_throughput: 56.848\n",
      "    sample_time_ms: 17590.783\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632127603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         2783.76</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-46-52\n",
      "  done: false\n",
      "  episode_len_mean: 988.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 249\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1934389476664367e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5640294932656817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.029312027479855857\n",
      "          policy_loss: -0.03411877482301659\n",
      "          total_loss: -0.04069791759053866\n",
      "          vf_explained_var: -0.7553772926330566\n",
      "          vf_loss: 0.009061152741196565\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.1923076923077\n",
      "    ram_util_percent: 63.892307692307696\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150277843880134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.401307338144134\n",
      "    mean_inference_ms: 1.4126534184353465\n",
      "    mean_raw_obs_processing_ms: 0.6775464364846842\n",
      "  time_since_restore: 2793.070920228958\n",
      "  time_this_iter_s: 9.313390493392944\n",
      "  time_total_s: 2793.070920228958\n",
      "  timers:\n",
      "    learn_throughput: 1714.833\n",
      "    learn_time_ms: 583.147\n",
      "    load_throughput: 131372.926\n",
      "    load_time_ms: 7.612\n",
      "    sample_throughput: 57.075\n",
      "    sample_time_ms: 17520.684\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1632127612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         2793.07</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 250\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2733052915996974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013649687988103251\n",
      "          policy_loss: 0.02321485471394327\n",
      "          total_loss: 0.0037688830660449132\n",
      "          vf_explained_var: -0.6538790464401245\n",
      "          vf_loss: 0.003287084162972557\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.278571428571425\n",
      "    ram_util_percent: 63.42142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04150073981901652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.397912757269026\n",
      "    mean_inference_ms: 1.412575534741673\n",
      "    mean_raw_obs_processing_ms: 0.6795711173731047\n",
      "  time_since_restore: 2802.635393857956\n",
      "  time_this_iter_s: 9.564473628997803\n",
      "  time_total_s: 2802.635393857956\n",
      "  timers:\n",
      "    learn_throughput: 1717.173\n",
      "    learn_time_ms: 582.352\n",
      "    load_throughput: 164061.74\n",
      "    load_time_ms: 6.095\n",
      "    sample_throughput: 63.625\n",
      "    sample_time_ms: 15717.038\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1632127622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         2802.64</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 251\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4012120167414346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013340791401880934\n",
      "          policy_loss: 0.018806481226864787\n",
      "          total_loss: -0.0038687469230757818\n",
      "          vf_explained_var: -0.2620822787284851\n",
      "          vf_loss: 0.0013368904313311861\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.38333333333333\n",
      "    ram_util_percent: 63.025\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149872325382249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.394490111652983\n",
      "    mean_inference_ms: 1.4124978410103923\n",
      "    mean_raw_obs_processing_ms: 0.6816031623621179\n",
      "  time_since_restore: 2811.220551967621\n",
      "  time_this_iter_s: 8.585158109664917\n",
      "  time_total_s: 2811.220551967621\n",
      "  timers:\n",
      "    learn_throughput: 1719.67\n",
      "    learn_time_ms: 581.507\n",
      "    load_throughput: 164046.34\n",
      "    load_time_ms: 6.096\n",
      "    sample_throughput: 64.537\n",
      "    sample_time_ms: 15495.01\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1632127630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         2811.22</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-19\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 252\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.371502078904046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011470526822001325\n",
      "          policy_loss: 0.02950486164126131\n",
      "          total_loss: 0.006639244821336534\n",
      "          vf_explained_var: -0.845244824886322\n",
      "          vf_loss: 0.0008494051651117237\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.11538461538461\n",
      "    ram_util_percent: 62.83076923076921\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04149660866702295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.391023699739872\n",
      "    mean_inference_ms: 1.4124167522710636\n",
      "    mean_raw_obs_processing_ms: 0.6836420732856487\n",
      "  time_since_restore: 2819.8632497787476\n",
      "  time_this_iter_s: 8.642697811126709\n",
      "  time_total_s: 2819.8632497787476\n",
      "  timers:\n",
      "    learn_throughput: 1721.856\n",
      "    learn_time_ms: 580.769\n",
      "    load_throughput: 164133.645\n",
      "    load_time_ms: 6.093\n",
      "    sample_throughput: 65.191\n",
      "    sample_time_ms: 15339.506\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632127639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         2819.86</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 253\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4022570927937825\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011198402979900221\n",
      "          policy_loss: 0.036891376972198485\n",
      "          total_loss: 0.01352921658092075\n",
      "          vf_explained_var: -0.6992425322532654\n",
      "          vf_loss: 0.0006604085731345954\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.708333333333336\n",
      "    ram_util_percent: 62.783333333333324\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414941727478452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.387475409377807\n",
      "    mean_inference_ms: 1.4123286275572091\n",
      "    mean_raw_obs_processing_ms: 0.6856878950266896\n",
      "  time_since_restore: 2828.5383722782135\n",
      "  time_this_iter_s: 8.675122499465942\n",
      "  time_total_s: 2828.5383722782135\n",
      "  timers:\n",
      "    learn_throughput: 1720.706\n",
      "    learn_time_ms: 581.157\n",
      "    load_throughput: 164102.821\n",
      "    load_time_ms: 6.094\n",
      "    sample_throughput: 65.889\n",
      "    sample_time_ms: 15177.044\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632127648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         2828.54</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 254\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4226600938373144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010361993852837594\n",
      "          policy_loss: 0.012375348971949683\n",
      "          total_loss: -0.011336577435334524\n",
      "          vf_explained_var: -0.22357600927352905\n",
      "          vf_loss: 0.0005146734536336024\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.28461538461538\n",
      "    ram_util_percent: 62.79230769230767\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041491741445741005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.383909141987424\n",
      "    mean_inference_ms: 1.412240553549091\n",
      "    mean_raw_obs_processing_ms: 0.6877403435897728\n",
      "  time_since_restore: 2837.1915674209595\n",
      "  time_this_iter_s: 8.653195142745972\n",
      "  time_total_s: 2837.1915674209595\n",
      "  timers:\n",
      "    learn_throughput: 1721.381\n",
      "    learn_time_ms: 580.929\n",
      "    load_throughput: 215934.102\n",
      "    load_time_ms: 4.631\n",
      "    sample_throughput: 78.809\n",
      "    sample_time_ms: 12688.827\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632127656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         2837.19</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 255\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5016263537936743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009493969702185644\n",
      "          policy_loss: -0.009025418841176563\n",
      "          total_loss: -0.03355965912342072\n",
      "          vf_explained_var: -0.9498552083969116\n",
      "          vf_loss: 0.0004820267473809913\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.50833333333335\n",
      "    ram_util_percent: 62.79166666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414893416659865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.380331892518283\n",
      "    mean_inference_ms: 1.4121531398485556\n",
      "    mean_raw_obs_processing_ms: 0.6897994256583841\n",
      "  time_since_restore: 2845.8280813694\n",
      "  time_this_iter_s: 8.636513948440552\n",
      "  time_total_s: 2845.8280813694\n",
      "  timers:\n",
      "    learn_throughput: 1721.528\n",
      "    learn_time_ms: 580.879\n",
      "    load_throughput: 216877.582\n",
      "    load_time_ms: 4.611\n",
      "    sample_throughput: 80.989\n",
      "    sample_time_ms: 12347.352\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632127665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         2845.83</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 256\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.476514373885261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013117079169570268\n",
      "          policy_loss: 0.006207507890131738\n",
      "          total_loss: -0.01785386653823985\n",
      "          vf_explained_var: -0.42324298620224\n",
      "          vf_loss: 0.0007037716386548709\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.333333333333336\n",
      "    ram_util_percent: 62.816666666666656\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041486988471335626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.376725155680278\n",
      "    mean_inference_ms: 1.412065959521101\n",
      "    mean_raw_obs_processing_ms: 0.6918651974836806\n",
      "  time_since_restore: 2854.496244907379\n",
      "  time_this_iter_s: 8.668163537979126\n",
      "  time_total_s: 2854.496244907379\n",
      "  timers:\n",
      "    learn_throughput: 1723.515\n",
      "    learn_time_ms: 580.209\n",
      "    load_throughput: 321767.522\n",
      "    load_time_ms: 3.108\n",
      "    sample_throughput: 114.888\n",
      "    sample_time_ms: 8704.157\n",
      "    update_time_ms: 1.622\n",
      "  timestamp: 1632127674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">          2854.5</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-02\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 257\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.545686059527927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00916759736339464\n",
      "          policy_loss: -0.006138137893544303\n",
      "          total_loss: -0.031242879976828893\n",
      "          vf_explained_var: -0.9865363836288452\n",
      "          vf_loss: 0.0003521169318547537\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.315384615384616\n",
      "    ram_util_percent: 62.89230769230768\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04148466807237287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.373095793480879\n",
      "    mean_inference_ms: 1.411979516300793\n",
      "    mean_raw_obs_processing_ms: 0.6939370107257217\n",
      "  time_since_restore: 2863.102025985718\n",
      "  time_this_iter_s: 8.605781078338623\n",
      "  time_total_s: 2863.102025985718\n",
      "  timers:\n",
      "    learn_throughput: 1722.61\n",
      "    learn_time_ms: 580.514\n",
      "    load_throughput: 315517.776\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 119.248\n",
      "    sample_time_ms: 8385.907\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632127682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">          2863.1</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-11\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 258\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.57900570763482\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010323200185181215\n",
      "          policy_loss: 0.004179228842258453\n",
      "          total_loss: -0.02099077320761151\n",
      "          vf_explained_var: -0.8319833278656006\n",
      "          vf_loss: 0.000620053486474919\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.475\n",
      "    ram_util_percent: 62.94166666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04148236503776181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.369439155275348\n",
      "    mean_inference_ms: 1.4118928944369782\n",
      "    mean_raw_obs_processing_ms: 0.6960147732591301\n",
      "  time_since_restore: 2871.8260765075684\n",
      "  time_this_iter_s: 8.724050521850586\n",
      "  time_total_s: 2871.8260765075684\n",
      "  timers:\n",
      "    learn_throughput: 1723.73\n",
      "    learn_time_ms: 580.137\n",
      "    load_throughput: 316095.591\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 121.675\n",
      "    sample_time_ms: 8218.6\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632127691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         2871.83</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 259\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5351978222529095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008653074561832398\n",
      "          policy_loss: -0.005164514978726705\n",
      "          total_loss: -0.02983258060283131\n",
      "          vf_explained_var: -0.5605766773223877\n",
      "          vf_loss: 0.0006839136837192604\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03846153846155\n",
      "    ram_util_percent: 63.00769230769231\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04148008873756688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.365791771400206\n",
      "    mean_inference_ms: 1.4118071833567956\n",
      "    mean_raw_obs_processing_ms: 0.6980984506538758\n",
      "  time_since_restore: 2880.774322986603\n",
      "  time_this_iter_s: 8.948246479034424\n",
      "  time_total_s: 2880.774322986603\n",
      "  timers:\n",
      "    learn_throughput: 1727.23\n",
      "    learn_time_ms: 578.962\n",
      "    load_throughput: 315950.343\n",
      "    load_time_ms: 3.165\n",
      "    sample_throughput: 122.199\n",
      "    sample_time_ms: 8183.347\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632127700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         2880.77</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 260\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5656768321990966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073712006362508\n",
      "          policy_loss: -0.013522982845703762\n",
      "          total_loss: -0.03862222383419673\n",
      "          vf_explained_var: -0.715560257434845\n",
      "          vf_loss: 0.0005575298351686797\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26923076923077\n",
      "    ram_util_percent: 63.0846153846154\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147783351925348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.362122910679492\n",
      "    mean_inference_ms: 1.4117222345299565\n",
      "    mean_raw_obs_processing_ms: 0.7001868840301766\n",
      "  time_since_restore: 2889.655866622925\n",
      "  time_this_iter_s: 8.881543636322021\n",
      "  time_total_s: 2889.655866622925\n",
      "  timers:\n",
      "    learn_throughput: 1727.226\n",
      "    learn_time_ms: 578.963\n",
      "    load_throughput: 314989.373\n",
      "    load_time_ms: 3.175\n",
      "    sample_throughput: 123.229\n",
      "    sample_time_ms: 8115.005\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632127709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         2889.66</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 261\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.790158421499654e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5950882964664035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003458307062311958\n",
      "          policy_loss: -0.022004890359110303\n",
      "          total_loss: -0.047575382391611735\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003803900155617157\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.275\n",
      "    ram_util_percent: 63.116666666666674\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147561038020023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.358447141165115\n",
      "    mean_inference_ms: 1.4116381631399697\n",
      "    mean_raw_obs_processing_ms: 0.7022806251309441\n",
      "  time_since_restore: 2898.4808666706085\n",
      "  time_this_iter_s: 8.825000047683716\n",
      "  time_total_s: 2898.4808666706085\n",
      "  timers:\n",
      "    learn_throughput: 1724.85\n",
      "    learn_time_ms: 579.76\n",
      "    load_throughput: 313700.712\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 122.878\n",
      "    sample_time_ms: 8138.176\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632127718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         2898.48</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 262\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.569267990854051\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008409666927043504\n",
      "          policy_loss: 0.03658289853483439\n",
      "          total_loss: 0.011425421738790142\n",
      "          vf_explained_var: -0.9977015852928162\n",
      "          vf_loss: 0.0005352074314537782\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.184615384615384\n",
      "    ram_util_percent: 63.19230769230771\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04147341460036788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.35476732071684\n",
      "    mean_inference_ms: 1.4115544595505576\n",
      "    mean_raw_obs_processing_ms: 0.7043796973450293\n",
      "  time_since_restore: 2907.35981297493\n",
      "  time_this_iter_s: 8.878946304321289\n",
      "  time_total_s: 2907.35981297493\n",
      "  timers:\n",
      "    learn_throughput: 1723.351\n",
      "    learn_time_ms: 580.265\n",
      "    load_throughput: 312064.581\n",
      "    load_time_ms: 3.204\n",
      "    sample_throughput: 122.529\n",
      "    sample_time_ms: 8161.31\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632127727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         2907.36</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-48-56\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 263\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5128849267959597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010090617284055152\n",
      "          policy_loss: -0.011992115692959892\n",
      "          total_loss: -0.036706556090050274\n",
      "          vf_explained_var: -0.8910685181617737\n",
      "          vf_loss: 0.0004144091646594461\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10769230769232\n",
      "    ram_util_percent: 63.25384615384614\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041471236443194476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.351075394733384\n",
      "    mean_inference_ms: 1.4114710948135367\n",
      "    mean_raw_obs_processing_ms: 0.7064833480014294\n",
      "  time_since_restore: 2916.313835144043\n",
      "  time_this_iter_s: 8.95402216911316\n",
      "  time_total_s: 2916.313835144043\n",
      "  timers:\n",
      "    learn_throughput: 1723.149\n",
      "    learn_time_ms: 580.333\n",
      "    load_throughput: 310935.63\n",
      "    load_time_ms: 3.216\n",
      "    sample_throughput: 122.113\n",
      "    sample_time_ms: 8189.127\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632127736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         2916.31</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 264\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.505356211132473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01126405727096148\n",
      "          policy_loss: 0.003148343786597252\n",
      "          total_loss: -0.021651703988512357\n",
      "          vf_explained_var: 0.003527984954416752\n",
      "          vf_loss: 0.0002535111821291341\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.85\n",
      "    ram_util_percent: 63.35833333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04146906619589605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.347362772521915\n",
      "    mean_inference_ms: 1.4113879243091976\n",
      "    mean_raw_obs_processing_ms: 0.708591336937736\n",
      "  time_since_restore: 2925.1231865882874\n",
      "  time_this_iter_s: 8.809351444244385\n",
      "  time_total_s: 2925.1231865882874\n",
      "  timers:\n",
      "    learn_throughput: 1721.847\n",
      "    learn_time_ms: 580.772\n",
      "    load_throughput: 311166.307\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 121.887\n",
      "    sample_time_ms: 8204.321\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632127745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         2925.12</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 265\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5383474614885118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011363145877560162\n",
      "          policy_loss: -0.015119804804109864\n",
      "          total_loss: -0.03982894791083203\n",
      "          vf_explained_var: -0.9035295248031616\n",
      "          vf_loss: 0.0006743323497681154\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.223076923076924\n",
      "    ram_util_percent: 63.4076923076923\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041466923370672995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.343615659915244\n",
      "    mean_inference_ms: 1.411305121657519\n",
      "    mean_raw_obs_processing_ms: 0.7107040325594126\n",
      "  time_since_restore: 2933.8502962589264\n",
      "  time_this_iter_s: 8.727109670639038\n",
      "  time_total_s: 2933.8502962589264\n",
      "  timers:\n",
      "    learn_throughput: 1720.665\n",
      "    learn_time_ms: 581.171\n",
      "    load_throughput: 310507.481\n",
      "    load_time_ms: 3.221\n",
      "    sample_throughput: 121.758\n",
      "    sample_time_ms: 8212.982\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632127753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         2933.85</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 266\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4547805786132812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007696310543571173\n",
      "          policy_loss: -0.052429661403099695\n",
      "          total_loss: -0.07651546866529518\n",
      "          vf_explained_var: -0.9054166078567505\n",
      "          vf_loss: 0.00046199906579810583\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35833333333333\n",
      "    ram_util_percent: 63.46666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04146480308415641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.339851864833086\n",
      "    mean_inference_ms: 1.4112228642637004\n",
      "    mean_raw_obs_processing_ms: 0.7128207513334504\n",
      "  time_since_restore: 2942.641856431961\n",
      "  time_this_iter_s: 8.791560173034668\n",
      "  time_total_s: 2942.641856431961\n",
      "  timers:\n",
      "    learn_throughput: 1719.861\n",
      "    learn_time_ms: 581.442\n",
      "    load_throughput: 310257.123\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 121.58\n",
      "    sample_time_ms: 8225.022\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632127762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         2942.64</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 267\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5033901664945812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01144971664084693\n",
      "          policy_loss: -0.038045931938621734\n",
      "          total_loss: -0.06255570161673758\n",
      "          vf_explained_var: -0.9719310402870178\n",
      "          vf_loss: 0.0005241341449921795\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.276923076923076\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04146271120795289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.336056885044352\n",
      "    mean_inference_ms: 1.4111411980246455\n",
      "    mean_raw_obs_processing_ms: 0.7149414017253694\n",
      "  time_since_restore: 2951.291335582733\n",
      "  time_this_iter_s: 8.649479150772095\n",
      "  time_total_s: 2951.291335582733\n",
      "  timers:\n",
      "    learn_throughput: 1721.833\n",
      "    learn_time_ms: 580.776\n",
      "    load_throughput: 316790.332\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 121.506\n",
      "    sample_time_ms: 8230.062\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632127771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         2951.29</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 268\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.288235815366109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006955607540415482\n",
      "          policy_loss: -0.03473443382730087\n",
      "          total_loss: -0.056039095752769046\n",
      "          vf_explained_var: -0.872223973274231\n",
      "          vf_loss: 0.0015776952562898967\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.23846153846153\n",
      "    ram_util_percent: 63.52307692307692\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041460636708269806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.332242624408757\n",
      "    mean_inference_ms: 1.4110598035665978\n",
      "    mean_raw_obs_processing_ms: 0.7170655872344347\n",
      "  time_since_restore: 2960.365454673767\n",
      "  time_this_iter_s: 9.074119091033936\n",
      "  time_total_s: 2960.365454673767\n",
      "  timers:\n",
      "    learn_throughput: 1721.046\n",
      "    learn_time_ms: 581.042\n",
      "    load_throughput: 315515.402\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 120.995\n",
      "    sample_time_ms: 8264.823\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632127780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         2960.37</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 269\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.489614258872138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01026463640270955\n",
      "          policy_loss: -0.039276812598109244\n",
      "          total_loss: -0.06365241048236688\n",
      "          vf_explained_var: -0.9996179342269897\n",
      "          vf_loss: 0.0005205451499528459\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.53333333333334\n",
      "    ram_util_percent: 63.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145857857484754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.328399639461566\n",
      "    mean_inference_ms: 1.4109786058364597\n",
      "    mean_raw_obs_processing_ms: 0.719192842523378\n",
      "  time_since_restore: 2969.161548614502\n",
      "  time_this_iter_s: 8.796093940734863\n",
      "  time_total_s: 2969.161548614502\n",
      "  timers:\n",
      "    learn_throughput: 1719.012\n",
      "    learn_time_ms: 581.729\n",
      "    load_throughput: 315624.619\n",
      "    load_time_ms: 3.168\n",
      "    sample_throughput: 121.228\n",
      "    sample_time_ms: 8248.921\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632127789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         2969.16</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-49-58\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 270\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4398334397210015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009267284950534426\n",
      "          policy_loss: -0.04380449144066208\n",
      "          total_loss: -0.06756750889536407\n",
      "          vf_explained_var: -0.9102381467819214\n",
      "          vf_loss: 0.000635316569565071\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25384615384615\n",
      "    ram_util_percent: 63.5846153846154\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145655675449939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.324547751965797\n",
      "    mean_inference_ms: 1.4108980269955498\n",
      "    mean_raw_obs_processing_ms: 0.7213237810700526\n",
      "  time_since_restore: 2978.248471260071\n",
      "  time_this_iter_s: 9.086922645568848\n",
      "  time_total_s: 2978.248471260071\n",
      "  timers:\n",
      "    learn_throughput: 1720.797\n",
      "    learn_time_ms: 581.126\n",
      "    load_throughput: 315363.574\n",
      "    load_time_ms: 3.171\n",
      "    sample_throughput: 120.918\n",
      "    sample_time_ms: 8270.091\n",
      "    update_time_ms: 1.621\n",
      "  timestamp: 1632127798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         2978.25</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-50-07\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 271\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4067115677727595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011135861013576331\n",
      "          policy_loss: -0.04857858991664317\n",
      "          total_loss: -0.0712165433085627\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0014291573088687276\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.22857142857144\n",
      "    ram_util_percent: 63.62857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145456177918799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.320702989076189\n",
      "    mean_inference_ms: 1.4108179662060047\n",
      "    mean_raw_obs_processing_ms: 0.7234573184617993\n",
      "  time_since_restore: 2987.6148822307587\n",
      "  time_this_iter_s: 9.366410970687866\n",
      "  time_total_s: 2987.6148822307587\n",
      "  timers:\n",
      "    learn_throughput: 1721.945\n",
      "    learn_time_ms: 580.739\n",
      "    load_throughput: 316043.191\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 120.126\n",
      "    sample_time_ms: 8324.612\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1632127807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         2987.61</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 272\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4015033774905734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012568986606382953\n",
      "          policy_loss: -0.024713923575149642\n",
      "          total_loss: -0.047548557010789715\n",
      "          vf_explained_var: -0.31914734840393066\n",
      "          vf_loss: 0.0011803984162624046\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.19230769230769\n",
      "    ram_util_percent: 63.6846153846154\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414525899314601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.316863381651617\n",
      "    mean_inference_ms: 1.4107384868525852\n",
      "    mean_raw_obs_processing_ms: 0.725593976849597\n",
      "  time_since_restore: 2997.1299393177032\n",
      "  time_this_iter_s: 9.51505708694458\n",
      "  time_total_s: 2997.1299393177032\n",
      "  timers:\n",
      "    learn_throughput: 1722.869\n",
      "    learn_time_ms: 580.427\n",
      "    load_throughput: 316021.76\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 119.211\n",
      "    sample_time_ms: 8388.513\n",
      "    update_time_ms: 1.621\n",
      "  timestamp: 1632127817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         2997.13</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 273\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4724652237362332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011467343303082187\n",
      "          policy_loss: -0.021688125220437844\n",
      "          total_loss: -0.04585779913597637\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005549781826428241\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.285714285714285\n",
      "    ram_util_percent: 63.7642857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04145063394857482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.313013690246905\n",
      "    mean_inference_ms: 1.410659250537644\n",
      "    mean_raw_obs_processing_ms: 0.7277347922586214\n",
      "  time_since_restore: 3006.383645296097\n",
      "  time_this_iter_s: 9.253705978393555\n",
      "  time_total_s: 3006.383645296097\n",
      "  timers:\n",
      "    learn_throughput: 1722.327\n",
      "    learn_time_ms: 580.61\n",
      "    load_throughput: 316740.094\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 118.789\n",
      "    sample_time_ms: 8418.289\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632127826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         3006.38</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-50-35\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 274\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4438891145918102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0083797400299289\n",
      "          policy_loss: -0.00839735513759984\n",
      "          total_loss: -0.03237607147958543\n",
      "          vf_explained_var: -0.8108252882957458\n",
      "          vf_loss: 0.00046017364652976134\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.307692307692314\n",
      "    ram_util_percent: 63.89230769230768\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04144870553535217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.309151139848389\n",
      "    mean_inference_ms: 1.4105805850952375\n",
      "    mean_raw_obs_processing_ms: 0.7298791464663174\n",
      "  time_since_restore: 3015.4716629981995\n",
      "  time_this_iter_s: 9.088017702102661\n",
      "  time_total_s: 3015.4716629981995\n",
      "  timers:\n",
      "    learn_throughput: 1722.138\n",
      "    learn_time_ms: 580.674\n",
      "    load_throughput: 316264.817\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 118.398\n",
      "    sample_time_ms: 8446.076\n",
      "    update_time_ms: 1.621\n",
      "  timestamp: 1632127835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         3015.47</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 989.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 275\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.379258394241333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013735404813741229\n",
      "          policy_loss: -0.060682261983553566\n",
      "          total_loss: -0.08390228417184618\n",
      "          vf_explained_var: -0.8417788743972778\n",
      "          vf_loss: 0.0005725599860953581\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.130769230769225\n",
      "    ram_util_percent: 63.91538461538461\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041446798168991716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.305289225328101\n",
      "    mean_inference_ms: 1.410502536589083\n",
      "    mean_raw_obs_processing_ms: 0.7320269014162157\n",
      "  time_since_restore: 3024.777662754059\n",
      "  time_this_iter_s: 9.305999755859375\n",
      "  time_total_s: 3024.777662754059\n",
      "  timers:\n",
      "    learn_throughput: 1723.22\n",
      "    learn_time_ms: 580.309\n",
      "    load_throughput: 315883.717\n",
      "    load_time_ms: 3.166\n",
      "    sample_throughput: 117.587\n",
      "    sample_time_ms: 8504.308\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632127845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         3024.78</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 276\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4243655602137246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013180816994280904\n",
      "          policy_loss: -0.042856953417261444\n",
      "          total_loss: -0.06669919614990552\n",
      "          vf_explained_var: -0.9810364842414856\n",
      "          vf_loss: 0.00040140939058296173\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.902702702702705\n",
      "    ram_util_percent: 63.47837837837839\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04144491299383199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.301420262556114\n",
      "    mean_inference_ms: 1.4104248380783182\n",
      "    mean_raw_obs_processing_ms: 0.7348029172007018\n",
      "  time_since_restore: 3051.0682055950165\n",
      "  time_this_iter_s: 26.29054284095764\n",
      "  time_total_s: 3051.0682055950165\n",
      "  timers:\n",
      "    learn_throughput: 1723.986\n",
      "    learn_time_ms: 580.051\n",
      "    load_throughput: 214520.458\n",
      "    load_time_ms: 4.662\n",
      "    sample_throughput: 97.533\n",
      "    sample_time_ms: 10252.968\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632127871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         3051.07</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 277\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.389793615871006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007846828824718738\n",
      "          policy_loss: 0.0069372158290611375\n",
      "          total_loss: -0.01651802584528923\n",
      "          vf_explained_var: -0.7634245753288269\n",
      "          vf_loss: 0.00044269308960388624\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.231249999999996\n",
      "    ram_util_percent: 64.2625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041443066061094484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.297613535742318\n",
      "    mean_inference_ms: 1.4103479151284355\n",
      "    mean_raw_obs_processing_ms: 0.7375796378168894\n",
      "  time_since_restore: 3062.1750144958496\n",
      "  time_this_iter_s: 11.10680890083313\n",
      "  time_total_s: 3062.1750144958496\n",
      "  timers:\n",
      "    learn_throughput: 1722.471\n",
      "    learn_time_ms: 580.561\n",
      "    load_throughput: 214229.005\n",
      "    load_time_ms: 4.668\n",
      "    sample_throughput: 95.255\n",
      "    sample_time_ms: 10498.184\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632127882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         3062.18</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-31\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 278\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.394805113474528\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011357976945811761\n",
      "          policy_loss: -0.021073949937191274\n",
      "          total_loss: -0.04431611365742154\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007058830794347968\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.57692307692309\n",
      "    ram_util_percent: 64.49230769230769\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04144125320236208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.293789970290147\n",
      "    mean_inference_ms: 1.4102719088991202\n",
      "    mean_raw_obs_processing_ms: 0.7403564692333598\n",
      "  time_since_restore: 3071.273223876953\n",
      "  time_this_iter_s: 9.098209381103516\n",
      "  time_total_s: 3071.273223876953\n",
      "  timers:\n",
      "    learn_throughput: 1721.307\n",
      "    learn_time_ms: 580.954\n",
      "    load_throughput: 214371.346\n",
      "    load_time_ms: 4.665\n",
      "    sample_throughput: 95.236\n",
      "    sample_time_ms: 10500.199\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632127891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         3071.27</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 279\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.441463494300842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013196323496470362\n",
      "          policy_loss: -0.03249064853621854\n",
      "          total_loss: -0.05649081841111183\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004144657933567133\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17142857142857\n",
      "    ram_util_percent: 64.47857142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414393985278683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.289945699575556\n",
      "    mean_inference_ms: 1.4101942684457227\n",
      "    mean_raw_obs_processing_ms: 0.7431330601222945\n",
      "  time_since_restore: 3080.4757328033447\n",
      "  time_this_iter_s: 9.202508926391602\n",
      "  time_total_s: 3080.4757328033447\n",
      "  timers:\n",
      "    learn_throughput: 1722.361\n",
      "    learn_time_ms: 580.598\n",
      "    load_throughput: 214283.729\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 94.866\n",
      "    sample_time_ms: 10541.14\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632127900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         3080.48</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-50\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 280\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3820389959547255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00961921489728979\n",
      "          policy_loss: -0.0252036412143045\n",
      "          total_loss: -0.04832779344999128\n",
      "          vf_explained_var: -0.8184988498687744\n",
      "          vf_loss: 0.0006962378431732456\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.24615384615385\n",
      "    ram_util_percent: 64.39230769230768\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041437544669254395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.286133149242174\n",
      "    mean_inference_ms: 1.410116416182518\n",
      "    mean_raw_obs_processing_ms: 0.7449648583012454\n",
      "  time_since_restore: 3089.985604286194\n",
      "  time_this_iter_s: 9.509871482849121\n",
      "  time_total_s: 3089.985604286194\n",
      "  timers:\n",
      "    learn_throughput: 1720.737\n",
      "    learn_time_ms: 581.146\n",
      "    load_throughput: 214445.876\n",
      "    load_time_ms: 4.663\n",
      "    sample_throughput: 94.492\n",
      "    sample_time_ms: 10582.917\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632127910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         3089.99</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 281\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.375773859024048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01246724081133325\n",
      "          policy_loss: -0.018945489823818207\n",
      "          total_loss: -0.04109142331613435\n",
      "          vf_explained_var: -0.49919426441192627\n",
      "          vf_loss: 0.0016118064244639956\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85\n",
      "    ram_util_percent: 64.25714285714288\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041435687259925934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.282259101159632\n",
      "    mean_inference_ms: 1.4100389195107719\n",
      "    mean_raw_obs_processing_ms: 0.7468018564435164\n",
      "  time_since_restore: 3099.488133907318\n",
      "  time_this_iter_s: 9.502529621124268\n",
      "  time_total_s: 3099.488133907318\n",
      "  timers:\n",
      "    learn_throughput: 1722.014\n",
      "    learn_time_ms: 580.715\n",
      "    load_throughput: 214611.563\n",
      "    load_time_ms: 4.66\n",
      "    sample_throughput: 94.366\n",
      "    sample_time_ms: 10596.989\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632127919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         3099.49</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-09\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 282\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3522069374720256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008684300793959841\n",
      "          policy_loss: -0.007122672783831756\n",
      "          total_loss: -0.029692257485455936\n",
      "          vf_explained_var: -0.9571011066436768\n",
      "          vf_loss: 0.0009524821328038039\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.23846153846153\n",
      "    ram_util_percent: 64.10769230769232\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143384299674355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.278405815582408\n",
      "    mean_inference_ms: 1.4099617322267353\n",
      "    mean_raw_obs_processing_ms: 0.7486436698873561\n",
      "  time_since_restore: 3109.0156211853027\n",
      "  time_this_iter_s: 9.52748727798462\n",
      "  time_total_s: 3109.0156211853027\n",
      "  timers:\n",
      "    learn_throughput: 1721.359\n",
      "    learn_time_ms: 580.936\n",
      "    load_throughput: 215284.613\n",
      "    load_time_ms: 4.645\n",
      "    sample_throughput: 94.357\n",
      "    sample_time_ms: 10598.033\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632127929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         3109.02</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-18\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 283\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3680029736624824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015012349820034071\n",
      "          policy_loss: 0.00826812916331821\n",
      "          total_loss: -0.014295448569787874\n",
      "          vf_explained_var: -0.6485196352005005\n",
      "          vf_loss: 0.0011164500055504808\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.971428571428575\n",
      "    ram_util_percent: 63.99285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143200980819147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.274576514987727\n",
      "    mean_inference_ms: 1.4098850690807507\n",
      "    mean_raw_obs_processing_ms: 0.7504900752460771\n",
      "  time_since_restore: 3118.367216348648\n",
      "  time_this_iter_s: 9.351595163345337\n",
      "  time_total_s: 3118.367216348648\n",
      "  timers:\n",
      "    learn_throughput: 1721.8\n",
      "    learn_time_ms: 580.788\n",
      "    load_throughput: 214248.703\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 94.269\n",
      "    sample_time_ms: 10607.934\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632127938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         3118.37</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 284\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.324253545867072\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011510727758047344\n",
      "          policy_loss: -0.042411960164705914\n",
      "          total_loss: -0.06473205466237333\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009224398200684744\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.871428571428574\n",
      "    ram_util_percent: 63.921428571428564\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04143019275114726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.2707832757281\n",
      "    mean_inference_ms: 1.4098092065245487\n",
      "    mean_raw_obs_processing_ms: 0.7523409659692138\n",
      "  time_since_restore: 3128.0423650741577\n",
      "  time_this_iter_s: 9.675148725509644\n",
      "  time_total_s: 3128.0423650741577\n",
      "  timers:\n",
      "    learn_throughput: 1720.469\n",
      "    learn_time_ms: 581.237\n",
      "    load_throughput: 214587.407\n",
      "    load_time_ms: 4.66\n",
      "    sample_throughput: 93.754\n",
      "    sample_time_ms: 10666.205\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632127948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         3128.04</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-38\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 285\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3313339286380343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010267656344616407\n",
      "          policy_loss: -0.02704360294673178\n",
      "          total_loss: -0.04962539548675219\n",
      "          vf_explained_var: -0.9915992617607117\n",
      "          vf_loss: 0.0007315458306240746\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.90000000000001\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142840054355026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.26703294515971\n",
      "    mean_inference_ms: 1.4097344543687624\n",
      "    mean_raw_obs_processing_ms: 0.7541961727012884\n",
      "  time_since_restore: 3137.6561682224274\n",
      "  time_this_iter_s: 9.613803148269653\n",
      "  time_total_s: 3137.6561682224274\n",
      "  timers:\n",
      "    learn_throughput: 1719.074\n",
      "    learn_time_ms: 581.709\n",
      "    load_throughput: 215170.857\n",
      "    load_time_ms: 4.647\n",
      "    sample_throughput: 93.488\n",
      "    sample_time_ms: 10696.53\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632127958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         3137.66</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-47\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 286\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3104964097340903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010360600897727343\n",
      "          policy_loss: -0.00651000551879406\n",
      "          total_loss: -0.02888220945994059\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007327606384125021\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99285714285714\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041426612743372136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.263335041007725\n",
      "    mean_inference_ms: 1.4096604745950552\n",
      "    mean_raw_obs_processing_ms: 0.7560555417066468\n",
      "  time_since_restore: 3147.2119965553284\n",
      "  time_this_iter_s: 9.555828332901001\n",
      "  time_total_s: 3147.2119965553284\n",
      "  timers:\n",
      "    learn_throughput: 1719.217\n",
      "    learn_time_ms: 581.66\n",
      "    load_throughput: 318111.794\n",
      "    load_time_ms: 3.144\n",
      "    sample_throughput: 110.808\n",
      "    sample_time_ms: 9024.631\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632127967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         3147.21</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 287\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3343385457992554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011350833382286889\n",
      "          policy_loss: -0.017053806864553028\n",
      "          total_loss: -0.03965656090941694\n",
      "          vf_explained_var: -0.8305292725563049\n",
      "          vf_loss: 0.0007406311719225616\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.114285714285714\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041424840035635534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.259695204554834\n",
      "    mean_inference_ms: 1.40958747449286\n",
      "    mean_raw_obs_processing_ms: 0.7579185846551374\n",
      "  time_since_restore: 3156.8526821136475\n",
      "  time_this_iter_s: 9.640685558319092\n",
      "  time_total_s: 3156.8526821136475\n",
      "  timers:\n",
      "    learn_throughput: 1720.912\n",
      "    learn_time_ms: 581.087\n",
      "    load_throughput: 318926.95\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 112.63\n",
      "    sample_time_ms: 8878.628\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632127977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         3156.85</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 288\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2909776634640164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01691257912777616\n",
      "          policy_loss: 0.0024274758994579314\n",
      "          total_loss: -0.019734515912002988\n",
      "          vf_explained_var: -0.8561962842941284\n",
      "          vf_loss: 0.0007477839131348042\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.971428571428575\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142308376035847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.256126195777332\n",
      "    mean_inference_ms: 1.4095155525506515\n",
      "    mean_raw_obs_processing_ms: 0.7597855156408493\n",
      "  time_since_restore: 3166.48237156868\n",
      "  time_this_iter_s: 9.629689455032349\n",
      "  time_total_s: 3166.48237156868\n",
      "  timers:\n",
      "    learn_throughput: 1721.748\n",
      "    learn_time_ms: 580.805\n",
      "    load_throughput: 319359.196\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 111.956\n",
      "    sample_time_ms: 8932.052\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632127987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         3166.48</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 289\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.356933681170146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013607771904375622\n",
      "          policy_loss: -0.011292488076206711\n",
      "          total_loss: -0.03373530196646849\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011265223355925022\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.11538461538461\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04142134811527321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.252635277020161\n",
      "    mean_inference_ms: 1.4094447856118049\n",
      "    mean_raw_obs_processing_ms: 0.7616562297199927\n",
      "  time_since_restore: 3176.1949820518494\n",
      "  time_this_iter_s: 9.712610483169556\n",
      "  time_total_s: 3176.1949820518494\n",
      "  timers:\n",
      "    learn_throughput: 1721.567\n",
      "    learn_time_ms: 580.866\n",
      "    load_throughput: 319334.881\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 111.321\n",
      "    sample_time_ms: 8983.029\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632127996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         3176.19</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 290\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1581618110338847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011891925113392255\n",
      "          policy_loss: -0.02463793522781796\n",
      "          total_loss: -0.044849153152770466\n",
      "          vf_explained_var: -0.9280676245689392\n",
      "          vf_loss: 0.0013704000933406253\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70666666666667\n",
      "    ram_util_percent: 63.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041419598044429656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.249210525532726\n",
      "    mean_inference_ms: 1.4093740743534975\n",
      "    mean_raw_obs_processing_ms: 0.7635305474010952\n",
      "  time_since_restore: 3186.1197533607483\n",
      "  time_this_iter_s: 9.924771308898926\n",
      "  time_total_s: 3186.1197533607483\n",
      "  timers:\n",
      "    learn_throughput: 1721.06\n",
      "    learn_time_ms: 581.037\n",
      "    load_throughput: 320195.432\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 110.812\n",
      "    sample_time_ms: 9024.334\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632128006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         3186.12</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 291\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1029404679934185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015746687919883425\n",
      "          policy_loss: -0.03193108526368936\n",
      "          total_loss: -0.05174879026081827\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0012116990343201905\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94285714285716\n",
      "    ram_util_percent: 63.964285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041417871766082665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.245855810825258\n",
      "    mean_inference_ms: 1.4093044323927275\n",
      "    mean_raw_obs_processing_ms: 0.7654083416888391\n",
      "  time_since_restore: 3196.100490808487\n",
      "  time_this_iter_s: 9.980737447738647\n",
      "  time_total_s: 3196.100490808487\n",
      "  timers:\n",
      "    learn_throughput: 1718.723\n",
      "    learn_time_ms: 581.827\n",
      "    load_throughput: 319249.81\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 110.237\n",
      "    sample_time_ms: 9071.34\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632128016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">          3196.1</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 292\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1527323034074572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009008502349817881\n",
      "          policy_loss: -0.055260285569561855\n",
      "          total_loss: -0.07546349014672968\n",
      "          vf_explained_var: -0.20174843072891235\n",
      "          vf_loss: 0.0013241143961850968\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.05000000000001\n",
      "    ram_util_percent: 64.03571428571429\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414161362908483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.242565571444871\n",
      "    mean_inference_ms: 1.4092353393561232\n",
      "    mean_raw_obs_processing_ms: 0.7672894114697678\n",
      "  time_since_restore: 3206.233707666397\n",
      "  time_this_iter_s: 10.133216857910156\n",
      "  time_total_s: 3206.233707666397\n",
      "  timers:\n",
      "    learn_throughput: 1716.894\n",
      "    learn_time_ms: 582.447\n",
      "    load_throughput: 319140.498\n",
      "    load_time_ms: 3.133\n",
      "    sample_throughput: 109.514\n",
      "    sample_time_ms: 9131.27\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632128026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         3206.23</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 293\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2632333199183146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014458344182126718\n",
      "          policy_loss: -0.026834586076438426\n",
      "          total_loss: -0.04862603356854783\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008408841554127219\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.68666666666666\n",
      "    ram_util_percent: 64.02666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141441087724946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.23934739340067\n",
      "    mean_inference_ms: 1.4091670851712477\n",
      "    mean_raw_obs_processing_ms: 0.7691740007996052\n",
      "  time_since_restore: 3216.186650276184\n",
      "  time_this_iter_s: 9.952942609786987\n",
      "  time_total_s: 3216.186650276184\n",
      "  timers:\n",
      "    learn_throughput: 1717.781\n",
      "    learn_time_ms: 582.146\n",
      "    load_throughput: 321112.251\n",
      "    load_time_ms: 3.114\n",
      "    sample_throughput: 108.793\n",
      "    sample_time_ms: 9191.749\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632128036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         3216.19</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 294\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2323268996344674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01413868073046483\n",
      "          policy_loss: -0.015069796558883455\n",
      "          total_loss: -0.036065067764785554\n",
      "          vf_explained_var: -0.9930346012115479\n",
      "          vf_loss: 0.0013279940008134063\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.942857142857136\n",
      "    ram_util_percent: 64.09285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04141270020943922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.236201236984943\n",
      "    mean_inference_ms: 1.4090997163664205\n",
      "    mean_raw_obs_processing_ms: 0.7710616804615067\n",
      "  time_since_restore: 3226.3551342487335\n",
      "  time_this_iter_s: 10.168483972549438\n",
      "  time_total_s: 3226.3551342487335\n",
      "  timers:\n",
      "    learn_throughput: 1719.498\n",
      "    learn_time_ms: 581.565\n",
      "    load_throughput: 320283.454\n",
      "    load_time_ms: 3.122\n",
      "    sample_throughput: 108.206\n",
      "    sample_time_ms: 9241.658\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632128047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         3226.36</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 295\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2354129791259765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011268117600415826\n",
      "          policy_loss: -0.01809670709901386\n",
      "          total_loss: -0.03977206870913506\n",
      "          vf_explained_var: -0.8475673198699951\n",
      "          vf_loss: 0.0006787663770309235\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.83571428571429\n",
      "    ram_util_percent: 64.14285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041411002257570155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.233117025861635\n",
      "    mean_inference_ms: 1.4090335786549328\n",
      "    mean_raw_obs_processing_ms: 0.7729523234092884\n",
      "  time_since_restore: 3236.301011323929\n",
      "  time_this_iter_s: 9.945877075195312\n",
      "  time_total_s: 3236.301011323929\n",
      "  timers:\n",
      "    learn_throughput: 1720.901\n",
      "    learn_time_ms: 581.091\n",
      "    load_throughput: 320095.243\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 107.813\n",
      "    sample_time_ms: 9275.325\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632128057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">          3236.3</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 296\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.182122270266215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009806161757987139\n",
      "          policy_loss: 0.014514822285208437\n",
      "          total_loss: -0.006204095213777489\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011023044599116677\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.693333333333335\n",
      "    ram_util_percent: 64.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140932425552131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.230097439224453\n",
      "    mean_inference_ms: 1.4089682014532452\n",
      "    mean_raw_obs_processing_ms: 0.774845876178463\n",
      "  time_since_restore: 3246.308695793152\n",
      "  time_this_iter_s: 10.007684469223022\n",
      "  time_total_s: 3246.308695793152\n",
      "  timers:\n",
      "    learn_throughput: 1719.887\n",
      "    learn_time_ms: 581.434\n",
      "    load_throughput: 320109.901\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 107.294\n",
      "    sample_time_ms: 9320.146\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632128067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         3246.31</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 297\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.098398049672445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014606004186640062\n",
      "          policy_loss: -0.005391879503925641\n",
      "          total_loss: -0.025152156295047867\n",
      "          vf_explained_var: -0.6692209243774414\n",
      "          vf_loss: 0.0012237035417961629\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.157142857142844\n",
      "    ram_util_percent: 64.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140763564539591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.227151119356455\n",
      "    mean_inference_ms: 1.4089036871450857\n",
      "    mean_raw_obs_processing_ms: 0.776742187335637\n",
      "  time_since_restore: 3256.2224056720734\n",
      "  time_this_iter_s: 9.913709878921509\n",
      "  time_total_s: 3256.2224056720734\n",
      "  timers:\n",
      "    learn_throughput: 1718.657\n",
      "    learn_time_ms: 581.85\n",
      "    load_throughput: 320239.437\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 106.986\n",
      "    sample_time_ms: 9347.051\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632128077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         3256.22</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 298\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.002440477742089\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01073700930420082\n",
      "          policy_loss: -0.030078403527537982\n",
      "          total_loss: -0.04831727362341351\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017855318787042052\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.24666666666667\n",
      "    ram_util_percent: 64.29333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140595188644692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.224274389691006\n",
      "    mean_inference_ms: 1.408840086470575\n",
      "    mean_raw_obs_processing_ms: 0.7786412087632425\n",
      "  time_since_restore: 3266.497560739517\n",
      "  time_this_iter_s: 10.275155067443848\n",
      "  time_total_s: 3266.497560739517\n",
      "  timers:\n",
      "    learn_throughput: 1720.28\n",
      "    learn_time_ms: 581.301\n",
      "    load_throughput: 320457.195\n",
      "    load_time_ms: 3.121\n",
      "    sample_throughput: 106.246\n",
      "    sample_time_ms: 9412.146\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632128087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">          3266.5</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-54-57\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 299\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9397928661770292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011174495830275294\n",
      "          policy_loss: -0.056919901818037036\n",
      "          total_loss: -0.07454494097166592\n",
      "          vf_explained_var: -0.7125994563102722\n",
      "          vf_loss: 0.001772889095850082\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.800000000000004\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140427334360262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.221471201596417\n",
      "    mean_inference_ms: 1.4087773152116783\n",
      "    mean_raw_obs_processing_ms: 0.7805426379695808\n",
      "  time_since_restore: 3276.8067140579224\n",
      "  time_this_iter_s: 10.309153318405151\n",
      "  time_total_s: 3276.8067140579224\n",
      "  timers:\n",
      "    learn_throughput: 1719.635\n",
      "    learn_time_ms: 581.519\n",
      "    load_throughput: 320611.518\n",
      "    load_time_ms: 3.119\n",
      "    sample_throughput: 105.579\n",
      "    sample_time_ms: 9471.577\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632128097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         3276.81</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 300\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.102639881769816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009247507444929168\n",
      "          policy_loss: -0.05681966236895985\n",
      "          total_loss: -0.07514926981594827\n",
      "          vf_explained_var: -0.48798954486846924\n",
      "          vf_loss: 0.0026967923098709435\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.733333333333334\n",
      "    ram_util_percent: 64.35999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0414026064686574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.218693449524125\n",
      "    mean_inference_ms: 1.4087152637477818\n",
      "    mean_raw_obs_processing_ms: 0.7824465051376288\n",
      "  time_since_restore: 3287.041332244873\n",
      "  time_this_iter_s: 10.234618186950684\n",
      "  time_total_s: 3287.041332244873\n",
      "  timers:\n",
      "    learn_throughput: 1719.801\n",
      "    learn_time_ms: 581.463\n",
      "    load_throughput: 320254.108\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 105.234\n",
      "    sample_time_ms: 9502.603\n",
      "    update_time_ms: 1.624\n",
      "  timestamp: 1632128107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         3287.04</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-18\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 301\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0985743602116904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013381520267875166\n",
      "          policy_loss: -0.013504946024881469\n",
      "          total_loss: -0.03207725433425771\n",
      "          vf_explained_var: -0.6296753287315369\n",
      "          vf_loss: 0.002413432545856469\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88666666666667\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04140095324308943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.215935136777794\n",
      "    mean_inference_ms: 1.4086538816794916\n",
      "    mean_raw_obs_processing_ms: 0.7843527442724664\n",
      "  time_since_restore: 3297.330821990967\n",
      "  time_this_iter_s: 10.28948974609375\n",
      "  time_total_s: 3297.330821990967\n",
      "  timers:\n",
      "    learn_throughput: 1722.008\n",
      "    learn_time_ms: 580.718\n",
      "    load_throughput: 321639.214\n",
      "    load_time_ms: 3.109\n",
      "    sample_throughput: 104.885\n",
      "    sample_time_ms: 9534.243\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1632128118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         3297.33</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-28\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 302\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1433238082461887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014248006860677146\n",
      "          policy_loss: -0.021517375194364124\n",
      "          total_loss: -0.04095552522275183\n",
      "          vf_explained_var: -0.6097330451011658\n",
      "          vf_loss: 0.001995087103730637\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.957142857142856\n",
      "    ram_util_percent: 64.41428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041399317631150276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.213180301602808\n",
      "    mean_inference_ms: 1.4085929143855496\n",
      "    mean_raw_obs_processing_ms: 0.786261191225838\n",
      "  time_since_restore: 3307.4143164157867\n",
      "  time_this_iter_s: 10.083494424819946\n",
      "  time_total_s: 3307.4143164157867\n",
      "  timers:\n",
      "    learn_throughput: 1722.24\n",
      "    learn_time_ms: 580.639\n",
      "    load_throughput: 321373.054\n",
      "    load_time_ms: 3.112\n",
      "    sample_throughput: 104.939\n",
      "    sample_time_ms: 9529.33\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632128128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         3307.41</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 303\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.204092110527886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011599038636277273\n",
      "          policy_loss: 0.005565102977885141\n",
      "          total_loss: -0.015368674157394303\n",
      "          vf_explained_var: -0.5321621298789978\n",
      "          vf_loss: 0.0011071437478272451\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 64.46428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04139770220889877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.210455935321754\n",
      "    mean_inference_ms: 1.4085326215096874\n",
      "    mean_raw_obs_processing_ms: 0.788172183355456\n",
      "  time_since_restore: 3317.3503041267395\n",
      "  time_this_iter_s: 9.935987710952759\n",
      "  time_total_s: 3317.3503041267395\n",
      "  timers:\n",
      "    learn_throughput: 1721.587\n",
      "    learn_time_ms: 580.859\n",
      "    load_throughput: 321814.429\n",
      "    load_time_ms: 3.107\n",
      "    sample_throughput: 104.96\n",
      "    sample_time_ms: 9527.417\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632128138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         3317.35</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-48\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 304\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.232060522503323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015532330189856757\n",
      "          policy_loss: 0.00504051542116536\n",
      "          total_loss: -0.016604626551270485\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000675463135768142\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.653333333333336\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04139610270748814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.207787985447364\n",
      "    mean_inference_ms: 1.4084732158489286\n",
      "    mean_raw_obs_processing_ms: 0.7900855063248875\n",
      "  time_since_restore: 3327.4369161129\n",
      "  time_this_iter_s: 10.086611986160278\n",
      "  time_total_s: 3327.4369161129\n",
      "  timers:\n",
      "    learn_throughput: 1721.612\n",
      "    learn_time_ms: 580.851\n",
      "    load_throughput: 320508.62\n",
      "    load_time_ms: 3.12\n",
      "    sample_throughput: 105.051\n",
      "    sample_time_ms: 9519.179\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632128148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         3327.44</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 989.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 305\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.249314702881707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013337176037753433\n",
      "          policy_loss: -0.11275712876684135\n",
      "          total_loss: -0.13467513442867332\n",
      "          vf_explained_var: -0.8719691634178162\n",
      "          vf_loss: 0.0005751420299121593\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.971428571428575\n",
      "    ram_util_percent: 64.52142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041394530028691576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.205168023083827\n",
      "    mean_inference_ms: 1.408414668981145\n",
      "    mean_raw_obs_processing_ms: 0.7920010806582966\n",
      "  time_since_restore: 3337.3212633132935\n",
      "  time_this_iter_s: 9.884347200393677\n",
      "  time_total_s: 3337.3212633132935\n",
      "  timers:\n",
      "    learn_throughput: 1721.054\n",
      "    learn_time_ms: 581.039\n",
      "    load_throughput: 320161.214\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 105.121\n",
      "    sample_time_ms: 9512.837\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632128158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         3337.32</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             989.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-56-25\n",
      "  done: false\n",
      "  episode_len_mean: 988.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 306\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.95079210749827e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.026560029718611\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020882477861461052\n",
      "          policy_loss: -0.02204453961716758\n",
      "          total_loss: -0.04071380934781498\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0015963306378883621\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.343589743589746\n",
      "    ram_util_percent: 64.55897435897437\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041392974552730416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.202613327944157\n",
      "    mean_inference_ms: 1.4083572657349834\n",
      "    mean_raw_obs_processing_ms: 0.7944762048298911\n",
      "  time_since_restore: 3364.6532196998596\n",
      "  time_this_iter_s: 27.331956386566162\n",
      "  time_total_s: 3364.6532196998596\n",
      "  timers:\n",
      "    learn_throughput: 1720.294\n",
      "    learn_time_ms: 581.296\n",
      "    load_throughput: 215391.852\n",
      "    load_time_ms: 4.643\n",
      "    sample_throughput: 88.94\n",
      "    sample_time_ms: 11243.484\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632128185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         3364.65</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-56-35\n",
      "  done: false\n",
      "  episode_len_mean: 988.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 307\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.062704602877299\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007512419996558448\n",
      "          policy_loss: -0.06508183860116534\n",
      "          total_loss: -0.08458720975452\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011216745149188986\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.40714285714285\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04139142641527144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.20009790625412\n",
      "    mean_inference_ms: 1.408300662491918\n",
      "    mean_raw_obs_processing_ms: 0.7969512765904411\n",
      "  time_since_restore: 3374.748497247696\n",
      "  time_this_iter_s: 10.095277547836304\n",
      "  time_total_s: 3374.748497247696\n",
      "  timers:\n",
      "    learn_throughput: 1721.405\n",
      "    learn_time_ms: 580.921\n",
      "    load_throughput: 215290.138\n",
      "    load_time_ms: 4.645\n",
      "    sample_throughput: 88.794\n",
      "    sample_time_ms: 11262.043\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632128195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         3374.75</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 988.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 308\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2280228005515204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01633597888107576\n",
      "          policy_loss: -0.03590422154714664\n",
      "          total_loss: -0.05721833058115509\n",
      "          vf_explained_var: -0.9997650384902954\n",
      "          vf_loss: 0.0009661203466950813\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.72142857142858\n",
      "    ram_util_percent: 64.31428571428569\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413899032470915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.19758561617531\n",
      "    mean_inference_ms: 1.4082447901318673\n",
      "    mean_raw_obs_processing_ms: 0.7994262185834902\n",
      "  time_since_restore: 3384.3590545654297\n",
      "  time_this_iter_s: 9.610557317733765\n",
      "  time_total_s: 3384.3590545654297\n",
      "  timers:\n",
      "    learn_throughput: 1720.51\n",
      "    learn_time_ms: 581.223\n",
      "    load_throughput: 214715.934\n",
      "    load_time_ms: 4.657\n",
      "    sample_throughput: 89.324\n",
      "    sample_time_ms: 11195.261\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632128205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         3384.36</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 988.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 309\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0457485874493915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01547536355771339\n",
      "          policy_loss: -0.09048487684792943\n",
      "          total_loss: -0.10775295462873247\n",
      "          vf_explained_var: -0.11402934044599533\n",
      "          vf_loss: 0.0031894069371951952\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.91333333333332\n",
      "    ram_util_percent: 64.32666666666664\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041388391415317985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.195094922836118\n",
      "    mean_inference_ms: 1.4081885862399557\n",
      "    mean_raw_obs_processing_ms: 0.8019009745807277\n",
      "  time_since_restore: 3394.5975375175476\n",
      "  time_this_iter_s: 10.23848295211792\n",
      "  time_total_s: 3394.5975375175476\n",
      "  timers:\n",
      "    learn_throughput: 1719.499\n",
      "    learn_time_ms: 581.565\n",
      "    load_throughput: 214531.431\n",
      "    load_time_ms: 4.661\n",
      "    sample_throughput: 89.383\n",
      "    sample_time_ms: 11187.855\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632128215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">          3394.6</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-06\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 310\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0913800345526803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011223826575213794\n",
      "          policy_loss: -0.07493143880532847\n",
      "          total_loss: -0.09472754407260153\n",
      "          vf_explained_var: -0.6622482538223267\n",
      "          vf_loss: 0.001117692325108995\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.933333333333344\n",
      "    ram_util_percent: 64.34666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04138689893402117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.192555694283817\n",
      "    mean_inference_ms: 1.4081330312074516\n",
      "    mean_raw_obs_processing_ms: 0.8035845302872514\n",
      "  time_since_restore: 3405.06064081192\n",
      "  time_this_iter_s: 10.463103294372559\n",
      "  time_total_s: 3405.06064081192\n",
      "  timers:\n",
      "    learn_throughput: 1719.853\n",
      "    learn_time_ms: 581.445\n",
      "    load_throughput: 214288.108\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 89.199\n",
      "    sample_time_ms: 11210.841\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632128226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         3405.06</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-16\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 311\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0436989068984985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011914767027591585\n",
      "          policy_loss: -0.04801127033101188\n",
      "          total_loss: -0.0673986677494314\n",
      "          vf_explained_var: -0.8374130129814148\n",
      "          vf_loss: 0.0010495891128407997\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.58666666666667\n",
      "    ram_util_percent: 64.33999999999997\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041385412475960566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.19004777620357\n",
      "    mean_inference_ms: 1.40807706532744\n",
      "    mean_raw_obs_processing_ms: 0.8052713642370736\n",
      "  time_since_restore: 3415.523448228836\n",
      "  time_this_iter_s: 10.462807416915894\n",
      "  time_total_s: 3415.523448228836\n",
      "  timers:\n",
      "    learn_throughput: 1718.551\n",
      "    learn_time_ms: 581.886\n",
      "    load_throughput: 214236.665\n",
      "    load_time_ms: 4.668\n",
      "    sample_throughput: 89.065\n",
      "    sample_time_ms: 11227.738\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632128236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         3415.52</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 312\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.117295103602939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019949126852692543\n",
      "          policy_loss: -0.05527011396156417\n",
      "          total_loss: -0.0752996675670147\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001143396084403826\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.978571428571435\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04138394053768026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.187541166774036\n",
      "    mean_inference_ms: 1.4080212817859732\n",
      "    mean_raw_obs_processing_ms: 0.8069617116865238\n",
      "  time_since_restore: 3425.608261346817\n",
      "  time_this_iter_s: 10.084813117980957\n",
      "  time_total_s: 3425.608261346817\n",
      "  timers:\n",
      "    learn_throughput: 1721.88\n",
      "    learn_time_ms: 580.761\n",
      "    load_throughput: 214293.582\n",
      "    load_time_ms: 4.666\n",
      "    sample_throughput: 89.055\n",
      "    sample_time_ms: 11229.032\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632128246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         3425.61</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-37\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 313\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0683085759480795\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010633903359365742\n",
      "          policy_loss: -0.0433898346291648\n",
      "          total_loss: -0.06305102759765255\n",
      "          vf_explained_var: -0.8942933678627014\n",
      "          vf_loss: 0.001021892023143462\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.81333333333334\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041382478693963984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.185040580432856\n",
      "    mean_inference_ms: 1.4079657176656695\n",
      "    mean_raw_obs_processing_ms: 0.8086553942800436\n",
      "  time_since_restore: 3435.9092452526093\n",
      "  time_this_iter_s: 10.300983905792236\n",
      "  time_total_s: 3435.9092452526093\n",
      "  timers:\n",
      "    learn_throughput: 1721.517\n",
      "    learn_time_ms: 580.883\n",
      "    load_throughput: 214471.097\n",
      "    load_time_ms: 4.663\n",
      "    sample_throughput: 88.767\n",
      "    sample_time_ms: 11265.402\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632128257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         3435.91</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 314\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3426188161247413e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.187079350153605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02086976355099157\n",
      "          policy_loss: -0.055055436160829335\n",
      "          total_loss: -0.07584635822309388\n",
      "          vf_explained_var: -0.9755364060401917\n",
      "          vf_loss: 0.001079868047640452\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85\n",
      "    ram_util_percent: 64.28571428571426\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041381027909867175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.182541018522299\n",
      "    mean_inference_ms: 1.4079103404490423\n",
      "    mean_raw_obs_processing_ms: 0.8103521687284541\n",
      "  time_since_restore: 3446.0328130722046\n",
      "  time_this_iter_s: 10.123567819595337\n",
      "  time_total_s: 3446.0328130722046\n",
      "  timers:\n",
      "    learn_throughput: 1721.399\n",
      "    learn_time_ms: 580.923\n",
      "    load_throughput: 215343.194\n",
      "    load_time_ms: 4.644\n",
      "    sample_throughput: 88.738\n",
      "    sample_time_ms: 11269.113\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632128267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         3446.03</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 315\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0003025081422594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010271766292336082\n",
      "          policy_loss: -0.06437144037336111\n",
      "          total_loss: -0.0833258282393217\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010486350724628816\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66666666666666\n",
      "    ram_util_percent: 64.28666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04137960279129341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.180045558493896\n",
      "    mean_inference_ms: 1.4078552646333105\n",
      "    mean_raw_obs_processing_ms: 0.8120519330500372\n",
      "  time_since_restore: 3456.4274015426636\n",
      "  time_this_iter_s: 10.394588470458984\n",
      "  time_total_s: 3456.4274015426636\n",
      "  timers:\n",
      "    learn_throughput: 1722.007\n",
      "    learn_time_ms: 580.718\n",
      "    load_throughput: 215469.308\n",
      "    load_time_ms: 4.641\n",
      "    sample_throughput: 88.342\n",
      "    sample_time_ms: 11319.669\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632128277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         3456.43</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-07\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 316\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.300928001933628\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010843408852153682\n",
      "          policy_loss: -0.08566222579942809\n",
      "          total_loss: -0.10737602727280723\n",
      "          vf_explained_var: -0.5477737784385681\n",
      "          vf_loss: 0.001295476079556263\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82666666666667\n",
      "    ram_util_percent: 64.26666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04137818870864335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.177554642038366\n",
      "    mean_inference_ms: 1.4078003637063496\n",
      "    mean_raw_obs_processing_ms: 0.81375488295227\n",
      "  time_since_restore: 3466.5514562129974\n",
      "  time_this_iter_s: 10.124054670333862\n",
      "  time_total_s: 3466.5514562129974\n",
      "  timers:\n",
      "    learn_throughput: 1724.188\n",
      "    learn_time_ms: 579.983\n",
      "    load_throughput: 320100.129\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 104.154\n",
      "    sample_time_ms: 9601.135\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632128287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         3466.55</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-18\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 317\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2134029706319174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015319977625346315\n",
      "          policy_loss: -0.05018196531261007\n",
      "          total_loss: -0.07098379149619076\n",
      "          vf_explained_var: -0.8537254929542542\n",
      "          vf_loss: 0.0013322040684417718\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.199999999999996\n",
      "    ram_util_percent: 64.29285714285713\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413767795549427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.17507094871889\n",
      "    mean_inference_ms: 1.4077457887377143\n",
      "    mean_raw_obs_processing_ms: 0.8154605830242829\n",
      "  time_since_restore: 3476.7601416110992\n",
      "  time_this_iter_s: 10.208685398101807\n",
      "  time_total_s: 3476.7601416110992\n",
      "  timers:\n",
      "    learn_throughput: 1724.316\n",
      "    learn_time_ms: 579.94\n",
      "    load_throughput: 319393.243\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 104.031\n",
      "    sample_time_ms: 9612.479\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632128298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         3476.76</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-28\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 318\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0998639133241443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01118889592750995\n",
      "          policy_loss: 0.10491653362082111\n",
      "          total_loss: 0.08516635994116466\n",
      "          vf_explained_var: -0.44353827834129333\n",
      "          vf_loss: 0.0012484650664393686\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66\n",
      "    ram_util_percent: 64.23333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04137539362424603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.172600313270536\n",
      "    mean_inference_ms: 1.4076915747275067\n",
      "    mean_raw_obs_processing_ms: 0.8171687824426449\n",
      "  time_since_restore: 3487.1889350414276\n",
      "  time_this_iter_s: 10.42879343032837\n",
      "  time_total_s: 3487.1889350414276\n",
      "  timers:\n",
      "    learn_throughput: 1725.018\n",
      "    learn_time_ms: 579.704\n",
      "    load_throughput: 318885.729\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 103.151\n",
      "    sample_time_ms: 9694.554\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632128308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         3487.19</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 319\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.999651273091634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01046915787037107\n",
      "          policy_loss: 0.003167700229419602\n",
      "          total_loss: -0.013338761321372456\n",
      "          vf_explained_var: -0.30277082324028015\n",
      "          vf_loss: 0.003490054148197588\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.346666666666664\n",
      "    ram_util_percent: 64.24666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04137401154272009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.170140983686775\n",
      "    mean_inference_ms: 1.407637576843274\n",
      "    mean_raw_obs_processing_ms: 0.8188795634968828\n",
      "  time_since_restore: 3497.592027425766\n",
      "  time_this_iter_s: 10.403092384338379\n",
      "  time_total_s: 3497.592027425766\n",
      "  timers:\n",
      "    learn_throughput: 1726.441\n",
      "    learn_time_ms: 579.226\n",
      "    load_throughput: 317902.029\n",
      "    load_time_ms: 3.146\n",
      "    sample_throughput: 102.971\n",
      "    sample_time_ms: 9711.512\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632128318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         3497.59</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 320\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0994546916749743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01357756778002769\n",
      "          policy_loss: -0.08296470650368266\n",
      "          total_loss: -0.1022138940791289\n",
      "          vf_explained_var: -0.6501365303993225\n",
      "          vf_loss: 0.0017453555315215554\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.686666666666675\n",
      "    ram_util_percent: 64.25999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041372643590393325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.167688745634802\n",
      "    mean_inference_ms: 1.4075838370228098\n",
      "    mean_raw_obs_processing_ms: 0.8205928896119148\n",
      "  time_since_restore: 3507.9948768615723\n",
      "  time_this_iter_s: 10.402849435806274\n",
      "  time_total_s: 3507.9948768615723\n",
      "  timers:\n",
      "    learn_throughput: 1727.527\n",
      "    learn_time_ms: 578.862\n",
      "    load_throughput: 318312.172\n",
      "    load_time_ms: 3.142\n",
      "    sample_throughput: 103.031\n",
      "    sample_time_ms: 9705.863\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632128329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         3507.99</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 321\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9345219559139675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0127443171520414\n",
      "          policy_loss: -0.08948735801710023\n",
      "          total_loss: -0.10778601004017724\n",
      "          vf_explained_var: -0.6205276250839233\n",
      "          vf_loss: 0.0010465697353033141\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.913333333333334\n",
      "    ram_util_percent: 64.27999999999997\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04137127820122103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.165296017819513\n",
      "    mean_inference_ms: 1.407530538765823\n",
      "    mean_raw_obs_processing_ms: 0.8223084117216344\n",
      "  time_since_restore: 3518.319002866745\n",
      "  time_this_iter_s: 10.32412600517273\n",
      "  time_total_s: 3518.319002866745\n",
      "  timers:\n",
      "    learn_throughput: 1727.492\n",
      "    learn_time_ms: 578.874\n",
      "    load_throughput: 317577.079\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 103.178\n",
      "    sample_time_ms: 9691.96\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632128339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         3518.32</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 322\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9840341713693408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015872220059137\n",
      "          policy_loss: -0.06600541406207615\n",
      "          total_loss: -0.08351151624487506\n",
      "          vf_explained_var: 0.24936996400356293\n",
      "          vf_loss: 0.002334240953334504\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.720000000000006\n",
      "    ram_util_percent: 64.34666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04136992293770681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.162934468936292\n",
      "    mean_inference_ms: 1.4074777457524237\n",
      "    mean_raw_obs_processing_ms: 0.824026505327669\n",
      "  time_since_restore: 3528.627253293991\n",
      "  time_this_iter_s: 10.308250427246094\n",
      "  time_total_s: 3528.627253293991\n",
      "  timers:\n",
      "    learn_throughput: 1726.874\n",
      "    learn_time_ms: 579.081\n",
      "    load_throughput: 318249.376\n",
      "    load_time_ms: 3.142\n",
      "    sample_throughput: 102.943\n",
      "    sample_time_ms: 9714.1\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632128350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         3528.63</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 323\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.770791945192549\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013751627675274432\n",
      "          policy_loss: -0.12050545646084679\n",
      "          total_loss: -0.13583121283186805\n",
      "          vf_explained_var: -0.09318994730710983\n",
      "          vf_loss: 0.0023821631659908843\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66666666666667\n",
      "    ram_util_percent: 64.38666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04136857636129153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.160649458841895\n",
      "    mean_inference_ms: 1.4074255377176887\n",
      "    mean_raw_obs_processing_ms: 0.8257470192735166\n",
      "  time_since_restore: 3539.210097551346\n",
      "  time_this_iter_s: 10.582844257354736\n",
      "  time_total_s: 3539.210097551346\n",
      "  timers:\n",
      "    learn_throughput: 1727.641\n",
      "    learn_time_ms: 578.824\n",
      "    load_throughput: 317800.862\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 102.642\n",
      "    sample_time_ms: 9742.565\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632128360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         3539.21</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 324\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8447987371020846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014751771552577984\n",
      "          policy_loss: -0.031162238948875005\n",
      "          total_loss: -0.04690084093146854\n",
      "          vf_explained_var: -0.4857788383960724\n",
      "          vf_loss: 0.0027093824723528493\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.960000000000015\n",
      "    ram_util_percent: 64.40666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041367247536336596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.158404729106557\n",
      "    mean_inference_ms: 1.407373950553542\n",
      "    mean_raw_obs_processing_ms: 0.827470044831951\n",
      "  time_since_restore: 3549.780827999115\n",
      "  time_this_iter_s: 10.570730447769165\n",
      "  time_total_s: 3549.780827999115\n",
      "  timers:\n",
      "    learn_throughput: 1727.738\n",
      "    learn_time_ms: 578.792\n",
      "    load_throughput: 317545.823\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 102.173\n",
      "    sample_time_ms: 9787.322\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632128371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         3549.78</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-59-41\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 325\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9177843888600667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009514682794037792\n",
      "          policy_loss: -0.1364943855545587\n",
      "          total_loss: -0.15436610980994173\n",
      "          vf_explained_var: -0.46343743801116943\n",
      "          vf_loss: 0.0013061176183530026\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.64\n",
      "    ram_util_percent: 64.50000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041365931584308645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.156236926544883\n",
      "    mean_inference_ms: 1.4073229805749372\n",
      "    mean_raw_obs_processing_ms: 0.8291954109958758\n",
      "  time_since_restore: 3560.1897382736206\n",
      "  time_this_iter_s: 10.408910274505615\n",
      "  time_total_s: 3560.1897382736206\n",
      "  timers:\n",
      "    learn_throughput: 1729.423\n",
      "    learn_time_ms: 578.228\n",
      "    load_throughput: 317447.285\n",
      "    load_time_ms: 3.15\n",
      "    sample_throughput: 102.145\n",
      "    sample_time_ms: 9790.015\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632128381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         3560.19</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_08-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 326\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6299496094385784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014096748314999116\n",
      "          policy_loss: 0.04138649163974656\n",
      "          total_loss: 0.026895137131214143\n",
      "          vf_explained_var: -0.8668630123138428\n",
      "          vf_loss: 0.0018081407424890333\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.942857142857136\n",
      "    ram_util_percent: 64.57142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04136462643149978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.15411412982731\n",
      "    mean_inference_ms: 1.4072725357057507\n",
      "    mean_raw_obs_processing_ms: 0.8309227698036866\n",
      "  time_since_restore: 3570.5333490371704\n",
      "  time_this_iter_s: 10.343610763549805\n",
      "  time_total_s: 3570.5333490371704\n",
      "  timers:\n",
      "    learn_throughput: 1728.059\n",
      "    learn_time_ms: 578.684\n",
      "    load_throughput: 316644.446\n",
      "    load_time_ms: 3.158\n",
      "    sample_throughput: 101.921\n",
      "    sample_time_ms: 9811.537\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632128392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         3570.53</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 327\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8960623489485846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015597530013518284\n",
      "          policy_loss: 0.0042761256297429405\n",
      "          total_loss: -0.012390322279598978\n",
      "          vf_explained_var: -0.6761796474456787\n",
      "          vf_loss: 0.0022941763777958434\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70666666666666\n",
      "    ram_util_percent: 64.58666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413633322612943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.15204797237407\n",
      "    mean_inference_ms: 1.4072227419703172\n",
      "    mean_raw_obs_processing_ms: 0.8326516757303977\n",
      "  time_since_restore: 3581.005145549774\n",
      "  time_this_iter_s: 10.47179651260376\n",
      "  time_total_s: 3581.005145549774\n",
      "  timers:\n",
      "    learn_throughput: 1711.91\n",
      "    learn_time_ms: 584.143\n",
      "    load_throughput: 317726.233\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 101.704\n",
      "    sample_time_ms: 9832.427\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632128402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         3581.01</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 328\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9237391630808511\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015095128649213101\n",
      "          policy_loss: -0.09256527506642871\n",
      "          total_loss: -0.11031974057356517\n",
      "          vf_explained_var: -0.9910832643508911\n",
      "          vf_loss: 0.0014829271000861707\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.059999999999995\n",
      "    ram_util_percent: 64.57333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041362048176509285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.150025838779118\n",
      "    mean_inference_ms: 1.4071732785192481\n",
      "    mean_raw_obs_processing_ms: 0.8343824525994249\n",
      "  time_since_restore: 3591.3395371437073\n",
      "  time_this_iter_s: 10.334391593933105\n",
      "  time_total_s: 3591.3395371437073\n",
      "  timers:\n",
      "    learn_throughput: 1712.137\n",
      "    learn_time_ms: 584.065\n",
      "    load_throughput: 320026.858\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 101.801\n",
      "    sample_time_ms: 9823.061\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632128412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         3591.34</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 329\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0319360865486993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016778605385882845\n",
      "          policy_loss: -0.0790940419667297\n",
      "          total_loss: -0.09816219992935657\n",
      "          vf_explained_var: -0.7156045436859131\n",
      "          vf_loss: 0.0012512039043940603\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79999999999999\n",
      "    ram_util_percent: 64.58666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04136077681632902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.148071319117447\n",
      "    mean_inference_ms: 1.4071243262967517\n",
      "    mean_raw_obs_processing_ms: 0.836115370861645\n",
      "  time_since_restore: 3601.774587392807\n",
      "  time_this_iter_s: 10.435050249099731\n",
      "  time_total_s: 3601.774587392807\n",
      "  timers:\n",
      "    learn_throughput: 1711.762\n",
      "    learn_time_ms: 584.193\n",
      "    load_throughput: 321666.347\n",
      "    load_time_ms: 3.109\n",
      "    sample_throughput: 101.77\n",
      "    sample_time_ms: 9826.115\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632128423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         3601.77</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-33\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 330\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.018252999252743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011271112338864124\n",
      "          policy_loss: -0.04546316969725821\n",
      "          total_loss: -0.06392278985844718\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0017229094396397058\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.666666666666664\n",
      "    ram_util_percent: 64.67333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04135952295770615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.146164109578892\n",
      "    mean_inference_ms: 1.4070760302168583\n",
      "    mean_raw_obs_processing_ms: 0.8378500996172878\n",
      "  time_since_restore: 3611.994149208069\n",
      "  time_this_iter_s: 10.21956181526184\n",
      "  time_total_s: 3611.994149208069\n",
      "  timers:\n",
      "    learn_throughput: 1710.317\n",
      "    learn_time_ms: 584.687\n",
      "    load_throughput: 322383.342\n",
      "    load_time_ms: 3.102\n",
      "    sample_throughput: 101.965\n",
      "    sample_time_ms: 9807.277\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632128433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         3611.99</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 331\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.055461588170793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017087448160798463\n",
      "          policy_loss: -0.013995432729522387\n",
      "          total_loss: -0.03360205880469746\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000947993131639022\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.70714285714286\n",
      "    ram_util_percent: 64.67857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041358286081209075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.144306448233818\n",
      "    mean_inference_ms: 1.4070285178010369\n",
      "    mean_raw_obs_processing_ms: 0.8395864955442786\n",
      "  time_since_restore: 3622.217326402664\n",
      "  time_this_iter_s: 10.223177194595337\n",
      "  time_total_s: 3622.217326402664\n",
      "  timers:\n",
      "    learn_throughput: 1710.798\n",
      "    learn_time_ms: 584.523\n",
      "    load_throughput: 322569.293\n",
      "    load_time_ms: 3.1\n",
      "    sample_throughput: 102.068\n",
      "    sample_time_ms: 9797.355\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632128443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         3622.22</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 332\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.869243366188473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010718171127281077\n",
      "          policy_loss: 0.0072994300681683754\n",
      "          total_loss: -0.010349637187189526\n",
      "          vf_explained_var: 0.194123312830925\n",
      "          vf_loss: 0.0010433669143822045\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.713333333333324\n",
      "    ram_util_percent: 64.69333333333336\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04135705153766952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.142447154396384\n",
      "    mean_inference_ms: 1.4069811919193256\n",
      "    mean_raw_obs_processing_ms: 0.8413244381700707\n",
      "  time_since_restore: 3632.436256170273\n",
      "  time_this_iter_s: 10.218929767608643\n",
      "  time_total_s: 3632.436256170273\n",
      "  timers:\n",
      "    learn_throughput: 1711.13\n",
      "    learn_time_ms: 584.409\n",
      "    load_throughput: 321888.521\n",
      "    load_time_ms: 3.107\n",
      "    sample_throughput: 102.16\n",
      "    sample_time_ms: 9788.54\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632128454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         3632.44</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 333\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.092982510725657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011210393196241607\n",
      "          policy_loss: -0.09096401292416785\n",
      "          total_loss: -0.11025383215811517\n",
      "          vf_explained_var: -0.8017458319664001\n",
      "          vf_loss: 0.0016400045566519516\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86\n",
      "    ram_util_percent: 64.70000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041355822863406576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.14058138398247\n",
      "    mean_inference_ms: 1.4069340314227727\n",
      "    mean_raw_obs_processing_ms: 0.8430640741271922\n",
      "  time_since_restore: 3642.6073050498962\n",
      "  time_this_iter_s: 10.171048879623413\n",
      "  time_total_s: 3642.6073050498962\n",
      "  timers:\n",
      "    learn_throughput: 1710.091\n",
      "    learn_time_ms: 584.764\n",
      "    load_throughput: 322368.476\n",
      "    load_time_ms: 3.102\n",
      "    sample_throughput: 102.596\n",
      "    sample_time_ms: 9746.99\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632128464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         3642.61</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 989.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 334\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8832682728767396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010684431351590007\n",
      "          policy_loss: -0.060662849847641254\n",
      "          total_loss: -0.07880938794049952\n",
      "          vf_explained_var: -0.6499091386795044\n",
      "          vf_loss: 0.0006861437380494964\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.62\n",
      "    ram_util_percent: 64.72000000000003\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04135459803087689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.138737149284738\n",
      "    mean_inference_ms: 1.4068871842842372\n",
      "    mean_raw_obs_processing_ms: 0.8448053460261984\n",
      "  time_since_restore: 3653.083920955658\n",
      "  time_this_iter_s: 10.476615905761719\n",
      "  time_total_s: 3653.083920955658\n",
      "  timers:\n",
      "    learn_throughput: 1709.561\n",
      "    learn_time_ms: 584.946\n",
      "    load_throughput: 322795.201\n",
      "    load_time_ms: 3.098\n",
      "    sample_throughput: 102.697\n",
      "    sample_time_ms: 9737.355\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1632128474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         3653.08</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 336\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6861766987376743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011740922374909365\n",
      "          policy_loss: 0.0827224136226707\n",
      "          total_loss: 0.06637022553218735\n",
      "          vf_explained_var: -0.04154425859451294\n",
      "          vf_loss: 0.000509578434866853\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.53076923076923\n",
      "    ram_util_percent: 64.30769230769229\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04135217054568903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.135044413939006\n",
      "    mean_inference_ms: 1.4067938925710675\n",
      "    mean_raw_obs_processing_ms: 0.8493488393663248\n",
      "  time_since_restore: 3680.825495004654\n",
      "  time_this_iter_s: 27.74157404899597\n",
      "  time_total_s: 3680.825495004654\n",
      "  timers:\n",
      "    learn_throughput: 1707.641\n",
      "    learn_time_ms: 585.603\n",
      "    load_throughput: 217540.118\n",
      "    load_time_ms: 4.597\n",
      "    sample_throughput: 87.196\n",
      "    sample_time_ms: 11468.478\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632128502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         3680.83</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-01-54\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 337\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0098135232925416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011404378065441279\n",
      "          policy_loss: -0.00696575144926707\n",
      "          total_loss: -0.024039374084936248\n",
      "          vf_explained_var: -0.38618576526641846\n",
      "          vf_loss: 0.0030245115126793583\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10588235294117\n",
      "    ram_util_percent: 64.51764705882354\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413509873159644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.133280133611876\n",
      "    mean_inference_ms: 1.406748136987123\n",
      "    mean_raw_obs_processing_ms: 0.8516314754329823\n",
      "  time_since_restore: 3692.962784051895\n",
      "  time_this_iter_s: 12.137289047241211\n",
      "  time_total_s: 3692.962784051895\n",
      "  timers:\n",
      "    learn_throughput: 1707.724\n",
      "    learn_time_ms: 585.575\n",
      "    load_throughput: 217580.744\n",
      "    load_time_ms: 4.596\n",
      "    sample_throughput: 85.853\n",
      "    sample_time_ms: 11647.873\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632128514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         3692.96</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-04\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 338\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.157129028108385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009757781414923351\n",
      "          policy_loss: -0.06758109662267897\n",
      "          total_loss: -0.08814562062422435\n",
      "          vf_explained_var: -0.7283045649528503\n",
      "          vf_loss: 0.001006766526390695\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.24\n",
      "    ram_util_percent: 64.53333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041349820849811626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.131532369873293\n",
      "    mean_inference_ms: 1.406702890625086\n",
      "    mean_raw_obs_processing_ms: 0.853913618772997\n",
      "  time_since_restore: 3703.18429851532\n",
      "  time_this_iter_s: 10.221514463424683\n",
      "  time_total_s: 3703.18429851532\n",
      "  timers:\n",
      "    learn_throughput: 1721.842\n",
      "    learn_time_ms: 580.774\n",
      "    load_throughput: 217516.427\n",
      "    load_time_ms: 4.597\n",
      "    sample_throughput: 86.002\n",
      "    sample_time_ms: 11627.641\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632128524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         3703.18</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 988.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 339\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.80059341457155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01311276498139065\n",
      "          policy_loss: -0.007885323133733538\n",
      "          total_loss: -0.02510514925751421\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007861073652748018\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.52666666666666\n",
      "    ram_util_percent: 64.48666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134865715477545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.129782124990479\n",
      "    mean_inference_ms: 1.4066578660305324\n",
      "    mean_raw_obs_processing_ms: 0.8554820413491414\n",
      "  time_since_restore: 3713.5260100364685\n",
      "  time_this_iter_s: 10.341711521148682\n",
      "  time_total_s: 3713.5260100364685\n",
      "  timers:\n",
      "    learn_throughput: 1720.216\n",
      "    learn_time_ms: 581.322\n",
      "    load_throughput: 217596.547\n",
      "    load_time_ms: 4.596\n",
      "    sample_throughput: 86.0\n",
      "    sample_time_ms: 11627.844\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632128535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         3713.53</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            988.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 989.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 340\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1155417025089265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017022155285288124\n",
      "          policy_loss: 0.04762985524204042\n",
      "          total_loss: 0.037294027540418835\n",
      "          vf_explained_var: -0.2400083690881729\n",
      "          vf_loss: 0.0008195892503459213\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.02857142857142\n",
      "    ram_util_percent: 64.38571428571429\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134748962256924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.128028601092254\n",
      "    mean_inference_ms: 1.4066124638387358\n",
      "    mean_raw_obs_processing_ms: 0.8570286225505183\n",
      "  time_since_restore: 3723.6726217269897\n",
      "  time_this_iter_s: 10.14661169052124\n",
      "  time_total_s: 3723.6726217269897\n",
      "  timers:\n",
      "    learn_throughput: 1721.163\n",
      "    learn_time_ms: 581.002\n",
      "    load_throughput: 217345.1\n",
      "    load_time_ms: 4.601\n",
      "    sample_throughput: 86.212\n",
      "    sample_time_ms: 11599.314\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632128545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         3723.67</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 989.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 341\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.631358073817359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011740357043651646\n",
      "          policy_loss: -0.005621825282772382\n",
      "          total_loss: -0.020600022044446734\n",
      "          vf_explained_var: -0.939819872379303\n",
      "          vf_loss: 0.001335383860471969\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.693333333333335\n",
      "    ram_util_percent: 64.20666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134632703482679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.126259332731443\n",
      "    mean_inference_ms: 1.4065675032082077\n",
      "    mean_raw_obs_processing_ms: 0.8585772050730789\n",
      "  time_since_restore: 3733.9056951999664\n",
      "  time_this_iter_s: 10.233073472976685\n",
      "  time_total_s: 3733.9056951999664\n",
      "  timers:\n",
      "    learn_throughput: 1723.212\n",
      "    learn_time_ms: 580.312\n",
      "    load_throughput: 217112.213\n",
      "    load_time_ms: 4.606\n",
      "    sample_throughput: 86.197\n",
      "    sample_time_ms: 11601.332\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1632128555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         3733.91</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 989.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 342\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.73986400630739\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0146552926150071\n",
      "          policy_loss: -0.013218393052617709\n",
      "          total_loss: -0.02949643979469935\n",
      "          vf_explained_var: -0.903781533241272\n",
      "          vf_loss: 0.001120594557788637\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.4\n",
      "    ram_util_percent: 64.07333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134517472957918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.124501084821757\n",
      "    mean_inference_ms: 1.4065226314926338\n",
      "    mean_raw_obs_processing_ms: 0.8601281488093894\n",
      "  time_since_restore: 3744.199479818344\n",
      "  time_this_iter_s: 10.293784618377686\n",
      "  time_total_s: 3744.199479818344\n",
      "  timers:\n",
      "    learn_throughput: 1723.123\n",
      "    learn_time_ms: 580.341\n",
      "    load_throughput: 217095.357\n",
      "    load_time_ms: 4.606\n",
      "    sample_throughput: 86.145\n",
      "    sample_time_ms: 11608.35\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632128566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">          3744.2</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-02-56\n",
      "  done: false\n",
      "  episode_len_mean: 989.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 343\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796901528040568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010816174865078566\n",
      "          policy_loss: 0.04513986470798651\n",
      "          total_loss: 0.027995184332960183\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008243355925919282\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84000000000001\n",
      "    ram_util_percent: 63.926666666666655\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04134403064508242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.122750611760866\n",
      "    mean_inference_ms: 1.4064780044675782\n",
      "    mean_raw_obs_processing_ms: 0.861681149641193\n",
      "  time_since_restore: 3754.523902654648\n",
      "  time_this_iter_s: 10.324422836303711\n",
      "  time_total_s: 3754.523902654648\n",
      "  timers:\n",
      "    learn_throughput: 1721.381\n",
      "    learn_time_ms: 580.929\n",
      "    load_throughput: 216627.793\n",
      "    load_time_ms: 4.616\n",
      "    sample_throughput: 86.071\n",
      "    sample_time_ms: 11618.279\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632128576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         3754.52</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">    0.12</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            989.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 990.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 344\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6041576411989\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00998040307328518\n",
      "          policy_loss: -0.046206901574300394\n",
      "          total_loss: -0.06064851118458642\n",
      "          vf_explained_var: -0.952265739440918\n",
      "          vf_loss: 0.0015999674344331855\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.81428571428571\n",
      "    ram_util_percent: 63.87857142857142\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041342896747650515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.121016146060544\n",
      "    mean_inference_ms: 1.4064336552310175\n",
      "    mean_raw_obs_processing_ms: 0.8622737412085361\n",
      "  time_since_restore: 3764.9221816062927\n",
      "  time_this_iter_s: 10.398278951644897\n",
      "  time_total_s: 3764.9221816062927\n",
      "  timers:\n",
      "    learn_throughput: 1722.056\n",
      "    learn_time_ms: 580.701\n",
      "    load_throughput: 216404.256\n",
      "    load_time_ms: 4.621\n",
      "    sample_throughput: 85.902\n",
      "    sample_time_ms: 11641.235\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1632128586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         3764.92</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">    0.08</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            990.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 990.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 345\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6834089504347907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01522900557341684\n",
      "          policy_loss: -0.02894689641478989\n",
      "          total_loss: -0.043433654639456004\n",
      "          vf_explained_var: -0.9027567505836487\n",
      "          vf_loss: 0.0023473309087825734\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79333333333332\n",
      "    ram_util_percent: 63.799999999999976\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041341772274989516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.119217853925042\n",
      "    mean_inference_ms: 1.4063894139592645\n",
      "    mean_raw_obs_processing_ms: 0.8628721516801192\n",
      "  time_since_restore: 3775.218107700348\n",
      "  time_this_iter_s: 10.295926094055176\n",
      "  time_total_s: 3775.218107700348\n",
      "  timers:\n",
      "    learn_throughput: 1723.632\n",
      "    learn_time_ms: 580.17\n",
      "    load_throughput: 216504.79\n",
      "    load_time_ms: 4.619\n",
      "    sample_throughput: 86.031\n",
      "    sample_time_ms: 11623.738\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1632128597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         3775.22</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            990.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 346\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7552874326705932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014131106007666282\n",
      "          policy_loss: -0.01969339499870936\n",
      "          total_loss: -0.03601232427689764\n",
      "          vf_explained_var: -0.7425684332847595\n",
      "          vf_loss: 0.0012339446732463936\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79333333333334\n",
      "    ram_util_percent: 63.793333333333315\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041340651780134435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.11741091306505\n",
      "    mean_inference_ms: 1.4063449980074967\n",
      "    mean_raw_obs_processing_ms: 0.8620727291369783\n",
      "  time_since_restore: 3785.5397667884827\n",
      "  time_this_iter_s: 10.321659088134766\n",
      "  time_total_s: 3785.5397667884827\n",
      "  timers:\n",
      "    learn_throughput: 1721.864\n",
      "    learn_time_ms: 580.766\n",
      "    load_throughput: 320241.882\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 101.188\n",
      "    sample_time_ms: 9882.636\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1632128607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         3785.54</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 347\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.847985037167867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016218574533739212\n",
      "          policy_loss: -0.018279135185811254\n",
      "          total_loss: -0.03405156458417575\n",
      "          vf_explained_var: -0.8972365260124207\n",
      "          vf_loss: 0.0027074175265928107\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.59333333333333\n",
      "    ram_util_percent: 63.799999999999976\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133953480359298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.115551772516348\n",
      "    mean_inference_ms: 1.4063005387120295\n",
      "    mean_raw_obs_processing_ms: 0.8612846562648637\n",
      "  time_since_restore: 3795.858516216278\n",
      "  time_this_iter_s: 10.31874942779541\n",
      "  time_total_s: 3795.858516216278\n",
      "  timers:\n",
      "    learn_throughput: 1721.79\n",
      "    learn_time_ms: 580.791\n",
      "    load_throughput: 320474.335\n",
      "    load_time_ms: 3.12\n",
      "    sample_throughput: 103.085\n",
      "    sample_time_ms: 9700.704\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1632128617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         3795.86</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 348\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.010981723997328\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011256163288154219\n",
      "          policy_loss: 0.0013257538278897603\n",
      "          total_loss: -0.01789643350574705\n",
      "          vf_explained_var: -0.9426462650299072\n",
      "          vf_loss: 0.0008876274527412736\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.942857142857136\n",
      "    ram_util_percent: 63.799999999999976\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133841589715035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.11369587240597\n",
      "    mean_inference_ms: 1.4062560438636107\n",
      "    mean_raw_obs_processing_ms: 0.860507989045636\n",
      "  time_since_restore: 3806.1567261219025\n",
      "  time_this_iter_s: 10.29820990562439\n",
      "  time_total_s: 3806.1567261219025\n",
      "  timers:\n",
      "    learn_throughput: 1720.392\n",
      "    learn_time_ms: 581.263\n",
      "    load_throughput: 320031.741\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 103.009\n",
      "    sample_time_ms: 9707.914\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632128628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         3806.16</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 349\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8261226892471314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012028761787515347\n",
      "          policy_loss: -0.08466058642499977\n",
      "          total_loss: -0.10219204119510121\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007297697519081541\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.7\n",
      "    ram_util_percent: 63.799999999999976\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041337307060409376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.111889535717491\n",
      "    mean_inference_ms: 1.4062117907222966\n",
      "    mean_raw_obs_processing_ms: 0.8597422995062632\n",
      "  time_since_restore: 3816.522838830948\n",
      "  time_this_iter_s: 10.36611270904541\n",
      "  time_total_s: 3816.522838830948\n",
      "  timers:\n",
      "    learn_throughput: 1721.589\n",
      "    learn_time_ms: 580.859\n",
      "    load_throughput: 319437.027\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 102.979\n",
      "    sample_time_ms: 9710.763\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632128638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         3816.52</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 350\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3449564576148987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009750839274406826\n",
      "          policy_loss: 0.021687880655129752\n",
      "          total_loss: 0.008809427420298258\n",
      "          vf_explained_var: -0.5450438261032104\n",
      "          vf_loss: 0.0005711102321381784\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72\n",
      "    ram_util_percent: 63.84666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133621004540596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.110127486485942\n",
      "    mean_inference_ms: 1.4061678617115965\n",
      "    mean_raw_obs_processing_ms: 0.8589878772888317\n",
      "  time_since_restore: 3827.1004662513733\n",
      "  time_this_iter_s: 10.577627420425415\n",
      "  time_total_s: 3827.1004662513733\n",
      "  timers:\n",
      "    learn_throughput: 1721.105\n",
      "    learn_time_ms: 581.022\n",
      "    load_throughput: 318592.643\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 102.525\n",
      "    sample_time_ms: 9753.691\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632128649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">          3827.1</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 351\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6863453639878168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010519714096266182\n",
      "          policy_loss: 0.021620048334201176\n",
      "          total_loss: 0.006174839205212063\n",
      "          vf_explained_var: -0.6187828779220581\n",
      "          vf_loss: 0.001418244550910054\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70666666666667\n",
      "    ram_util_percent: 63.93999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041335127478095444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.10844193728291\n",
      "    mean_inference_ms: 1.4061245303068282\n",
      "    mean_raw_obs_processing_ms: 0.8582445977489559\n",
      "  time_since_restore: 3837.453268766403\n",
      "  time_this_iter_s: 10.352802515029907\n",
      "  time_total_s: 3837.453268766403\n",
      "  timers:\n",
      "    learn_throughput: 1719.083\n",
      "    learn_time_ms: 581.705\n",
      "    load_throughput: 317418.456\n",
      "    load_time_ms: 3.15\n",
      "    sample_throughput: 102.407\n",
      "    sample_time_ms: 9765.001\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632128659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         3837.45</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-04-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 352\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6633166432380677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007931560774546122\n",
      "          policy_loss: -0.006783666171961361\n",
      "          total_loss: -0.022806470634208785\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006103645528330365\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79333333333333\n",
      "    ram_util_percent: 63.919999999999995\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133406131393029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.106828854044952\n",
      "    mean_inference_ms: 1.4060817299445145\n",
      "    mean_raw_obs_processing_ms: 0.8575123492409797\n",
      "  time_since_restore: 3847.7510962486267\n",
      "  time_this_iter_s: 10.29782748222351\n",
      "  time_total_s: 3847.7510962486267\n",
      "  timers:\n",
      "    learn_throughput: 1719.098\n",
      "    learn_time_ms: 581.7\n",
      "    load_throughput: 317743.082\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 102.402\n",
      "    sample_time_ms: 9765.426\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632128669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         3847.75</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 353\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.752258343166775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00869888092202989\n",
      "          policy_loss: -0.0033029888653092914\n",
      "          total_loss: -0.020373736073573432\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00045183534126004414\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.42\n",
      "    ram_util_percent: 64.00666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041333006121811254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.105287684667365\n",
      "    mean_inference_ms: 1.4060388912260817\n",
      "    mean_raw_obs_processing_ms: 0.856790446998479\n",
      "  time_since_restore: 3858.0725214481354\n",
      "  time_this_iter_s: 10.321425199508667\n",
      "  time_total_s: 3858.0725214481354\n",
      "  timers:\n",
      "    learn_throughput: 1719.875\n",
      "    learn_time_ms: 581.438\n",
      "    load_throughput: 318353.245\n",
      "    load_time_ms: 3.141\n",
      "    sample_throughput: 102.402\n",
      "    sample_time_ms: 9765.422\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1632128680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         3858.07</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-04-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 354\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8676979568269518\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01804413396074934\n",
      "          policy_loss: -0.013724397122859954\n",
      "          total_loss: -0.03205017372965813\n",
      "          vf_explained_var: -0.9989205002784729\n",
      "          vf_loss: 0.00035120367522015135\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.760000000000005\n",
      "    ram_util_percent: 64.04666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04133196064325986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.10381907051475\n",
      "    mean_inference_ms: 1.4059967687766437\n",
      "    mean_raw_obs_processing_ms: 0.8560793234052078\n",
      "  time_since_restore: 3868.4574258327484\n",
      "  time_this_iter_s: 10.384904384613037\n",
      "  time_total_s: 3868.4574258327484\n",
      "  timers:\n",
      "    learn_throughput: 1720.057\n",
      "    learn_time_ms: 581.376\n",
      "    load_throughput: 318597.483\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 102.415\n",
      "    sample_time_ms: 9764.148\n",
      "    update_time_ms: 1.619\n",
      "  timestamp: 1632128690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         3868.46</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 355\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9584301657146879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010644265267026595\n",
      "          policy_loss: 0.017947673425078393\n",
      "          total_loss: -0.001247640699148178\n",
      "          vf_explained_var: -0.908778190612793\n",
      "          vf_loss: 0.0003889875425051691\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.08571428571428\n",
      "    ram_util_percent: 64.09285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413309305982397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.102423014799477\n",
      "    mean_inference_ms: 1.4059552573762366\n",
      "    mean_raw_obs_processing_ms: 0.8553785589179417\n",
      "  time_since_restore: 3878.8062517642975\n",
      "  time_this_iter_s: 10.348825931549072\n",
      "  time_total_s: 3878.8062517642975\n",
      "  timers:\n",
      "    learn_throughput: 1718.678\n",
      "    learn_time_ms: 581.842\n",
      "    load_throughput: 317803.27\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 102.365\n",
      "    sample_time_ms: 9768.972\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1632128700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         3878.81</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 356\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8060213605562845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01228680911946248\n",
      "          policy_loss: -0.042320972349908614\n",
      "          total_loss: -0.060035730939772394\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00034545447997516023\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.779999999999994\n",
      "    ram_util_percent: 64.11333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132988811795921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.101097709560193\n",
      "    mean_inference_ms: 1.4059143585963292\n",
      "    mean_raw_obs_processing_ms: 0.8546877360113028\n",
      "  time_since_restore: 3889.1554729938507\n",
      "  time_this_iter_s: 10.349221229553223\n",
      "  time_total_s: 3889.1554729938507\n",
      "  timers:\n",
      "    learn_throughput: 1720.746\n",
      "    learn_time_ms: 581.143\n",
      "    load_throughput: 318109.381\n",
      "    load_time_ms: 3.144\n",
      "    sample_throughput: 102.329\n",
      "    sample_time_ms: 9772.431\n",
      "    update_time_ms: 1.616\n",
      "  timestamp: 1632128711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         3889.16</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 357\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8955860561794704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013961451595268393\n",
      "          policy_loss: -0.05520150578684277\n",
      "          total_loss: -0.07386516883141464\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00029219357683258647\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66\n",
      "    ram_util_percent: 64.19333333333336\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132885389751701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.099842514402384\n",
      "    mean_inference_ms: 1.4058739662452655\n",
      "    mean_raw_obs_processing_ms: 0.8540069635069071\n",
      "  time_since_restore: 3899.419178724289\n",
      "  time_this_iter_s: 10.263705730438232\n",
      "  time_total_s: 3899.419178724289\n",
      "  timers:\n",
      "    learn_throughput: 1719.631\n",
      "    learn_time_ms: 581.52\n",
      "    load_throughput: 318222.816\n",
      "    load_time_ms: 3.142\n",
      "    sample_throughput: 102.39\n",
      "    sample_time_ms: 9766.58\n",
      "    update_time_ms: 1.618\n",
      "  timestamp: 1632128721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         3899.42</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 358\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8901180028915405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009938167271196442\n",
      "          policy_loss: -0.04352460551179117\n",
      "          total_loss: -0.0622464744374156\n",
      "          vf_explained_var: -0.7982991337776184\n",
      "          vf_loss: 0.00017931225536611036\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.964285714285715\n",
      "    ram_util_percent: 64.23571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132783380320571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.098652695161274\n",
      "    mean_inference_ms: 1.4058339702310583\n",
      "    mean_raw_obs_processing_ms: 0.8533361356960905\n",
      "  time_since_restore: 3909.703518629074\n",
      "  time_this_iter_s: 10.284339904785156\n",
      "  time_total_s: 3909.703518629074\n",
      "  timers:\n",
      "    learn_throughput: 1722.452\n",
      "    learn_time_ms: 580.568\n",
      "    load_throughput: 318522.479\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 102.395\n",
      "    sample_time_ms: 9766.126\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632128731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">          3909.7</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 359\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8760122484631008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012122510533265185\n",
      "          policy_loss: -0.059611778230302864\n",
      "          total_loss: -0.07806617522405254\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00030572271837930507\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.713333333333324\n",
      "    ram_util_percent: 64.25999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132682525453301\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09752057477149\n",
      "    mean_inference_ms: 1.4057943939106974\n",
      "    mean_raw_obs_processing_ms: 0.8526748930772332\n",
      "  time_since_restore: 3920.039530515671\n",
      "  time_this_iter_s: 10.33601188659668\n",
      "  time_total_s: 3920.039530515671\n",
      "  timers:\n",
      "    learn_throughput: 1721.283\n",
      "    learn_time_ms: 580.962\n",
      "    load_throughput: 318740.33\n",
      "    load_time_ms: 3.137\n",
      "    sample_throughput: 102.431\n",
      "    sample_time_ms: 9762.707\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632128742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         3920.04</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 360\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8127271387312147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012520109630799976\n",
      "          policy_loss: -0.03161048835350407\n",
      "          total_loss: -0.04927269696361489\n",
      "          vf_explained_var: -0.7498488426208496\n",
      "          vf_loss: 0.00046506053590241614\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.806666666666665\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413258265825983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.096448418973019\n",
      "    mean_inference_ms: 1.4057550174348616\n",
      "    mean_raw_obs_processing_ms: 0.8520236336984434\n",
      "  time_since_restore: 3930.3736176490784\n",
      "  time_this_iter_s: 10.334087133407593\n",
      "  time_total_s: 3930.3736176490784\n",
      "  timers:\n",
      "    learn_throughput: 1722.627\n",
      "    learn_time_ms: 580.509\n",
      "    load_throughput: 319975.588\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 102.682\n",
      "    sample_time_ms: 9738.808\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632128752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         3930.37</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 361\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8401539007822671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00822200914964968\n",
      "          policy_loss: -0.04551645898156696\n",
      "          total_loss: -0.06371384888059563\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00020414852778129797\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.553333333333335\n",
      "    ram_util_percent: 64.31333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132482886192735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09543812348568\n",
      "    mean_inference_ms: 1.4057158022381373\n",
      "    mean_raw_obs_processing_ms: 0.8513819350257092\n",
      "  time_since_restore: 3940.7150230407715\n",
      "  time_this_iter_s: 10.341405391693115\n",
      "  time_total_s: 3940.7150230407715\n",
      "  timers:\n",
      "    learn_throughput: 1722.189\n",
      "    learn_time_ms: 580.656\n",
      "    load_throughput: 321678.682\n",
      "    load_time_ms: 3.109\n",
      "    sample_throughput: 102.696\n",
      "    sample_time_ms: 9737.498\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632128763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         3940.72</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 362\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.018242863814036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01379720221978579\n",
      "          policy_loss: 0.00730291861626837\n",
      "          total_loss: -0.012738024608956443\n",
      "          vf_explained_var: -0.5952121019363403\n",
      "          vf_loss: 0.00014148283654422914\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.785714285714285\n",
      "    ram_util_percent: 64.33571428571426\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132384115501133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.094484147189394\n",
      "    mean_inference_ms: 1.4056770284941433\n",
      "    mean_raw_obs_processing_ms: 0.8507494575027068\n",
      "  time_since_restore: 3950.9441471099854\n",
      "  time_this_iter_s: 10.229124069213867\n",
      "  time_total_s: 3950.9441471099854\n",
      "  timers:\n",
      "    learn_throughput: 1721.879\n",
      "    learn_time_ms: 580.761\n",
      "    load_throughput: 321787.271\n",
      "    load_time_ms: 3.108\n",
      "    sample_throughput: 102.769\n",
      "    sample_time_ms: 9730.532\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632128773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         3950.94</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-06-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 363\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7608356727494134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011436990897925704\n",
      "          policy_loss: -0.04346255974637137\n",
      "          total_loss: -0.060747055812842315\n",
      "          vf_explained_var: -0.9222167730331421\n",
      "          vf_loss: 0.0003238599901022907\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.620000000000005\n",
      "    ram_util_percent: 64.39333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041322864114925834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09358399472902\n",
      "    mean_inference_ms: 1.4056386253055728\n",
      "    mean_raw_obs_processing_ms: 0.8501266062055086\n",
      "  time_since_restore: 3961.2131583690643\n",
      "  time_this_iter_s: 10.26901125907898\n",
      "  time_total_s: 3961.2131583690643\n",
      "  timers:\n",
      "    learn_throughput: 1722.622\n",
      "    learn_time_ms: 580.51\n",
      "    load_throughput: 321582.494\n",
      "    load_time_ms: 3.11\n",
      "    sample_throughput: 102.822\n",
      "    sample_time_ms: 9725.508\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632128783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         3961.21</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 364\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.013928224187112e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8843482984436883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020601910182461856\n",
      "          policy_loss: -0.025961029902100564\n",
      "          total_loss: -0.04415833467824592\n",
      "          vf_explained_var: -0.9952840209007263\n",
      "          vf_loss: 0.0006461766496714619\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.50000000000001\n",
      "    ram_util_percent: 64.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041321901935202694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092745912126883\n",
      "    mean_inference_ms: 1.4056007617150053\n",
      "    mean_raw_obs_processing_ms: 0.8495133223302409\n",
      "  time_since_restore: 3971.610676765442\n",
      "  time_this_iter_s: 10.397518396377563\n",
      "  time_total_s: 3971.610676765442\n",
      "  timers:\n",
      "    learn_throughput: 1721.866\n",
      "    learn_time_ms: 580.765\n",
      "    load_throughput: 321989.836\n",
      "    load_time_ms: 3.106\n",
      "    sample_throughput: 102.812\n",
      "    sample_time_ms: 9726.489\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632128793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         3971.61</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 365\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8940867529975043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008345625955042815\n",
      "          policy_loss: -0.0035451754099792903\n",
      "          total_loss: -0.02223045258886284\n",
      "          vf_explained_var: -0.37951958179473877\n",
      "          vf_loss: 0.0002555903723178845\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.519999999999996\n",
      "    ram_util_percent: 64.46666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041320949358426846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091970569328376\n",
      "    mean_inference_ms: 1.4055634382012898\n",
      "    mean_raw_obs_processing_ms: 0.8489089830905838\n",
      "  time_since_restore: 3981.9108176231384\n",
      "  time_this_iter_s: 10.300140857696533\n",
      "  time_total_s: 3981.9108176231384\n",
      "  timers:\n",
      "    learn_throughput: 1723.493\n",
      "    learn_time_ms: 580.217\n",
      "    load_throughput: 322465.134\n",
      "    load_time_ms: 3.101\n",
      "    sample_throughput: 102.858\n",
      "    sample_time_ms: 9722.165\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632128804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         3981.91</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-07-11\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 366\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9107410351435343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01443156061392392\n",
      "          policy_loss: 0.0036829425228966607\n",
      "          total_loss: -0.015132850118809276\n",
      "          vf_explained_var: -0.909946858882904\n",
      "          vf_loss: 0.0002916176770264024\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.56666666666666\n",
      "    ram_util_percent: 64.53333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04132001227914731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09125634035094\n",
      "    mean_inference_ms: 1.4055266533006345\n",
      "    mean_raw_obs_processing_ms: 0.8487850868672617\n",
      "  time_since_restore: 4009.4372594356537\n",
      "  time_this_iter_s: 27.52644181251526\n",
      "  time_total_s: 4009.4372594356537\n",
      "  timers:\n",
      "    learn_throughput: 1724.005\n",
      "    learn_time_ms: 580.045\n",
      "    load_throughput: 217348.479\n",
      "    load_time_ms: 4.601\n",
      "    sample_throughput: 87.424\n",
      "    sample_time_ms: 11438.546\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632128831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         4009.44</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 367\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7947554654545255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017920075823055253\n",
      "          policy_loss: -0.0311989544166459\n",
      "          total_loss: -0.04891816746029589\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00022834085838338878\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01666666666667\n",
      "    ram_util_percent: 64.44444444444444\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041319080867915664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09065769138216\n",
      "    mean_inference_ms: 1.4054903659818785\n",
      "    mean_raw_obs_processing_ms: 0.8486690149348949\n",
      "  time_since_restore: 4021.630481481552\n",
      "  time_this_iter_s: 12.193222045898438\n",
      "  time_total_s: 4021.630481481552\n",
      "  timers:\n",
      "    learn_throughput: 1724.275\n",
      "    learn_time_ms: 579.954\n",
      "    load_throughput: 217352.984\n",
      "    load_time_ms: 4.601\n",
      "    sample_throughput: 85.973\n",
      "    sample_time_ms: 11631.613\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632128844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         4021.63</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-07-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 368\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9446747501691182\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015439438547156294\n",
      "          policy_loss: -0.024097419312844672\n",
      "          total_loss: -0.04309532623738051\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00044884000833715415\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.52142857142858\n",
      "    ram_util_percent: 64.54285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04131816091961913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090107777155435\n",
      "    mean_inference_ms: 1.4054546457178572\n",
      "    mean_raw_obs_processing_ms: 0.8485607803300792\n",
      "  time_since_restore: 4031.9787187576294\n",
      "  time_this_iter_s: 10.34823727607727\n",
      "  time_total_s: 4031.9787187576294\n",
      "  timers:\n",
      "    learn_throughput: 1723.387\n",
      "    learn_time_ms: 580.253\n",
      "    load_throughput: 217162.798\n",
      "    load_time_ms: 4.605\n",
      "    sample_throughput: 85.928\n",
      "    sample_time_ms: 11637.699\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632128854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         4031.98</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 369\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9696792748239305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01442175566649979\n",
      "          policy_loss: -0.07470869256390465\n",
      "          total_loss: -0.09398553768793742\n",
      "          vf_explained_var: -0.6688672304153442\n",
      "          vf_loss: 0.00041994651773064913\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.76\n",
      "    ram_util_percent: 64.59333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04131725575618783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089617629251157\n",
      "    mean_inference_ms: 1.4054195279414423\n",
      "    mean_raw_obs_processing_ms: 0.8484602515786726\n",
      "  time_since_restore: 4042.3608813285828\n",
      "  time_this_iter_s: 10.38216257095337\n",
      "  time_total_s: 4042.3608813285828\n",
      "  timers:\n",
      "    learn_throughput: 1723.136\n",
      "    learn_time_ms: 580.337\n",
      "    load_throughput: 217062.775\n",
      "    load_time_ms: 4.607\n",
      "    sample_throughput: 85.894\n",
      "    sample_time_ms: 11642.219\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632128864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         4042.36</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 370\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9751529852549234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01411486182110604\n",
      "          policy_loss: -0.07122141482929388\n",
      "          total_loss: -0.09072381841639678\n",
      "          vf_explained_var: -0.7635213136672974\n",
      "          vf_loss: 0.0002491248732743164\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70666666666666\n",
      "    ram_util_percent: 64.53333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041316359751515215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089174657961797\n",
      "    mean_inference_ms: 1.4053847026007447\n",
      "    mean_raw_obs_processing_ms: 0.8483670666168159\n",
      "  time_since_restore: 4052.718781232834\n",
      "  time_this_iter_s: 10.357899904251099\n",
      "  time_total_s: 4052.718781232834\n",
      "  timers:\n",
      "    learn_throughput: 1721.551\n",
      "    learn_time_ms: 580.871\n",
      "    load_throughput: 216617.724\n",
      "    load_time_ms: 4.616\n",
      "    sample_throughput: 85.881\n",
      "    sample_time_ms: 11644.086\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632128875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         4052.72</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 371\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7838236742549471\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008548316015033643\n",
      "          policy_loss: -0.027441255665487715\n",
      "          total_loss: -0.044171701040532856\n",
      "          vf_explained_var: -0.5473139882087708\n",
      "          vf_loss: 0.0011077897697557798\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.63999999999999\n",
      "    ram_util_percent: 64.40666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04131546883959006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088765751892392\n",
      "    mean_inference_ms: 1.4053501658542837\n",
      "    mean_raw_obs_processing_ms: 0.8482810842414882\n",
      "  time_since_restore: 4062.934862613678\n",
      "  time_this_iter_s: 10.216081380844116\n",
      "  time_total_s: 4062.934862613678\n",
      "  timers:\n",
      "    learn_throughput: 1722.955\n",
      "    learn_time_ms: 580.398\n",
      "    load_throughput: 216587.522\n",
      "    load_time_ms: 4.617\n",
      "    sample_throughput: 85.969\n",
      "    sample_time_ms: 11632.039\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632128885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         4062.93</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 372\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8879713773727418\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01686876286866238\n",
      "          policy_loss: -0.04249337104459604\n",
      "          total_loss: -0.061081312720974286\n",
      "          vf_explained_var: -0.914550244808197\n",
      "          vf_loss: 0.0002917717775239402\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87857142857143\n",
      "    ram_util_percent: 64.30714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04131458663573657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088387861477242\n",
      "    mean_inference_ms: 1.405315808424258\n",
      "    mean_raw_obs_processing_ms: 0.8482025187926788\n",
      "  time_since_restore: 4073.2488675117493\n",
      "  time_this_iter_s: 10.314004898071289\n",
      "  time_total_s: 4073.2488675117493\n",
      "  timers:\n",
      "    learn_throughput: 1723.16\n",
      "    learn_time_ms: 580.329\n",
      "    load_throughput: 216327.242\n",
      "    load_time_ms: 4.623\n",
      "    sample_throughput: 85.906\n",
      "    sample_time_ms: 11640.586\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632128895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4073.25</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 373\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1834172507127125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010065540335926887\n",
      "          policy_loss: -0.03541559477647146\n",
      "          total_loss: -0.04293858036398888\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.004311185464676883\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.666666666666664\n",
      "    ram_util_percent: 64.21333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041313709567527654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088046007599708\n",
      "    mean_inference_ms: 1.4052818364654096\n",
      "    mean_raw_obs_processing_ms: 0.8481300386146075\n",
      "  time_since_restore: 4083.378923892975\n",
      "  time_this_iter_s: 10.130056381225586\n",
      "  time_total_s: 4083.378923892975\n",
      "  timers:\n",
      "    learn_throughput: 1720.606\n",
      "    learn_time_ms: 581.191\n",
      "    load_throughput: 216638.982\n",
      "    load_time_ms: 4.616\n",
      "    sample_throughput: 86.015\n",
      "    sample_time_ms: 11625.871\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632128905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         4083.38</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 374\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8831527670224508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008604920252512511\n",
      "          policy_loss: -0.05704192680617173\n",
      "          total_loss: -0.07541844488845931\n",
      "          vf_explained_var: -0.8026520609855652\n",
      "          vf_loss: 0.0004550106084530449\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.028571428571446\n",
      "    ram_util_percent: 64.12142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041312834579325886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087749529040279\n",
      "    mean_inference_ms: 1.4052481604638356\n",
      "    mean_raw_obs_processing_ms: 0.848064044882472\n",
      "  time_since_restore: 4093.6524307727814\n",
      "  time_this_iter_s: 10.273506879806519\n",
      "  time_total_s: 4093.6524307727814\n",
      "  timers:\n",
      "    learn_throughput: 1721.003\n",
      "    learn_time_ms: 581.057\n",
      "    load_throughput: 215806.334\n",
      "    load_time_ms: 4.634\n",
      "    sample_throughput: 86.106\n",
      "    sample_time_ms: 11613.569\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632128916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         4093.65</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 375\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.262693785296546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01515705861166246\n",
      "          policy_loss: -0.05432668384164572\n",
      "          total_loss: -0.06453993349439568\n",
      "          vf_explained_var: -0.6152443885803223\n",
      "          vf_loss: 0.0024136875882201517\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.39333333333333\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0413119660278765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087484202309463\n",
      "    mean_inference_ms: 1.4052148060237468\n",
      "    mean_raw_obs_processing_ms: 0.8480044337079715\n",
      "  time_since_restore: 4103.722919225693\n",
      "  time_this_iter_s: 10.070488452911377\n",
      "  time_total_s: 4103.722919225693\n",
      "  timers:\n",
      "    learn_throughput: 1720.563\n",
      "    learn_time_ms: 581.205\n",
      "    load_throughput: 215496.984\n",
      "    load_time_ms: 4.64\n",
      "    sample_throughput: 86.278\n",
      "    sample_time_ms: 11590.428\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1632128926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         4103.72</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 376\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0041294627719455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019824417719006386\n",
      "          policy_loss: -0.025136521955331167\n",
      "          total_loss: -0.044563160671128164\n",
      "          vf_explained_var: -0.7615750432014465\n",
      "          vf_loss: 0.0006146559462649748\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.62\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041311104663693116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087268114930787\n",
      "    mean_inference_ms: 1.4051818516710783\n",
      "    mean_raw_obs_processing_ms: 0.847326102902601\n",
      "  time_since_restore: 4114.160555839539\n",
      "  time_this_iter_s: 10.437636613845825\n",
      "  time_total_s: 4114.160555839539\n",
      "  timers:\n",
      "    learn_throughput: 1721.099\n",
      "    learn_time_ms: 581.024\n",
      "    load_throughput: 317723.826\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 101.182\n",
      "    sample_time_ms: 9883.22\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632128936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         4114.16</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 377\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9977169659402636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01244499084020752\n",
      "          policy_loss: 0.0011213242179817623\n",
      "          total_loss: -0.017559928033086987\n",
      "          vf_explained_var: -0.19856683909893036\n",
      "          vf_loss: 0.0012959182566393995\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.64666666666666\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04131023807774155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087024919830387\n",
      "    mean_inference_ms: 1.405148766711597\n",
      "    mean_raw_obs_processing_ms: 0.8466564032171209\n",
      "  time_since_restore: 4124.479876995087\n",
      "  time_this_iter_s: 10.319321155548096\n",
      "  time_total_s: 4124.479876995087\n",
      "  timers:\n",
      "    learn_throughput: 1721.84\n",
      "    learn_time_ms: 580.774\n",
      "    load_throughput: 317401.642\n",
      "    load_time_ms: 3.151\n",
      "    sample_throughput: 103.135\n",
      "    sample_time_ms: 9696.054\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1632128947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         4124.48</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 378\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0742574863963656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011141728384185147\n",
      "          policy_loss: -0.021854462433192466\n",
      "          total_loss: -0.041986641743116906\n",
      "          vf_explained_var: -0.5035164952278137\n",
      "          vf_loss: 0.0006103945512829038\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.89999999999999\n",
      "    ram_util_percent: 64.09285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130936573897388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086826857957476\n",
      "    mean_inference_ms: 1.4051155934961526\n",
      "    mean_raw_obs_processing_ms: 0.8459957362851867\n",
      "  time_since_restore: 4134.79470205307\n",
      "  time_this_iter_s: 10.314825057983398\n",
      "  time_total_s: 4134.79470205307\n",
      "  timers:\n",
      "    learn_throughput: 1722.344\n",
      "    learn_time_ms: 580.604\n",
      "    load_throughput: 316567.97\n",
      "    load_time_ms: 3.159\n",
      "    sample_throughput: 103.169\n",
      "    sample_time_ms: 9692.853\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632128957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         4134.79</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 379\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9896836837132772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016999348225088796\n",
      "          policy_loss: -0.022110924797339573\n",
      "          total_loss: -0.04134418129300078\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006635789892041228\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66\n",
      "    ram_util_percent: 64.09333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130849460212563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08666925776882\n",
      "    mean_inference_ms: 1.4050827679250801\n",
      "    mean_raw_obs_processing_ms: 0.8453440926053136\n",
      "  time_since_restore: 4145.116405963898\n",
      "  time_this_iter_s: 10.321703910827637\n",
      "  time_total_s: 4145.116405963898\n",
      "  timers:\n",
      "    learn_throughput: 1723.282\n",
      "    learn_time_ms: 580.288\n",
      "    load_throughput: 316761.623\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 103.23\n",
      "    sample_time_ms: 9687.14\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632128967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         4145.12</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 380\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9655896716647678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013959467392985264\n",
      "          policy_loss: 0.0073762545569075475\n",
      "          total_loss: -0.011407380054394404\n",
      "          vf_explained_var: -0.8866796493530273\n",
      "          vf_loss: 0.0008722611450745414\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72\n",
      "    ram_util_percent: 64.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130762825931748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.0865355519733\n",
      "    mean_inference_ms: 1.4050502541143763\n",
      "    mean_raw_obs_processing_ms: 0.8447010947824364\n",
      "  time_since_restore: 4155.229012727737\n",
      "  time_this_iter_s: 10.112606763839722\n",
      "  time_total_s: 4155.229012727737\n",
      "  timers:\n",
      "    learn_throughput: 1723.753\n",
      "    learn_time_ms: 580.129\n",
      "    load_throughput: 316857.341\n",
      "    load_time_ms: 3.156\n",
      "    sample_throughput: 103.49\n",
      "    sample_time_ms: 9662.764\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632128977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         4155.23</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 381\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1679245948791506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010193247935443904\n",
      "          policy_loss: -0.05607626001454062\n",
      "          total_loss: -0.07728864571286573\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004668598362412821\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.792857142857144\n",
      "    ram_util_percent: 64.12857142857145\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130676559811725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086430005724056\n",
      "    mean_inference_ms: 1.4050182331885979\n",
      "    mean_raw_obs_processing_ms: 0.8440666215019325\n",
      "  time_since_restore: 4165.491199493408\n",
      "  time_this_iter_s: 10.262186765670776\n",
      "  time_total_s: 4165.491199493408\n",
      "  timers:\n",
      "    learn_throughput: 1722.251\n",
      "    learn_time_ms: 580.636\n",
      "    load_throughput: 316305.363\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 103.446\n",
      "    sample_time_ms: 9666.88\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632128988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         4165.49</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 382\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1276786539289687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014007082463956759\n",
      "          policy_loss: 0.07180367733041446\n",
      "          total_loss: 0.05156209899319543\n",
      "          vf_explained_var: -0.7932374477386475\n",
      "          vf_loss: 0.001035207939437694\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.573333333333345\n",
      "    ram_util_percent: 64.22\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041305909162803356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086351783552344\n",
      "    mean_inference_ms: 1.4049866315881883\n",
      "    mean_raw_obs_processing_ms: 0.8434408842487574\n",
      "  time_since_restore: 4175.7887403965\n",
      "  time_this_iter_s: 10.29754090309143\n",
      "  time_total_s: 4175.7887403965\n",
      "  timers:\n",
      "    learn_throughput: 1721.414\n",
      "    learn_time_ms: 580.918\n",
      "    load_throughput: 316759.231\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 103.467\n",
      "    sample_time_ms: 9664.961\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632128998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         4175.79</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-10-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 383\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1272888739903766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01321182049193668\n",
      "          policy_loss: -0.005614537476665444\n",
      "          total_loss: -0.0261874218367868\n",
      "          vf_explained_var: -0.16491971909999847\n",
      "          vf_loss: 0.0007000008804930581\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.04\n",
      "    ram_util_percent: 64.27333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130505852036558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086306785237477\n",
      "    mean_inference_ms: 1.40495538370875\n",
      "    mean_raw_obs_processing_ms: 0.8428237757520253\n",
      "  time_since_restore: 4186.07533788681\n",
      "  time_this_iter_s: 10.286597490310669\n",
      "  time_total_s: 4186.07533788681\n",
      "  timers:\n",
      "    learn_throughput: 1722.882\n",
      "    learn_time_ms: 580.423\n",
      "    load_throughput: 316105.12\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 103.294\n",
      "    sample_time_ms: 9681.064\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1632129008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         4186.08</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 384\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.12733338409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018021790656041696\n",
      "          policy_loss: -0.04921826480163468\n",
      "          total_loss: -0.06978708472516802\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007045122461729786\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.81333333333334\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041304214573910404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086285918347372\n",
      "    mean_inference_ms: 1.4049242920771048\n",
      "    mean_raw_obs_processing_ms: 0.8422151972589837\n",
      "  time_since_restore: 4196.447505712509\n",
      "  time_this_iter_s: 10.372167825698853\n",
      "  time_total_s: 4196.447505712509\n",
      "  timers:\n",
      "    learn_throughput: 1722.455\n",
      "    learn_time_ms: 580.567\n",
      "    load_throughput: 316329.218\n",
      "    load_time_ms: 3.161\n",
      "    sample_throughput: 103.19\n",
      "    sample_time_ms: 9690.845\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1632129019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         4196.45</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 385\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0930887725618152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009002141618687896\n",
      "          policy_loss: -0.00790451533264584\n",
      "          total_loss: -0.027915518689486715\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009198867153221121\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95\n",
      "    ram_util_percent: 64.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130336781400988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086288078530263\n",
      "    mean_inference_ms: 1.4048933283649823\n",
      "    mean_raw_obs_processing_ms: 0.8416150402612318\n",
      "  time_since_restore: 4206.704664230347\n",
      "  time_this_iter_s: 10.257158517837524\n",
      "  time_total_s: 4206.704664230347\n",
      "  timers:\n",
      "    learn_throughput: 1721.009\n",
      "    learn_time_ms: 581.054\n",
      "    load_throughput: 316948.328\n",
      "    load_time_ms: 3.155\n",
      "    sample_throughput: 102.997\n",
      "    sample_time_ms: 9709.039\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632129029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">          4206.7</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 386\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.183514263894823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011655875579318142\n",
      "          policy_loss: -0.015509348776605394\n",
      "          total_loss: -0.03652883134782314\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008156601145553092\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.586666666666666\n",
      "    ram_util_percent: 64.37333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130252972962327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086314284757382\n",
      "    mean_inference_ms: 1.404862813659546\n",
      "    mean_raw_obs_processing_ms: 0.8410232428847293\n",
      "  time_since_restore: 4216.942412853241\n",
      "  time_this_iter_s: 10.237748622894287\n",
      "  time_total_s: 4216.942412853241\n",
      "  timers:\n",
      "    learn_throughput: 1720.289\n",
      "    learn_time_ms: 581.298\n",
      "    load_throughput: 317771.969\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 103.212\n",
      "    sample_time_ms: 9688.823\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632129039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         4216.94</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 387\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1962050278981526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015049018227102057\n",
      "          policy_loss: -0.02935176950155033\n",
      "          total_loss: -0.050669442489743234\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006443746145426606\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.55333333333333\n",
      "    ram_util_percent: 64.40666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130169767330618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086362428460339\n",
      "    mean_inference_ms: 1.4048325899719674\n",
      "    mean_raw_obs_processing_ms: 0.8404396847833298\n",
      "  time_since_restore: 4227.219882011414\n",
      "  time_this_iter_s: 10.277469158172607\n",
      "  time_total_s: 4227.219882011414\n",
      "  timers:\n",
      "    learn_throughput: 1720.051\n",
      "    learn_time_ms: 581.378\n",
      "    load_throughput: 318350.828\n",
      "    load_time_ms: 3.141\n",
      "    sample_throughput: 103.257\n",
      "    sample_time_ms: 9684.573\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1632129050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         4227.22</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 388\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9845051553514268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01108235381229682\n",
      "          policy_loss: -0.01955809982286559\n",
      "          total_loss: -0.03806338699327575\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0013397639119325\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.957142857142856\n",
      "    ram_util_percent: 64.47857142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04130086611245638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086431222039517\n",
      "    mean_inference_ms: 1.404802536603643\n",
      "    mean_raw_obs_processing_ms: 0.839864300029863\n",
      "  time_since_restore: 4237.42386174202\n",
      "  time_this_iter_s: 10.203979730606079\n",
      "  time_total_s: 4237.42386174202\n",
      "  timers:\n",
      "    learn_throughput: 1721.145\n",
      "    learn_time_ms: 581.009\n",
      "    load_throughput: 318798.474\n",
      "    load_time_ms: 3.137\n",
      "    sample_throughput: 103.371\n",
      "    sample_time_ms: 9673.903\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632129060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         4237.42</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 389\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6920095403989157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01067661874949353\n",
      "          policy_loss: -0.01796481278207567\n",
      "          total_loss: -0.03184242389268345\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0030424835723048696\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.67333333333333\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041300037579939844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086518699861116\n",
      "    mean_inference_ms: 1.4047726858496639\n",
      "    mean_raw_obs_processing_ms: 0.8392970227423401\n",
      "  time_since_restore: 4247.684767961502\n",
      "  time_this_iter_s: 10.260906219482422\n",
      "  time_total_s: 4247.684767961502\n",
      "  timers:\n",
      "    learn_throughput: 1720.15\n",
      "    learn_time_ms: 581.345\n",
      "    load_throughput: 319133.214\n",
      "    load_time_ms: 3.133\n",
      "    sample_throughput: 103.44\n",
      "    sample_time_ms: 9667.481\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632129070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         4247.68</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 390\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0679650518629287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008847225883726474\n",
      "          policy_loss: -0.02650242687927352\n",
      "          total_loss: -0.04638468482428127\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007973933857606931\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.626666666666665\n",
      "    ram_util_percent: 64.51333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041299215763123155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086619352049155\n",
      "    mean_inference_ms: 1.4047431277525855\n",
      "    mean_raw_obs_processing_ms: 0.8387377693355017\n",
      "  time_since_restore: 4258.036036014557\n",
      "  time_this_iter_s: 10.35126805305481\n",
      "  time_total_s: 4258.036036014557\n",
      "  timers:\n",
      "    learn_throughput: 1719.609\n",
      "    learn_time_ms: 581.528\n",
      "    load_throughput: 318956.054\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 103.187\n",
      "    sample_time_ms: 9691.15\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632129081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         4258.04</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 391\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1893095864189998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011778919438998336\n",
      "          policy_loss: -0.01575683289104038\n",
      "          total_loss: -0.037202921095821594\n",
      "          vf_explained_var: -0.9992836117744446\n",
      "          vf_loss: 0.0004470042473662438\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87857142857142\n",
      "    ram_util_percent: 64.57142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041298396267465474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086730297561084\n",
      "    mean_inference_ms: 1.4047137910120466\n",
      "    mean_raw_obs_processing_ms: 0.8381864221522275\n",
      "  time_since_restore: 4268.324615478516\n",
      "  time_this_iter_s: 10.28857946395874\n",
      "  time_total_s: 4268.324615478516\n",
      "  timers:\n",
      "    learn_throughput: 1719.424\n",
      "    learn_time_ms: 581.59\n",
      "    load_throughput: 319235.231\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 103.159\n",
      "    sample_time_ms: 9693.737\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632129091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         4268.32</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 392\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.51615815560023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01178433627454106\n",
      "          policy_loss: 0.009203121024701331\n",
      "          total_loss: -0.004008252835936017\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.001950208559477081\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71333333333334\n",
      "    ram_util_percent: 64.60000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412975776278672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086846754030603\n",
      "    mean_inference_ms: 1.4046841764241484\n",
      "    mean_raw_obs_processing_ms: 0.8376429080916665\n",
      "  time_since_restore: 4278.613797426224\n",
      "  time_this_iter_s: 10.28918194770813\n",
      "  time_total_s: 4278.613797426224\n",
      "  timers:\n",
      "    learn_throughput: 1720.546\n",
      "    learn_time_ms: 581.211\n",
      "    load_throughput: 318454.764\n",
      "    load_time_ms: 3.14\n",
      "    sample_throughput: 103.164\n",
      "    sample_time_ms: 9693.283\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632129101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         4278.61</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-11-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 393\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.120890692869822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008948308325880502\n",
      "          policy_loss: -0.03489333970679177\n",
      "          total_loss: -0.05575164585477776\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003506001143250614\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87333333333333\n",
      "    ram_util_percent: 64.74666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129675923184278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086976280295321\n",
      "    mean_inference_ms: 1.4046548650355783\n",
      "    mean_raw_obs_processing_ms: 0.837106887144263\n",
      "  time_since_restore: 4288.977060317993\n",
      "  time_this_iter_s: 10.36326289176941\n",
      "  time_total_s: 4288.977060317993\n",
      "  timers:\n",
      "    learn_throughput: 1721.647\n",
      "    learn_time_ms: 580.839\n",
      "    load_throughput: 318859.063\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 103.079\n",
      "    sample_time_ms: 9701.328\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         4288.98</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-12-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 394\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0590346309873793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008891718271663823\n",
      "          policy_loss: -0.023455757151047387\n",
      "          total_loss: -0.043687461606330344\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003586399415022849\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71333333333333\n",
      "    ram_util_percent: 64.99333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041295947717296035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08710869380152\n",
      "    mean_inference_ms: 1.4046257645831013\n",
      "    mean_raw_obs_processing_ms: 0.8365785335825674\n",
      "  time_since_restore: 4299.249499320984\n",
      "  time_this_iter_s: 10.272439002990723\n",
      "  time_total_s: 4299.249499320984\n",
      "  timers:\n",
      "    learn_throughput: 1720.658\n",
      "    learn_time_ms: 581.173\n",
      "    load_throughput: 320144.108\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 103.188\n",
      "    sample_time_ms: 9691.013\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632129122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         4299.25</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 395\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5141222463713753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01793582571966519\n",
      "          policy_loss: 0.014688601013686922\n",
      "          total_loss: 0.0011882549358738794\n",
      "          vf_explained_var: -0.8888610005378723\n",
      "          vf_loss: 0.0016408760055330479\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.08571428571429\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041295136869447954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087248183481249\n",
      "    mean_inference_ms: 1.4045965571180254\n",
      "    mean_raw_obs_processing_ms: 0.8360577551665169\n",
      "  time_since_restore: 4309.369588375092\n",
      "  time_this_iter_s: 10.120089054107666\n",
      "  time_total_s: 4309.369588375092\n",
      "  timers:\n",
      "    learn_throughput: 1722.107\n",
      "    learn_time_ms: 580.684\n",
      "    load_throughput: 319310.571\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 103.329\n",
      "    sample_time_ms: 9677.808\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632129132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         4309.37</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 396\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.127577112780677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015793868331261347\n",
      "          policy_loss: -0.010362546932366158\n",
      "          total_loss: -0.030955348246627382\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006829679371245826\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.274358974358975\n",
      "    ram_util_percent: 64.57179487179489\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129433690151936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087400165439275\n",
      "    mean_inference_ms: 1.4045675941579228\n",
      "    mean_raw_obs_processing_ms: 0.8359740850405472\n",
      "  time_since_restore: 4336.743382930756\n",
      "  time_this_iter_s: 27.373794555664062\n",
      "  time_total_s: 4336.743382930756\n",
      "  timers:\n",
      "    learn_throughput: 1720.995\n",
      "    learn_time_ms: 581.059\n",
      "    load_throughput: 202522.61\n",
      "    load_time_ms: 4.938\n",
      "    sample_throughput: 87.802\n",
      "    sample_time_ms: 11389.217\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632129159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         4336.74</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 397\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7617388248443604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007103509349581769\n",
      "          policy_loss: -0.008674983431895574\n",
      "          total_loss: -0.02496060993936327\n",
      "          vf_explained_var: -0.5118249654769897\n",
      "          vf_loss: 0.0013317615550477058\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26470588235295\n",
      "    ram_util_percent: 64.98235294117646\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129355051556532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087609121311433\n",
      "    mean_inference_ms: 1.404539112567372\n",
      "    mean_raw_obs_processing_ms: 0.8358968289318176\n",
      "  time_since_restore: 4348.819228410721\n",
      "  time_this_iter_s: 12.07584547996521\n",
      "  time_total_s: 4348.819228410721\n",
      "  timers:\n",
      "    learn_throughput: 1722.419\n",
      "    learn_time_ms: 580.579\n",
      "    load_throughput: 202560.754\n",
      "    load_time_ms: 4.937\n",
      "    sample_throughput: 86.434\n",
      "    sample_time_ms: 11569.515\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632129171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         4348.82</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 398\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.842695865366194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0190077024573955\n",
      "          policy_loss: -0.01963281962606642\n",
      "          total_loss: -0.03584142393536038\n",
      "          vf_explained_var: -0.357391357421875\n",
      "          vf_loss: 0.002218351809359673\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.800000000000004\n",
      "    ram_util_percent: 65.02666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129277075509332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087819235879472\n",
      "    mean_inference_ms: 1.4045109245434046\n",
      "    mean_raw_obs_processing_ms: 0.8358258822096687\n",
      "  time_since_restore: 4359.181033372879\n",
      "  time_this_iter_s: 10.361804962158203\n",
      "  time_total_s: 4359.181033372879\n",
      "  timers:\n",
      "    learn_throughput: 1721.615\n",
      "    learn_time_ms: 580.85\n",
      "    load_throughput: 202923.355\n",
      "    load_time_ms: 4.928\n",
      "    sample_throughput: 86.318\n",
      "    sample_time_ms: 11585.03\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632129182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         4359.18</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 399\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.887318774064382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015217271962013355\n",
      "          policy_loss: 0.042066564452317025\n",
      "          total_loss: 0.02390293065044615\n",
      "          vf_explained_var: -0.012113730423152447\n",
      "          vf_loss: 0.0007095546047720644\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92142857142857\n",
      "    ram_util_percent: 64.99999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129199989000529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088017603155668\n",
      "    mean_inference_ms: 1.4044830303425595\n",
      "    mean_raw_obs_processing_ms: 0.8357611470491447\n",
      "  time_since_restore: 4369.068667411804\n",
      "  time_this_iter_s: 9.887634038925171\n",
      "  time_total_s: 4369.068667411804\n",
      "  timers:\n",
      "    learn_throughput: 1721.744\n",
      "    learn_time_ms: 580.806\n",
      "    load_throughput: 202680.171\n",
      "    load_time_ms: 4.934\n",
      "    sample_throughput: 86.597\n",
      "    sample_time_ms: 11547.755\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632129192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         4369.07</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 400\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.248635032441881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011649609874276488\n",
      "          policy_loss: -0.04934788718819618\n",
      "          total_loss: -0.07101227976381778\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008219565410399809\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.779999999999994\n",
      "    ram_util_percent: 64.91333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04129124136557307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08821620603615\n",
      "    mean_inference_ms: 1.404455307890027\n",
      "    mean_raw_obs_processing_ms: 0.8357025626243069\n",
      "  time_since_restore: 4379.322667360306\n",
      "  time_this_iter_s: 10.253999948501587\n",
      "  time_total_s: 4379.322667360306\n",
      "  timers:\n",
      "    learn_throughput: 1722.482\n",
      "    learn_time_ms: 580.558\n",
      "    load_throughput: 203033.372\n",
      "    load_time_ms: 4.925\n",
      "    sample_throughput: 86.668\n",
      "    sample_time_ms: 11538.296\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632129202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         4379.32</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 401\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8245564301808674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010889118865228713\n",
      "          policy_loss: -0.04277408296863238\n",
      "          total_loss: -0.06005518306046724\n",
      "          vf_explained_var: -0.3481798470020294\n",
      "          vf_loss: 0.0009644647471658472\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95\n",
      "    ram_util_percent: 64.7857142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041290492675037795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08840500004057\n",
      "    mean_inference_ms: 1.4044278544219042\n",
      "    mean_raw_obs_processing_ms: 0.835650074091469\n",
      "  time_since_restore: 4389.263936519623\n",
      "  time_this_iter_s: 9.941269159317017\n",
      "  time_total_s: 4389.263936519623\n",
      "  timers:\n",
      "    learn_throughput: 1723.039\n",
      "    learn_time_ms: 580.37\n",
      "    load_throughput: 202821.304\n",
      "    load_time_ms: 4.93\n",
      "    sample_throughput: 86.928\n",
      "    sample_time_ms: 11503.712\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632129212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         4389.26</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 402\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.88220444255405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012566795575613732\n",
      "          policy_loss: -0.03787773030085696\n",
      "          total_loss: -0.056046359530753556\n",
      "          vf_explained_var: -0.586470901966095\n",
      "          vf_loss: 0.0006534134742752131\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.64\n",
      "    ram_util_percent: 64.66666666666669\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128974875841467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08859473317534\n",
      "    mean_inference_ms: 1.4044008344228265\n",
      "    mean_raw_obs_processing_ms: 0.8356033569317186\n",
      "  time_since_restore: 4399.333008050919\n",
      "  time_this_iter_s: 10.069071531295776\n",
      "  time_total_s: 4399.333008050919\n",
      "  timers:\n",
      "    learn_throughput: 1722.702\n",
      "    learn_time_ms: 580.483\n",
      "    load_throughput: 202956.741\n",
      "    load_time_ms: 4.927\n",
      "    sample_throughput: 87.096\n",
      "    sample_time_ms: 11481.573\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1632129222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         4399.33</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-13-52\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 403\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7362230446603564\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0076764311162143195\n",
      "          policy_loss: -0.002258036761648125\n",
      "          total_loss: -0.018769424657026926\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008508415863616392\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07142857142857\n",
      "    ram_util_percent: 64.56428571428572\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128900574577387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08879044401459\n",
      "    mean_inference_ms: 1.4043740737503287\n",
      "    mean_raw_obs_processing_ms: 0.8355621708002591\n",
      "  time_since_restore: 4409.4081308841705\n",
      "  time_this_iter_s: 10.075122833251953\n",
      "  time_total_s: 4409.4081308841705\n",
      "  timers:\n",
      "    learn_throughput: 1722.548\n",
      "    learn_time_ms: 580.535\n",
      "    load_throughput: 202239.42\n",
      "    load_time_ms: 4.945\n",
      "    sample_throughput: 87.316\n",
      "    sample_time_ms: 11452.7\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632129232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         4409.41</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 404\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5239896880255805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007638299864328271\n",
      "          policy_loss: -0.13202940714028147\n",
      "          total_loss: -0.1464924776719676\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007768257249457141\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88571428571429\n",
      "    ram_util_percent: 64.52142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128827148306863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08898558929776\n",
      "    mean_inference_ms: 1.4043475202075708\n",
      "    mean_raw_obs_processing_ms: 0.8355266018167259\n",
      "  time_since_restore: 4419.423124790192\n",
      "  time_this_iter_s: 10.014993906021118\n",
      "  time_total_s: 4419.423124790192\n",
      "  timers:\n",
      "    learn_throughput: 1724.906\n",
      "    learn_time_ms: 579.742\n",
      "    load_throughput: 202380.916\n",
      "    load_time_ms: 4.941\n",
      "    sample_throughput: 87.506\n",
      "    sample_time_ms: 11427.747\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632129242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         4419.42</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 405\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.153659506638845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014773792924781612\n",
      "          policy_loss: -0.041073141247034074\n",
      "          total_loss: -0.06176530093782478\n",
      "          vf_explained_var: -0.8249997496604919\n",
      "          vf_loss: 0.0008444342073441173\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82666666666667\n",
      "    ram_util_percent: 64.48666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128754320979526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089186618217779\n",
      "    mean_inference_ms: 1.4043215041523056\n",
      "    mean_raw_obs_processing_ms: 0.835496572525578\n",
      "  time_since_restore: 4429.437235832214\n",
      "  time_this_iter_s: 10.014111042022705\n",
      "  time_total_s: 4429.437235832214\n",
      "  timers:\n",
      "    learn_throughput: 1725.804\n",
      "    learn_time_ms: 579.44\n",
      "    load_throughput: 202492.3\n",
      "    load_time_ms: 4.938\n",
      "    sample_throughput: 87.585\n",
      "    sample_time_ms: 11417.438\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632129252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         4429.44</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 406\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8758321907785205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014594222640385773\n",
      "          policy_loss: -0.023162182172139487\n",
      "          total_loss: -0.04119448024365637\n",
      "          vf_explained_var: -0.6061234474182129\n",
      "          vf_loss: 0.0007260251146767082\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.89999999999999\n",
      "    ram_util_percent: 64.47142857142856\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128681931071682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08938360112423\n",
      "    mean_inference_ms: 1.4042953281564623\n",
      "    mean_raw_obs_processing_ms: 0.8349145676128596\n",
      "  time_since_restore: 4439.599681377411\n",
      "  time_this_iter_s: 10.162445545196533\n",
      "  time_total_s: 4439.599681377411\n",
      "  timers:\n",
      "    learn_throughput: 1725.358\n",
      "    learn_time_ms: 579.59\n",
      "    load_throughput: 319031.262\n",
      "    load_time_ms: 3.134\n",
      "    sample_throughput: 103.114\n",
      "    sample_time_ms: 9697.958\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632129262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">          4439.6</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 407\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9603298015064663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014249025497860614\n",
      "          policy_loss: -0.08462123697002728\n",
      "          total_loss: -0.10354243737335006\n",
      "          vf_explained_var: -0.6235842108726501\n",
      "          vf_loss: 0.0006820975599111989\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92857142857142\n",
      "    ram_util_percent: 64.47857142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128610302546434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08957468989136\n",
      "    mean_inference_ms: 1.4042693405085975\n",
      "    mean_raw_obs_processing_ms: 0.8343400400088141\n",
      "  time_since_restore: 4449.423554897308\n",
      "  time_this_iter_s: 9.823873519897461\n",
      "  time_total_s: 4449.423554897308\n",
      "  timers:\n",
      "    learn_throughput: 1726.271\n",
      "    learn_time_ms: 579.283\n",
      "    load_throughput: 319067.666\n",
      "    load_time_ms: 3.134\n",
      "    sample_throughput: 105.562\n",
      "    sample_time_ms: 9473.113\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632129272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         4449.42</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 408\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7175604343414306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008648766008316539\n",
      "          policy_loss: 0.0005195174780156877\n",
      "          total_loss: -0.015671607934766345\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000984480088421454\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63333333333333\n",
      "    ram_util_percent: 64.50666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128539240389765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089782803130587\n",
      "    mean_inference_ms: 1.4042437156409713\n",
      "    mean_raw_obs_processing_ms: 0.8337729758833531\n",
      "  time_since_restore: 4459.542903184891\n",
      "  time_this_iter_s: 10.119348287582397\n",
      "  time_total_s: 4459.542903184891\n",
      "  timers:\n",
      "    learn_throughput: 1728.516\n",
      "    learn_time_ms: 578.531\n",
      "    load_throughput: 318476.526\n",
      "    load_time_ms: 3.14\n",
      "    sample_throughput: 105.824\n",
      "    sample_time_ms: 9449.619\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632129282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         4459.54</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 409\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.88604410621855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008752411050349067\n",
      "          policy_loss: -0.012963990287648306\n",
      "          total_loss: -0.031180314264363712\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006441161637970557\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.792857142857144\n",
      "    ram_util_percent: 64.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128468920994889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089986209114054\n",
      "    mean_inference_ms: 1.4042181348938185\n",
      "    mean_raw_obs_processing_ms: 0.8332132317713641\n",
      "  time_since_restore: 4469.5950310230255\n",
      "  time_this_iter_s: 10.052127838134766\n",
      "  time_total_s: 4469.5950310230255\n",
      "  timers:\n",
      "    learn_throughput: 1728.96\n",
      "    learn_time_ms: 578.383\n",
      "    load_throughput: 318718.531\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 105.639\n",
      "    sample_time_ms: 9466.208\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632129293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">          4469.6</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 410\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8149643659591674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01414745965735734\n",
      "          policy_loss: 0.019650072211192714\n",
      "          total_loss: 0.0023350149186121094\n",
      "          vf_explained_var: -0.822951078414917\n",
      "          vf_loss: 0.000834584978616072\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07142857142858\n",
      "    ram_util_percent: 64.50714285714285\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041283997190314066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090174288468475\n",
      "    mean_inference_ms: 1.4041922183909017\n",
      "    mean_raw_obs_processing_ms: 0.8326603449893207\n",
      "  time_since_restore: 4479.489777088165\n",
      "  time_this_iter_s: 9.89474606513977\n",
      "  time_total_s: 4479.489777088165\n",
      "  timers:\n",
      "    learn_throughput: 1729.059\n",
      "    learn_time_ms: 578.349\n",
      "    load_throughput: 317776.784\n",
      "    load_time_ms: 3.147\n",
      "    sample_throughput: 106.041\n",
      "    sample_time_ms: 9430.321\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         4479.49</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 411\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8500281651814778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00927901399502375\n",
      "          policy_loss: -0.0012130794632765982\n",
      "          total_loss: -0.018756543948418563\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009568152897473838\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.71333333333334\n",
      "    ram_util_percent: 64.60666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128331846915625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090355076693093\n",
      "    mean_inference_ms: 1.404166808497659\n",
      "    mean_raw_obs_processing_ms: 0.8321141582266932\n",
      "  time_since_restore: 4489.733150959015\n",
      "  time_this_iter_s: 10.24337387084961\n",
      "  time_total_s: 4489.733150959015\n",
      "  timers:\n",
      "    learn_throughput: 1729.158\n",
      "    learn_time_ms: 578.316\n",
      "    load_throughput: 318895.428\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 105.701\n",
      "    sample_time_ms: 9460.615\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632129313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         4489.73</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 412\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7751640677452087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013256705602378658\n",
      "          policy_loss: 0.014572377047604985\n",
      "          total_loss: -0.0024089750937289663\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007702889360694422\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92857142857143\n",
      "    ram_util_percent: 64.66428571428573\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128265294998554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090538921160926\n",
      "    mean_inference_ms: 1.4041417131671472\n",
      "    mean_raw_obs_processing_ms: 0.831574815903846\n",
      "  time_since_restore: 4499.864204883575\n",
      "  time_this_iter_s: 10.131053924560547\n",
      "  time_total_s: 4499.864204883575\n",
      "  timers:\n",
      "    learn_throughput: 1729.034\n",
      "    learn_time_ms: 578.358\n",
      "    load_throughput: 319417.566\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 105.633\n",
      "    sample_time_ms: 9466.773\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632129323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         4499.86</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 413\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9810076859262256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011751336577877226\n",
      "          policy_loss: -0.018397787503070303\n",
      "          total_loss: -0.0373637487904893\n",
      "          vf_explained_var: -0.8074290156364441\n",
      "          vf_loss: 0.0008441138123291441\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.58000000000001\n",
      "    ram_util_percent: 64.68000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128199960464969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09071835815651\n",
      "    mean_inference_ms: 1.4041167736513216\n",
      "    mean_raw_obs_processing_ms: 0.8310422801910978\n",
      "  time_since_restore: 4510.005358457565\n",
      "  time_this_iter_s: 10.141153573989868\n",
      "  time_total_s: 4510.005358457565\n",
      "  timers:\n",
      "    learn_throughput: 1728.859\n",
      "    learn_time_ms: 578.416\n",
      "    load_throughput: 321476.508\n",
      "    load_time_ms: 3.111\n",
      "    sample_throughput: 105.559\n",
      "    sample_time_ms: 9473.348\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632129333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         4510.01</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 414\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9855541825294494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014923661271975483\n",
      "          policy_loss: 0.0046419286893473735\n",
      "          total_loss: -0.014035247804390059\n",
      "          vf_explained_var: -0.9170695543289185\n",
      "          vf_loss: 0.0011783669566890846\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94285714285713\n",
      "    ram_util_percent: 64.70714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041281357217203836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090890140794977\n",
      "    mean_inference_ms: 1.4040921753064939\n",
      "    mean_raw_obs_processing_ms: 0.830516658652266\n",
      "  time_since_restore: 4519.796245098114\n",
      "  time_this_iter_s: 9.790886640548706\n",
      "  time_total_s: 4519.796245098114\n",
      "  timers:\n",
      "    learn_throughput: 1727.591\n",
      "    learn_time_ms: 578.841\n",
      "    load_throughput: 321230.298\n",
      "    load_time_ms: 3.113\n",
      "    sample_throughput: 105.814\n",
      "    sample_time_ms: 9450.523\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632129343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">          4519.8</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 415\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9512968500455221\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011033304839286847\n",
      "          policy_loss: 0.038532055252128175\n",
      "          total_loss: 0.01989890717797809\n",
      "          vf_explained_var: -0.8815438151359558\n",
      "          vf_loss: 0.0008798206128024807\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 64.70714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128072133803983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091053000187905\n",
      "    mean_inference_ms: 1.4040678417160368\n",
      "    mean_raw_obs_processing_ms: 0.8299979369135936\n",
      "  time_since_restore: 4529.878953456879\n",
      "  time_this_iter_s: 10.082708358764648\n",
      "  time_total_s: 4529.878953456879\n",
      "  timers:\n",
      "    learn_throughput: 1726.554\n",
      "    learn_time_ms: 579.188\n",
      "    load_throughput: 320601.715\n",
      "    load_time_ms: 3.119\n",
      "    sample_throughput: 105.741\n",
      "    sample_time_ms: 9457.036\n",
      "    update_time_ms: 1.625\n",
      "  timestamp: 1632129353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         4529.88</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 416\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0534922348128424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012572720858917648\n",
      "          policy_loss: 0.07020752835604879\n",
      "          total_loss: 0.05013747604356872\n",
      "          vf_explained_var: -0.7662250995635986\n",
      "          vf_loss: 0.0004648681733265726\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.62666666666666\n",
      "    ram_util_percent: 64.78\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04128009631320001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091212719079273\n",
      "    mean_inference_ms: 1.4040436349665006\n",
      "    mean_raw_obs_processing_ms: 0.8294857570353763\n",
      "  time_since_restore: 4539.837486028671\n",
      "  time_this_iter_s: 9.958532571792603\n",
      "  time_total_s: 4539.837486028671\n",
      "  timers:\n",
      "    learn_throughput: 1727.714\n",
      "    learn_time_ms: 578.8\n",
      "    load_throughput: 320805.244\n",
      "    load_time_ms: 3.117\n",
      "    sample_throughput: 105.965\n",
      "    sample_time_ms: 9437.039\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632129363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         4539.84</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 417\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7658489227294922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008979541021363453\n",
      "          policy_loss: -0.04591336213052273\n",
      "          total_loss: -0.062014836859371925\n",
      "          vf_explained_var: -0.7272603511810303\n",
      "          vf_loss: 0.0015570134092639718\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 64.80714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041279480145229724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091372947144187\n",
      "    mean_inference_ms: 1.4040196119862178\n",
      "    mean_raw_obs_processing_ms: 0.8289803215226128\n",
      "  time_since_restore: 4550.069847583771\n",
      "  time_this_iter_s: 10.232361555099487\n",
      "  time_total_s: 4550.069847583771\n",
      "  timers:\n",
      "    learn_throughput: 1726.822\n",
      "    learn_time_ms: 579.099\n",
      "    load_throughput: 319787.739\n",
      "    load_time_ms: 3.127\n",
      "    sample_throughput: 105.512\n",
      "    sample_time_ms: 9477.557\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         4550.07</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 418\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2604023377100626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016887869376378577\n",
      "          policy_loss: 0.02624931074678898\n",
      "          total_loss: 0.004148087567753262\n",
      "          vf_explained_var: -0.5186280012130737\n",
      "          vf_loss: 0.0005027988024974345\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.05714285714286\n",
      "    ram_util_percent: 64.85714285714285\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041278870097163065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091518793564017\n",
      "    mean_inference_ms: 1.4039957734259945\n",
      "    mean_raw_obs_processing_ms: 0.8284815754364174\n",
      "  time_since_restore: 4559.976639986038\n",
      "  time_this_iter_s: 9.906792402267456\n",
      "  time_total_s: 4559.976639986038\n",
      "  timers:\n",
      "    learn_throughput: 1724.545\n",
      "    learn_time_ms: 579.863\n",
      "    load_throughput: 320214.988\n",
      "    load_time_ms: 3.123\n",
      "    sample_throughput: 105.758\n",
      "    sample_time_ms: 9455.51\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632129383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         4559.98</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 419\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9015836225615608\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01033781954703825\n",
      "          policy_loss: -0.05674770399928093\n",
      "          total_loss: -0.07474231637186474\n",
      "          vf_explained_var: -0.9916958212852478\n",
      "          vf_loss: 0.001021224367690997\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.606666666666676\n",
      "    ram_util_percent: 64.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127827018694694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091660824894419\n",
      "    mean_inference_ms: 1.4039721832493692\n",
      "    mean_raw_obs_processing_ms: 0.8279894622303625\n",
      "  time_since_restore: 4570.288822174072\n",
      "  time_this_iter_s: 10.312182188034058\n",
      "  time_total_s: 4570.288822174072\n",
      "  timers:\n",
      "    learn_throughput: 1723.954\n",
      "    learn_time_ms: 580.062\n",
      "    load_throughput: 319963.383\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 105.471\n",
      "    sample_time_ms: 9481.321\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         4570.29</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 420\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2653036726845635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009037512351818627\n",
      "          policy_loss: 0.017232450884249476\n",
      "          total_loss: -0.0049455128610134125\n",
      "          vf_explained_var: -0.7787889838218689\n",
      "          vf_loss: 0.00047507262335986725\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.83571428571429\n",
      "    ram_util_percent: 64.96428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041277682167440856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09179024739722\n",
      "    mean_inference_ms: 1.4039489223174326\n",
      "    mean_raw_obs_processing_ms: 0.827503871853436\n",
      "  time_since_restore: 4580.233345985413\n",
      "  time_this_iter_s: 9.944523811340332\n",
      "  time_total_s: 4580.233345985413\n",
      "  timers:\n",
      "    learn_throughput: 1724.868\n",
      "    learn_time_ms: 579.755\n",
      "    load_throughput: 320447.402\n",
      "    load_time_ms: 3.121\n",
      "    sample_throughput: 105.412\n",
      "    sample_time_ms: 9486.591\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632129403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         4580.23</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 421\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1327550027105544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014642114727824316\n",
      "          policy_loss: 0.02356784012582567\n",
      "          total_loss: 0.003103982015616364\n",
      "          vf_explained_var: -0.9752809405326843\n",
      "          vf_loss: 0.000863692108153676\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84666666666667\n",
      "    ram_util_percent: 65.00666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127710364745789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091917478741584\n",
      "    mean_inference_ms: 1.4039259865202343\n",
      "    mean_raw_obs_processing_ms: 0.8270247604440334\n",
      "  time_since_restore: 4590.507935523987\n",
      "  time_this_iter_s: 10.274589538574219\n",
      "  time_total_s: 4590.507935523987\n",
      "  timers:\n",
      "    learn_throughput: 1724.459\n",
      "    learn_time_ms: 579.892\n",
      "    load_throughput: 320320.144\n",
      "    load_time_ms: 3.122\n",
      "    sample_throughput: 105.379\n",
      "    sample_time_ms: 9489.584\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632129414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         4590.51</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-17-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 422\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6354303465949165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008617446109906623\n",
      "          policy_loss: -0.038220737170841956\n",
      "          total_loss: -0.05342960419754187\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011454374393603454\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.63333333333333\n",
      "    ram_util_percent: 65.06666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412765341805664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092042828000608\n",
      "    mean_inference_ms: 1.40390323257522\n",
      "    mean_raw_obs_processing_ms: 0.8265518704234449\n",
      "  time_since_restore: 4600.762713670731\n",
      "  time_this_iter_s: 10.254778146743774\n",
      "  time_total_s: 4600.762713670731\n",
      "  timers:\n",
      "    learn_throughput: 1725.15\n",
      "    learn_time_ms: 579.66\n",
      "    load_throughput: 320114.787\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 105.239\n",
      "    sample_time_ms: 9502.196\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632129424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         4600.76</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-17-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 423\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2718797551261054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011064328333370099\n",
      "          policy_loss: -0.06528171077370644\n",
      "          total_loss: -0.08745999932289124\n",
      "          vf_explained_var: -0.19362862408161163\n",
      "          vf_loss: 0.0005405061257786454\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.02857142857143\n",
      "    ram_util_percent: 65.07857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412759687172111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092148978099054\n",
      "    mean_inference_ms: 1.4038807493153167\n",
      "    mean_raw_obs_processing_ms: 0.8260851976976024\n",
      "  time_since_restore: 4610.621111869812\n",
      "  time_this_iter_s: 9.858398199081421\n",
      "  time_total_s: 4610.621111869812\n",
      "  timers:\n",
      "    learn_throughput: 1725.04\n",
      "    learn_time_ms: 579.697\n",
      "    load_throughput: 319995.117\n",
      "    load_time_ms: 3.125\n",
      "    sample_throughput: 105.553\n",
      "    sample_time_ms: 9473.876\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632129434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         4610.62</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-17-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 424\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.26447815100352\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016040847887281984\n",
      "          policy_loss: -0.10919456978638967\n",
      "          total_loss: -0.13119404655363825\n",
      "          vf_explained_var: -0.8435884118080139\n",
      "          vf_loss: 0.0006453021714454128\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.14285714285713\n",
      "    ram_util_percent: 65.11428571428571\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041275409554291154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092237077701919\n",
      "    mean_inference_ms: 1.4038584199936424\n",
      "    mean_raw_obs_processing_ms: 0.8256244419231901\n",
      "  time_since_restore: 4620.573124170303\n",
      "  time_this_iter_s: 9.952012300491333\n",
      "  time_total_s: 4620.573124170303\n",
      "  timers:\n",
      "    learn_throughput: 1722.091\n",
      "    learn_time_ms: 580.689\n",
      "    load_throughput: 319368.923\n",
      "    load_time_ms: 3.131\n",
      "    sample_throughput: 105.385\n",
      "    sample_time_ms: 9489.006\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1632129444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         4620.57</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 425\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.418157214588589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013828412672681109\n",
      "          policy_loss: 0.059731859465440115\n",
      "          total_loss: 0.03582818251517084\n",
      "          vf_explained_var: -0.6588250398635864\n",
      "          vf_loss: 0.0002778951925898178\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.2\n",
      "    ram_util_percent: 65.23846153846155\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127486304515398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092299828485212\n",
      "    mean_inference_ms: 1.4038361356251243\n",
      "    mean_raw_obs_processing_ms: 0.8251700061792229\n",
      "  time_since_restore: 4629.975694179535\n",
      "  time_this_iter_s: 9.402570009231567\n",
      "  time_total_s: 4629.975694179535\n",
      "  timers:\n",
      "    learn_throughput: 1720.979\n",
      "    learn_time_ms: 581.065\n",
      "    load_throughput: 319873.097\n",
      "    load_time_ms: 3.126\n",
      "    sample_throughput: 106.15\n",
      "    sample_time_ms: 9420.616\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         4629.98</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 426\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3883513265185887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015359644521368824\n",
      "          policy_loss: -0.014467311857475174\n",
      "          total_loss: -0.037395282255278696\n",
      "          vf_explained_var: -0.7941173315048218\n",
      "          vf_loss: 0.0009555438301403127\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.22105263157894\n",
      "    ram_util_percent: 65.1657894736842\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041274318863676676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092330370412196\n",
      "    mean_inference_ms: 1.4038139009048651\n",
      "    mean_raw_obs_processing_ms: 0.8251210031589521\n",
      "  time_since_restore: 4655.924045562744\n",
      "  time_this_iter_s: 25.94835138320923\n",
      "  time_total_s: 4655.924045562744\n",
      "  timers:\n",
      "    learn_throughput: 1719.815\n",
      "    learn_time_ms: 581.458\n",
      "    load_throughput: 215109.06\n",
      "    load_time_ms: 4.649\n",
      "    sample_throughput: 90.763\n",
      "    sample_time_ms: 11017.668\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632129479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         4655.92</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 427\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.423251173231337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01425986056848781\n",
      "          policy_loss: -0.06936195741097133\n",
      "          total_loss: -0.09320947585834397\n",
      "          vf_explained_var: -0.7597981691360474\n",
      "          vf_loss: 0.00038499316376853837\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.82666666666667\n",
      "    ram_util_percent: 65.09333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041273790117614444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092374643411317\n",
      "    mean_inference_ms: 1.4037918384288899\n",
      "    mean_raw_obs_processing_ms: 0.8250771556398411\n",
      "  time_since_restore: 4666.990873575211\n",
      "  time_this_iter_s: 11.06682801246643\n",
      "  time_total_s: 4666.990873575211\n",
      "  timers:\n",
      "    learn_throughput: 1716.075\n",
      "    learn_time_ms: 582.725\n",
      "    load_throughput: 215255.887\n",
      "    load_time_ms: 4.646\n",
      "    sample_throughput: 90.092\n",
      "    sample_time_ms: 11099.809\n",
      "    update_time_ms: 1.635\n",
      "  timestamp: 1632129490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         4666.99</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-18-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 428\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.449131366941664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010963478689821353\n",
      "          policy_loss: 0.0709134767866797\n",
      "          total_loss: 0.047034753517558175\n",
      "          vf_explained_var: -0.729590117931366\n",
      "          vf_loss: 0.0006125893084370343\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.02142857142858\n",
      "    ram_util_percent: 65.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127327828150225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.09239705343955\n",
      "    mean_inference_ms: 1.4037700571683254\n",
      "    mean_raw_obs_processing_ms: 0.8250386477437381\n",
      "  time_since_restore: 4676.448466300964\n",
      "  time_this_iter_s: 9.457592725753784\n",
      "  time_total_s: 4676.448466300964\n",
      "  timers:\n",
      "    learn_throughput: 1715.15\n",
      "    learn_time_ms: 583.039\n",
      "    load_throughput: 215501.413\n",
      "    load_time_ms: 4.64\n",
      "    sample_throughput: 90.46\n",
      "    sample_time_ms: 11054.598\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         4676.45</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 429\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.477964260843065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013855818491856513\n",
      "          policy_loss: -0.0676032529781676\n",
      "          total_loss: -0.09202060780177514\n",
      "          vf_explained_var: -0.6305869817733765\n",
      "          vf_loss: 0.0003622862968768459\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99285714285714\n",
      "    ram_util_percent: 65.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127277140523015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092398333781023\n",
      "    mean_inference_ms: 1.4037482358386948\n",
      "    mean_raw_obs_processing_ms: 0.8250049545158217\n",
      "  time_since_restore: 4686.017431974411\n",
      "  time_this_iter_s: 9.568965673446655\n",
      "  time_total_s: 4686.017431974411\n",
      "  timers:\n",
      "    learn_throughput: 1715.823\n",
      "    learn_time_ms: 582.811\n",
      "    load_throughput: 215051.708\n",
      "    load_time_ms: 4.65\n",
      "    sample_throughput: 91.071\n",
      "    sample_time_ms: 10980.49\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632129509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         4686.02</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-18-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 430\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.343858427471585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009112387109810684\n",
      "          policy_loss: -0.10426198744939433\n",
      "          total_loss: -0.12649172983235782\n",
      "          vf_explained_var: -0.9401228427886963\n",
      "          vf_loss: 0.0012088412909684444\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1357142857143\n",
      "    ram_util_percent: 65.12142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041272274214507194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092396705889179\n",
      "    mean_inference_ms: 1.4037264630570423\n",
      "    mean_raw_obs_processing_ms: 0.8249761804633363\n",
      "  time_since_restore: 4696.092390298843\n",
      "  time_this_iter_s: 10.074958324432373\n",
      "  time_total_s: 4696.092390298843\n",
      "  timers:\n",
      "    learn_throughput: 1714.441\n",
      "    learn_time_ms: 583.28\n",
      "    load_throughput: 215272.459\n",
      "    load_time_ms: 4.645\n",
      "    sample_throughput: 90.967\n",
      "    sample_time_ms: 10993.056\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632129519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         4696.09</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-18-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 431\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9171402547094556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016752931209076824\n",
      "          policy_loss: 0.02415258079353306\n",
      "          total_loss: 0.007075648133953412\n",
      "          vf_explained_var: -0.43910983204841614\n",
      "          vf_loss: 0.002094467815348051\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.63333333333334\n",
      "    ram_util_percent: 64.95333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127177532878759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092406137021289\n",
      "    mean_inference_ms: 1.4037047139150467\n",
      "    mean_raw_obs_processing_ms: 0.8249526869365209\n",
      "  time_since_restore: 4706.786563396454\n",
      "  time_this_iter_s: 10.694173097610474\n",
      "  time_total_s: 4706.786563396454\n",
      "  timers:\n",
      "    learn_throughput: 1715.765\n",
      "    learn_time_ms: 582.831\n",
      "    load_throughput: 214909.564\n",
      "    load_time_ms: 4.653\n",
      "    sample_throughput: 90.617\n",
      "    sample_time_ms: 11035.434\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632129530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         4706.79</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 432\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.020892336280666e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1103263020515444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03103956128686652\n",
      "          policy_loss: 0.0019431047141551972\n",
      "          total_loss: -0.01776521445976363\n",
      "          vf_explained_var: -0.44215795397758484\n",
      "          vf_loss: 0.0013949446030892432\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.74666666666667\n",
      "    ram_util_percent: 64.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412712835927272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092411597970122\n",
      "    mean_inference_ms: 1.4036831218817227\n",
      "    mean_raw_obs_processing_ms: 0.8249343330465321\n",
      "  time_since_restore: 4716.835374593735\n",
      "  time_this_iter_s: 10.048811197280884\n",
      "  time_total_s: 4716.835374593735\n",
      "  timers:\n",
      "    learn_throughput: 1717.071\n",
      "    learn_time_ms: 582.387\n",
      "    load_throughput: 214829.209\n",
      "    load_time_ms: 4.655\n",
      "    sample_throughput: 90.783\n",
      "    sample_time_ms: 11015.289\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         4716.84</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-09\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 433\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.531338504421002e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3913918018341063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015975208837678816\n",
      "          policy_loss: -0.005431823949846957\n",
      "          total_loss: -0.028695551223225063\n",
      "          vf_explained_var: -0.7926995754241943\n",
      "          vf_loss: 0.0006501909013751275\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0923076923077\n",
      "    ram_util_percent: 64.76153846153845\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04127079340021542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092393664790261\n",
      "    mean_inference_ms: 1.403661531420261\n",
      "    mean_raw_obs_processing_ms: 0.8249205621292387\n",
      "  time_since_restore: 4725.952689886093\n",
      "  time_this_iter_s: 9.117315292358398\n",
      "  time_total_s: 4725.952689886093\n",
      "  timers:\n",
      "    learn_throughput: 1716.565\n",
      "    learn_time_ms: 582.559\n",
      "    load_throughput: 214575.331\n",
      "    load_time_ms: 4.66\n",
      "    sample_throughput: 91.399\n",
      "    sample_time_ms: 10941.007\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632129549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         4725.95</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-19\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 434\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.531338504421002e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4539592213100856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01366429765036158\n",
      "          policy_loss: -0.052888586454921296\n",
      "          total_loss: -0.07701334555943808\n",
      "          vf_explained_var: -0.7365373373031616\n",
      "          vf_loss: 0.0004148297796240917\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.223076923076924\n",
      "    ram_util_percent: 64.73846153846155\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412703092305923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092344067978292\n",
      "    mean_inference_ms: 1.4036399830977562\n",
      "    mean_raw_obs_processing_ms: 0.8249113808149998\n",
      "  time_since_restore: 4735.091597080231\n",
      "  time_this_iter_s: 9.138907194137573\n",
      "  time_total_s: 4735.091597080231\n",
      "  timers:\n",
      "    learn_throughput: 1720.563\n",
      "    learn_time_ms: 581.205\n",
      "    load_throughput: 214842.414\n",
      "    load_time_ms: 4.655\n",
      "    sample_throughput: 92.072\n",
      "    sample_time_ms: 10861.039\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632129559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         4735.09</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 435\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.531338504421002e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5078128655751546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012648511831503366\n",
      "          policy_loss: 0.0030092047941353587\n",
      "          total_loss: -0.0213298582782348\n",
      "          vf_explained_var: -0.9660525918006897\n",
      "          vf_loss: 0.0007390664229799565\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.964285714285715\n",
      "    ram_util_percent: 64.68571428571431\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041269831061719414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092276094860944\n",
      "    mean_inference_ms: 1.4036184592438419\n",
      "    mean_raw_obs_processing_ms: 0.8243887392208875\n",
      "  time_since_restore: 4744.753979444504\n",
      "  time_this_iter_s: 9.662382364273071\n",
      "  time_total_s: 4744.753979444504\n",
      "  timers:\n",
      "    learn_throughput: 1720.769\n",
      "    learn_time_ms: 581.136\n",
      "    load_throughput: 215231.586\n",
      "    load_time_ms: 4.646\n",
      "    sample_throughput: 91.852\n",
      "    sample_time_ms: 10887.109\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632129568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         4744.75</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 436\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.531338504421002e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.41903334458669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02248162878612929\n",
      "          policy_loss: -0.042180736838943425\n",
      "          total_loss: -0.06605845321383741\n",
      "          vf_explained_var: -0.5307083129882812\n",
      "          vf_loss: 0.0003126164611684443\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26153846153847\n",
      "    ram_util_percent: 64.68461538461541\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126934500047171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092186355012563\n",
      "    mean_inference_ms: 1.4035965528636407\n",
      "    mean_raw_obs_processing_ms: 0.8238504815774148\n",
      "  time_since_restore: 4754.065899133682\n",
      "  time_this_iter_s: 9.311919689178467\n",
      "  time_total_s: 4754.065899133682\n",
      "  timers:\n",
      "    learn_throughput: 1721.161\n",
      "    learn_time_ms: 581.003\n",
      "    load_throughput: 320065.932\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 108.399\n",
      "    sample_time_ms: 9225.161\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632129578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         4754.07</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 437\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.286051016383701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01939365002079206\n",
      "          policy_loss: -0.012248830041951604\n",
      "          total_loss: -0.034814508135120076\n",
      "          vf_explained_var: -0.5995168685913086\n",
      "          vf_loss: 0.0002948288839257253\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.123076923076916\n",
      "    ram_util_percent: 64.68461538461541\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412688565263171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.092015586556943\n",
      "    mean_inference_ms: 1.403574530749988\n",
      "    mean_raw_obs_processing_ms: 0.823318176890852\n",
      "  time_since_restore: 4763.202540636063\n",
      "  time_this_iter_s: 9.136641502380371\n",
      "  time_total_s: 4763.202540636063\n",
      "  timers:\n",
      "    learn_throughput: 1724.32\n",
      "    learn_time_ms: 579.939\n",
      "    load_throughput: 319909.693\n",
      "    load_time_ms: 3.126\n",
      "    sample_throughput: 110.702\n",
      "    sample_time_ms: 9033.239\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">          4763.2</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-19-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 438\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.135689093006982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010407115137578889\n",
      "          policy_loss: 0.008046145861347517\n",
      "          total_loss: -0.012887900819381078\n",
      "          vf_explained_var: -0.2901643216609955\n",
      "          vf_loss: 0.00042284196438736724\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94285714285714\n",
      "    ram_util_percent: 64.72142857142859\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126835951391938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091832020988774\n",
      "    mean_inference_ms: 1.4035524383247162\n",
      "    mean_raw_obs_processing_ms: 0.8227923575344767\n",
      "  time_since_restore: 4772.8275282382965\n",
      "  time_this_iter_s: 9.624987602233887\n",
      "  time_total_s: 4772.8275282382965\n",
      "  timers:\n",
      "    learn_throughput: 1725.295\n",
      "    learn_time_ms: 579.611\n",
      "    load_throughput: 319259.53\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 110.494\n",
      "    sample_time_ms: 9050.299\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632129596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         4772.83</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 439\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.260105383396149\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01041690857483941\n",
      "          policy_loss: -0.02864573618604077\n",
      "          total_loss: -0.05089643576906787\n",
      "          vf_explained_var: -0.9666826725006104\n",
      "          vf_loss: 0.0003503555294325148\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.43846153846154\n",
      "    ram_util_percent: 64.70000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126786276011557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091627796579761\n",
      "    mean_inference_ms: 1.4035303289005001\n",
      "    mean_raw_obs_processing_ms: 0.8222730869874769\n",
      "  time_since_restore: 4782.280333280563\n",
      "  time_this_iter_s: 9.452805042266846\n",
      "  time_total_s: 4782.280333280563\n",
      "  timers:\n",
      "    learn_throughput: 1724.844\n",
      "    learn_time_ms: 579.763\n",
      "    load_throughput: 320283.454\n",
      "    load_time_ms: 3.122\n",
      "    sample_throughput: 110.638\n",
      "    sample_time_ms: 9038.495\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1632129606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         4782.28</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 440\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9847181306944952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015732842634762968\n",
      "          policy_loss: -0.01738196810086568\n",
      "          total_loss: -0.036423375374741024\n",
      "          vf_explained_var: -0.4350675940513611\n",
      "          vf_loss: 0.0008057752918426154\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.65333333333332\n",
      "    ram_util_percent: 64.72\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126736332436048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091420130235184\n",
      "    mean_inference_ms: 1.4035085712351005\n",
      "    mean_raw_obs_processing_ms: 0.8217600427247815\n",
      "  time_since_restore: 4792.2425129413605\n",
      "  time_this_iter_s: 9.96217966079712\n",
      "  time_total_s: 4792.2425129413605\n",
      "  timers:\n",
      "    learn_throughput: 1722.817\n",
      "    learn_time_ms: 580.445\n",
      "    load_throughput: 320051.278\n",
      "    load_time_ms: 3.124\n",
      "    sample_throughput: 110.784\n",
      "    sample_time_ms: 9026.567\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         4792.24</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 441\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.568567763434516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010842136160838529\n",
      "          policy_loss: -0.0722047969698906\n",
      "          total_loss: -0.08682446430126826\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010660115273721101\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.050000000000004\n",
      "    ram_util_percent: 64.77142857142856\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126686929531131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091213471209636\n",
      "    mean_inference_ms: 1.4034870145194243\n",
      "    mean_raw_obs_processing_ms: 0.821253128878504\n",
      "  time_since_restore: 4802.499764442444\n",
      "  time_this_iter_s: 10.257251501083374\n",
      "  time_total_s: 4802.499764442444\n",
      "  timers:\n",
      "    learn_throughput: 1722.899\n",
      "    learn_time_ms: 580.417\n",
      "    load_throughput: 317500.151\n",
      "    load_time_ms: 3.15\n",
      "    sample_throughput: 111.323\n",
      "    sample_time_ms: 8982.859\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632129626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">          4802.5</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 442\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5455938180287678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008495242507657885\n",
      "          policy_loss: -0.06916968810061613\n",
      "          total_loss: -0.083742249591483\n",
      "          vf_explained_var: -0.8965606093406677\n",
      "          vf_loss: 0.0008833760848372346\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.586666666666666\n",
      "    ram_util_percent: 64.79999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126637126329649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.091006424313731\n",
      "    mean_inference_ms: 1.4034655754833252\n",
      "    mean_raw_obs_processing_ms: 0.8207523074914224\n",
      "  time_since_restore: 4812.767915725708\n",
      "  time_this_iter_s: 10.26815128326416\n",
      "  time_total_s: 4812.767915725708\n",
      "  timers:\n",
      "    learn_throughput: 1720.491\n",
      "    learn_time_ms: 581.229\n",
      "    load_throughput: 317528.995\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 111.063\n",
      "    sample_time_ms: 9003.926\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632129636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         4812.77</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 443\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0090637697113887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01435259235531241\n",
      "          policy_loss: -0.04711400953431924\n",
      "          total_loss: -0.06647835713293818\n",
      "          vf_explained_var: -0.5098564624786377\n",
      "          vf_loss: 0.0007262896369664101\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63571428571429\n",
      "    ram_util_percent: 64.87142857142855\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126587329307963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090791010452616\n",
      "    mean_inference_ms: 1.4034442376149046\n",
      "    mean_raw_obs_processing_ms: 0.8202574689261073\n",
      "  time_since_restore: 4822.713404417038\n",
      "  time_this_iter_s: 9.945488691329956\n",
      "  time_total_s: 4822.713404417038\n",
      "  timers:\n",
      "    learn_throughput: 1721.047\n",
      "    learn_time_ms: 581.042\n",
      "    load_throughput: 316546.467\n",
      "    load_time_ms: 3.159\n",
      "    sample_throughput: 110.048\n",
      "    sample_time_ms: 9086.939\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632129646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         4822.71</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-20-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 444\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7003364112642076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009381924906991893\n",
      "          policy_loss: 0.006033558481269413\n",
      "          total_loss: -0.010090028618772824\n",
      "          vf_explained_var: -0.9948444962501526\n",
      "          vf_loss: 0.0008797740117491534\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.76666666666667\n",
      "    ram_util_percent: 64.89333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126536942908841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090572185886128\n",
      "    mean_inference_ms: 1.4034230035070576\n",
      "    mean_raw_obs_processing_ms: 0.8197685637263654\n",
      "  time_since_restore: 4832.9827716350555\n",
      "  time_this_iter_s: 10.269367218017578\n",
      "  time_total_s: 4832.9827716350555\n",
      "  timers:\n",
      "    learn_throughput: 1719.507\n",
      "    learn_time_ms: 581.562\n",
      "    load_throughput: 316140.859\n",
      "    load_time_ms: 3.163\n",
      "    sample_throughput: 108.702\n",
      "    sample_time_ms: 9199.446\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632129657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         4832.98</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 445\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7645020100805495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013975933429776216\n",
      "          policy_loss: 0.02170085629655255\n",
      "          total_loss: 0.004935256267587344\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008794171651566608\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.78571428571429\n",
      "    ram_util_percent: 64.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126486145683997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090352647858442\n",
      "    mean_inference_ms: 1.4034018592367372\n",
      "    mean_raw_obs_processing_ms: 0.8192855698290559\n",
      "  time_since_restore: 4843.240824460983\n",
      "  time_this_iter_s: 10.258052825927734\n",
      "  time_total_s: 4843.240824460983\n",
      "  timers:\n",
      "    learn_throughput: 1719.432\n",
      "    learn_time_ms: 581.588\n",
      "    load_throughput: 314665.626\n",
      "    load_time_ms: 3.178\n",
      "    sample_throughput: 108.004\n",
      "    sample_time_ms: 9258.953\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1632129667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         4843.24</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 446\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8919011765056186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009144859059296372\n",
      "          policy_loss: -0.0335427795847257\n",
      "          total_loss: -0.05168735128309992\n",
      "          vf_explained_var: -0.8090489506721497\n",
      "          vf_loss: 0.0007744388580451616\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.56666666666667\n",
      "    ram_util_percent: 64.93333333333334\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126435414227072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.090125139483789\n",
      "    mean_inference_ms: 1.4033808693657486\n",
      "    mean_raw_obs_processing_ms: 0.818808437656695\n",
      "  time_since_restore: 4853.201415538788\n",
      "  time_this_iter_s: 9.960591077804565\n",
      "  time_total_s: 4853.201415538788\n",
      "  timers:\n",
      "    learn_throughput: 1719.131\n",
      "    learn_time_ms: 581.689\n",
      "    load_throughput: 314762.444\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 107.253\n",
      "    sample_time_ms: 9323.716\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632129677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">          4853.2</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 447\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9693574521276687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013708490805028836\n",
      "          policy_loss: 0.010003396537568834\n",
      "          total_loss: -0.008823192119598389\n",
      "          vf_explained_var: -0.49612298607826233\n",
      "          vf_loss: 0.0008669860783912655\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.03571428571428\n",
      "    ram_util_percent: 64.98571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041263851317550025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08988890008812\n",
      "    mean_inference_ms: 1.40335998257159\n",
      "    mean_raw_obs_processing_ms: 0.81833711015722\n",
      "  time_since_restore: 4863.116844892502\n",
      "  time_this_iter_s: 9.91542935371399\n",
      "  time_total_s: 4863.116844892502\n",
      "  timers:\n",
      "    learn_throughput: 1720.646\n",
      "    learn_time_ms: 581.177\n",
      "    load_throughput: 312269.035\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 106.36\n",
      "    sample_time_ms: 9402.069\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632129687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         4863.12</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 448\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7114546007580227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011298628747698361\n",
      "          policy_loss: -0.06625102447966734\n",
      "          total_loss: -0.08206606879830361\n",
      "          vf_explained_var: -0.4813457131385803\n",
      "          vf_loss: 0.0012995030232963876\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87142857142858\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126334656303208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089644183397674\n",
      "    mean_inference_ms: 1.40333906187435\n",
      "    mean_raw_obs_processing_ms: 0.8178713716165149\n",
      "  time_since_restore: 4873.000205993652\n",
      "  time_this_iter_s: 9.883361101150513\n",
      "  time_total_s: 4873.000205993652\n",
      "  timers:\n",
      "    learn_throughput: 1720.116\n",
      "    learn_time_ms: 581.356\n",
      "    load_throughput: 311561.558\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 106.07\n",
      "    sample_time_ms: 9427.707\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632129697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">            4873</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 449\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7693897432751127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01543101340630852\n",
      "          policy_loss: -0.005809616545836131\n",
      "          total_loss: -0.02266032596429189\n",
      "          vf_explained_var: -0.5947757959365845\n",
      "          vf_loss: 0.0008431879476928669\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.74\n",
      "    ram_util_percent: 65.02666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412628513146387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089394556818736\n",
      "    mean_inference_ms: 1.4033182977128436\n",
      "    mean_raw_obs_processing_ms: 0.8174113403633505\n",
      "  time_since_restore: 4883.16063117981\n",
      "  time_this_iter_s: 10.160425186157227\n",
      "  time_total_s: 4883.16063117981\n",
      "  timers:\n",
      "    learn_throughput: 1719.426\n",
      "    learn_time_ms: 581.589\n",
      "    load_throughput: 311281.773\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 105.282\n",
      "    sample_time_ms: 9498.265\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632129707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         4883.16</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-21-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 450\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9774523496627807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01216032046299781\n",
      "          policy_loss: -0.07607257589697838\n",
      "          total_loss: -0.09524503176410994\n",
      "          vf_explained_var: -0.3953596353530884\n",
      "          vf_loss: 0.0006020670423620484\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.199999999999996\n",
      "    ram_util_percent: 65.07857142857144\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126235977637225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.089130615904681\n",
      "    mean_inference_ms: 1.4032977136521672\n",
      "    mean_raw_obs_processing_ms: 0.8169570191520837\n",
      "  time_since_restore: 4893.169783830643\n",
      "  time_this_iter_s: 10.00915265083313\n",
      "  time_total_s: 4893.169783830643\n",
      "  timers:\n",
      "    learn_throughput: 1721.104\n",
      "    learn_time_ms: 581.023\n",
      "    load_throughput: 311196.32\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 105.224\n",
      "    sample_time_ms: 9503.515\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1632129717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         4893.17</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 451\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.087748302353753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012773258139804117\n",
      "          policy_loss: 0.044452520459890364\n",
      "          total_loss: 0.02377018787794643\n",
      "          vf_explained_var: -0.624605655670166\n",
      "          vf_loss: 0.00019514752744321918\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.85\n",
      "    ram_util_percent: 65.11428571428573\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126187029730191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088857698783883\n",
      "    mean_inference_ms: 1.4032772675121663\n",
      "    mean_raw_obs_processing_ms: 0.8165082708186694\n",
      "  time_since_restore: 4903.1175582408905\n",
      "  time_this_iter_s: 9.947774410247803\n",
      "  time_total_s: 4903.1175582408905\n",
      "  timers:\n",
      "    learn_throughput: 1721.014\n",
      "    learn_time_ms: 581.053\n",
      "    load_throughput: 313126.936\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 105.568\n",
      "    sample_time_ms: 9472.552\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632129727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         4903.12</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 452\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4170363585154218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01120937941222052\n",
      "          policy_loss: 0.01051494831012355\n",
      "          total_loss: -0.013267899428804715\n",
      "          vf_explained_var: -0.7716747522354126\n",
      "          vf_loss: 0.00038751446867940506\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.83571428571429\n",
      "    ram_util_percent: 65.18571428571431\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126138256614386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088566174766694\n",
      "    mean_inference_ms: 1.4032568988734568\n",
      "    mean_raw_obs_processing_ms: 0.8160650313345909\n",
      "  time_since_restore: 4912.560192108154\n",
      "  time_this_iter_s: 9.442633867263794\n",
      "  time_total_s: 4912.560192108154\n",
      "  timers:\n",
      "    learn_throughput: 1721.004\n",
      "    learn_time_ms: 581.056\n",
      "    load_throughput: 313157.329\n",
      "    load_time_ms: 3.193\n",
      "    sample_throughput: 106.496\n",
      "    sample_time_ms: 9390.044\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1632129736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         4912.56</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 453\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.388682656817966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01195347111753667\n",
      "          policy_loss: -0.049484156693021454\n",
      "          total_loss: -0.07305189081364208\n",
      "          vf_explained_var: -0.6841434836387634\n",
      "          vf_loss: 0.0003190926534039641\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17692307692308\n",
      "    ram_util_percent: 65.20000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126089438789771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.088253627257608\n",
      "    mean_inference_ms: 1.4032364227749954\n",
      "    mean_raw_obs_processing_ms: 0.8156272710872323\n",
      "  time_since_restore: 4921.909807443619\n",
      "  time_this_iter_s: 9.349615335464478\n",
      "  time_total_s: 4921.909807443619\n",
      "  timers:\n",
      "    learn_throughput: 1721.335\n",
      "    learn_time_ms: 580.945\n",
      "    load_throughput: 314158.896\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 107.175\n",
      "    sample_time_ms: 9330.552\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1632129746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         4921.91</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-22-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 454\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3048805210325454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015102858598513237\n",
      "          policy_loss: 0.0006448766630556849\n",
      "          total_loss: -0.021824907697737216\n",
      "          vf_explained_var: -0.546601414680481\n",
      "          vf_loss: 0.0005790211499364281\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72857142857142\n",
      "    ram_util_percent: 65.23571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04126040800454041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087922150803804\n",
      "    mean_inference_ms: 1.4032159189282152\n",
      "    mean_raw_obs_processing_ms: 0.8151949304121493\n",
      "  time_since_restore: 4931.435373067856\n",
      "  time_this_iter_s: 9.52556562423706\n",
      "  time_total_s: 4931.435373067856\n",
      "  timers:\n",
      "    learn_throughput: 1722.736\n",
      "    learn_time_ms: 580.472\n",
      "    load_throughput: 314597.181\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 108.03\n",
      "    sample_time_ms: 9256.646\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632129755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         4931.44</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-22-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 455\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3656553294923572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012919769279380777\n",
      "          policy_loss: -0.10339580968850189\n",
      "          total_loss: -0.12659327576143875\n",
      "          vf_explained_var: -0.36324769258499146\n",
      "          vf_loss: 0.0004590850171451974\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.964285714285715\n",
      "    ram_util_percent: 65.29285714285712\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125991980691102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.087580525835627\n",
      "    mean_inference_ms: 1.4031954900512664\n",
      "    mean_raw_obs_processing_ms: 0.8147679683458162\n",
      "  time_since_restore: 4941.303871393204\n",
      "  time_this_iter_s: 9.8684983253479\n",
      "  time_total_s: 4941.303871393204\n",
      "  timers:\n",
      "    learn_throughput: 1724.697\n",
      "    learn_time_ms: 579.812\n",
      "    load_throughput: 315313.787\n",
      "    load_time_ms: 3.171\n",
      "    sample_throughput: 108.479\n",
      "    sample_time_ms: 9218.387\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632129765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">          4941.3</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 456\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.259736919403076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012451699654216379\n",
      "          policy_loss: -0.031916112090564434\n",
      "          total_loss: -0.05416588160312838\n",
      "          vf_explained_var: -0.48556509613990784\n",
      "          vf_loss: 0.0003475989650824987\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.52894736842104\n",
      "    ram_util_percent: 64.89736842105259\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125944663567513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08721810469525\n",
      "    mean_inference_ms: 1.403175473563205\n",
      "    mean_raw_obs_processing_ms: 0.814727382026538\n",
      "  time_since_restore: 4968.038277626038\n",
      "  time_this_iter_s: 26.734406232833862\n",
      "  time_total_s: 4968.038277626038\n",
      "  timers:\n",
      "    learn_throughput: 1722.922\n",
      "    learn_time_ms: 580.409\n",
      "    load_throughput: 217018.974\n",
      "    load_time_ms: 4.608\n",
      "    sample_throughput: 91.796\n",
      "    sample_time_ms: 10893.707\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1632129792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         4968.04</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 457\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2058662705951266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009511522100241561\n",
      "          policy_loss: -0.03613249427742428\n",
      "          total_loss: -0.05774686526921061\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004442907897909107\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.23846153846153\n",
      "    ram_util_percent: 65.05384615384615\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125898172794025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086838755410021\n",
      "    mean_inference_ms: 1.4031556925535404\n",
      "    mean_raw_obs_processing_ms: 0.8146911982082787\n",
      "  time_since_restore: 4977.494436979294\n",
      "  time_this_iter_s: 9.456159353256226\n",
      "  time_total_s: 4977.494436979294\n",
      "  timers:\n",
      "    learn_throughput: 1721.137\n",
      "    learn_time_ms: 581.011\n",
      "    load_throughput: 218368.033\n",
      "    load_time_ms: 4.579\n",
      "    sample_throughput: 92.19\n",
      "    sample_time_ms: 10847.171\n",
      "    update_time_ms: 1.657\n",
      "  timestamp: 1632129802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         4977.49</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 458\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.242859817875756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008730028994885923\n",
      "          policy_loss: -0.010526755193455351\n",
      "          total_loss: -0.032601955926252736\n",
      "          vf_explained_var: -0.3887447416782379\n",
      "          vf_loss: 0.000353395647562138\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26153846153846\n",
      "    ram_util_percent: 64.86153846153844\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125851523273524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.08643431081897\n",
      "    mean_inference_ms: 1.4031359283803144\n",
      "    mean_raw_obs_processing_ms: 0.8146593937934736\n",
      "  time_since_restore: 4986.591811180115\n",
      "  time_this_iter_s: 9.097374200820923\n",
      "  time_total_s: 4986.591811180115\n",
      "  timers:\n",
      "    learn_throughput: 1722.901\n",
      "    learn_time_ms: 580.417\n",
      "    load_throughput: 218863.703\n",
      "    load_time_ms: 4.569\n",
      "    sample_throughput: 92.858\n",
      "    sample_time_ms: 10769.179\n",
      "    update_time_ms: 1.659\n",
      "  timestamp: 1632129811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         4986.59</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 459\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.215830541981591\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013998886139311455\n",
      "          policy_loss: -0.037177417344517176\n",
      "          total_loss: -0.05893782203396161\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00039789953548784575\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.01428571428572\n",
      "    ram_util_percent: 64.77857142857142\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125804723592602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.086011384176908\n",
      "    mean_inference_ms: 1.4031162625426312\n",
      "    mean_raw_obs_processing_ms: 0.8146319547308596\n",
      "  time_since_restore: 4996.06098484993\n",
      "  time_this_iter_s: 9.469173669815063\n",
      "  time_total_s: 4996.06098484993\n",
      "  timers:\n",
      "    learn_throughput: 1724.002\n",
      "    learn_time_ms: 580.046\n",
      "    load_throughput: 219083.198\n",
      "    load_time_ms: 4.564\n",
      "    sample_throughput: 93.454\n",
      "    sample_time_ms: 10700.442\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1632129820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         4996.06</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-23-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 460\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0463319460550946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011247222517476955\n",
      "          policy_loss: -0.01344573615739743\n",
      "          total_loss: -0.03336643133726385\n",
      "          vf_explained_var: -0.49064725637435913\n",
      "          vf_loss: 0.0005426224044640549\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.857142857142854\n",
      "    ram_util_percent: 64.7214285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125758254260095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.085578905049184\n",
      "    mean_inference_ms: 1.4030968497649987\n",
      "    mean_raw_obs_processing_ms: 0.8146088107138512\n",
      "  time_since_restore: 5005.9435222148895\n",
      "  time_this_iter_s: 9.882537364959717\n",
      "  time_total_s: 5005.9435222148895\n",
      "  timers:\n",
      "    learn_throughput: 1724.94\n",
      "    learn_time_ms: 579.73\n",
      "    load_throughput: 219086.631\n",
      "    load_time_ms: 4.564\n",
      "    sample_throughput: 93.562\n",
      "    sample_time_ms: 10688.073\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632129830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         5005.94</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 461\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9565904921955533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011919071832220693\n",
      "          policy_loss: -0.003655697074201372\n",
      "          total_loss: -0.022593611851334572\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006279861800269121\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.10714285714287\n",
      "    ram_util_percent: 64.65\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041257124667869724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.085135148177782\n",
      "    mean_inference_ms: 1.4030776428234963\n",
      "    mean_raw_obs_processing_ms: 0.8145901230170532\n",
      "  time_since_restore: 5015.7390122413635\n",
      "  time_this_iter_s: 9.795490026473999\n",
      "  time_total_s: 5015.7390122413635\n",
      "  timers:\n",
      "    learn_throughput: 1723.847\n",
      "    learn_time_ms: 580.098\n",
      "    load_throughput: 219659.171\n",
      "    load_time_ms: 4.553\n",
      "    sample_throughput: 93.699\n",
      "    sample_time_ms: 10672.522\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632129840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">         5015.74</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 462\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6827359331978693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01552720478268582\n",
      "          policy_loss: 0.026513351582818562\n",
      "          total_loss: 0.010561253668533431\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008752631217551728\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82142857142857\n",
      "    ram_util_percent: 64.60714285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125666890892891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.084685327481784\n",
      "    mean_inference_ms: 1.4030587329643232\n",
      "    mean_raw_obs_processing_ms: 0.8145756906397756\n",
      "  time_since_restore: 5025.646691083908\n",
      "  time_this_iter_s: 9.907678842544556\n",
      "  time_total_s: 5025.646691083908\n",
      "  timers:\n",
      "    learn_throughput: 1723.884\n",
      "    learn_time_ms: 580.085\n",
      "    load_throughput: 219796.15\n",
      "    load_time_ms: 4.55\n",
      "    sample_throughput: 93.292\n",
      "    sample_time_ms: 10719.046\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632129850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         5025.65</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-20\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 463\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8010846787028842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011761632729729498\n",
      "          policy_loss: 0.004497931152582168\n",
      "          total_loss: -0.012713662617736392\n",
      "          vf_explained_var: -0.6172215938568115\n",
      "          vf_loss: 0.0007992538972757757\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.67333333333333\n",
      "    ram_util_percent: 64.59333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412562134511197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.084229484445988\n",
      "    mean_inference_ms: 1.4030400424487928\n",
      "    mean_raw_obs_processing_ms: 0.8145654460627049\n",
      "  time_since_restore: 5035.612632989883\n",
      "  time_this_iter_s: 9.965941905975342\n",
      "  time_total_s: 5035.612632989883\n",
      "  timers:\n",
      "    learn_throughput: 1721.08\n",
      "    learn_time_ms: 581.03\n",
      "    load_throughput: 219934.454\n",
      "    load_time_ms: 4.547\n",
      "    sample_throughput: 92.767\n",
      "    sample_time_ms: 10779.669\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         5035.61</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-30\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 464\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5617087496651543\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008638206374475023\n",
      "          policy_loss: 0.024909722846415308\n",
      "          total_loss: 0.010237038259704907\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0009444002120289951\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.99285714285714\n",
      "    ram_util_percent: 64.59285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125576396365531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.083764778841502\n",
      "    mean_inference_ms: 1.4030216567079925\n",
      "    mean_raw_obs_processing_ms: 0.8145593833519249\n",
      "  time_since_restore: 5045.602333307266\n",
      "  time_this_iter_s: 9.989700317382812\n",
      "  time_total_s: 5045.602333307266\n",
      "  timers:\n",
      "    learn_throughput: 1719.935\n",
      "    learn_time_ms: 581.417\n",
      "    load_throughput: 219920.616\n",
      "    load_time_ms: 4.547\n",
      "    sample_throughput: 92.373\n",
      "    sample_time_ms: 10825.698\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632129870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">          5045.6</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 994.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 465\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6516528407732645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00920619789485465\n",
      "          policy_loss: -0.07727794936961598\n",
      "          total_loss: -0.09296866191758049\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0008258154842122976\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.878571428571426\n",
      "    ram_util_percent: 64.64285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125531587246753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.083295027389038\n",
      "    mean_inference_ms: 1.4030035587956737\n",
      "    mean_raw_obs_processing_ms: 0.8145574402548402\n",
      "  time_since_restore: 5055.647972822189\n",
      "  time_this_iter_s: 10.045639514923096\n",
      "  time_total_s: 5055.647972822189\n",
      "  timers:\n",
      "    learn_throughput: 1718.181\n",
      "    learn_time_ms: 582.011\n",
      "    load_throughput: 220005.98\n",
      "    load_time_ms: 4.545\n",
      "    sample_throughput: 92.227\n",
      "    sample_time_ms: 10842.807\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632129880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         5055.65</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 466\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9257187472449409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01258320549393495\n",
      "          policy_loss: -0.05585459290693204\n",
      "          total_loss: -0.0745204960513446\n",
      "          vf_explained_var: -0.9717624187469482\n",
      "          vf_loss: 0.0005912831121046717\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.66666666666666\n",
      "    ram_util_percent: 64.64000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125487408378035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.082817804491055\n",
      "    mean_inference_ms: 1.4029857102320256\n",
      "    mean_raw_obs_processing_ms: 0.8140884630388942\n",
      "  time_since_restore: 5065.641436576843\n",
      "  time_this_iter_s: 9.99346375465393\n",
      "  time_total_s: 5065.641436576843\n",
      "  timers:\n",
      "    learn_throughput: 1719.77\n",
      "    learn_time_ms: 581.473\n",
      "    load_throughput: 322321.407\n",
      "    load_time_ms: 3.102\n",
      "    sample_throughput: 109.043\n",
      "    sample_time_ms: 9170.715\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632129890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         5065.64</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 467\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.892923194832272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015336668037296913\n",
      "          policy_loss: -0.045721357357170846\n",
      "          total_loss: -0.06419985323316521\n",
      "          vf_explained_var: -0.9979424476623535\n",
      "          vf_loss: 0.000450734882744857\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.93571428571429\n",
      "    ram_util_percent: 64.65714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125443935200367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.082285832203645\n",
      "    mean_inference_ms: 1.402968011641863\n",
      "    mean_raw_obs_processing_ms: 0.8136248377536873\n",
      "  time_since_restore: 5075.750483036041\n",
      "  time_this_iter_s: 10.109046459197998\n",
      "  time_total_s: 5075.750483036041\n",
      "  timers:\n",
      "    learn_throughput: 1721.137\n",
      "    learn_time_ms: 581.011\n",
      "    load_throughput: 322029.391\n",
      "    load_time_ms: 3.105\n",
      "    sample_throughput: 108.266\n",
      "    sample_time_ms: 9236.523\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         5075.75</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 468\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.934077591366238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017083000432900283\n",
      "          policy_loss: -0.043163849827316074\n",
      "          total_loss: -0.06168154233859645\n",
      "          vf_explained_var: -0.9979824423789978\n",
      "          vf_loss: 0.0008230834670636492\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.92142857142858\n",
      "    ram_util_percent: 64.70000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125400720553988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.081748410689176\n",
      "    mean_inference_ms: 1.4029504046343106\n",
      "    mean_raw_obs_processing_ms: 0.8131665159317216\n",
      "  time_since_restore: 5085.8115670681\n",
      "  time_this_iter_s: 10.061084032058716\n",
      "  time_total_s: 5085.8115670681\n",
      "  timers:\n",
      "    learn_throughput: 1719.159\n",
      "    learn_time_ms: 581.68\n",
      "    load_throughput: 320636.027\n",
      "    load_time_ms: 3.119\n",
      "    sample_throughput: 107.155\n",
      "    sample_time_ms: 9332.242\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         5085.81</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 469\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8162436657481724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013868368925198125\n",
      "          policy_loss: 0.005949488346878853\n",
      "          total_loss: -0.011680571646947\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005323782931858053\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.68\n",
      "    ram_util_percent: 64.70000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125357722255368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.081203423796001\n",
      "    mean_inference_ms: 1.402932971945195\n",
      "    mean_raw_obs_processing_ms: 0.8127134486244186\n",
      "  time_since_restore: 5095.824142694473\n",
      "  time_this_iter_s: 10.012575626373291\n",
      "  time_total_s: 5095.824142694473\n",
      "  timers:\n",
      "    learn_throughput: 1718.402\n",
      "    learn_time_ms: 581.936\n",
      "    load_throughput: 320584.562\n",
      "    load_time_ms: 3.119\n",
      "    sample_throughput: 106.538\n",
      "    sample_time_ms: 9386.317\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632129920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         5095.82</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 470\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.077725999885135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014453238954064633\n",
      "          policy_loss: 0.0040002857645352686\n",
      "          total_loss: -0.016187688211599986\n",
      "          vf_explained_var: -0.35351210832595825\n",
      "          vf_loss: 0.0005892849504663092\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.0\n",
      "    ram_util_percent: 64.73571428571428\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125314887650968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.080655792197359\n",
      "    mean_inference_ms: 1.4029157212280128\n",
      "    mean_raw_obs_processing_ms: 0.8122655721238972\n",
      "  time_since_restore: 5105.9846115112305\n",
      "  time_this_iter_s: 10.160468816757202\n",
      "  time_total_s: 5105.9846115112305\n",
      "  timers:\n",
      "    learn_throughput: 1717.286\n",
      "    learn_time_ms: 582.314\n",
      "    load_throughput: 320489.027\n",
      "    load_time_ms: 3.12\n",
      "    sample_throughput: 106.228\n",
      "    sample_time_ms: 9413.749\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">         5105.98</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 471\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.933920509285397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011371345654184259\n",
      "          policy_loss: -0.029654191020462246\n",
      "          total_loss: -0.048557238239381045\n",
      "          vf_explained_var: -0.9963827133178711\n",
      "          vf_loss: 0.00043615883041638883\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.739999999999995\n",
      "    ram_util_percent: 64.79333333333331\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125272752947263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.080105565376833\n",
      "    mean_inference_ms: 1.4028987944436515\n",
      "    mean_raw_obs_processing_ms: 0.811822860906831\n",
      "  time_since_restore: 5116.025741815567\n",
      "  time_this_iter_s: 10.041130304336548\n",
      "  time_total_s: 5116.025741815567\n",
      "  timers:\n",
      "    learn_throughput: 1717.473\n",
      "    learn_time_ms: 582.251\n",
      "    load_throughput: 318704.001\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 105.951\n",
      "    sample_time_ms: 9438.327\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         5116.03</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 472\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.944882082939148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015703349876049995\n",
      "          policy_loss: 0.04583418005042606\n",
      "          total_loss: 0.026818938387764824\n",
      "          vf_explained_var: -0.5446791052818298\n",
      "          vf_loss: 0.0004335815573641513\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.91428571428571\n",
      "    ram_util_percent: 64.80714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041252307516793485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.079553069696871\n",
      "    mean_inference_ms: 1.4028823589237636\n",
      "    mean_raw_obs_processing_ms: 0.8113853219962531\n",
      "  time_since_restore: 5126.212927103043\n",
      "  time_this_iter_s: 10.187185287475586\n",
      "  time_total_s: 5126.212927103043\n",
      "  timers:\n",
      "    learn_throughput: 1717.086\n",
      "    learn_time_ms: 582.382\n",
      "    load_throughput: 318597.483\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 105.64\n",
      "    sample_time_ms: 9466.135\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         5126.21</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 473\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.905033704969618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00890186540449061\n",
      "          policy_loss: -0.004585054020086924\n",
      "          total_loss: -0.02316024870508247\n",
      "          vf_explained_var: -0.6371240615844727\n",
      "          vf_loss: 0.0004751376028353762\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.84\n",
      "    ram_util_percent: 64.89333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125189455968086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.07900126347842\n",
      "    mean_inference_ms: 1.4028660201990772\n",
      "    mean_raw_obs_processing_ms: 0.8109528636958355\n",
      "  time_since_restore: 5136.281423091888\n",
      "  time_this_iter_s: 10.068495988845825\n",
      "  time_total_s: 5136.281423091888\n",
      "  timers:\n",
      "    learn_throughput: 1719.272\n",
      "    learn_time_ms: 581.641\n",
      "    load_throughput: 319099.222\n",
      "    load_time_ms: 3.134\n",
      "    sample_throughput: 105.516\n",
      "    sample_time_ms: 9477.217\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632129961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         5136.28</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 474\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.802848419878218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013480005541406994\n",
      "          policy_loss: -0.027982462611463334\n",
      "          total_loss: -0.04541212846007612\n",
      "          vf_explained_var: -0.9169938564300537\n",
      "          vf_loss: 0.0005988165933457721\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.82857142857142\n",
      "    ram_util_percent: 64.89285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125148874366448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.078445740155344\n",
      "    mean_inference_ms: 1.4028498247527423\n",
      "    mean_raw_obs_processing_ms: 0.810525409442819\n",
      "  time_since_restore: 5146.327242612839\n",
      "  time_this_iter_s: 10.045819520950317\n",
      "  time_total_s: 5146.327242612839\n",
      "  timers:\n",
      "    learn_throughput: 1719.935\n",
      "    learn_time_ms: 581.417\n",
      "    load_throughput: 316713.785\n",
      "    load_time_ms: 3.157\n",
      "    sample_throughput: 105.451\n",
      "    sample_time_ms: 9483.065\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632129971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         5146.33</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 475\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7251477784580656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01106804888727295\n",
      "          policy_loss: 0.002729059424665239\n",
      "          total_loss: -0.014118910332520803\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0004035097778897681\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.949999999999996\n",
      "    ram_util_percent: 64.94285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125108914469653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.077891488678063\n",
      "    mean_inference_ms: 1.402833755292896\n",
      "    mean_raw_obs_processing_ms: 0.8101029460423013\n",
      "  time_since_restore: 5156.371641635895\n",
      "  time_this_iter_s: 10.04439902305603\n",
      "  time_total_s: 5156.371641635895\n",
      "  timers:\n",
      "    learn_throughput: 1720.337\n",
      "    learn_time_ms: 581.281\n",
      "    load_throughput: 316984.258\n",
      "    load_time_ms: 3.155\n",
      "    sample_throughput: 105.451\n",
      "    sample_time_ms: 9483.053\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1632129981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         5156.37</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 476\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.7970077566315016e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7805971609221565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021569777871498868\n",
      "          policy_loss: -0.0627669180639916\n",
      "          total_loss: -0.08025646495322386\n",
      "          vf_explained_var: -0.6695361733436584\n",
      "          vf_loss: 0.0003164243790201403\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70666666666668\n",
      "    ram_util_percent: 64.99333333333333\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125069421225947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.077328087030713\n",
      "    mean_inference_ms: 1.4028179007277215\n",
      "    mean_raw_obs_processing_ms: 0.8096854185335494\n",
      "  time_since_restore: 5166.3787317276\n",
      "  time_this_iter_s: 10.007090091705322\n",
      "  time_total_s: 5166.3787317276\n",
      "  timers:\n",
      "    learn_throughput: 1721.154\n",
      "    learn_time_ms: 581.006\n",
      "    load_throughput: 316250.509\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 105.433\n",
      "    sample_time_ms: 9484.672\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632129991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">         5166.38</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 477\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0195511634947252e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7631557292408413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013932146677234374\n",
      "          policy_loss: -0.03940409264630741\n",
      "          total_loss: -0.05669055986735556\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003450917254667729\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.05714285714286\n",
      "    ram_util_percent: 65.04285714285716\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04125030327033634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.076759452370249\n",
      "    mean_inference_ms: 1.4028023822738134\n",
      "    mean_raw_obs_processing_ms: 0.8092728068182218\n",
      "  time_since_restore: 5176.415181875229\n",
      "  time_this_iter_s: 10.036450147628784\n",
      "  time_total_s: 5176.415181875229\n",
      "  timers:\n",
      "    learn_throughput: 1720.516\n",
      "    learn_time_ms: 581.221\n",
      "    load_throughput: 317368.019\n",
      "    load_time_ms: 3.151\n",
      "    sample_throughput: 105.517\n",
      "    sample_time_ms: 9477.13\n",
      "    update_time_ms: 1.631\n",
      "  timestamp: 1632130001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         5176.42</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-26-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 478\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0195511634947252e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6493221256468031\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023531216210221452\n",
      "          policy_loss: -0.04396275199121899\n",
      "          total_loss: -0.060012577805254194\n",
      "          vf_explained_var: -0.4020581841468811\n",
      "          vf_loss: 0.00044339323552170147\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.957142857142856\n",
      "    ram_util_percent: 65.10000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041249913652367874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.076186205619875\n",
      "    mean_inference_ms: 1.4027869152449488\n",
      "    mean_raw_obs_processing_ms: 0.8088650400151914\n",
      "  time_since_restore: 5186.460919618607\n",
      "  time_this_iter_s: 10.045737743377686\n",
      "  time_total_s: 5186.460919618607\n",
      "  timers:\n",
      "    learn_throughput: 1721.772\n",
      "    learn_time_ms: 580.797\n",
      "    load_throughput: 313869.732\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 105.53\n",
      "    sample_time_ms: 9475.942\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632130011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         5186.46</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 479\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5361509746975368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01306769425534237\n",
      "          policy_loss: -0.007808530703186989\n",
      "          total_loss: -0.022628798625535435\n",
      "          vf_explained_var: -0.6442742943763733\n",
      "          vf_loss: 0.0005412418603858289\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75333333333332\n",
      "    ram_util_percent: 65.12000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041249529519253876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.075607680708927\n",
      "    mean_inference_ms: 1.4027715982394215\n",
      "    mean_raw_obs_processing_ms: 0.8084620825490308\n",
      "  time_since_restore: 5196.51299905777\n",
      "  time_this_iter_s: 10.052079439163208\n",
      "  time_total_s: 5196.51299905777\n",
      "  timers:\n",
      "    learn_throughput: 1721.102\n",
      "    learn_time_ms: 581.023\n",
      "    load_throughput: 314368.461\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 105.505\n",
      "    sample_time_ms: 9478.233\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632130021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         5196.51</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 480\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5289128224054973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010428096054684961\n",
      "          policy_loss: -0.042887321818206044\n",
      "          total_loss: -0.0576901132447852\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.000486337648342467\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87857142857143\n",
      "    ram_util_percent: 65.19285714285716\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124915036792214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.075025929476805\n",
      "    mean_inference_ms: 1.4027564589377257\n",
      "    mean_raw_obs_processing_ms: 0.8080638905633226\n",
      "  time_since_restore: 5206.3804812431335\n",
      "  time_this_iter_s: 9.86748218536377\n",
      "  time_total_s: 5206.3804812431335\n",
      "  timers:\n",
      "    learn_throughput: 1723.182\n",
      "    learn_time_ms: 580.322\n",
      "    load_throughput: 302822.529\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 105.826\n",
      "    sample_time_ms: 9449.512\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632130031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         5206.38</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 481\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5019888308313158\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0097515820177757\n",
      "          policy_loss: -0.07938576348953777\n",
      "          total_loss: -0.09401956146789922\n",
      "          vf_explained_var: -0.6705144643783569\n",
      "          vf_loss: 0.00038608935315601734\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.93571428571429\n",
      "    ram_util_percent: 65.19285714285716\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124877566548392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.074439298228667\n",
      "    mean_inference_ms: 1.4027414802181255\n",
      "    mean_raw_obs_processing_ms: 0.8076704445461518\n",
      "  time_since_restore: 5216.3519723415375\n",
      "  time_this_iter_s: 9.97149109840393\n",
      "  time_total_s: 5216.3519723415375\n",
      "  timers:\n",
      "    learn_throughput: 1722.92\n",
      "    learn_time_ms: 580.41\n",
      "    load_throughput: 303791.982\n",
      "    load_time_ms: 3.292\n",
      "    sample_throughput: 105.904\n",
      "    sample_time_ms: 9442.487\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632130041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         5216.35</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 482\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7106860505210029\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017212884254770543\n",
      "          policy_loss: -0.029388487918509378\n",
      "          total_loss: -0.046075493469834326\n",
      "          vf_explained_var: -0.7144879698753357\n",
      "          vf_loss: 0.0004198533217681365\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.921428571428564\n",
      "    ram_util_percent: 65.22142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041248405134102134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.073847926833986\n",
      "    mean_inference_ms: 1.4027266468160267\n",
      "    mean_raw_obs_processing_ms: 0.8072816563959304\n",
      "  time_since_restore: 5226.3664174079895\n",
      "  time_this_iter_s: 10.014445066452026\n",
      "  time_total_s: 5226.3664174079895\n",
      "  timers:\n",
      "    learn_throughput: 1724.53\n",
      "    learn_time_ms: 579.868\n",
      "    load_throughput: 300645.402\n",
      "    load_time_ms: 3.326\n",
      "    sample_throughput: 106.093\n",
      "    sample_time_ms: 9425.718\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632130051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         5226.37</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 483\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5050791263580323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01321849673413785\n",
      "          policy_loss: -0.07455893572833804\n",
      "          total_loss: -0.08914613674084346\n",
      "          vf_explained_var: -0.9881923198699951\n",
      "          vf_loss: 0.0004635915393009782\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.686666666666675\n",
      "    ram_util_percent: 65.22\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124803188555418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.07324959138366\n",
      "    mean_inference_ms: 1.4027120263341348\n",
      "    mean_raw_obs_processing_ms: 0.8068975106315992\n",
      "  time_since_restore: 5236.270180940628\n",
      "  time_this_iter_s: 9.90376353263855\n",
      "  time_total_s: 5236.270180940628\n",
      "  timers:\n",
      "    learn_throughput: 1724.47\n",
      "    learn_time_ms: 579.888\n",
      "    load_throughput: 300531.23\n",
      "    load_time_ms: 3.327\n",
      "    sample_throughput: 106.279\n",
      "    sample_time_ms: 9409.23\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         5236.27</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-27-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 484\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.647081572479672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013362733620186872\n",
      "          policy_loss: -0.008098665955993864\n",
      "          total_loss: -0.024178038040796917\n",
      "          vf_explained_var: -0.6823546886444092\n",
      "          vf_loss: 0.0003914407920092344\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.942857142857136\n",
      "    ram_util_percent: 65.25714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124766142798826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.072645906312998\n",
      "    mean_inference_ms: 1.402697661999657\n",
      "    mean_raw_obs_processing_ms: 0.8065179832907272\n",
      "  time_since_restore: 5246.357610464096\n",
      "  time_this_iter_s: 10.087429523468018\n",
      "  time_total_s: 5246.357610464096\n",
      "  timers:\n",
      "    learn_throughput: 1725.061\n",
      "    learn_time_ms: 579.69\n",
      "    load_throughput: 302625.887\n",
      "    load_time_ms: 3.304\n",
      "    sample_throughput: 106.229\n",
      "    sample_time_ms: 9413.61\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632130071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         5246.36</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 485\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6946034722858005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009633218054999778\n",
      "          policy_loss: 0.054920496584640606\n",
      "          total_loss: 0.03847826792755061\n",
      "          vf_explained_var: -0.6909838914871216\n",
      "          vf_loss: 0.0005038056771607242\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.07142857142856\n",
      "    ram_util_percent: 65.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124729383264964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.072038108225977\n",
      "    mean_inference_ms: 1.4026834862316417\n",
      "    mean_raw_obs_processing_ms: 0.8061430392610273\n",
      "  time_since_restore: 5256.358438253403\n",
      "  time_this_iter_s: 10.00082778930664\n",
      "  time_total_s: 5256.358438253403\n",
      "  timers:\n",
      "    learn_throughput: 1724.031\n",
      "    learn_time_ms: 580.036\n",
      "    load_throughput: 302518.933\n",
      "    load_time_ms: 3.306\n",
      "    sample_throughput: 106.282\n",
      "    sample_time_ms: 9408.929\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632130081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         5256.36</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 486\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6549524439705743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014259250210196504\n",
      "          policy_loss: -0.04051943802171283\n",
      "          total_loss: -0.05659324843436479\n",
      "          vf_explained_var: -0.5792539119720459\n",
      "          vf_loss: 0.00047571294675839856\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.4525\n",
      "    ram_util_percent: 65.34750000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041246932034390935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.071424152767426\n",
      "    mean_inference_ms: 1.402669760523279\n",
      "    mean_raw_obs_processing_ms: 0.8061378999771286\n",
      "  time_since_restore: 5283.950488805771\n",
      "  time_this_iter_s: 27.592050552368164\n",
      "  time_total_s: 5283.950488805771\n",
      "  timers:\n",
      "    learn_throughput: 1723.475\n",
      "    learn_time_ms: 580.223\n",
      "    load_throughput: 204349.991\n",
      "    load_time_ms: 4.894\n",
      "    sample_throughput: 89.56\n",
      "    sample_time_ms: 11165.652\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632130109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         5283.95</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 487\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7315335591634116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007213116864365788\n",
      "          policy_loss: -0.07842130503720707\n",
      "          total_loss: -0.09531787729097738\n",
      "          vf_explained_var: -0.7817779779434204\n",
      "          vf_loss: 0.00041876235821594794\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.65\n",
      "    ram_util_percent: 65.22142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124657905934367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.0708050258684\n",
      "    mean_inference_ms: 1.4026562430212783\n",
      "    mean_raw_obs_processing_ms: 0.8061365011634392\n",
      "  time_since_restore: 5293.9229946136475\n",
      "  time_this_iter_s: 9.972505807876587\n",
      "  time_total_s: 5293.9229946136475\n",
      "  timers:\n",
      "    learn_throughput: 1722.321\n",
      "    learn_time_ms: 580.612\n",
      "    load_throughput: 203287.257\n",
      "    load_time_ms: 4.919\n",
      "    sample_throughput: 89.615\n",
      "    sample_time_ms: 11158.901\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632130119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         5293.92</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 488\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6044758492045932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008668197586590163\n",
      "          policy_loss: -0.001070071632663409\n",
      "          total_loss: -0.016530112591054705\n",
      "          vf_explained_var: -0.7608753442764282\n",
      "          vf_loss: 0.0005847178852289087\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.028571428571425\n",
      "    ram_util_percent: 65.1357142857143\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124623181289679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.070180154823559\n",
      "    mean_inference_ms: 1.4026429726442442\n",
      "    mean_raw_obs_processing_ms: 0.8061388119076035\n",
      "  time_since_restore: 5303.777963638306\n",
      "  time_this_iter_s: 9.854969024658203\n",
      "  time_total_s: 5303.777963638306\n",
      "  timers:\n",
      "    learn_throughput: 1722.852\n",
      "    learn_time_ms: 580.433\n",
      "    load_throughput: 205284.142\n",
      "    load_time_ms: 4.871\n",
      "    sample_throughput: 89.766\n",
      "    sample_time_ms: 11140.055\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         5303.78</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-28-58\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 489\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4547195348474715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012708920445717033\n",
      "          policy_loss: 0.012241434802611669\n",
      "          total_loss: -0.001934227099021276\n",
      "          vf_explained_var: -0.8497382402420044\n",
      "          vf_loss: 0.0003715325814684749\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.9\n",
      "    ram_util_percent: 65.08571428571429\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124588630929607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.069548909823327\n",
      "    mean_inference_ms: 1.402629893523101\n",
      "    mean_raw_obs_processing_ms: 0.806144782071212\n",
      "  time_since_restore: 5313.661194562912\n",
      "  time_this_iter_s: 9.883230924606323\n",
      "  time_total_s: 5313.661194562912\n",
      "  timers:\n",
      "    learn_throughput: 1724.159\n",
      "    learn_time_ms: 579.993\n",
      "    load_throughput: 204399.784\n",
      "    load_time_ms: 4.892\n",
      "    sample_throughput: 89.887\n",
      "    sample_time_ms: 11125.022\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632130138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         5313.66</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 490\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.653827542728848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01727756781854792\n",
      "          policy_loss: -0.09177121197183927\n",
      "          total_loss: -0.10800680820312765\n",
      "          vf_explained_var: -0.6452397108078003\n",
      "          vf_loss: 0.0003026800471060495\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.05714285714286\n",
      "    ram_util_percent: 64.95714285714284\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124554336202173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.068904740683504\n",
      "    mean_inference_ms: 1.402617011512363\n",
      "    mean_raw_obs_processing_ms: 0.806154363913523\n",
      "  time_since_restore: 5323.320987462997\n",
      "  time_this_iter_s: 9.65979290008545\n",
      "  time_total_s: 5323.320987462997\n",
      "  timers:\n",
      "    learn_throughput: 1722.193\n",
      "    learn_time_ms: 580.655\n",
      "    load_throughput: 209704.715\n",
      "    load_time_ms: 4.769\n",
      "    sample_throughput: 90.06\n",
      "    sample_time_ms: 11103.735\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632130148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         5323.32</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 491\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4617874383926392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013130867264516234\n",
      "          policy_loss: -0.012443531966871686\n",
      "          total_loss: -0.026569155603647233\n",
      "          vf_explained_var: -0.45342686772346497\n",
      "          vf_loss: 0.0004922493752221473\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.871428571428574\n",
      "    ram_util_percent: 64.8642857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124520567403967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.068249828018425\n",
      "    mean_inference_ms: 1.40260423645397\n",
      "    mean_raw_obs_processing_ms: 0.8061676491948385\n",
      "  time_since_restore: 5333.025365829468\n",
      "  time_this_iter_s: 9.704378366470337\n",
      "  time_total_s: 5333.025365829468\n",
      "  timers:\n",
      "    learn_throughput: 1723.743\n",
      "    learn_time_ms: 580.133\n",
      "    load_throughput: 209577.926\n",
      "    load_time_ms: 4.771\n",
      "    sample_throughput: 90.273\n",
      "    sample_time_ms: 11077.552\n",
      "    update_time_ms: 1.632\n",
      "  timestamp: 1632130158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         5333.03</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-28\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 492\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.568265402317047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01652992091110228\n",
      "          policy_loss: -0.056309168537457786\n",
      "          total_loss: -0.07145534687572055\n",
      "          vf_explained_var: -0.31749090552330017\n",
      "          vf_loss: 0.0005364733570281209\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.79285714285714\n",
      "    ram_util_percent: 64.77142857142857\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124487269001016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.067587300542197\n",
      "    mean_inference_ms: 1.4025916071237727\n",
      "    mean_raw_obs_processing_ms: 0.8061844966391832\n",
      "  time_since_restore: 5342.902178525925\n",
      "  time_this_iter_s: 9.87681269645691\n",
      "  time_total_s: 5342.902178525925\n",
      "  timers:\n",
      "    learn_throughput: 1721.846\n",
      "    learn_time_ms: 580.772\n",
      "    load_throughput: 210868.652\n",
      "    load_time_ms: 4.742\n",
      "    sample_throughput: 90.39\n",
      "    sample_time_ms: 11063.167\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632130168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">          5342.9</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-38\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 493\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5711463292439778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012408593810486302\n",
      "          policy_loss: -0.006427999917003844\n",
      "          total_loss: -0.02155305668711662\n",
      "          vf_explained_var: -0.44675835967063904\n",
      "          vf_loss: 0.0005864087883512386\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.94285714285714\n",
      "    ram_util_percent: 64.70000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124454489000659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.066915660143305\n",
      "    mean_inference_ms: 1.4025794532157974\n",
      "    mean_raw_obs_processing_ms: 0.8062048639727968\n",
      "  time_since_restore: 5352.8030734062195\n",
      "  time_this_iter_s: 9.9008948802948\n",
      "  time_total_s: 5352.8030734062195\n",
      "  timers:\n",
      "    learn_throughput: 1721.168\n",
      "    learn_time_ms: 581.001\n",
      "    load_throughput: 210731.982\n",
      "    load_time_ms: 4.745\n",
      "    sample_throughput: 90.395\n",
      "    sample_time_ms: 11062.54\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1632130178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">          5352.8</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 494\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6869897656970554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012479776045445905\n",
      "          policy_loss: -0.0074359136323134106\n",
      "          total_loss: -0.023907452821731567\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003983574414936205\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.72142857142857\n",
      "    ram_util_percent: 64.65000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041244216590059626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.066236705740126\n",
      "    mean_inference_ms: 1.4025673713271127\n",
      "    mean_raw_obs_processing_ms: 0.8062287344317518\n",
      "  time_since_restore: 5362.637064218521\n",
      "  time_this_iter_s: 9.833990812301636\n",
      "  time_total_s: 5362.637064218521\n",
      "  timers:\n",
      "    learn_throughput: 1720.732\n",
      "    learn_time_ms: 581.148\n",
      "    load_throughput: 210267.151\n",
      "    load_time_ms: 4.756\n",
      "    sample_throughput: 90.604\n",
      "    sample_time_ms: 11037.027\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632130188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         5362.64</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-29-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 495\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5921069741249085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017197808363138577\n",
      "          policy_loss: 0.012053318487273323\n",
      "          total_loss: -0.0030637984888421165\n",
      "          vf_explained_var: -0.5344363451004028\n",
      "          vf_loss: 0.0008039527061757528\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.857142857142854\n",
      "    ram_util_percent: 64.61428571428573\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124389032741916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.065553339821657\n",
      "    mean_inference_ms: 1.402555432993222\n",
      "    mean_raw_obs_processing_ms: 0.8062560760040027\n",
      "  time_since_restore: 5372.435510873795\n",
      "  time_this_iter_s: 9.798446655273438\n",
      "  time_total_s: 5372.435510873795\n",
      "  timers:\n",
      "    learn_throughput: 1722.751\n",
      "    learn_time_ms: 580.467\n",
      "    load_throughput: 210320.924\n",
      "    load_time_ms: 4.755\n",
      "    sample_throughput: 90.765\n",
      "    sample_time_ms: 11017.489\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632130197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         5372.44</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 496\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4744999647140502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009258614723842287\n",
      "          policy_loss: -0.0001405870955851343\n",
      "          total_loss: -0.014300698497229153\n",
      "          vf_explained_var: -0.5082854628562927\n",
      "          vf_loss: 0.0005848894168189468\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.95333333333333\n",
      "    ram_util_percent: 64.62666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124356125081628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.064863325456113\n",
      "    mean_inference_ms: 1.402543534497045\n",
      "    mean_raw_obs_processing_ms: 0.8058572649857347\n",
      "  time_since_restore: 5382.511660575867\n",
      "  time_this_iter_s: 10.076149702072144\n",
      "  time_total_s: 5382.511660575867\n",
      "  timers:\n",
      "    learn_throughput: 1721.56\n",
      "    learn_time_ms: 580.869\n",
      "    load_throughput: 315513.029\n",
      "    load_time_ms: 3.169\n",
      "    sample_throughput: 107.909\n",
      "    sample_time_ms: 9267.086\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         5382.51</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 497\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6347027394506666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01984400967316707\n",
      "          policy_loss: 0.001965460139844153\n",
      "          total_loss: -0.013362510171201493\n",
      "          vf_explained_var: -0.7603388428688049\n",
      "          vf_loss: 0.001019056511318518\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.70714285714285\n",
      "    ram_util_percent: 64.60714285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041243230517768084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.064121235639702\n",
      "    mean_inference_ms: 1.4025315214572531\n",
      "    mean_raw_obs_processing_ms: 0.8054629116157109\n",
      "  time_since_restore: 5392.382195949554\n",
      "  time_this_iter_s: 9.870535373687744\n",
      "  time_total_s: 5392.382195949554\n",
      "  timers:\n",
      "    learn_throughput: 1723.438\n",
      "    learn_time_ms: 580.236\n",
      "    load_throughput: 317565.057\n",
      "    load_time_ms: 3.149\n",
      "    sample_throughput: 108.02\n",
      "    sample_time_ms: 9257.569\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632130217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         5392.38</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 498\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5816919194327461\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012695351528617928\n",
      "          policy_loss: -0.015245817746553156\n",
      "          total_loss: -0.03043913058936596\n",
      "          vf_explained_var: -0.6369250416755676\n",
      "          vf_loss: 0.0006236043706950214\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.81428571428571\n",
      "    ram_util_percent: 64.64285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041242897427159406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.063369188046094\n",
      "    mean_inference_ms: 1.4025194892110668\n",
      "    mean_raw_obs_processing_ms: 0.8050729766959587\n",
      "  time_since_restore: 5402.189400196075\n",
      "  time_this_iter_s: 9.807204246520996\n",
      "  time_total_s: 5402.189400196075\n",
      "  timers:\n",
      "    learn_throughput: 1722.724\n",
      "    learn_time_ms: 580.476\n",
      "    load_throughput: 317334.403\n",
      "    load_time_ms: 3.151\n",
      "    sample_throughput: 108.078\n",
      "    sample_time_ms: 9252.559\n",
      "    update_time_ms: 1.644\n",
      "  timestamp: 1632130227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         5402.19</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 499\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5427910725275675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017858496281681367\n",
      "          policy_loss: 0.037488394934270114\n",
      "          total_loss: 0.02251978673868709\n",
      "          vf_explained_var: -0.6215006709098816\n",
      "          vf_loss: 0.0004593013764760043\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.407142857142865\n",
      "    ram_util_percent: 64.65714285714287\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124256339672741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.062620288135205\n",
      "    mean_inference_ms: 1.4025074547667713\n",
      "    mean_raw_obs_processing_ms: 0.804687428918709\n",
      "  time_since_restore: 5412.079299449921\n",
      "  time_this_iter_s: 9.889899253845215\n",
      "  time_total_s: 5412.079299449921\n",
      "  timers:\n",
      "    learn_throughput: 1714.798\n",
      "    learn_time_ms: 583.159\n",
      "    load_throughput: 319189.072\n",
      "    load_time_ms: 3.133\n",
      "    sample_throughput: 108.101\n",
      "    sample_time_ms: 9250.574\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         5412.08</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 500\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.732596680853102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009776743288154914\n",
      "          policy_loss: -0.026673438772559167\n",
      "          total_loss: -0.04312060620221827\n",
      "          vf_explained_var: -0.9460955858230591\n",
      "          vf_loss: 0.0008788001257926225\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.83571428571428\n",
      "    ram_util_percent: 64.72142857142858\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124222444163919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.061868033393626\n",
      "    mean_inference_ms: 1.4024955009694924\n",
      "    mean_raw_obs_processing_ms: 0.8043062299839527\n",
      "  time_since_restore: 5422.0927567481995\n",
      "  time_this_iter_s: 10.013457298278809\n",
      "  time_total_s: 5422.0927567481995\n",
      "  timers:\n",
      "    learn_throughput: 1713.478\n",
      "    learn_time_ms: 583.608\n",
      "    load_throughput: 318931.8\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 107.695\n",
      "    sample_time_ms: 9285.477\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         5422.09</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 501\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5927435649765862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015536762221639893\n",
      "          policy_loss: -0.04435652411646313\n",
      "          total_loss: -0.059596933217512235\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006870256051317685\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.90714285714285\n",
      "    ram_util_percent: 64.84285714285713\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124188944818283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.06111408729171\n",
      "    mean_inference_ms: 1.4024836259176723\n",
      "    mean_raw_obs_processing_ms: 0.803929311619889\n",
      "  time_since_restore: 5431.785785198212\n",
      "  time_this_iter_s: 9.693028450012207\n",
      "  time_total_s: 5431.785785198212\n",
      "  timers:\n",
      "    learn_throughput: 1712.319\n",
      "    learn_time_ms: 584.003\n",
      "    load_throughput: 318689.471\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 107.713\n",
      "    sample_time_ms: 9283.936\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1632130257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         5431.79</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 502\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9371164507336087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015447299311593376\n",
      "          policy_loss: -0.03995449576112959\n",
      "          total_loss: -0.05866869986057281\n",
      "          vf_explained_var: -0.5348568558692932\n",
      "          vf_loss: 0.0006569604205045229\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.779999999999994\n",
      "    ram_util_percent: 64.79999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124155826642645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.060366939983448\n",
      "    mean_inference_ms: 1.4024717789084031\n",
      "    mean_raw_obs_processing_ms: 0.8035566569200329\n",
      "  time_since_restore: 5442.060136079788\n",
      "  time_this_iter_s: 10.274350881576538\n",
      "  time_total_s: 5442.060136079788\n",
      "  timers:\n",
      "    learn_throughput: 1714.168\n",
      "    learn_time_ms: 583.373\n",
      "    load_throughput: 319308.14\n",
      "    load_time_ms: 3.132\n",
      "    sample_throughput: 107.246\n",
      "    sample_time_ms: 9324.345\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1632130267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         5442.06</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 503\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6818943791919285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014771592902090132\n",
      "          policy_loss: -0.020652170934610897\n",
      "          total_loss: -0.03716460205614567\n",
      "          vf_explained_var: -0.1733488142490387\n",
      "          vf_loss: 0.00030651225485295677\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.77333333333333\n",
      "    ram_util_percent: 64.87333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124122851546407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.059627653021055\n",
      "    mean_inference_ms: 1.4024600039939432\n",
      "    mean_raw_obs_processing_ms: 0.8031885428981722\n",
      "  time_since_restore: 5452.420058488846\n",
      "  time_this_iter_s: 10.359922409057617\n",
      "  time_total_s: 5452.420058488846\n",
      "  timers:\n",
      "    learn_throughput: 1715.117\n",
      "    learn_time_ms: 583.05\n",
      "    load_throughput: 319748.733\n",
      "    load_time_ms: 3.127\n",
      "    sample_throughput: 106.716\n",
      "    sample_time_ms: 9370.644\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632130278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">         5452.42</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 504\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4767320262061225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011275887010103286\n",
      "          policy_loss: -0.006295013758871291\n",
      "          total_loss: -0.020615073790152868\n",
      "          vf_explained_var: -0.9084663987159729\n",
      "          vf_loss: 0.00044725895309562073\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.878571428571426\n",
      "    ram_util_percent: 64.90714285714286\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041240901730948805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.05889632545471\n",
      "    mean_inference_ms: 1.4024482452597973\n",
      "    mean_raw_obs_processing_ms: 0.8028246517285237\n",
      "  time_since_restore: 5462.72377872467\n",
      "  time_this_iter_s: 10.303720235824585\n",
      "  time_total_s: 5462.72377872467\n",
      "  timers:\n",
      "    learn_throughput: 1713.821\n",
      "    learn_time_ms: 583.491\n",
      "    load_throughput: 318834.825\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 106.215\n",
      "    sample_time_ms: 9414.877\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1632130288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         5462.72</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 505\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8629927105373807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007479332141671414\n",
      "          policy_loss: -0.020399030215210386\n",
      "          total_loss: -0.03872802679737409\n",
      "          vf_explained_var: -0.8670327663421631\n",
      "          vf_loss: 0.00030093173425282455\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.74666666666667\n",
      "    ram_util_percent: 64.96000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124057378901473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.058177652265789\n",
      "    mean_inference_ms: 1.4024362258586656\n",
      "    mean_raw_obs_processing_ms: 0.8024649564776977\n",
      "  time_since_restore: 5473.222369670868\n",
      "  time_this_iter_s: 10.49859094619751\n",
      "  time_total_s: 5473.222369670868\n",
      "  timers:\n",
      "    learn_throughput: 1714.725\n",
      "    learn_time_ms: 583.184\n",
      "    load_throughput: 317663.667\n",
      "    load_time_ms: 3.148\n",
      "    sample_throughput: 105.428\n",
      "    sample_time_ms: 9485.178\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1632130298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         5473.22</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 506\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7064217858844333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019406288906903216\n",
      "          policy_loss: -0.027086992147896026\n",
      "          total_loss: -0.04384486228227615\n",
      "          vf_explained_var: -0.6394553184509277\n",
      "          vf_loss: 0.00030634692035770664\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.87333333333333\n",
      "    ram_util_percent: 65.02666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124024659918362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.057461800302217\n",
      "    mean_inference_ms: 1.4024242304957115\n",
      "    mean_raw_obs_processing_ms: 0.8021097041215974\n",
      "  time_since_restore: 5483.4358949661255\n",
      "  time_this_iter_s: 10.213525295257568\n",
      "  time_total_s: 5483.4358949661255\n",
      "  timers:\n",
      "    learn_throughput: 1716.768\n",
      "    learn_time_ms: 582.49\n",
      "    load_throughput: 318362.91\n",
      "    load_time_ms: 3.141\n",
      "    sample_throughput: 105.268\n",
      "    sample_time_ms: 9499.605\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1632130309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         5483.44</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-31-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 507\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7636322114202712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018415168416045843\n",
      "          policy_loss: -0.12986082955160075\n",
      "          total_loss: -0.14711832075069348\n",
      "          vf_explained_var: -0.9961034059524536\n",
      "          vf_loss: 0.00037882995481292405\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.726666666666674\n",
      "    ram_util_percent: 65.03333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123992346940272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.056762611633662\n",
      "    mean_inference_ms: 1.4024123086934444\n",
      "    mean_raw_obs_processing_ms: 0.8017585760153715\n",
      "  time_since_restore: 5493.928753376007\n",
      "  time_this_iter_s: 10.492858409881592\n",
      "  time_total_s: 5493.928753376007\n",
      "  timers:\n",
      "    learn_throughput: 1714.482\n",
      "    learn_time_ms: 583.266\n",
      "    load_throughput: 315095.859\n",
      "    load_time_ms: 3.174\n",
      "    sample_throughput: 104.591\n",
      "    sample_time_ms: 9561.016\n",
      "    update_time_ms: 3.248\n",
      "  timestamp: 1632130319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         5493.93</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 508\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0159029603004455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013777586213073947\n",
      "          policy_loss: 0.004927580720848508\n",
      "          total_loss: -0.014672123061286079\n",
      "          vf_explained_var: -0.502028226852417\n",
      "          vf_loss: 0.0005593216835728122\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.51333333333332\n",
      "    ram_util_percent: 65.08000000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123959966160024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.056072668459338\n",
      "    mean_inference_ms: 1.4024003718479088\n",
      "    mean_raw_obs_processing_ms: 0.8014115177594234\n",
      "  time_since_restore: 5504.411414384842\n",
      "  time_this_iter_s: 10.482661008834839\n",
      "  time_total_s: 5504.411414384842\n",
      "  timers:\n",
      "    learn_throughput: 1714.862\n",
      "    learn_time_ms: 583.137\n",
      "    load_throughput: 314182.428\n",
      "    load_time_ms: 3.183\n",
      "    sample_throughput: 103.856\n",
      "    sample_time_ms: 9628.688\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1632130330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         5504.41</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-32-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 509\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.008553495672014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016694273627690315\n",
      "          policy_loss: 0.03298587509327465\n",
      "          total_loss: 0.013351456655396356\n",
      "          vf_explained_var: -0.6966861486434937\n",
      "          vf_loss: 0.0004511141646718089\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.673333333333325\n",
      "    ram_util_percent: 65.12000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123927180885089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.055395650345494\n",
      "    mean_inference_ms: 1.4023884620402558\n",
      "    mean_raw_obs_processing_ms: 0.8010684774946393\n",
      "  time_since_restore: 5514.988891363144\n",
      "  time_this_iter_s: 10.577476978302002\n",
      "  time_total_s: 5514.988891363144\n",
      "  timers:\n",
      "    learn_throughput: 1723.308\n",
      "    learn_time_ms: 580.279\n",
      "    load_throughput: 314067.152\n",
      "    load_time_ms: 3.184\n",
      "    sample_throughput: 103.089\n",
      "    sample_time_ms: 9700.311\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1632130340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         5514.99</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 510\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6817149268256293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01681070107591817\n",
      "          policy_loss: 0.01832950164874395\n",
      "          total_loss: 0.0020534948342376285\n",
      "          vf_explained_var: -0.13512089848518372\n",
      "          vf_loss: 0.0005411417419155542\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.926666666666655\n",
      "    ram_util_percent: 65.15333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123894115846602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.054733698963311\n",
      "    mean_inference_ms: 1.402376663920481\n",
      "    mean_raw_obs_processing_ms: 0.8007294371860898\n",
      "  time_since_restore: 5525.489872217178\n",
      "  time_this_iter_s: 10.500980854034424\n",
      "  time_total_s: 5525.489872217178\n",
      "  timers:\n",
      "    learn_throughput: 1723.132\n",
      "    learn_time_ms: 580.339\n",
      "    load_throughput: 314267.175\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 102.575\n",
      "    sample_time_ms: 9748.999\n",
      "    update_time_ms: 3.254\n",
      "  timestamp: 1632130351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         5525.49</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 511\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5293267452420883e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4063277436627282\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023485064118752813\n",
      "          policy_loss: -0.14196485098865297\n",
      "          total_loss: -0.15567471235990524\n",
      "          vf_explained_var: 0.12288360297679901\n",
      "          vf_loss: 0.00035341617719192677\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.74666666666667\n",
      "    ram_util_percent: 65.17333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123860793939104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.054074580261267\n",
      "    mean_inference_ms: 1.4023648712873977\n",
      "    mean_raw_obs_processing_ms: 0.8003944082606936\n",
      "  time_since_restore: 5535.808897733688\n",
      "  time_this_iter_s: 10.31902551651001\n",
      "  time_total_s: 5535.808897733688\n",
      "  timers:\n",
      "    learn_throughput: 1722.976\n",
      "    learn_time_ms: 580.391\n",
      "    load_throughput: 315218.999\n",
      "    load_time_ms: 3.172\n",
      "    sample_throughput: 101.921\n",
      "    sample_time_ms: 9811.559\n",
      "    update_time_ms: 3.249\n",
      "  timestamp: 1632130361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         5535.81</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 512\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.649394200907813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008138222984469617\n",
      "          policy_loss: -0.0492942977282736\n",
      "          total_loss: -0.06537696876459652\n",
      "          vf_explained_var: -0.7613017559051514\n",
      "          vf_loss: 0.00041127143308080527\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.9\n",
      "    ram_util_percent: 65.24285714285715\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041238271020555814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.053418599610165\n",
      "    mean_inference_ms: 1.4023530769956707\n",
      "    mean_raw_obs_processing_ms: 0.8000633415603885\n",
      "  time_since_restore: 5545.991065502167\n",
      "  time_this_iter_s: 10.182167768478394\n",
      "  time_total_s: 5545.991065502167\n",
      "  timers:\n",
      "    learn_throughput: 1723.165\n",
      "    learn_time_ms: 580.328\n",
      "    load_throughput: 314906.601\n",
      "    load_time_ms: 3.176\n",
      "    sample_throughput: 102.016\n",
      "    sample_time_ms: 9802.399\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1632130371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         5545.99</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 513\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7181570172309875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014462423582675557\n",
      "          policy_loss: -0.05935398894879553\n",
      "          total_loss: -0.07616094847520193\n",
      "          vf_explained_var: -0.3955064117908478\n",
      "          vf_loss: 0.00037460701633891503\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.63333333333334\n",
      "    ram_util_percent: 65.27999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123793558326391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.052767576519116\n",
      "    mean_inference_ms: 1.402341441191369\n",
      "    mean_raw_obs_processing_ms: 0.7997361900273242\n",
      "  time_since_restore: 5556.294675827026\n",
      "  time_this_iter_s: 10.30361032485962\n",
      "  time_total_s: 5556.294675827026\n",
      "  timers:\n",
      "    learn_throughput: 1722.667\n",
      "    learn_time_ms: 580.495\n",
      "    load_throughput: 313874.429\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 102.076\n",
      "    sample_time_ms: 9796.622\n",
      "    update_time_ms: 3.245\n",
      "  timestamp: 1632130382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         5556.29</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-33-12\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 514\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.820177310042911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012151052681242823\n",
      "          policy_loss: 0.00750099155637953\n",
      "          total_loss: -0.010158007840315501\n",
      "          vf_explained_var: -0.09459666907787323\n",
      "          vf_loss: 0.0005427770652911729\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.68666666666666\n",
      "    ram_util_percent: 65.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123760117685924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.052134005407803\n",
      "    mean_inference_ms: 1.4023299384223415\n",
      "    mean_raw_obs_processing_ms: 0.7994129483933214\n",
      "  time_since_restore: 5566.802749633789\n",
      "  time_this_iter_s: 10.508073806762695\n",
      "  time_total_s: 5566.802749633789\n",
      "  timers:\n",
      "    learn_throughput: 1722.929\n",
      "    learn_time_ms: 580.407\n",
      "    load_throughput: 314248.339\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 101.839\n",
      "    sample_time_ms: 9819.426\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">          5566.8</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 995.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 515\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8307423803541396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019276558432237245\n",
      "          policy_loss: -0.0006210726996262868\n",
      "          total_loss: -0.01829266490207778\n",
      "          vf_explained_var: 0.08862311393022537\n",
      "          vf_loss: 0.0006358334235200244\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.7\n",
      "    ram_util_percent: 65.29999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123726279574755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.051509317091693\n",
      "    mean_inference_ms: 1.402318427232353\n",
      "    mean_raw_obs_processing_ms: 0.7990935618406203\n",
      "  time_since_restore: 5577.23063993454\n",
      "  time_this_iter_s: 10.427890300750732\n",
      "  time_total_s: 5577.23063993454\n",
      "  timers:\n",
      "    learn_throughput: 1720.964\n",
      "    learn_time_ms: 581.07\n",
      "    load_throughput: 314580.665\n",
      "    load_time_ms: 3.179\n",
      "    sample_throughput: 101.919\n",
      "    sample_time_ms: 9811.693\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632130403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">         5577.23</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 516\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2744330141279432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008154313372230743\n",
      "          policy_loss: 0.01233999952673912\n",
      "          total_loss: -0.00017782627708382077\n",
      "          vf_explained_var: -0.5348114371299744\n",
      "          vf_loss: 0.00022650501972141986\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.28780487804878\n",
      "    ram_util_percent: 64.96829268292683\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123692603850431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.050916274895181\n",
      "    mean_inference_ms: 1.4023069645746093\n",
      "    mean_raw_obs_processing_ms: 0.7991097378585742\n",
      "  time_since_restore: 5605.74739074707\n",
      "  time_this_iter_s: 28.516750812530518\n",
      "  time_total_s: 5605.74739074707\n",
      "  timers:\n",
      "    learn_throughput: 1720.012\n",
      "    learn_time_ms: 581.391\n",
      "    load_throughput: 213585.367\n",
      "    load_time_ms: 4.682\n",
      "    sample_throughput: 85.909\n",
      "    sample_time_ms: 11640.186\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632130431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         5605.75</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-02\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 517\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.516224686967002\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012456725673577808\n",
      "          policy_loss: -0.09554641818006833\n",
      "          total_loss: -0.11032354864809248\n",
      "          vf_explained_var: -0.25111913681030273\n",
      "          vf_loss: 0.0003851165524489867\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.08666666666667\n",
      "    ram_util_percent: 65.28666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123659614618119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.050328099908459\n",
      "    mean_inference_ms: 1.4022956930113712\n",
      "    mean_raw_obs_processing_ms: 0.7991290723898894\n",
      "  time_since_restore: 5616.177535057068\n",
      "  time_this_iter_s: 10.430144309997559\n",
      "  time_total_s: 5616.177535057068\n",
      "  timers:\n",
      "    learn_throughput: 1719.569\n",
      "    learn_time_ms: 581.541\n",
      "    load_throughput: 215039.58\n",
      "    load_time_ms: 4.65\n",
      "    sample_throughput: 85.956\n",
      "    sample_time_ms: 11633.794\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632130442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         5616.18</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 518\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4033665855725606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01845003799822962\n",
      "          policy_loss: -0.03795247086220317\n",
      "          total_loss: -0.05161098179717859\n",
      "          vf_explained_var: 0.44704386591911316\n",
      "          vf_loss: 0.00037515405662513025\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.31428571428571\n",
      "    ram_util_percent: 65.2785714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041236266987002314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04974524987633\n",
      "    mean_inference_ms: 1.4022843728184562\n",
      "    mean_raw_obs_processing_ms: 0.7991514994510301\n",
      "  time_since_restore: 5626.204580545425\n",
      "  time_this_iter_s: 10.027045488357544\n",
      "  time_total_s: 5626.204580545425\n",
      "  timers:\n",
      "    learn_throughput: 1718.998\n",
      "    learn_time_ms: 581.734\n",
      "    load_throughput: 215709.775\n",
      "    load_time_ms: 4.636\n",
      "    sample_throughput: 86.296\n",
      "    sample_time_ms: 11588.036\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632130452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">          5626.2</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 519\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.824060282442305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011834628690359948\n",
      "          policy_loss: -0.08641264956030581\n",
      "          total_loss: -0.1041807693325811\n",
      "          vf_explained_var: -0.21760979294776917\n",
      "          vf_loss: 0.00047248398607027616\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.75999999999999\n",
      "    ram_util_percent: 65.28666666666665\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123593555439139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04916580459536\n",
      "    mean_inference_ms: 1.4022729658613522\n",
      "    mean_raw_obs_processing_ms: 0.7991770199089047\n",
      "  time_since_restore: 5636.636628627777\n",
      "  time_this_iter_s: 10.432048082351685\n",
      "  time_total_s: 5636.636628627777\n",
      "  timers:\n",
      "    learn_throughput: 1718.014\n",
      "    learn_time_ms: 582.067\n",
      "    load_throughput: 215407.339\n",
      "    load_time_ms: 4.642\n",
      "    sample_throughput: 86.407\n",
      "    sample_time_ms: 11573.152\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632130462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         5636.64</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 520\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.293990117863131e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3166317303975423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0483830928062081\n",
      "          policy_loss: 0.08979354856742752\n",
      "          total_loss: 0.07708512660529879\n",
      "          vf_explained_var: -0.11406131088733673\n",
      "          vf_loss: 0.0004578979349591666\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.707142857142856\n",
      "    ram_util_percent: 65.20714285714288\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123559820393211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.048588523945545\n",
      "    mean_inference_ms: 1.4022614085094285\n",
      "    mean_raw_obs_processing_ms: 0.7992056101495975\n",
      "  time_since_restore: 5646.550801277161\n",
      "  time_this_iter_s: 9.914172649383545\n",
      "  time_total_s: 5646.550801277161\n",
      "  timers:\n",
      "    learn_throughput: 1720.408\n",
      "    learn_time_ms: 581.257\n",
      "    load_throughput: 215446.065\n",
      "    load_time_ms: 4.642\n",
      "    sample_throughput: 86.841\n",
      "    sample_time_ms: 11515.298\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632130472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         5646.55</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 521\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7112243122524686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017090850780693105\n",
      "          policy_loss: -0.06471010289258428\n",
      "          total_loss: -0.08141226205560896\n",
      "          vf_explained_var: -0.5841163396835327\n",
      "          vf_loss: 0.0004100830643437803\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.873333333333335\n",
      "    ram_util_percent: 65.12000000000002\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041235264048924476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04801871599164\n",
      "    mean_inference_ms: 1.402249827695817\n",
      "    mean_raw_obs_processing_ms: 0.7992372478054566\n",
      "  time_since_restore: 5657.148066997528\n",
      "  time_this_iter_s: 10.597265720367432\n",
      "  time_total_s: 5657.148066997528\n",
      "  timers:\n",
      "    learn_throughput: 1721.618\n",
      "    learn_time_ms: 580.849\n",
      "    load_throughput: 214991.081\n",
      "    load_time_ms: 4.651\n",
      "    sample_throughput: 86.629\n",
      "    sample_time_ms: 11543.519\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1632130483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         5657.15</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 522\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.721665174431271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01460496366815794\n",
      "          policy_loss: 0.02308639900551902\n",
      "          total_loss: 0.0062247569362322485\n",
      "          vf_explained_var: -0.2449001669883728\n",
      "          vf_loss: 0.0003550080123305735\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.666666666666664\n",
      "    ram_util_percent: 65.0\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123493110875007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.047448563081735\n",
      "    mean_inference_ms: 1.4022383275551027\n",
      "    mean_raw_obs_processing_ms: 0.7992720551645753\n",
      "  time_since_restore: 5667.33073091507\n",
      "  time_this_iter_s: 10.182663917541504\n",
      "  time_total_s: 5667.33073091507\n",
      "  timers:\n",
      "    learn_throughput: 1722.332\n",
      "    learn_time_ms: 580.608\n",
      "    load_throughput: 215130.023\n",
      "    load_time_ms: 4.648\n",
      "    sample_throughput: 86.627\n",
      "    sample_time_ms: 11543.808\n",
      "    update_time_ms: 1.647\n",
      "  timestamp: 1632130493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         5667.33</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 523\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.789780514770084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013745413222324704\n",
      "          policy_loss: -0.046956950177749\n",
      "          total_loss: -0.06454777403010262\n",
      "          vf_explained_var: -0.7374032139778137\n",
      "          vf_loss: 0.0003069770566879823\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.5\n",
      "    ram_util_percent: 64.89999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041234602250710815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04689913939452\n",
      "    mean_inference_ms: 1.4022266282776321\n",
      "    mean_raw_obs_processing_ms: 0.7993098479580497\n",
      "  time_since_restore: 5678.099280595779\n",
      "  time_this_iter_s: 10.768549680709839\n",
      "  time_total_s: 5678.099280595779\n",
      "  timers:\n",
      "    learn_throughput: 1723.308\n",
      "    learn_time_ms: 580.279\n",
      "    load_throughput: 215523.56\n",
      "    load_time_ms: 4.64\n",
      "    sample_throughput: 86.277\n",
      "    sample_time_ms: 11590.632\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1632130504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">          5678.1</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 524\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7007594916555617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013470711330082649\n",
      "          policy_loss: -0.0005066297948360443\n",
      "          total_loss: -0.017254981936679945\n",
      "          vf_explained_var: -0.8338697552680969\n",
      "          vf_loss: 0.00025924251595925955\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.65\n",
      "    ram_util_percent: 64.8\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123426951851199\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04636587026521\n",
      "    mean_inference_ms: 1.402214897820538\n",
      "    mean_raw_obs_processing_ms: 0.7993507280572031\n",
      "  time_since_restore: 5688.70095038414\n",
      "  time_this_iter_s: 10.601669788360596\n",
      "  time_total_s: 5688.70095038414\n",
      "  timers:\n",
      "    learn_throughput: 1725.26\n",
      "    learn_time_ms: 579.623\n",
      "    load_throughput: 216187.865\n",
      "    load_time_ms: 4.626\n",
      "    sample_throughput: 86.202\n",
      "    sample_time_ms: 11600.669\n",
      "    update_time_ms: 1.66\n",
      "  timestamp: 1632130514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">          5688.7</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 525\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.755155372619629\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014916294761707347\n",
      "          policy_loss: 0.08674262596501245\n",
      "          total_loss: 0.06950102121465736\n",
      "          vf_explained_var: -0.44371071457862854\n",
      "          vf_loss: 0.00030994884453118885\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.480000000000004\n",
      "    ram_util_percent: 64.79999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041233935494964805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04586422228923\n",
      "    mean_inference_ms: 1.4022033191065908\n",
      "    mean_raw_obs_processing_ms: 0.7993942367114364\n",
      "  time_since_restore: 5699.484457492828\n",
      "  time_this_iter_s: 10.783507108688354\n",
      "  time_total_s: 5699.484457492828\n",
      "  timers:\n",
      "    learn_throughput: 1725.982\n",
      "    learn_time_ms: 579.38\n",
      "    load_throughput: 216600.944\n",
      "    load_time_ms: 4.617\n",
      "    sample_throughput: 85.937\n",
      "    sample_time_ms: 11636.488\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632130525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         5699.48</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 526\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7369939062330457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010922096971461947\n",
      "          policy_loss: 0.055234671549664605\n",
      "          total_loss: 0.0380757932861646\n",
      "          vf_explained_var: -0.04997142031788826\n",
      "          vf_loss: 0.0002110650861545259\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.953333333333326\n",
      "    ram_util_percent: 64.79999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041233606861248856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.045402707208375\n",
      "    mean_inference_ms: 1.4021922445349986\n",
      "    mean_raw_obs_processing_ms: 0.79904158590789\n",
      "  time_since_restore: 5710.276670455933\n",
      "  time_this_iter_s: 10.792212963104248\n",
      "  time_total_s: 5710.276670455933\n",
      "  timers:\n",
      "    learn_throughput: 1725.835\n",
      "    learn_time_ms: 579.43\n",
      "    load_throughput: 320837.145\n",
      "    load_time_ms: 3.117\n",
      "    sample_throughput: 101.363\n",
      "    sample_time_ms: 9865.495\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1632130536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         5710.28</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 527\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.455030651887258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012113539279359426\n",
      "          policy_loss: -0.01952913486295276\n",
      "          total_loss: -0.0336103359858195\n",
      "          vf_explained_var: -0.31534910202026367\n",
      "          vf_loss: 0.0004691060567792091\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.17333333333333\n",
      "    ram_util_percent: 64.87333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123328868968096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04492539230045\n",
      "    mean_inference_ms: 1.4021815735382046\n",
      "    mean_raw_obs_processing_ms: 0.798693051418041\n",
      "  time_since_restore: 5720.706924200058\n",
      "  time_this_iter_s: 10.430253744125366\n",
      "  time_total_s: 5720.706924200058\n",
      "  timers:\n",
      "    learn_throughput: 1719.128\n",
      "    learn_time_ms: 581.69\n",
      "    load_throughput: 314989.373\n",
      "    load_time_ms: 3.175\n",
      "    sample_throughput: 101.387\n",
      "    sample_time_ms: 9863.173\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632130546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         5720.71</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-35-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 528\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4409851767946974e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8174761560228136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026236924782481946\n",
      "          policy_loss: -0.025860002181596228\n",
      "          total_loss: -0.043571420055296683\n",
      "          vf_explained_var: -0.7690131068229675\n",
      "          vf_loss: 0.0004633452555733836\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.32941176470589\n",
      "    ram_util_percent: 65.65294117647058\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123303808069369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.0444967101344\n",
      "    mean_inference_ms: 1.4021726468769558\n",
      "    mean_raw_obs_processing_ms: 0.7983482953173724\n",
      "  time_since_restore: 5732.613456964493\n",
      "  time_this_iter_s: 11.906532764434814\n",
      "  time_total_s: 5732.613456964493\n",
      "  timers:\n",
      "    learn_throughput: 1711.0\n",
      "    learn_time_ms: 584.453\n",
      "    load_throughput: 314234.213\n",
      "    load_time_ms: 3.182\n",
      "    sample_throughput: 99.519\n",
      "    sample_time_ms: 10048.367\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632130558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         5732.61</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-36-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 529\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1614777651920484e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8033158858617147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012086879863492012\n",
      "          policy_loss: 0.02713029028640853\n",
      "          total_loss: 0.009732127851910062\n",
      "          vf_explained_var: -0.37302398681640625\n",
      "          vf_loss: 0.00063499551518665\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.847058823529416\n",
      "    ram_util_percent: 65.62941176470588\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041232840890314225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04410626697288\n",
      "    mean_inference_ms: 1.4021651237072819\n",
      "    mean_raw_obs_processing_ms: 0.7980073813015106\n",
      "  time_since_restore: 5744.126065969467\n",
      "  time_this_iter_s: 11.512609004974365\n",
      "  time_total_s: 5744.126065969467\n",
      "  timers:\n",
      "    learn_throughput: 1698.331\n",
      "    learn_time_ms: 588.813\n",
      "    load_throughput: 310275.485\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 98.508\n",
      "    sample_time_ms: 10151.455\n",
      "    update_time_ms: 1.673\n",
      "  timestamp: 1632130570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         5744.13</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 530\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1614777651920484e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8396692633628846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012756249347907407\n",
      "          policy_loss: -0.06930708272589578\n",
      "          total_loss: -0.08745374066962136\n",
      "          vf_explained_var: 0.4763726592063904\n",
      "          vf_loss: 0.0002500351123873972\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.68124999999999\n",
      "    ram_util_percent: 65.95625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041232685058079185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.043741862221871\n",
      "    mean_inference_ms: 1.4021587946058722\n",
      "    mean_raw_obs_processing_ms: 0.7976701268704016\n",
      "  time_since_restore: 5755.55308675766\n",
      "  time_this_iter_s: 11.427020788192749\n",
      "  time_total_s: 5755.55308675766\n",
      "  timers:\n",
      "    learn_throughput: 1697.528\n",
      "    learn_time_ms: 589.092\n",
      "    load_throughput: 308968.118\n",
      "    load_time_ms: 3.237\n",
      "    sample_throughput: 97.064\n",
      "    sample_time_ms: 10302.434\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1632130581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         5755.55</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-36-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 531\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.1614777651920484e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5735668725437588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03547799045243513\n",
      "          policy_loss: -0.02324621267616749\n",
      "          total_loss: -0.03844743462072479\n",
      "          vf_explained_var: 0.16187423467636108\n",
      "          vf_loss: 0.000534445383042718\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.84375\n",
      "    ram_util_percent: 65.89375000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123254361265634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.043378170333392\n",
      "    mean_inference_ms: 1.402152823423914\n",
      "    mean_raw_obs_processing_ms: 0.7973364471867914\n",
      "  time_since_restore: 5766.364885568619\n",
      "  time_this_iter_s: 10.811798810958862\n",
      "  time_total_s: 5766.364885568619\n",
      "  timers:\n",
      "    learn_throughput: 1696.054\n",
      "    learn_time_ms: 589.604\n",
      "    load_throughput: 310011.752\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 96.867\n",
      "    sample_time_ms: 10323.395\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1632130592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         5766.36</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-36-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 532\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.742216647788067e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6719449175728691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010481240984655532\n",
      "          policy_loss: -0.029657909232709143\n",
      "          total_loss: -0.0460732070936097\n",
      "          vf_explained_var: -0.508866548538208\n",
      "          vf_loss: 0.0003041514449351881\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.04\n",
      "    ram_util_percent: 65.85333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123242386149319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.043029721986084\n",
      "    mean_inference_ms: 1.4021474706596033\n",
      "    mean_raw_obs_processing_ms: 0.7970063898697565\n",
      "  time_since_restore: 5777.178883075714\n",
      "  time_this_iter_s: 10.813997507095337\n",
      "  time_total_s: 5777.178883075714\n",
      "  timers:\n",
      "    learn_throughput: 1692.843\n",
      "    learn_time_ms: 590.722\n",
      "    load_throughput: 307425.916\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 96.289\n",
      "    sample_time_ms: 10385.377\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1632130603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         5777.18</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-36-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 533\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.742216647788067e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.733893174595303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013291893653796846\n",
      "          policy_loss: 0.034200799299610986\n",
      "          total_loss: 0.017252844365106688\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003909764591500991\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.7\n",
      "    ram_util_percent: 65.8\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041232334531543374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042720347165497\n",
      "    mean_inference_ms: 1.4021430761458256\n",
      "    mean_raw_obs_processing_ms: 0.7966800663863808\n",
      "  time_since_restore: 5788.141246080399\n",
      "  time_this_iter_s: 10.962363004684448\n",
      "  time_total_s: 5788.141246080399\n",
      "  timers:\n",
      "    learn_throughput: 1687.291\n",
      "    learn_time_ms: 592.666\n",
      "    load_throughput: 304776.521\n",
      "    load_time_ms: 3.281\n",
      "    sample_throughput: 96.128\n",
      "    sample_time_ms: 10402.789\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1632130614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         5788.14</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-37-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 534\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.742216647788067e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7584427224265204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022079993267602464\n",
      "          policy_loss: -0.034447061187691155\n",
      "          total_loss: -0.05149232819676399\n",
      "          vf_explained_var: -0.7939902544021606\n",
      "          vf_loss: 0.000539159754892656\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.8625\n",
      "    ram_util_percent: 66.13125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123226587297563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04245362502958\n",
      "    mean_inference_ms: 1.402139475189488\n",
      "    mean_raw_obs_processing_ms: 0.7963574670075413\n",
      "  time_since_restore: 5799.311172246933\n",
      "  time_this_iter_s: 11.169926166534424\n",
      "  time_total_s: 5799.311172246933\n",
      "  timers:\n",
      "    learn_throughput: 1686.691\n",
      "    learn_time_ms: 592.877\n",
      "    load_throughput: 302885.946\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 95.608\n",
      "    sample_time_ms: 10459.361\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1632130625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">         5799.31</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 535\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1613324971682104e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.889826946788364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021431169470420584\n",
      "          policy_loss: 0.01697747463153468\n",
      "          total_loss: -0.0014415838238265779\n",
      "          vf_explained_var: -0.7793727517127991\n",
      "          vf_loss: 0.00047920737189189016\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.09333333333335\n",
      "    ram_util_percent: 66.10666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041232205210839146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042215145187495\n",
      "    mean_inference_ms: 1.4021363066129269\n",
      "    mean_raw_obs_processing_ms: 0.7960386550019248\n",
      "  time_since_restore: 5810.343528747559\n",
      "  time_this_iter_s: 11.03235650062561\n",
      "  time_total_s: 5810.343528747559\n",
      "  timers:\n",
      "    learn_throughput: 1687.425\n",
      "    learn_time_ms: 592.619\n",
      "    load_throughput: 301950.514\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 95.379\n",
      "    sample_time_ms: 10484.484\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1632130636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         5810.34</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-37-27\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 536\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7419987457523165e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8005528145366245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0208422932062081\n",
      "          policy_loss: -0.08376469781829252\n",
      "          total_loss: -0.10146849134729968\n",
      "          vf_explained_var: -0.4976228177547455\n",
      "          vf_loss: 0.0003017353083123453\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.86875\n",
      "    ram_util_percent: 66.14375000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123215308743707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042013181571821\n",
      "    mean_inference_ms: 1.4021336541237244\n",
      "    mean_raw_obs_processing_ms: 0.7957233525738614\n",
      "  time_since_restore: 5821.384072542191\n",
      "  time_this_iter_s: 11.040543794631958\n",
      "  time_total_s: 5821.384072542191\n",
      "  timers:\n",
      "    learn_throughput: 1688.963\n",
      "    learn_time_ms: 592.079\n",
      "    load_throughput: 302135.396\n",
      "    load_time_ms: 3.31\n",
      "    sample_throughput: 95.149\n",
      "    sample_time_ms: 10509.863\n",
      "    update_time_ms: 1.67\n",
      "  timestamp: 1632130647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         5821.38</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-37-38\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 537\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7203731642829048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01695822750650496\n",
      "          policy_loss: -0.040853596147563724\n",
      "          total_loss: -0.057736505278282696\n",
      "          vf_explained_var: -0.5775948166847229\n",
      "          vf_loss: 0.00032082261160313566\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.51875\n",
      "    ram_util_percent: 66.2\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123210810747314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041851701032517\n",
      "    mean_inference_ms: 1.4021315550795321\n",
      "    mean_raw_obs_processing_ms: 0.7954118010442405\n",
      "  time_since_restore: 5832.496656179428\n",
      "  time_this_iter_s: 11.112583637237549\n",
      "  time_total_s: 5832.496656179428\n",
      "  timers:\n",
      "    learn_throughput: 1694.748\n",
      "    learn_time_ms: 590.058\n",
      "    load_throughput: 306722.244\n",
      "    load_time_ms: 3.26\n",
      "    sample_throughput: 94.516\n",
      "    sample_time_ms: 10580.191\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1632130658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">          5832.5</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 538\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9497089465459188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015481398047737778\n",
      "          policy_loss: 0.03353211962514453\n",
      "          total_loss: 0.014314390139447318\n",
      "          vf_explained_var: -0.16035322844982147\n",
      "          vf_loss: 0.00027935696562053636\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.6875\n",
      "    ram_util_percent: 66.28125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123207312104861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041719639029829\n",
      "    mean_inference_ms: 1.4021298380368383\n",
      "    mean_raw_obs_processing_ms: 0.795103883401677\n",
      "  time_since_restore: 5843.566557168961\n",
      "  time_this_iter_s: 11.06990098953247\n",
      "  time_total_s: 5843.566557168961\n",
      "  timers:\n",
      "    learn_throughput: 1702.573\n",
      "    learn_time_ms: 587.346\n",
      "    load_throughput: 305426.795\n",
      "    load_time_ms: 3.274\n",
      "    sample_throughput: 95.245\n",
      "    sample_time_ms: 10499.246\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1632130669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         5843.57</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 539\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8802519665824042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018039226841811463\n",
      "          policy_loss: 0.01307272066672643\n",
      "          total_loss: -0.005495048065980276\n",
      "          vf_explained_var: -0.2869774401187897\n",
      "          vf_loss: 0.00023475030369202917\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.266666666666666\n",
      "    ram_util_percent: 66.35333333333332\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123205948909317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041614822823005\n",
      "    mean_inference_ms: 1.4021288333279087\n",
      "    mean_raw_obs_processing_ms: 0.794799358527072\n",
      "  time_since_restore: 5854.402535915375\n",
      "  time_this_iter_s: 10.835978746414185\n",
      "  time_total_s: 5854.402535915375\n",
      "  timers:\n",
      "    learn_throughput: 1693.018\n",
      "    learn_time_ms: 590.661\n",
      "    load_throughput: 307951.836\n",
      "    load_time_ms: 3.247\n",
      "    sample_throughput: 95.888\n",
      "    sample_time_ms: 10428.867\n",
      "    update_time_ms: 1.634\n",
      "  timestamp: 1632130680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">          5854.4</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 540\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9637380454275344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015027174151984695\n",
      "          policy_loss: 0.0652838862604565\n",
      "          total_loss: 0.04599358547064993\n",
      "          vf_explained_var: -0.36159271001815796\n",
      "          vf_loss: 0.0003470764611847699\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.81875\n",
      "    ram_util_percent: 66.33749999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123206518828149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041531866755038\n",
      "    mean_inference_ms: 1.402128283781126\n",
      "    mean_raw_obs_processing_ms: 0.7944985686425342\n",
      "  time_since_restore: 5865.499875068665\n",
      "  time_this_iter_s: 11.097339153289795\n",
      "  time_total_s: 5865.499875068665\n",
      "  timers:\n",
      "    learn_throughput: 1692.688\n",
      "    learn_time_ms: 590.776\n",
      "    load_throughput: 309043.244\n",
      "    load_time_ms: 3.236\n",
      "    sample_throughput: 96.193\n",
      "    sample_time_ms: 10395.815\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632130691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">          5865.5</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 541\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8791547457377116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01586210640706555\n",
      "          policy_loss: -0.09110939494437642\n",
      "          total_loss: -0.10958868894312117\n",
      "          vf_explained_var: -0.342478483915329\n",
      "          vf_loss: 0.00031225111953163935\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.8125\n",
      "    ram_util_percent: 66.39375000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123207363832899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04145963835633\n",
      "    mean_inference_ms: 1.4021279312242276\n",
      "    mean_raw_obs_processing_ms: 0.7942017090547644\n",
      "  time_since_restore: 5876.372856616974\n",
      "  time_this_iter_s: 10.872981548309326\n",
      "  time_total_s: 5876.372856616974\n",
      "  timers:\n",
      "    learn_throughput: 1692.063\n",
      "    learn_time_ms: 590.994\n",
      "    load_throughput: 307868.2\n",
      "    load_time_ms: 3.248\n",
      "    sample_throughput: 96.138\n",
      "    sample_time_ms: 10401.7\n",
      "    update_time_ms: 1.618\n",
      "  timestamp: 1632130702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         5876.37</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 542\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7805566840701632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01760120283474293\n",
      "          policy_loss: 0.02707177698612213\n",
      "          total_loss: 0.009837522192133798\n",
      "          vf_explained_var: -0.42798516154289246\n",
      "          vf_loss: 0.0005713084843591787\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.6625\n",
      "    ram_util_percent: 66.46875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123211389686631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041400018377528\n",
      "    mean_inference_ms: 1.4021283607240052\n",
      "    mean_raw_obs_processing_ms: 0.7939084937531163\n",
      "  time_since_restore: 5887.463999032974\n",
      "  time_this_iter_s: 11.091142416000366\n",
      "  time_total_s: 5887.463999032974\n",
      "  timers:\n",
      "    learn_throughput: 1668.534\n",
      "    learn_time_ms: 599.329\n",
      "    load_throughput: 305535.814\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 95.959\n",
      "    sample_time_ms: 10421.069\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632130713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         5887.46</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 543\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7691873033841452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01281144103692695\n",
      "          policy_loss: 0.01709805859459771\n",
      "          total_loss: -0.00017973101801342433\n",
      "          vf_explained_var: -0.538998544216156\n",
      "          vf_loss: 0.00041408026105879496\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.01875\n",
      "    ram_util_percent: 66.79375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123219495999948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04136774462228\n",
      "    mean_inference_ms: 1.4021298445048171\n",
      "    mean_raw_obs_processing_ms: 0.7936190529493834\n",
      "  time_since_restore: 5898.902269363403\n",
      "  time_this_iter_s: 11.438270330429077\n",
      "  time_total_s: 5898.902269363403\n",
      "  timers:\n",
      "    learn_throughput: 1672.146\n",
      "    learn_time_ms: 598.034\n",
      "    load_throughput: 305901.264\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 95.513\n",
      "    sample_time_ms: 10469.764\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632130725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">          5898.9</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 544\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.62322678565979\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01984798812388296\n",
      "          policy_loss: -0.04217687555485301\n",
      "          total_loss: -0.058087134030130175\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003220078634007627\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.64705882352941\n",
      "    ram_util_percent: 66.98823529411764\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123234599005755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041367346271379\n",
      "    mean_inference_ms: 1.402132938940509\n",
      "    mean_raw_obs_processing_ms: 0.793333544717441\n",
      "  time_since_restore: 5911.05171251297\n",
      "  time_this_iter_s: 12.14944314956665\n",
      "  time_total_s: 5911.05171251297\n",
      "  timers:\n",
      "    learn_throughput: 1663.931\n",
      "    learn_time_ms: 600.986\n",
      "    load_throughput: 307407.891\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 94.654\n",
      "    sample_time_ms: 10564.763\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632130737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         5911.05</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-39-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 546\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.606622823079427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010630187248818304\n",
      "          policy_loss: -0.08160747107532289\n",
      "          total_loss: -0.09753550808462831\n",
      "          vf_explained_var: -0.5860159397125244\n",
      "          vf_loss: 0.0001381919152562558\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.423809523809524\n",
      "    ram_util_percent: 66.89285714285714\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123276248440844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041425286402639\n",
      "    mean_inference_ms: 1.4021425657207316\n",
      "    mean_raw_obs_processing_ms: 0.7934145204991653\n",
      "  time_since_restore: 5940.026571989059\n",
      "  time_this_iter_s: 28.974859476089478\n",
      "  time_total_s: 5940.026571989059\n",
      "  timers:\n",
      "    learn_throughput: 1645.321\n",
      "    learn_time_ms: 607.784\n",
      "    load_throughput: 221746.031\n",
      "    load_time_ms: 4.51\n",
      "    sample_throughput: 80.965\n",
      "    sample_time_ms: 12350.95\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         5940.03</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-39-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 547\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8668488383293151\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011243722870336568\n",
      "          policy_loss: -0.0002521674045258098\n",
      "          total_loss: -0.018669794851707087\n",
      "          vf_explained_var: -0.03315213695168495\n",
      "          vf_loss: 0.00025086043832642544\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.46666666666667\n",
      "    ram_util_percent: 66.52222222222223\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123299107711498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041509728927409\n",
      "    mean_inference_ms: 1.4021481569799874\n",
      "    mean_raw_obs_processing_ms: 0.7934654432675378\n",
      "  time_since_restore: 5952.76841378212\n",
      "  time_this_iter_s: 12.741841793060303\n",
      "  time_total_s: 5952.76841378212\n",
      "  timers:\n",
      "    learn_throughput: 1644.558\n",
      "    learn_time_ms: 608.066\n",
      "    load_throughput: 220056.768\n",
      "    load_time_ms: 4.544\n",
      "    sample_throughput: 79.867\n",
      "    sample_time_ms: 12520.758\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1632130779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">         5952.77</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 548\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9139534539646572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011841528407173247\n",
      "          policy_loss: 0.057873949739668105\n",
      "          total_loss: 0.0388746546374427\n",
      "          vf_explained_var: -0.5891544222831726\n",
      "          vf_loss: 0.00014024031996895145\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.66875\n",
      "    ram_util_percent: 66.47500000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123323569774252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041615572637062\n",
      "    mean_inference_ms: 1.4021547723644927\n",
      "    mean_raw_obs_processing_ms: 0.7935192944254158\n",
      "  time_since_restore: 5963.851534843445\n",
      "  time_this_iter_s: 11.083121061325073\n",
      "  time_total_s: 5963.851534843445\n",
      "  timers:\n",
      "    learn_throughput: 1637.721\n",
      "    learn_time_ms: 610.605\n",
      "    load_throughput: 219416.71\n",
      "    load_time_ms: 4.558\n",
      "    sample_throughput: 79.903\n",
      "    sample_time_ms: 12515.216\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1632130790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         5963.85</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 549\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7366684238115946\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014497370160237387\n",
      "          policy_loss: 0.04473285612960656\n",
      "          total_loss: 0.027470564221342406\n",
      "          vf_explained_var: -0.997613251209259\n",
      "          vf_loss: 0.0001043928915654154\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.20666666666667\n",
      "    ram_util_percent: 66.63333333333335\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123348333487737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041731143417172\n",
      "    mean_inference_ms: 1.4021616240670327\n",
      "    mean_raw_obs_processing_ms: 0.7935757751397248\n",
      "  time_since_restore: 5974.550756216049\n",
      "  time_this_iter_s: 10.69922137260437\n",
      "  time_total_s: 5974.550756216049\n",
      "  timers:\n",
      "    learn_throughput: 1635.871\n",
      "    learn_time_ms: 611.295\n",
      "    load_throughput: 220173.438\n",
      "    load_time_ms: 4.542\n",
      "    sample_throughput: 80.145\n",
      "    sample_time_ms: 12477.46\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1632130801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         5974.55</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 550\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7497562898529901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010235401723821664\n",
      "          policy_loss: 0.1569976195693016\n",
      "          total_loss: 0.13958757428659332\n",
      "          vf_explained_var: -0.3981219530105591\n",
      "          vf_loss: 8.75151018084984e-05\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.306250000000006\n",
      "    ram_util_percent: 66.5625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041233745332200034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.041860584937522\n",
      "    mean_inference_ms: 1.4021688351637067\n",
      "    mean_raw_obs_processing_ms: 0.7936351491626212\n",
      "  time_since_restore: 5985.316654443741\n",
      "  time_this_iter_s: 10.76589822769165\n",
      "  time_total_s: 5985.316654443741\n",
      "  timers:\n",
      "    learn_throughput: 1656.684\n",
      "    learn_time_ms: 603.615\n",
      "    load_throughput: 219898.709\n",
      "    load_time_ms: 4.548\n",
      "    sample_throughput: 80.14\n",
      "    sample_time_ms: 12478.111\n",
      "    update_time_ms: 1.664\n",
      "  timestamp: 1632130811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         5985.32</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-22\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 551\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8228864047262403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014687230547986355\n",
      "          policy_loss: 0.020581128199895223\n",
      "          total_loss: 0.0025708562797970244\n",
      "          vf_explained_var: -0.4214443266391754\n",
      "          vf_loss: 0.00021859308965051444\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.69333333333332\n",
      "    ram_util_percent: 66.42666666666666\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123401700738558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04200878200516\n",
      "    mean_inference_ms: 1.4021764428136856\n",
      "    mean_raw_obs_processing_ms: 0.7936974378787087\n",
      "  time_since_restore: 5996.286650180817\n",
      "  time_this_iter_s: 10.969995737075806\n",
      "  time_total_s: 5996.286650180817\n",
      "  timers:\n",
      "    learn_throughput: 1656.209\n",
      "    learn_time_ms: 603.788\n",
      "    load_throughput: 220120.286\n",
      "    load_time_ms: 4.543\n",
      "    sample_throughput: 80.224\n",
      "    sample_time_ms: 12465.166\n",
      "    update_time_ms: 1.668\n",
      "  timestamp: 1632130822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         5996.29</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 552\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.6129981186284726e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8525299059020148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03542004830947469\n",
      "          policy_loss: -0.042590905560387506\n",
      "          total_loss: -0.05978998003734483\n",
      "          vf_explained_var: -0.45524710416793823\n",
      "          vf_loss: 0.0013262202696447882\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.23125\n",
      "    ram_util_percent: 66.275\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123429268787094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04218967950485\n",
      "    mean_inference_ms: 1.4021847128113054\n",
      "    mean_raw_obs_processing_ms: 0.7937628271197696\n",
      "  time_since_restore: 6007.434423208237\n",
      "  time_this_iter_s: 11.147773027420044\n",
      "  time_total_s: 6007.434423208237\n",
      "  timers:\n",
      "    learn_throughput: 1657.25\n",
      "    learn_time_ms: 603.409\n",
      "    load_throughput: 220553.183\n",
      "    load_time_ms: 4.534\n",
      "    sample_throughput: 80.045\n",
      "    sample_time_ms: 12493.045\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1632130834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         6007.43</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 553\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.91949717794271e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9382588426272074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018258706905567912\n",
      "          policy_loss: 0.054732538759708405\n",
      "          total_loss: 0.03554486185312271\n",
      "          vf_explained_var: -0.07418433576822281\n",
      "          vf_loss: 0.0001949107161231546\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.868750000000006\n",
      "    ram_util_percent: 66.18125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123458008320667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042401760571664\n",
      "    mean_inference_ms: 1.4021935520974151\n",
      "    mean_raw_obs_processing_ms: 0.7938309142562474\n",
      "  time_since_restore: 6018.385977506638\n",
      "  time_this_iter_s: 10.951554298400879\n",
      "  time_total_s: 6018.385977506638\n",
      "  timers:\n",
      "    learn_throughput: 1681.779\n",
      "    learn_time_ms: 594.608\n",
      "    load_throughput: 222874.845\n",
      "    load_time_ms: 4.487\n",
      "    sample_throughput: 80.089\n",
      "    sample_time_ms: 12486.132\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1632130845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         6018.39</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-40-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 554\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.91949717794271e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.001694361368815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03790795744340277\n",
      "          policy_loss: -0.08129692069358296\n",
      "          total_loss: -0.10118562140398556\n",
      "          vf_explained_var: -0.2663594186306\n",
      "          vf_loss: 0.00012823245360777947\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.306666666666665\n",
      "    ram_util_percent: 66.12666666666668\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041234872567604566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042638556618979\n",
      "    mean_inference_ms: 1.4022026791066622\n",
      "    mean_raw_obs_processing_ms: 0.7939019962739429\n",
      "  time_since_restore: 6029.202309846878\n",
      "  time_this_iter_s: 10.816332340240479\n",
      "  time_total_s: 6029.202309846878\n",
      "  timers:\n",
      "    learn_throughput: 1682.759\n",
      "    learn_time_ms: 594.262\n",
      "    load_throughput: 223907.582\n",
      "    load_time_ms: 4.466\n",
      "    sample_throughput: 80.486\n",
      "    sample_time_ms: 12424.494\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1632130855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">          6029.2</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 994.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 555\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.934646807776557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012819531105788763\n",
      "          policy_loss: -0.0005844672313994831\n",
      "          total_loss: -0.019719114609890512\n",
      "          vf_explained_var: -0.575294554233551\n",
      "          vf_loss: 0.0002118187194759634\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14375\n",
      "    ram_util_percent: 66.05625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123518250738749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.042901430182212\n",
      "    mean_inference_ms: 1.4022124066012207\n",
      "    mean_raw_obs_processing_ms: 0.7939756931166333\n",
      "  time_since_restore: 6040.503711223602\n",
      "  time_this_iter_s: 11.301401376724243\n",
      "  time_total_s: 6040.503711223602\n",
      "  timers:\n",
      "    learn_throughput: 1690.464\n",
      "    learn_time_ms: 591.554\n",
      "    load_throughput: 224278.74\n",
      "    load_time_ms: 4.459\n",
      "    sample_throughput: 81.021\n",
      "    sample_time_ms: 12342.412\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1632130867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">          6040.5</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 556\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8596319463517932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019310699345396483\n",
      "          policy_loss: -0.0076688617467880246\n",
      "          total_loss: -0.0257643005086316\n",
      "          vf_explained_var: -0.6761561632156372\n",
      "          vf_loss: 0.0005008752237901919\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.743750000000006\n",
      "    ram_util_percent: 66.03125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123549218850673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04319808209595\n",
      "    mean_inference_ms: 1.4022227089461003\n",
      "    mean_raw_obs_processing_ms: 0.793670989899421\n",
      "  time_since_restore: 6051.671102523804\n",
      "  time_this_iter_s: 11.167391300201416\n",
      "  time_total_s: 6051.671102523804\n",
      "  timers:\n",
      "    learn_throughput: 1708.902\n",
      "    learn_time_ms: 585.171\n",
      "    load_throughput: 312657.771\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 94.614\n",
      "    sample_time_ms: 10569.304\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1632130878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         6051.67</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-41-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 557\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8384420567088657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01389034146258492\n",
      "          policy_loss: 0.02553042910165257\n",
      "          total_loss: 0.0074436268872684905\n",
      "          vf_explained_var: -0.685002863407135\n",
      "          vf_loss: 0.00029761640034848825\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.849999999999994\n",
      "    ram_util_percent: 66.06875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041235805302282935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.043527215046574\n",
      "    mean_inference_ms: 1.4022333450392663\n",
      "    mean_raw_obs_processing_ms: 0.7933698020797868\n",
      "  time_since_restore: 6062.820221185684\n",
      "  time_this_iter_s: 11.149118661880493\n",
      "  time_total_s: 6062.820221185684\n",
      "  timers:\n",
      "    learn_throughput: 1707.513\n",
      "    learn_time_ms: 585.647\n",
      "    load_throughput: 316074.152\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 96.065\n",
      "    sample_time_ms: 10409.591\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1632130889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         6062.82</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-41-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 558\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.042949589093526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019596665868998404\n",
      "          policy_loss: 0.11508881251017253\n",
      "          total_loss: 0.0948163530892796\n",
      "          vf_explained_var: -0.2996551990509033\n",
      "          vf_loss: 0.00015702896170195245\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.864705882352936\n",
      "    ram_util_percent: 66.07058823529411\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041236134497544485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.043899002219785\n",
      "    mean_inference_ms: 1.4022448007657333\n",
      "    mean_raw_obs_processing_ms: 0.7930723972328753\n",
      "  time_since_restore: 6074.157033920288\n",
      "  time_this_iter_s: 11.336812734603882\n",
      "  time_total_s: 6074.157033920288\n",
      "  timers:\n",
      "    learn_throughput: 1716.107\n",
      "    learn_time_ms: 582.714\n",
      "    load_throughput: 318169.709\n",
      "    load_time_ms: 3.143\n",
      "    sample_throughput: 95.804\n",
      "    sample_time_ms: 10437.945\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1632130901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         6074.16</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 559\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9053052902221679\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012576074923019655\n",
      "          policy_loss: 0.023574694825543297\n",
      "          total_loss: 0.004779105219576094\n",
      "          vf_explained_var: -0.8919441103935242\n",
      "          vf_loss: 0.00025745641744126464\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95625\n",
      "    ram_util_percent: 66.08749999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123647224246587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.044301869024586\n",
      "    mean_inference_ms: 1.4022566914203087\n",
      "    mean_raw_obs_processing_ms: 0.7927784797903763\n",
      "  time_since_restore: 6085.271783351898\n",
      "  time_this_iter_s: 11.114749431610107\n",
      "  time_total_s: 6085.271783351898\n",
      "  timers:\n",
      "    learn_throughput: 1715.989\n",
      "    learn_time_ms: 582.754\n",
      "    load_throughput: 316960.303\n",
      "    load_time_ms: 3.155\n",
      "    sample_throughput: 95.425\n",
      "    sample_time_ms: 10479.457\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1632130912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         6085.27</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 560\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8288131872812907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01708975763897073\n",
      "          policy_loss: 0.00212913379073143\n",
      "          total_loss: -0.015711813254488838\n",
      "          vf_explained_var: -0.873997151851654\n",
      "          vf_loss: 0.0004471809129528184\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.40625\n",
      "    ram_util_percent: 66.29375\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0412368374764846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.044732554521003\n",
      "    mean_inference_ms: 1.4022694486024796\n",
      "    mean_raw_obs_processing_ms: 0.792487884317898\n",
      "  time_since_restore: 6096.744604587555\n",
      "  time_this_iter_s: 11.472821235656738\n",
      "  time_total_s: 6096.744604587555\n",
      "  timers:\n",
      "    learn_throughput: 1712.375\n",
      "    learn_time_ms: 583.984\n",
      "    load_throughput: 317983.973\n",
      "    load_time_ms: 3.145\n",
      "    sample_throughput: 94.796\n",
      "    sample_time_ms: 10548.938\n",
      "    update_time_ms: 1.674\n",
      "  timestamp: 1632130923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">         6096.74</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 561\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8245607693990071\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014301688414182341\n",
      "          policy_loss: 0.08216993591437736\n",
      "          total_loss: 0.06420100807315773\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00027667499283173433\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.90625\n",
      "    ram_util_percent: 66.40625\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123721421499643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.045190308164104\n",
      "    mean_inference_ms: 1.4022829344313765\n",
      "    mean_raw_obs_processing_ms: 0.7922003789809569\n",
      "  time_since_restore: 6108.0533452034\n",
      "  time_this_iter_s: 11.308740615844727\n",
      "  time_total_s: 6108.0533452034\n",
      "  timers:\n",
      "    learn_throughput: 1710.752\n",
      "    learn_time_ms: 584.538\n",
      "    load_throughput: 317348.809\n",
      "    load_time_ms: 3.151\n",
      "    sample_throughput: 94.497\n",
      "    sample_time_ms: 10582.292\n",
      "    update_time_ms: 1.672\n",
      "  timestamp: 1632130934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">         6108.05</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 562\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8216285096274483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015115944175682077\n",
      "          policy_loss: -0.004565116763114929\n",
      "          total_loss: -0.02244378150337272\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0003376151963796777\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.33125\n",
      "    ram_util_percent: 66.31250000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123759232208004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04566962290789\n",
      "    mean_inference_ms: 1.402296583059617\n",
      "    mean_raw_obs_processing_ms: 0.7919161906152573\n",
      "  time_since_restore: 6119.194926977158\n",
      "  time_this_iter_s: 11.141581773757935\n",
      "  time_total_s: 6119.194926977158\n",
      "  timers:\n",
      "    learn_throughput: 1701.029\n",
      "    learn_time_ms: 587.879\n",
      "    load_throughput: 317341.606\n",
      "    load_time_ms: 3.151\n",
      "    sample_throughput: 94.536\n",
      "    sample_time_ms: 10577.993\n",
      "    update_time_ms: 1.675\n",
      "  timestamp: 1632130946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         6119.19</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 563\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8422652681668599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015334910057826607\n",
      "          policy_loss: -0.027466909939216244\n",
      "          total_loss: -0.045464713809390864\n",
      "          vf_explained_var: -0.9988772869110107\n",
      "          vf_loss: 0.00042484218574827536\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.70625\n",
      "    ram_util_percent: 66.28125\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123798030998956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.046167514757885\n",
      "    mean_inference_ms: 1.402310535663756\n",
      "    mean_raw_obs_processing_ms: 0.791635326044322\n",
      "  time_since_restore: 6130.227823734283\n",
      "  time_this_iter_s: 11.032896757125854\n",
      "  time_total_s: 6130.227823734283\n",
      "  timers:\n",
      "    learn_throughput: 1697.34\n",
      "    learn_time_ms: 589.157\n",
      "    load_throughput: 315764.812\n",
      "    load_time_ms: 3.167\n",
      "    sample_throughput: 94.459\n",
      "    sample_time_ms: 10586.638\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632130957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         6130.23</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 564\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.636758765909407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013452751794372376\n",
      "          policy_loss: -0.02230395062102212\n",
      "          total_loss: -0.03842282729844252\n",
      "          vf_explained_var: -0.5877410173416138\n",
      "          vf_loss: 0.0002487095616136988\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.09375\n",
      "    ram_util_percent: 66.35\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123837398768332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.046683583598472\n",
      "    mean_inference_ms: 1.4023249212472881\n",
      "    mean_raw_obs_processing_ms: 0.7913577623977237\n",
      "  time_since_restore: 6141.273492097855\n",
      "  time_this_iter_s: 11.045668363571167\n",
      "  time_total_s: 6141.273492097855\n",
      "  timers:\n",
      "    learn_throughput: 1696.466\n",
      "    learn_time_ms: 589.461\n",
      "    load_throughput: 312718.38\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 94.258\n",
      "    sample_time_ms: 10609.222\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632130968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         6141.27</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 565\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6357253988583882\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019691870606193252\n",
      "          policy_loss: 0.0472115026993884\n",
      "          total_loss: 0.031019250634643766\n",
      "          vf_explained_var: -0.6909124255180359\n",
      "          vf_loss: 0.00016499659678730595\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.17999999999999\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123877146084358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.047216091556743\n",
      "    mean_inference_ms: 1.402339421263748\n",
      "    mean_raw_obs_processing_ms: 0.7910833426949861\n",
      "  time_since_restore: 6152.26366019249\n",
      "  time_this_iter_s: 10.99016809463501\n",
      "  time_total_s: 6152.26366019249\n",
      "  timers:\n",
      "    learn_throughput: 1697.098\n",
      "    learn_time_ms: 589.241\n",
      "    load_throughput: 312362.058\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 94.533\n",
      "    sample_time_ms: 10578.351\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1632130979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         6152.26</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 566\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7644202378061082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01866033626612591\n",
      "          policy_loss: 0.05850597317847941\n",
      "          total_loss: 0.04130306939284007\n",
      "          vf_explained_var: -0.20493032038211823\n",
      "          vf_loss: 0.0004412929305949041\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.800000000000004\n",
      "    ram_util_percent: 66.45625000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123917212103219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.047763356354558\n",
      "    mean_inference_ms: 1.4023542965509705\n",
      "    mean_raw_obs_processing_ms: 0.7908121751233617\n",
      "  time_since_restore: 6163.1244764328\n",
      "  time_this_iter_s: 10.860816240310669\n",
      "  time_total_s: 6163.1244764328\n",
      "  timers:\n",
      "    learn_throughput: 1695.13\n",
      "    learn_time_ms: 589.925\n",
      "    load_throughput: 312998.418\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 94.813\n",
      "    sample_time_ms: 10547.044\n",
      "    update_time_ms: 1.629\n",
      "  timestamp: 1632130990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         6163.12</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-43-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 567\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.879245766914066e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8250151144133673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02076608074482006\n",
      "          policy_loss: 0.003927288535568449\n",
      "          total_loss: -0.01411140412092209\n",
      "          vf_explained_var: -0.31205296516418457\n",
      "          vf_loss: 0.00021145302008436476\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.6875\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123958165536821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.048324878234888\n",
      "    mean_inference_ms: 1.4023695051390581\n",
      "    mean_raw_obs_processing_ms: 0.7905442381988292\n",
      "  time_since_restore: 6174.094194889069\n",
      "  time_this_iter_s: 10.96971845626831\n",
      "  time_total_s: 6174.094194889069\n",
      "  timers:\n",
      "    learn_throughput: 1695.102\n",
      "    learn_time_ms: 589.935\n",
      "    load_throughput: 313061.496\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 94.975\n",
      "    sample_time_ms: 10529.089\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632131001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">         6174.09</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-43-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 568\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0161198708746166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012921477399749411\n",
      "          policy_loss: -0.02109611084063848\n",
      "          total_loss: -0.04107950607107745\n",
      "          vf_explained_var: -0.7835261821746826\n",
      "          vf_loss: 0.00017779714081471966\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53333333333333\n",
      "    ram_util_percent: 66.50666666666667\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04123999742204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.048903084193062\n",
      "    mean_inference_ms: 1.4023848895815376\n",
      "    mean_raw_obs_processing_ms: 0.790279495027424\n",
      "  time_since_restore: 6185.14465379715\n",
      "  time_this_iter_s: 11.050458908081055\n",
      "  time_total_s: 6185.14465379715\n",
      "  timers:\n",
      "    learn_throughput: 1694.073\n",
      "    learn_time_ms: 590.293\n",
      "    load_throughput: 311800.117\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 95.237\n",
      "    sample_time_ms: 10500.079\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1632131012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         6185.14</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 569\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0162943813535903\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015576942203805188\n",
      "          policy_loss: 0.012814555565516155\n",
      "          total_loss: -0.0067603514840205515\n",
      "          vf_explained_var: -0.6009588241577148\n",
      "          vf_loss: 0.0005880305103750693\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.71875\n",
      "    ram_util_percent: 66.51875000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124042932557225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.04949671337872\n",
      "    mean_inference_ms: 1.4024007343609883\n",
      "    mean_raw_obs_processing_ms: 0.790018009266455\n",
      "  time_since_restore: 6196.082263946533\n",
      "  time_this_iter_s: 10.937610149383545\n",
      "  time_total_s: 6196.082263946533\n",
      "  timers:\n",
      "    learn_throughput: 1696.091\n",
      "    learn_time_ms: 589.591\n",
      "    load_throughput: 313014.769\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 95.392\n",
      "    sample_time_ms: 10483.043\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1632131023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         6196.08</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 570\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.107930972841051\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01663315617962324\n",
      "          policy_loss: -0.010402882099151611\n",
      "          total_loss: -0.03127944982714123\n",
      "          vf_explained_var: -0.4427327513694763\n",
      "          vf_loss: 0.00020273750027020773\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.223529411764716\n",
      "    ram_util_percent: 66.74705882352941\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124089515251332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.050117118941465\n",
      "    mean_inference_ms: 1.402417484399157\n",
      "    mean_raw_obs_processing_ms: 0.7897597663137783\n",
      "  time_since_restore: 6207.906257867813\n",
      "  time_this_iter_s: 11.823993921279907\n",
      "  time_total_s: 6207.906257867813\n",
      "  timers:\n",
      "    learn_throughput: 1698.355\n",
      "    learn_time_ms: 588.805\n",
      "    load_throughput: 313229.827\n",
      "    load_time_ms: 3.193\n",
      "    sample_throughput: 95.067\n",
      "    sample_time_ms: 10518.935\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1632131035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">         6207.91</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-44-06\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 571\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.786157402727339\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018501187957275414\n",
      "          policy_loss: -0.024851637995905347\n",
      "          total_loss: -0.04247980962196986\n",
      "          vf_explained_var: -0.8551881313323975\n",
      "          vf_loss: 0.00023339352968226496\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.4375\n",
      "    ram_util_percent: 66.775\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041241374096310075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.050754705851448\n",
      "    mean_inference_ms: 1.402434535134599\n",
      "    mean_raw_obs_processing_ms: 0.7895045614160765\n",
      "  time_since_restore: 6219.02025103569\n",
      "  time_this_iter_s: 11.113993167877197\n",
      "  time_total_s: 6219.02025103569\n",
      "  timers:\n",
      "    learn_throughput: 1687.677\n",
      "    learn_time_ms: 592.53\n",
      "    load_throughput: 312378.342\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 95.277\n",
      "    sample_time_ms: 10495.724\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632131046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         6219.02</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-44-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 572\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8727183394961886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018443763013880874\n",
      "          policy_loss: -0.10049565368228489\n",
      "          total_loss: -0.11898306707541148\n",
      "          vf_explained_var: -0.7013654708862305\n",
      "          vf_loss: 0.0002397601895305949\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.075\n",
      "    ram_util_percent: 67.21875\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041241882646889404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.051412209587745\n",
      "    mean_inference_ms: 1.402452172377709\n",
      "    mean_raw_obs_processing_ms: 0.7892525258249441\n",
      "  time_since_restore: 6230.47666144371\n",
      "  time_this_iter_s: 11.45641040802002\n",
      "  time_total_s: 6230.47666144371\n",
      "  timers:\n",
      "    learn_throughput: 1690.189\n",
      "    learn_time_ms: 591.65\n",
      "    load_throughput: 304597.24\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 94.982\n",
      "    sample_time_ms: 10528.309\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632131057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         6230.48</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-44-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 573\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8338791211446126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015814754745605182\n",
      "          policy_loss: 0.006299596445428\n",
      "          total_loss: -0.011862144867579142\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00017704120934164774\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.949999999999996\n",
      "    ram_util_percent: 67.26875000000001\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124240370231406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.052086923201324\n",
      "    mean_inference_ms: 1.4024704089297677\n",
      "    mean_raw_obs_processing_ms: 0.7890034690447919\n",
      "  time_since_restore: 6241.618680000305\n",
      "  time_this_iter_s: 11.142018556594849\n",
      "  time_total_s: 6241.618680000305\n",
      "  timers:\n",
      "    learn_throughput: 1687.661\n",
      "    learn_time_ms: 592.536\n",
      "    load_throughput: 306323.508\n",
      "    load_time_ms: 3.265\n",
      "    sample_throughput: 94.891\n",
      "    sample_time_ms: 10538.362\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1632131068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         6241.62</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-44-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 574\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9797335134612188\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012517578366847618\n",
      "          policy_loss: 0.08590640475352605\n",
      "          total_loss: 0.06620685255361927\n",
      "          vf_explained_var: -0.39608103036880493\n",
      "          vf_loss: 9.777731752870345e-05\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.14666666666666\n",
      "    ram_util_percent: 67.35999999999999\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.041242936086699176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.052770669002603\n",
      "    mean_inference_ms: 1.4024889605958644\n",
      "    mean_raw_obs_processing_ms: 0.7887573719878987\n",
      "  time_since_restore: 6252.242333173752\n",
      "  time_this_iter_s: 10.623653173446655\n",
      "  time_total_s: 6252.242333173752\n",
      "  timers:\n",
      "    learn_throughput: 1685.675\n",
      "    learn_time_ms: 593.234\n",
      "    load_throughput: 309787.36\n",
      "    load_time_ms: 3.228\n",
      "    sample_throughput: 95.279\n",
      "    sample_time_ms: 10495.516\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1632131079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">         6252.24</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2935_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-20_09-44-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 575\n",
      "  experiment_id: 47489c5ff1344717988ec3ea9512dd6e\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.818868650371094e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.941737511422899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014499802969125931\n",
      "          policy_loss: 0.020591356646683483\n",
      "          total_loss: 0.0013539570073286692\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00017996860875023736\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.18\n",
      "    ram_util_percent: 67.43999999999998\n",
      "  pid: 491044\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04124346951850046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.053461433168096\n",
      "    mean_inference_ms: 1.4025074649908047\n",
      "    mean_raw_obs_processing_ms: 0.7885141924435546\n",
      "  time_since_restore: 6262.680402755737\n",
      "  time_this_iter_s: 10.438069581985474\n",
      "  time_total_s: 6262.680402755737\n",
      "  timers:\n",
      "    learn_throughput: 1685.542\n",
      "    learn_time_ms: 593.281\n",
      "    load_throughput: 309661.568\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 95.783\n",
      "    sample_time_ms: 10440.245\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632131089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: c2935_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/6.08 GiB heap, 0.0/3.04 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-20_08-00-02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2935_00000</td><td>RUNNING </td><td>192.168.1.100:491044</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         6262.68</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=491049)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained (AlinaCNN)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
