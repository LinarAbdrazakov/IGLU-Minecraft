{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(features_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(128, action_space.n)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.mlp.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(obs)\n",
    "        features = self.mlp(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C32']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-10-09 14:51:38,435\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-09 14:51:38,449\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=202262)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202262)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C32 pretrained (AngelaCNN + MLP) (3 noops after placement)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/67f57_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/67f57_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211009_145139-67f57_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202262)\u001b[0m 2021-10-09 14:51:41,877\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=202262)\u001b[0m 2021-10-09 14:51:41,877\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=202262)\u001b[0m 2021-10-09 14:51:47,687\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 424.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7.0\n",
      "  episode_reward_mean: -8.5\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.880436126391093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004079058113112211\n",
      "          policy_loss: 0.03850563491384188\n",
      "          total_loss: 0.6827736003531351\n",
      "          vf_explained_var: 0.07799533754587173\n",
      "          vf_loss: 0.672256518734826\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.923478260869565\n",
      "    ram_util_percent: 62.28434782608696\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03825439201606499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 77.51308121047654\n",
      "    mean_inference_ms: 1.794723364023062\n",
      "    mean_raw_obs_processing_ms: 0.18616084690456028\n",
      "  time_since_restore: 80.30446696281433\n",
      "  time_this_iter_s: 80.30446696281433\n",
      "  time_total_s: 80.30446696281433\n",
      "  timers:\n",
      "    learn_throughput: 1598.962\n",
      "    learn_time_ms: 625.406\n",
      "    load_throughput: 60142.876\n",
      "    load_time_ms: 16.627\n",
      "    sample_throughput: 12.554\n",
      "    sample_time_ms: 79656.02\n",
      "    update_time_ms: 2.807\n",
      "  timestamp: 1633791187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         80.3045</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">    -8.5</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">               424</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-53-29\n",
      "  done: false\n",
      "  episode_len_mean: 412.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -6.25\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 4\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.875970220565796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0027537142600012584\n",
      "          policy_loss: 0.052672796448071796\n",
      "          total_loss: 0.3165993462006251\n",
      "          vf_explained_var: 0.139669731259346\n",
      "          vf_loss: 0.29241087711416186\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.04666666666666\n",
      "    ram_util_percent: 69.60666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037821547944002984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 62.7882700198431\n",
      "    mean_inference_ms: 1.7736168627745736\n",
      "    mean_raw_obs_processing_ms: 0.18384486331486838\n",
      "  time_since_restore: 101.4998869895935\n",
      "  time_this_iter_s: 21.195420026779175\n",
      "  time_total_s: 101.4998869895935\n",
      "  timers:\n",
      "    learn_throughput: 1588.983\n",
      "    learn_time_ms: 629.333\n",
      "    load_throughput: 60044.722\n",
      "    load_time_ms: 16.654\n",
      "    sample_throughput: 19.961\n",
      "    sample_time_ms: 50097.937\n",
      "    update_time_ms: 2.639\n",
      "  timestamp: 1633791209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           101.5</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   -6.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            412.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-53-49\n",
      "  done: false\n",
      "  episode_len_mean: 413.42857142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -3.5714285714285716\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 7\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8746728261311847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004421391519707324\n",
      "          policy_loss: -0.1367511188818349\n",
      "          total_loss: -0.14056452131933636\n",
      "          vf_explained_var: 0.29280728101730347\n",
      "          vf_loss: 0.02471225268414451\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.79310344827586\n",
      "    ram_util_percent: 69.19310344827586\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751610461392251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.11385276372511\n",
      "    mean_inference_ms: 1.7580316968372316\n",
      "    mean_raw_obs_processing_ms: 0.18269072860511293\n",
      "  time_since_restore: 121.61828851699829\n",
      "  time_this_iter_s: 20.118401527404785\n",
      "  time_total_s: 121.61828851699829\n",
      "  timers:\n",
      "    learn_throughput: 1581.479\n",
      "    learn_time_ms: 632.319\n",
      "    load_throughput: 59622.503\n",
      "    load_time_ms: 16.772\n",
      "    sample_throughput: 25.072\n",
      "    sample_time_ms: 39884.65\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1633791229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         121.618</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-3.57143</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           413.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-54-09\n",
      "  done: false\n",
      "  episode_len_mean: 415.44444444444446\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -2.7777777777777777\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.024999999999999994\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8570196363661022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00488493117107459\n",
      "          policy_loss: -0.10939110103580686\n",
      "          total_loss: -0.12975025177001953\n",
      "          vf_explained_var: 0.5112901329994202\n",
      "          vf_loss: 0.008088923230146369\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87586206896552\n",
      "    ram_util_percent: 69.01379310344828\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03737707319835511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.8215717233957\n",
      "    mean_inference_ms: 1.7514753516765404\n",
      "    mean_raw_obs_processing_ms: 0.18024656804470052\n",
      "  time_since_restore: 141.72050547599792\n",
      "  time_this_iter_s: 20.102216958999634\n",
      "  time_total_s: 141.72050547599792\n",
      "  timers:\n",
      "    learn_throughput: 1583.957\n",
      "    learn_time_ms: 631.33\n",
      "    load_throughput: 60918.121\n",
      "    load_time_ms: 16.415\n",
      "    sample_throughput: 28.755\n",
      "    sample_time_ms: 34776.827\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1633791249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         141.721</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-2.77778</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           415.444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 416.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -2.0833333333333335\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012499999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8502304368548925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003543588223740003\n",
      "          policy_loss: 0.010007439967658785\n",
      "          total_loss: -0.014476039715939098\n",
      "          vf_explained_var: 0.174942746758461\n",
      "          vf_loss: 0.003974528271161641\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.02400000000001\n",
      "    ram_util_percent: 69.024\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037233480842526555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.18550718465415\n",
      "    mean_inference_ms: 1.7436548005172898\n",
      "    mean_raw_obs_processing_ms: 0.17814516743174844\n",
      "  time_since_restore: 159.4749789237976\n",
      "  time_this_iter_s: 17.754473447799683\n",
      "  time_total_s: 159.4749789237976\n",
      "  timers:\n",
      "    learn_throughput: 1585.647\n",
      "    learn_time_ms: 630.657\n",
      "    load_throughput: 65757.108\n",
      "    load_time_ms: 15.207\n",
      "    sample_throughput: 32.006\n",
      "    sample_time_ms: 31243.655\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633791267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         159.475</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-2.08333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            416.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 414.57142857142856\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.7857142857142858\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.812695919142829\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0058011812608874\n",
      "          policy_loss: -0.025078250902394454\n",
      "          total_loss: -0.05151641898685032\n",
      "          vf_explained_var: -0.2602919936180115\n",
      "          vf_loss: 0.0016525333960695813\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 69.03571428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716340657412568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.89789656431079\n",
      "    mean_inference_ms: 1.7397239943351492\n",
      "    mean_raw_obs_processing_ms: 0.17655365303757908\n",
      "  time_since_restore: 178.6741271018982\n",
      "  time_this_iter_s: 19.199148178100586\n",
      "  time_total_s: 178.6741271018982\n",
      "  timers:\n",
      "    learn_throughput: 1585.323\n",
      "    learn_time_ms: 630.786\n",
      "    load_throughput: 64588.533\n",
      "    load_time_ms: 15.483\n",
      "    sample_throughput: 34.332\n",
      "    sample_time_ms: 29127.34\n",
      "    update_time_ms: 2.224\n",
      "  timestamp: 1633791286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         178.674</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-1.78571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           414.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 418.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.5625\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 16\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7877673864364625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007438848466373763\n",
      "          policy_loss: -0.06832649923033185\n",
      "          total_loss: -0.09399162381887435\n",
      "          vf_explained_var: -0.09468455612659454\n",
      "          vf_loss: 0.0021660564887699568\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.04999999999999\n",
      "    ram_util_percent: 69.125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710566963567022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.960729359501485\n",
      "    mean_inference_ms: 1.7363201013328635\n",
      "    mean_raw_obs_processing_ms: 0.17493940936536773\n",
      "  time_since_restore: 195.9891164302826\n",
      "  time_this_iter_s: 17.3149893283844\n",
      "  time_total_s: 195.9891164302826\n",
      "  timers:\n",
      "    learn_throughput: 1582.952\n",
      "    learn_time_ms: 631.731\n",
      "    load_throughput: 69981.38\n",
      "    load_time_ms: 14.29\n",
      "    sample_throughput: 36.567\n",
      "    sample_time_ms: 27347.045\n",
      "    update_time_ms: 2.198\n",
      "  timestamp: 1633791303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         195.989</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\"> -1.5625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            418.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-55-22\n",
      "  done: false\n",
      "  episode_len_mean: 417.57894736842104\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.3157894736842106\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 19\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.807117811838786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008930719075401801\n",
      "          policy_loss: -0.008763201120826934\n",
      "          total_loss: -0.0355679704911179\n",
      "          vf_explained_var: 0.07574610412120819\n",
      "          vf_loss: 0.0012105925015122112\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77407407407407\n",
      "    ram_util_percent: 69.23333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703761920075239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.64030866942534\n",
      "    mean_inference_ms: 1.732165740401915\n",
      "    mean_raw_obs_processing_ms: 0.17331169893658135\n",
      "  time_since_restore: 214.87349438667297\n",
      "  time_this_iter_s: 18.88437795639038\n",
      "  time_total_s: 214.87349438667297\n",
      "  timers:\n",
      "    learn_throughput: 1585.752\n",
      "    learn_time_ms: 630.616\n",
      "    load_throughput: 68543.059\n",
      "    load_time_ms: 14.589\n",
      "    sample_throughput: 38.155\n",
      "    sample_time_ms: 26208.623\n",
      "    update_time_ms: 2.184\n",
      "  timestamp: 1633791322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         214.873</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-1.31579</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           417.579</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-55-40\n",
      "  done: false\n",
      "  episode_len_mean: 417.3809523809524\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.1904761904761905\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 21\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.797969272401598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007604839293667275\n",
      "          policy_loss: -0.08543911029895147\n",
      "          total_loss: -0.11236110279957454\n",
      "          vf_explained_var: -0.024789465591311455\n",
      "          vf_loss: 0.0010101681122452848\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.18461538461539\n",
      "    ram_util_percent: 69.2576923076923\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699462372066684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.37134830134557\n",
      "    mean_inference_ms: 1.729790244009254\n",
      "    mean_raw_obs_processing_ms: 0.1722443116158811\n",
      "  time_since_restore: 233.14602828025818\n",
      "  time_this_iter_s: 18.272533893585205\n",
      "  time_total_s: 233.14602828025818\n",
      "  timers:\n",
      "    learn_throughput: 1587.557\n",
      "    learn_time_ms: 629.899\n",
      "    load_throughput: 67386.797\n",
      "    load_time_ms: 14.84\n",
      "    sample_throughput: 39.598\n",
      "    sample_time_ms: 25253.83\n",
      "    update_time_ms: 3.376\n",
      "  timestamp: 1633791340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         233.146</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-1.19048</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           417.381</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 416.4166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0416666666666667\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 24\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8071422550413345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007195428385372674\n",
      "          policy_loss: 0.0961694684293535\n",
      "          total_loss: 0.06872350097530418\n",
      "          vf_explained_var: -0.5981001257896423\n",
      "          vf_loss: 0.0005804844284688846\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84074074074074\n",
      "    ram_util_percent: 69.34814814814816\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03693742343822384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.7705734208378\n",
      "    mean_inference_ms: 1.7267306154089859\n",
      "    mean_raw_obs_processing_ms: 0.17108544648646573\n",
      "  time_since_restore: 251.53252339363098\n",
      "  time_this_iter_s: 18.386495113372803\n",
      "  time_total_s: 251.53252339363098\n",
      "  timers:\n",
      "    learn_throughput: 1585.643\n",
      "    learn_time_ms: 630.659\n",
      "    load_throughput: 67147.218\n",
      "    load_time_ms: 14.893\n",
      "    sample_throughput: 40.814\n",
      "    sample_time_ms: 24501.26\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1633791359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         251.533</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-1.04167</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           416.417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 416.38461538461536\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9615384615384616\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 26\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8235095103581744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005562907965229572\n",
      "          policy_loss: -0.013870128782259093\n",
      "          total_loss: -0.041662323930197295\n",
      "          vf_explained_var: -0.582188069820404\n",
      "          vf_loss: 0.0004081333272754111\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.3\n",
      "    ram_util_percent: 69.35384615384615\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03690558238312401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.86166263144883\n",
      "    mean_inference_ms: 1.725022481848234\n",
      "    mean_raw_obs_processing_ms: 0.17032157164671408\n",
      "  time_since_restore: 269.94094228744507\n",
      "  time_this_iter_s: 18.408418893814087\n",
      "  time_total_s: 269.94094228744507\n",
      "  timers:\n",
      "    learn_throughput: 1583.368\n",
      "    learn_time_ms: 631.565\n",
      "    load_throughput: 67077.954\n",
      "    load_time_ms: 14.908\n",
      "    sample_throughput: 54.612\n",
      "    sample_time_ms: 18310.851\n",
      "    update_time_ms: 3.175\n",
      "  timestamp: 1633791377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         269.941</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.961538</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           416.385</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-56-35\n",
      "  done: false\n",
      "  episode_len_mean: 414.07142857142856\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8928571428571429\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 28\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.006249999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.83771960205502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0048517568324885785\n",
      "          policy_loss: -0.055631975498464374\n",
      "          total_loss: -0.08358178801006741\n",
      "          vf_explained_var: -0.37071260809898376\n",
      "          vf_loss: 0.0003970581467405686\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.044\n",
      "    ram_util_percent: 69.46\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03687600877453775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.04256986993338\n",
      "    mean_inference_ms: 1.7234147739785826\n",
      "    mean_raw_obs_processing_ms: 0.16955759633503664\n",
      "  time_since_restore: 287.7065472602844\n",
      "  time_this_iter_s: 17.765604972839355\n",
      "  time_total_s: 287.7065472602844\n",
      "  timers:\n",
      "    learn_throughput: 1584.203\n",
      "    learn_time_ms: 631.232\n",
      "    load_throughput: 67322.681\n",
      "    load_time_ms: 14.854\n",
      "    sample_throughput: 55.654\n",
      "    sample_time_ms: 17968.299\n",
      "    update_time_ms: 3.132\n",
      "  timestamp: 1633791395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         287.707</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-0.892857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           414.071</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 412.4516129032258\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8064516129032258\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 31\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.812476791275872\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009241330296108647\n",
      "          policy_loss: -0.015302751378880607\n",
      "          total_loss: -0.043122512764400905\n",
      "          vf_explained_var: -0.9126608371734619\n",
      "          vf_loss: 0.0002761285365623836\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.76538461538462\n",
      "    ram_util_percent: 68.68846153846152\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03684263720079903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.970499225425232\n",
      "    mean_inference_ms: 1.7214763887682025\n",
      "    mean_raw_obs_processing_ms: 0.3022097278998021\n",
      "  time_since_restore: 324.0550458431244\n",
      "  time_this_iter_s: 36.348498582839966\n",
      "  time_total_s: 324.0550458431244\n",
      "  timers:\n",
      "    learn_throughput: 1586.394\n",
      "    learn_time_ms: 630.36\n",
      "    load_throughput: 66484.758\n",
      "    load_time_ms: 15.041\n",
      "    sample_throughput: 51.041\n",
      "    sample_time_ms: 19591.961\n",
      "    update_time_ms: 3.134\n",
      "  timestamp: 1633791431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         324.055</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.806452</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           412.452</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-57-32\n",
      "  done: false\n",
      "  episode_len_mean: 409.1764705882353\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7352941176470589\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 34\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7884134186638727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008132114732154132\n",
      "          policy_loss: -0.06150064818147156\n",
      "          total_loss: -0.0891132962786489\n",
      "          vf_explained_var: -0.7315559387207031\n",
      "          vf_loss: 0.0002460756238886259\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.78620689655173\n",
      "    ram_util_percent: 69.96896551724137\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681847257318345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.065705013990843\n",
      "    mean_inference_ms: 1.7200072973490714\n",
      "    mean_raw_obs_processing_ms: 0.4029205494304178\n",
      "  time_since_restore: 344.17502307891846\n",
      "  time_this_iter_s: 20.119977235794067\n",
      "  time_total_s: 344.17502307891846\n",
      "  timers:\n",
      "    learn_throughput: 1584.432\n",
      "    learn_time_ms: 631.141\n",
      "    load_throughput: 65840.409\n",
      "    load_time_ms: 15.188\n",
      "    sample_throughput: 51.039\n",
      "    sample_time_ms: 19592.788\n",
      "    update_time_ms: 3.143\n",
      "  timestamp: 1633791452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         344.175</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.735294</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           409.176</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-57-54\n",
      "  done: false\n",
      "  episode_len_mean: 404.4864864864865\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6756756756756757\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 37\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.740892590416802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007131264831832339\n",
      "          policy_loss: 0.04433749947283003\n",
      "          total_loss: 0.01726107680135303\n",
      "          vf_explained_var: -0.539760410785675\n",
      "          vf_loss: 0.00031021600564902957\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.584375\n",
      "    ram_util_percent: 69.74375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680054542617663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.303395943848667\n",
      "    mean_inference_ms: 1.718897055342547\n",
      "    mean_raw_obs_processing_ms: 0.4804348397909777\n",
      "  time_since_restore: 366.738107919693\n",
      "  time_this_iter_s: 22.563084840774536\n",
      "  time_total_s: 366.738107919693\n",
      "  timers:\n",
      "    learn_throughput: 1581.12\n",
      "    learn_time_ms: 632.463\n",
      "    load_throughput: 61829.332\n",
      "    load_time_ms: 16.174\n",
      "    sample_throughput: 49.822\n",
      "    sample_time_ms: 20071.318\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1633791474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         366.738</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-0.675676</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           404.486</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-58-16\n",
      "  done: false\n",
      "  episode_len_mean: 402.02564102564105\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6410256410256411\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 39\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7316100226508246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008605314044495296\n",
      "          policy_loss: 0.04419074174430635\n",
      "          total_loss: 0.01727470623122321\n",
      "          vf_explained_var: -0.37534114718437195\n",
      "          vf_loss: 0.00037317395294343843\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.609375\n",
      "    ram_util_percent: 69.71875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036790180863827536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.856751355000252\n",
      "    mean_inference_ms: 1.7182948390199795\n",
      "    mean_raw_obs_processing_ms: 0.5215925556467659\n",
      "  time_since_restore: 388.91419291496277\n",
      "  time_this_iter_s: 22.176084995269775\n",
      "  time_total_s: 388.91419291496277\n",
      "  timers:\n",
      "    learn_throughput: 1577.397\n",
      "    learn_time_ms: 633.956\n",
      "    load_throughput: 61552.767\n",
      "    load_time_ms: 16.246\n",
      "    sample_throughput: 49.098\n",
      "    sample_time_ms: 20367.415\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1633791496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         388.914</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-0.641026</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           402.026</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 399.6190476190476\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5952380952380952\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 42\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7270883825090197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007140369169875971\n",
      "          policy_loss: 0.03136580172512266\n",
      "          total_loss: 0.004654419091012742\n",
      "          vf_explained_var: 0.1879970133304596\n",
      "          vf_loss: 0.0005371882479974172\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.638709677419364\n",
      "    ram_util_percent: 69.67741935483872\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036775096510683544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.261644006041706\n",
      "    mean_inference_ms: 1.717551428875589\n",
      "    mean_raw_obs_processing_ms: 0.5713262357678014\n",
      "  time_since_restore: 410.96077275276184\n",
      "  time_this_iter_s: 22.046579837799072\n",
      "  time_total_s: 410.96077275276184\n",
      "  timers:\n",
      "    learn_throughput: 1577.593\n",
      "    learn_time_ms: 633.877\n",
      "    load_throughput: 57441.898\n",
      "    load_time_ms: 17.409\n",
      "    sample_throughput: 47.986\n",
      "    sample_time_ms: 20839.481\n",
      "    update_time_ms: 3.177\n",
      "  timestamp: 1633791518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         410.961</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.595238</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           399.619</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 397.46666666666664\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5555555555555556\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 45\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7432665189107257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007594163663337896\n",
      "          policy_loss: -0.0014285477499167123\n",
      "          total_loss: -0.0285578191280365\n",
      "          vf_explained_var: -0.8837210536003113\n",
      "          vf_loss: 0.0002796622691676021\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.584375\n",
      "    ram_util_percent: 69.796875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676331128288625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.741978077454178\n",
      "    mean_inference_ms: 1.7169571525053937\n",
      "    mean_raw_obs_processing_ms: 0.6105526629192822\n",
      "  time_since_restore: 433.0492877960205\n",
      "  time_this_iter_s: 22.088515043258667\n",
      "  time_total_s: 433.0492877960205\n",
      "  timers:\n",
      "    learn_throughput: 1574.399\n",
      "    learn_time_ms: 635.163\n",
      "    load_throughput: 56273.021\n",
      "    load_time_ms: 17.771\n",
      "    sample_throughput: 47.263\n",
      "    sample_time_ms: 21158.249\n",
      "    update_time_ms: 3.171\n",
      "  timestamp: 1633791541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         433.049</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.555556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           397.467</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 396.1489361702128\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5319148936170213\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 47\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.747673291630215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01125860078927467\n",
      "          policy_loss: 0.02531888335943222\n",
      "          total_loss: -0.0015795002174046305\n",
      "          vf_explained_var: -0.18570055067539215\n",
      "          vf_loss: 0.0005431667024418453\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60666666666667\n",
      "    ram_util_percent: 70.05999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675626067718143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.428424849736718\n",
      "    mean_inference_ms: 1.7166075797228297\n",
      "    mean_raw_obs_processing_ms: 0.6316461601380647\n",
      "  time_since_restore: 454.32713294029236\n",
      "  time_this_iter_s: 21.27784514427185\n",
      "  time_total_s: 454.32713294029236\n",
      "  timers:\n",
      "    learn_throughput: 1570.483\n",
      "    learn_time_ms: 636.747\n",
      "    load_throughput: 55781.847\n",
      "    load_time_ms: 17.927\n",
      "    sample_throughput: 46.602\n",
      "    sample_time_ms: 21458.099\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633791562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         454.327</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.531915</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           396.149</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_14-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 394.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 50\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.766861006948683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011424539744888424\n",
      "          policy_loss: 0.042797945274247066\n",
      "          total_loss: 0.13151526716020373\n",
      "          vf_explained_var: -0.5058658719062805\n",
      "          vf_loss: 0.11635023388080298\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59677419354838\n",
      "    ram_util_percent: 70.32258064516131\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367464523001948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.001174303476972\n",
      "    mean_inference_ms: 1.7161543128282823\n",
      "    mean_raw_obs_processing_ms: 0.6573451863918646\n",
      "  time_since_restore: 475.998051404953\n",
      "  time_this_iter_s: 21.670918464660645\n",
      "  time_total_s: 475.998051404953\n",
      "  timers:\n",
      "    learn_throughput: 1573.389\n",
      "    learn_time_ms: 635.571\n",
      "    load_throughput: 54434.084\n",
      "    load_time_ms: 18.371\n",
      "    sample_throughput: 45.898\n",
      "    sample_time_ms: 21787.289\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633791584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         475.998</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            394.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-00-06\n",
      "  done: false\n",
      "  episode_len_mean: 393.6981132075472\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5660377358490566\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 53\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7506067593892416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073867216094372625\n",
      "          policy_loss: 0.03165206834673882\n",
      "          total_loss: 0.007483401811785168\n",
      "          vf_explained_var: -0.5710800290107727\n",
      "          vf_loss: 0.0033143184807461995\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.496874999999996\n",
      "    ram_util_percent: 70.46875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673790642606775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.620338178133103\n",
      "    mean_inference_ms: 1.7157808534024088\n",
      "    mean_raw_obs_processing_ms: 0.6777432254350727\n",
      "  time_since_restore: 498.19399333000183\n",
      "  time_this_iter_s: 22.195941925048828\n",
      "  time_total_s: 498.19399333000183\n",
      "  timers:\n",
      "    learn_throughput: 1573.332\n",
      "    learn_time_ms: 635.594\n",
      "    load_throughput: 54135.349\n",
      "    load_time_ms: 18.472\n",
      "    sample_throughput: 45.114\n",
      "    sample_time_ms: 22165.892\n",
      "    update_time_ms: 2.097\n",
      "  timestamp: 1633791606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         498.194</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.566038</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           393.698</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 392.26785714285717\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5357142857142857\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 56\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.752124145295885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006560099821742415\n",
      "          policy_loss: -0.018064670885602634\n",
      "          total_loss: -0.04396636502610313\n",
      "          vf_explained_var: -0.16852886974811554\n",
      "          vf_loss: 0.0015990456786110169\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.713333333333345\n",
      "    ram_util_percent: 70.54999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673031417872164\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.276613575805534\n",
      "    mean_inference_ms: 1.7154509216066158\n",
      "    mean_raw_obs_processing_ms: 0.6938922040451192\n",
      "  time_since_restore: 519.5618257522583\n",
      "  time_this_iter_s: 21.36783242225647\n",
      "  time_total_s: 519.5618257522583\n",
      "  timers:\n",
      "    learn_throughput: 1570.013\n",
      "    learn_time_ms: 636.938\n",
      "    load_throughput: 53586.879\n",
      "    load_time_ms: 18.661\n",
      "    sample_throughput: 44.396\n",
      "    sample_time_ms: 22524.579\n",
      "    update_time_ms: 2.093\n",
      "  timestamp: 1633791627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         519.562</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.535714</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           392.268</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 391.87931034482756\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5172413793103449\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 58\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.767403973473443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007218802583168424\n",
      "          policy_loss: 0.060201653382844396\n",
      "          total_loss: 0.03341350058714549\n",
      "          vf_explained_var: -0.6969187259674072\n",
      "          vf_loss: 0.0008633287455369201\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71333333333334\n",
      "    ram_util_percent: 70.64333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367254693653997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.06435673419066\n",
      "    mean_inference_ms: 1.7152501156385334\n",
      "    mean_raw_obs_processing_ms: 0.702476990048222\n",
      "  time_since_restore: 540.4365842342377\n",
      "  time_this_iter_s: 20.87475848197937\n",
      "  time_total_s: 540.4365842342377\n",
      "  timers:\n",
      "    learn_throughput: 1567.592\n",
      "    learn_time_ms: 637.921\n",
      "    load_throughput: 53860.825\n",
      "    load_time_ms: 18.566\n",
      "    sample_throughput: 47.673\n",
      "    sample_time_ms: 20976.296\n",
      "    update_time_ms: 2.106\n",
      "  timestamp: 1633791648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         540.437</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.517241</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           391.879</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 390.5245901639344\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4918032786885246\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 61\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.740456872516208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009922235102946115\n",
      "          policy_loss: -0.029735145252197982\n",
      "          total_loss: -0.050676388666033745\n",
      "          vf_explained_var: -0.4238753318786621\n",
      "          vf_loss: 0.006432316200031588\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.33965517241379\n",
      "    ram_util_percent: 70.76034482758621\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036719698786120206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.773805595776363\n",
      "    mean_inference_ms: 1.7149980281090176\n",
      "    mean_raw_obs_processing_ms: 0.7474365156799081\n",
      "  time_since_restore: 580.9667773246765\n",
      "  time_this_iter_s: 40.53019309043884\n",
      "  time_total_s: 580.9667773246765\n",
      "  timers:\n",
      "    learn_throughput: 1567.207\n",
      "    learn_time_ms: 638.078\n",
      "    load_throughput: 53507.307\n",
      "    load_time_ms: 18.689\n",
      "    sample_throughput: 43.446\n",
      "    sample_time_ms: 23017.018\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1633791689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         580.967</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.491803</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           390.525</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 389.59375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.46875\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 64\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7847859064737954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007692616565414302\n",
      "          policy_loss: -0.02243415390451749\n",
      "          total_loss: -0.049794700576199426\n",
      "          vf_explained_var: -0.19469104707241058\n",
      "          vf_loss: 0.000463271867192816\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.477419354838716\n",
      "    ram_util_percent: 71.0774193548387\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671497627504395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.50804289961207\n",
      "    mean_inference_ms: 1.7147931385781225\n",
      "    mean_raw_obs_processing_ms: 0.7854749138053299\n",
      "  time_since_restore: 602.439090013504\n",
      "  time_this_iter_s: 21.472312688827515\n",
      "  time_total_s: 602.439090013504\n",
      "  timers:\n",
      "    learn_throughput: 1567.974\n",
      "    learn_time_ms: 637.766\n",
      "    load_throughput: 53506.351\n",
      "    load_time_ms: 18.689\n",
      "    sample_throughput: 43.652\n",
      "    sample_time_ms: 22908.27\n",
      "    update_time_ms: 2.102\n",
      "  timestamp: 1633791710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         602.439</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-0.46875</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           389.594</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-02-11\n",
      "  done: false\n",
      "  episode_len_mean: 389.3181818181818\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.45454545454545453\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 66\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.735106462902493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009047288283071648\n",
      "          policy_loss: -0.027194186713960435\n",
      "          total_loss: -0.05413402964671453\n",
      "          vf_explained_var: -0.37447962164878845\n",
      "          vf_loss: 0.00038295090117672874\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.803448275862074\n",
      "    ram_util_percent: 71.03448275862071\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036711655753835955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.34189978352905\n",
      "    mean_inference_ms: 1.714665391399752\n",
      "    mean_raw_obs_processing_ms: 0.8072632607715328\n",
      "  time_since_restore: 623.057012796402\n",
      "  time_this_iter_s: 20.61792278289795\n",
      "  time_total_s: 623.057012796402\n",
      "  timers:\n",
      "    learn_throughput: 1572.969\n",
      "    learn_time_ms: 635.74\n",
      "    load_throughput: 53442.198\n",
      "    load_time_ms: 18.712\n",
      "    sample_throughput: 43.947\n",
      "    sample_time_ms: 22754.46\n",
      "    update_time_ms: 2.091\n",
      "  timestamp: 1633791731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         623.057</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.454545</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           389.318</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 389.40579710144925\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.43478260869565216\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 69\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7030738645129735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009617463435051574\n",
      "          policy_loss: -0.039091720494131246\n",
      "          total_loss: -0.06551256372282903\n",
      "          vf_explained_var: -0.7427729368209839\n",
      "          vf_loss: 0.0005798376753874537\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.599999999999994\n",
      "    ram_util_percent: 70.68387096774192\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670704773359221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.108965254575804\n",
      "    mean_inference_ms: 1.7144892651830048\n",
      "    mean_raw_obs_processing_ms: 0.835434437385301\n",
      "  time_since_restore: 644.5927457809448\n",
      "  time_this_iter_s: 21.535732984542847\n",
      "  time_total_s: 644.5927457809448\n",
      "  timers:\n",
      "    learn_throughput: 1571.469\n",
      "    learn_time_ms: 636.347\n",
      "    load_throughput: 53576.68\n",
      "    load_time_ms: 18.665\n",
      "    sample_throughput: 44.047\n",
      "    sample_time_ms: 22702.795\n",
      "    update_time_ms: 2.089\n",
      "  timestamp: 1633791752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         644.593</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.434783</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           389.406</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 389.59154929577466\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4225352112676056\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 71\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7095376518037586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008672632755561733\n",
      "          policy_loss: -0.0034881720112429726\n",
      "          total_loss: -0.030031785948408975\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005246591699283777\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67857142857144\n",
      "    ram_util_percent: 70.59642857142855\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670416967790876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.961493653786803\n",
      "    mean_inference_ms: 1.714379060919627\n",
      "    mean_raw_obs_processing_ms: 0.8515730950680492\n",
      "  time_since_restore: 664.0515630245209\n",
      "  time_this_iter_s: 19.45881724357605\n",
      "  time_total_s: 664.0515630245209\n",
      "  timers:\n",
      "    learn_throughput: 1571.42\n",
      "    learn_time_ms: 636.367\n",
      "    load_throughput: 54713.142\n",
      "    load_time_ms: 18.277\n",
      "    sample_throughput: 44.563\n",
      "    sample_time_ms: 22440.174\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1633791772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         664.052</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.422535</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           389.592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 390.56756756756755\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.40540540540540543\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 74\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.675137554274665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008410771023606906\n",
      "          policy_loss: 0.0076065704123013545\n",
      "          total_loss: -0.018484293959207004\n",
      "          vf_explained_var: 0.17392580211162567\n",
      "          vf_loss: 0.0006342246727500525\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67586206896553\n",
      "    ram_util_percent: 70.61724137931033\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670006997284143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.752281230723586\n",
      "    mean_inference_ms: 1.7142255697794209\n",
      "    mean_raw_obs_processing_ms: 0.8724459896330843\n",
      "  time_since_restore: 684.43887591362\n",
      "  time_this_iter_s: 20.38731288909912\n",
      "  time_total_s: 684.43887591362\n",
      "  timers:\n",
      "    learn_throughput: 1572.661\n",
      "    learn_time_ms: 635.865\n",
      "    load_throughput: 54871.544\n",
      "    load_time_ms: 18.224\n",
      "    sample_throughput: 44.739\n",
      "    sample_time_ms: 22351.679\n",
      "    update_time_ms: 2.101\n",
      "  timestamp: 1633791792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         684.439</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.405405</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           390.568</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 391.6842105263158\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.39473684210526316\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 76\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6804011556837293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00912146905846015\n",
      "          policy_loss: -0.0026126065601905185\n",
      "          total_loss: -0.029108975165420107\n",
      "          vf_explained_var: -0.09562104195356369\n",
      "          vf_loss: 0.0002791376622755908\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87307692307692\n",
      "    ram_util_percent: 70.73076923076923\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669750445696973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.61841049562762\n",
      "    mean_inference_ms: 1.7141254766496088\n",
      "    mean_raw_obs_processing_ms: 0.8843828593866487\n",
      "  time_since_restore: 702.7495617866516\n",
      "  time_this_iter_s: 18.310685873031616\n",
      "  time_total_s: 702.7495617866516\n",
      "  timers:\n",
      "    learn_throughput: 1570.711\n",
      "    learn_time_ms: 636.654\n",
      "    load_throughput: 55035.848\n",
      "    load_time_ms: 18.17\n",
      "    sample_throughput: 45.424\n",
      "    sample_time_ms: 22014.913\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633791811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          702.75</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-0.394737</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           391.684</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 393.0769230769231\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.38461538461538464\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 78\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.600885017712911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012501135565939675\n",
      "          policy_loss: -0.05911074603597323\n",
      "          total_loss: -0.08481464948919085\n",
      "          vf_explained_var: -0.4906626343727112\n",
      "          vf_loss: 0.0002658796223436689\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.548148148148144\n",
      "    ram_util_percent: 70.78518518518518\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669516745795077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.488474326765928\n",
      "    mean_inference_ms: 1.714028728327321\n",
      "    mean_raw_obs_processing_ms: 0.8947327954942389\n",
      "  time_since_restore: 721.4381873607635\n",
      "  time_this_iter_s: 18.68862557411194\n",
      "  time_total_s: 721.4381873607635\n",
      "  timers:\n",
      "    learn_throughput: 1573.481\n",
      "    learn_time_ms: 635.534\n",
      "    load_throughput: 54825.067\n",
      "    load_time_ms: 18.24\n",
      "    sample_throughput: 46.157\n",
      "    sample_time_ms: 21665.238\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633791829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         721.438</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.384615</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           393.077</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-04-07\n",
      "  done: false\n",
      "  episode_len_mean: 394.58024691358025\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.37037037037037035\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 81\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.625839363204108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011962924006429827\n",
      "          policy_loss: -0.02648068124221431\n",
      "          total_loss: -0.052480397497614226\n",
      "          vf_explained_var: -0.9319910407066345\n",
      "          vf_loss: 0.00022129082620570746\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.959999999999994\n",
      "    ram_util_percent: 70.86800000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669193844536213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.3005222424868\n",
      "    mean_inference_ms: 1.7138833213246027\n",
      "    mean_raw_obs_processing_ms: 0.9080206439607305\n",
      "  time_since_restore: 739.2158718109131\n",
      "  time_this_iter_s: 17.777684450149536\n",
      "  time_total_s: 739.2158718109131\n",
      "  timers:\n",
      "    learn_throughput: 1574.075\n",
      "    learn_time_ms: 635.294\n",
      "    load_throughput: 54239.589\n",
      "    load_time_ms: 18.437\n",
      "    sample_throughput: 46.935\n",
      "    sample_time_ms: 21306.259\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633791847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         739.216</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.37037</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            394.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 395.2168674698795\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3614457831325301\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 83\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6739298264185587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00970005384822152\n",
      "          policy_loss: -0.03814136665314436\n",
      "          total_loss: -0.06442279605608847\n",
      "          vf_explained_var: -0.5449137687683105\n",
      "          vf_loss: 0.0004275552344754235\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.78518518518519\n",
      "    ram_util_percent: 70.9962962962963\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366899286776163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.18064415694399\n",
      "    mean_inference_ms: 1.7137943694888733\n",
      "    mean_raw_obs_processing_ms: 0.9155475895131363\n",
      "  time_since_restore: 758.3044607639313\n",
      "  time_this_iter_s: 19.08858895301819\n",
      "  time_total_s: 758.3044607639313\n",
      "  timers:\n",
      "    learn_throughput: 1577.657\n",
      "    learn_time_ms: 633.851\n",
      "    load_throughput: 54125.499\n",
      "    load_time_ms: 18.476\n",
      "    sample_throughput: 47.328\n",
      "    sample_time_ms: 21129.085\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633791866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         758.304</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.361446</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           395.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 395.94117647058823\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.35294117647058826\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 85\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5921804507573447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010409862298858904\n",
      "          policy_loss: -0.017092528608110218\n",
      "          total_loss: -0.042759988953669864\n",
      "          vf_explained_var: -0.44365450739860535\n",
      "          vf_loss: 0.00022181553609294092\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30799999999999\n",
      "    ram_util_percent: 71.03199999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668801590977624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.06330070876787\n",
      "    mean_inference_ms: 1.7137017318507308\n",
      "    mean_raw_obs_processing_ms: 0.9219822602468756\n",
      "  time_since_restore: 775.7045586109161\n",
      "  time_this_iter_s: 17.400097846984863\n",
      "  time_total_s: 775.7045586109161\n",
      "  timers:\n",
      "    learn_throughput: 1577.994\n",
      "    learn_time_ms: 633.716\n",
      "    load_throughput: 57525.013\n",
      "    load_time_ms: 17.384\n",
      "    sample_throughput: 53.143\n",
      "    sample_time_ms: 18817.329\n",
      "    update_time_ms: 2.032\n",
      "  timestamp: 1633791884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         775.705</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.352941</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           395.941</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-05-03\n",
      "  done: false\n",
      "  episode_len_mean: 396.45454545454544\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3409090909090909\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 88\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.610579138331943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008524496946236977\n",
      "          policy_loss: -0.07018508745564354\n",
      "          total_loss: -0.09603870428270764\n",
      "          vf_explained_var: -0.6862525343894958\n",
      "          vf_loss: 0.00022553145293689644\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84285714285714\n",
      "    ram_util_percent: 71.20357142857144\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668542106940608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.894788951196315\n",
      "    mean_inference_ms: 1.7135687882342021\n",
      "    mean_raw_obs_processing_ms: 0.9301206573342052\n",
      "  time_since_restore: 794.9540371894836\n",
      "  time_this_iter_s: 19.249478578567505\n",
      "  time_total_s: 794.9540371894836\n",
      "  timers:\n",
      "    learn_throughput: 1580.576\n",
      "    learn_time_ms: 632.681\n",
      "    load_throughput: 58289.439\n",
      "    load_time_ms: 17.156\n",
      "    sample_throughput: 53.774\n",
      "    sample_time_ms: 18596.285\n",
      "    update_time_ms: 2.037\n",
      "  timestamp: 1633791903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         794.954</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.340909</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           396.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 396.2888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 90\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5691409720314873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010456974825015121\n",
      "          policy_loss: -0.004042189175056087\n",
      "          total_loss: -0.029506944782204097\n",
      "          vf_explained_var: 0.19727815687656403\n",
      "          vf_loss: 0.00019397541424647595\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.7530612244898\n",
      "    ram_util_percent: 71.27755102040815\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036683889792117334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.78642644678341\n",
      "    mean_inference_ms: 1.7134800332835647\n",
      "    mean_raw_obs_processing_ms: 0.9449466641218092\n",
      "  time_since_restore: 829.7162780761719\n",
      "  time_this_iter_s: 34.76224088668823\n",
      "  time_total_s: 829.7162780761719\n",
      "  timers:\n",
      "    learn_throughput: 1576.913\n",
      "    learn_time_ms: 634.151\n",
      "    load_throughput: 58344.251\n",
      "    load_time_ms: 17.14\n",
      "    sample_throughput: 49.977\n",
      "    sample_time_ms: 20009.247\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1633791938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         829.716</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           396.289</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 397.1847826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.32608695652173914\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 92\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5853434483210247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009228429939026563\n",
      "          policy_loss: 0.0009428006493382983\n",
      "          total_loss: -0.024618245164553323\n",
      "          vf_explained_var: -0.7402529120445251\n",
      "          vf_loss: 0.00026354858297660634\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.93600000000001\n",
      "    ram_util_percent: 71.212\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668269365042637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.68029403297569\n",
      "    mean_inference_ms: 1.7133956613461512\n",
      "    mean_raw_obs_processing_ms: 0.9582865276113668\n",
      "  time_since_restore: 847.1150786876678\n",
      "  time_this_iter_s: 17.39880061149597\n",
      "  time_total_s: 847.1150786876678\n",
      "  timers:\n",
      "    learn_throughput: 1577.273\n",
      "    learn_time_ms: 634.006\n",
      "    load_throughput: 58424.465\n",
      "    load_time_ms: 17.116\n",
      "    sample_throughput: 51.032\n",
      "    sample_time_ms: 19595.725\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633791955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         847.115</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.326087</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           397.185</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-06-10\n",
      "  done: false\n",
      "  episode_len_mean: 399.12631578947367\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3157894736842105\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 95\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6117271131939357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008926623224501481\n",
      "          policy_loss: -0.06115295332339075\n",
      "          total_loss: -0.08695888167454137\n",
      "          vf_explained_var: -0.8222525715827942\n",
      "          vf_loss: 0.0002834466031345073\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.622727272727275\n",
      "    ram_util_percent: 71.22727272727273\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036680946596232854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.524319380894642\n",
      "    mean_inference_ms: 1.713261060695302\n",
      "    mean_raw_obs_processing_ms: 0.9761104011465156\n",
      "  time_since_restore: 862.4199697971344\n",
      "  time_this_iter_s: 15.304891109466553\n",
      "  time_total_s: 862.4199697971344\n",
      "  timers:\n",
      "    learn_throughput: 1574.879\n",
      "    learn_time_ms: 634.969\n",
      "    load_throughput: 63271.761\n",
      "    load_time_ms: 15.805\n",
      "    sample_throughput: 52.136\n",
      "    sample_time_ms: 19180.701\n",
      "    update_time_ms: 2.01\n",
      "  timestamp: 1633791970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">          862.42</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.315789</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           399.126</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.5773195876289\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.30927835051546393\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 97\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.560220138231913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011391810410234725\n",
      "          policy_loss: -0.04460733286622498\n",
      "          total_loss: -0.06951179318130016\n",
      "          vf_explained_var: -0.52155601978302\n",
      "          vf_loss: 0.0006621407713383734\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.013043478260876\n",
      "    ram_util_percent: 71.0913043478261\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667987617262341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.422894820926423\n",
      "    mean_inference_ms: 1.7131701134276862\n",
      "    mean_raw_obs_processing_ms: 0.9866536170316406\n",
      "  time_since_restore: 878.42085313797\n",
      "  time_this_iter_s: 16.00088334083557\n",
      "  time_total_s: 878.42085313797\n",
      "  timers:\n",
      "    learn_throughput: 1575.531\n",
      "    learn_time_ms: 634.707\n",
      "    load_throughput: 65694.282\n",
      "    load_time_ms: 15.222\n",
      "    sample_throughput: 53.354\n",
      "    sample_time_ms: 18742.903\n",
      "    update_time_ms: 2.008\n",
      "  timestamp: 1633791986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         878.421</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.309278</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           400.577</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 401.4141414141414\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.30303030303030304\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 99\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.543030203713311\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009256789285862588\n",
      "          policy_loss: -0.028998024264971414\n",
      "          total_loss: -0.054068875561157866\n",
      "          vf_explained_var: 0.05609817057847977\n",
      "          vf_loss: 0.00033052176997646005\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58400000000002\n",
      "    ram_util_percent: 70.828\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667883511616107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.323561246388355\n",
      "    mean_inference_ms: 1.7130781340881907\n",
      "    mean_raw_obs_processing_ms: 0.9961003923250547\n",
      "  time_since_restore: 895.6248228549957\n",
      "  time_this_iter_s: 17.203969717025757\n",
      "  time_total_s: 895.6248228549957\n",
      "  timers:\n",
      "    learn_throughput: 1574.489\n",
      "    learn_time_ms: 635.127\n",
      "    load_throughput: 71326.849\n",
      "    load_time_ms: 14.02\n",
      "    sample_throughput: 53.668\n",
      "    sample_time_ms: 18632.991\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1633792004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         895.625</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-0.30303</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">           401.414</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-07-03\n",
      "  done: false\n",
      "  episode_len_mean: 402.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 101\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4645522091123793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007379306928377361\n",
      "          policy_loss: -0.10928868498239253\n",
      "          total_loss: -0.1337312719060315\n",
      "          vf_explained_var: 0.4714851379394531\n",
      "          vf_loss: 0.00017987502748534706\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.548148148148144\n",
      "    ram_util_percent: 70.75555555555555\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03666192302187472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.684422112720437\n",
      "    mean_inference_ms: 1.7121728062268284\n",
      "    mean_raw_obs_processing_ms: 1.0127324827771809\n",
      "  time_since_restore: 914.897554397583\n",
      "  time_this_iter_s: 19.27273154258728\n",
      "  time_total_s: 914.897554397583\n",
      "  timers:\n",
      "    learn_throughput: 1571.931\n",
      "    learn_time_ms: 636.16\n",
      "    load_throughput: 70926.887\n",
      "    load_time_ms: 14.099\n",
      "    sample_throughput: 53.504\n",
      "    sample_time_ms: 18690.283\n",
      "    update_time_ms: 2.014\n",
      "  timestamp: 1633792023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         914.898</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            402.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 403.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 103\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4417466825909084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009368467625314765\n",
      "          policy_loss: 0.1549471513264709\n",
      "          total_loss: 0.1307942472398281\n",
      "          vf_explained_var: -0.6844379901885986\n",
      "          vf_loss: 0.00023528710407845211\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64642857142856\n",
      "    ram_util_percent: 70.81785714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366377904628171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.79718026065533\n",
      "    mean_inference_ms: 1.7108733656771609\n",
      "    mean_raw_obs_processing_ms: 1.0369179292812063\n",
      "  time_since_restore: 934.49387550354\n",
      "  time_this_iter_s: 19.59632110595703\n",
      "  time_total_s: 934.49387550354\n",
      "  timers:\n",
      "    learn_throughput: 1572.75\n",
      "    learn_time_ms: 635.829\n",
      "    load_throughput: 72106.601\n",
      "    load_time_ms: 13.868\n",
      "    sample_throughput: 52.987\n",
      "    sample_time_ms: 18872.669\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633792043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         934.494</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            403.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-07-41\n",
      "  done: false\n",
      "  episode_len_mean: 404.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 106\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4648349205652873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010184675331895246\n",
      "          policy_loss: 0.005063496612840229\n",
      "          total_loss: -0.01929294860197438\n",
      "          vf_explained_var: -0.5766132473945618\n",
      "          vf_loss: 0.00026007627796692154\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56296296296296\n",
      "    ram_util_percent: 70.89259259259259\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662025324479908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.110096279990945\n",
      "    mean_inference_ms: 1.7098629142923525\n",
      "    mean_raw_obs_processing_ms: 1.072433067671075\n",
      "  time_since_restore: 953.229444026947\n",
      "  time_this_iter_s: 18.735568523406982\n",
      "  time_total_s: 953.229444026947\n",
      "  timers:\n",
      "    learn_throughput: 1571.578\n",
      "    learn_time_ms: 636.303\n",
      "    load_throughput: 72034.898\n",
      "    load_time_ms: 13.882\n",
      "    sample_throughput: 53.087\n",
      "    sample_time_ms: 18836.843\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633792061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         953.229</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            404.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 404.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 108\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4350251065360173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009678510861413327\n",
      "          policy_loss: -0.06777897030115128\n",
      "          total_loss: -0.09183820059729947\n",
      "          vf_explained_var: -0.48909294605255127\n",
      "          vf_loss: 0.0002607744219454212\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81428571428571\n",
      "    ram_util_percent: 71.00357142857142\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366126197497728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.77023974762238\n",
      "    mean_inference_ms: 1.7093810901479674\n",
      "    mean_raw_obs_processing_ms: 1.095659933340925\n",
      "  time_since_restore: 973.085191488266\n",
      "  time_this_iter_s: 19.85574746131897\n",
      "  time_total_s: 973.085191488266\n",
      "  timers:\n",
      "    learn_throughput: 1571.54\n",
      "    learn_time_ms: 636.319\n",
      "    load_throughput: 66389.205\n",
      "    load_time_ms: 15.063\n",
      "    sample_throughput: 52.408\n",
      "    sample_time_ms: 19081.041\n",
      "    update_time_ms: 2.148\n",
      "  timestamp: 1633792081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         973.085</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             404.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 404.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 110\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4407159778806897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008025338339744377\n",
      "          policy_loss: -0.002143677406840854\n",
      "          total_loss: -0.02620335966348648\n",
      "          vf_explained_var: -0.883249044418335\n",
      "          vf_loss: 0.00032239296787237335\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.165517241379305\n",
      "    ram_util_percent: 71.35862068965515\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660832721795766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.516156841642502\n",
      "    mean_inference_ms: 1.709078547473043\n",
      "    mean_raw_obs_processing_ms: 1.11845767574886\n",
      "  time_since_restore: 993.4520170688629\n",
      "  time_this_iter_s: 20.366825580596924\n",
      "  time_total_s: 993.4520170688629\n",
      "  timers:\n",
      "    learn_throughput: 1568.583\n",
      "    learn_time_ms: 637.518\n",
      "    load_throughput: 66282.402\n",
      "    load_time_ms: 15.087\n",
      "    sample_throughput: 52.106\n",
      "    sample_time_ms: 19191.556\n",
      "    update_time_ms: 2.148\n",
      "  timestamp: 1633792102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         993.452</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            404.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 404.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 113\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.509034609794617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013224536997784645\n",
      "          policy_loss: -0.05826425130168597\n",
      "          total_loss: -0.08275274624013239\n",
      "          vf_explained_var: -0.9794014692306519\n",
      "          vf_loss: 0.0005605244346144092\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.696296296296296\n",
      "    ram_util_percent: 71.26296296296296\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036603757838979956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.207456290426222\n",
      "    mean_inference_ms: 1.7087910393653436\n",
      "    mean_raw_obs_processing_ms: 1.1519739065884373\n",
      "  time_since_restore: 1012.2068753242493\n",
      "  time_this_iter_s: 18.754858255386353\n",
      "  time_total_s: 1012.2068753242493\n",
      "  timers:\n",
      "    learn_throughput: 1568.704\n",
      "    learn_time_ms: 637.469\n",
      "    load_throughput: 65796.513\n",
      "    load_time_ms: 15.198\n",
      "    sample_throughput: 56.848\n",
      "    sample_time_ms: 17590.77\n",
      "    update_time_ms: 2.146\n",
      "  timestamp: 1633792120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1012.21</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            404.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-09-00\n",
      "  done: false\n",
      "  episode_len_mean: 405.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 115\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.464280676841736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008412976091808222\n",
      "          policy_loss: -0.05256644818517897\n",
      "          total_loss: -0.07686393136779467\n",
      "          vf_explained_var: -0.683610737323761\n",
      "          vf_loss: 0.00031903141126450563\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.278571428571425\n",
      "    ram_util_percent: 71.3\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660182106602639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.04681080284074\n",
      "    mean_inference_ms: 1.7086901934406376\n",
      "    mean_raw_obs_processing_ms: 1.1739366861101401\n",
      "  time_since_restore: 1031.6013610363007\n",
      "  time_this_iter_s: 19.39448571205139\n",
      "  time_total_s: 1031.6013610363007\n",
      "  timers:\n",
      "    learn_throughput: 1570.45\n",
      "    learn_time_ms: 636.76\n",
      "    load_throughput: 66701.505\n",
      "    load_time_ms: 14.992\n",
      "    sample_throughput: 56.208\n",
      "    sample_time_ms: 17791.215\n",
      "    update_time_ms: 2.166\n",
      "  timestamp: 1633792140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          1031.6</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            405.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 405.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4705659680896335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009018507567389342\n",
      "          policy_loss: -0.07552649525718556\n",
      "          total_loss: -0.09990826495405701\n",
      "          vf_explained_var: -0.7982491254806519\n",
      "          vf_loss: 0.0002957066276899746\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62592592592593\n",
      "    ram_util_percent: 71.37407407407409\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660047716039742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.914333817290835\n",
      "    mean_inference_ms: 1.7086521514393487\n",
      "    mean_raw_obs_processing_ms: 1.1954615662489718\n",
      "  time_since_restore: 1050.2822506427765\n",
      "  time_this_iter_s: 18.68088960647583\n",
      "  time_total_s: 1050.2822506427765\n",
      "  timers:\n",
      "    learn_throughput: 1569.929\n",
      "    learn_time_ms: 636.972\n",
      "    load_throughput: 60718.464\n",
      "    load_time_ms: 16.469\n",
      "    sample_throughput: 55.166\n",
      "    sample_time_ms: 18127.102\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1633792158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1050.28</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            405.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-09-55\n",
      "  done: false\n",
      "  episode_len_mean: 406.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3414367304907904\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015159994577862143\n",
      "          policy_loss: -0.030672432233889897\n",
      "          total_loss: -0.05375654813316133\n",
      "          vf_explained_var: 0.07882977277040482\n",
      "          vf_loss: 0.00028287810241130906\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.292307692307695\n",
      "    ram_util_percent: 71.44230769230771\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036599811767052425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.741291039412715\n",
      "    mean_inference_ms: 1.7086635726483224\n",
      "    mean_raw_obs_processing_ms: 1.237668829122291\n",
      "  time_since_restore: 1086.9735119342804\n",
      "  time_this_iter_s: 36.691261291503906\n",
      "  time_total_s: 1086.9735119342804\n",
      "  timers:\n",
      "    learn_throughput: 1567.455\n",
      "    learn_time_ms: 637.977\n",
      "    load_throughput: 58431.871\n",
      "    load_time_ms: 17.114\n",
      "    sample_throughput: 49.519\n",
      "    sample_time_ms: 20194.471\n",
      "    update_time_ms: 2.177\n",
      "  timestamp: 1633792195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1086.97</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            406.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 406.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 122\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4732459439171683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012000583473084988\n",
      "          policy_loss: -0.04025476121654113\n",
      "          total_loss: -0.06471049425502619\n",
      "          vf_explained_var: -0.12225598096847534\n",
      "          vf_loss: 0.0002392240292263321\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.49999999999999\n",
      "    ram_util_percent: 71.43928571428573\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036601095161837176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.645495905979384\n",
      "    mean_inference_ms: 1.7087303388136694\n",
      "    mean_raw_obs_processing_ms: 1.2652709719173887\n",
      "  time_since_restore: 1106.280281305313\n",
      "  time_this_iter_s: 19.306769371032715\n",
      "  time_total_s: 1106.280281305313\n",
      "  timers:\n",
      "    learn_throughput: 1569.063\n",
      "    learn_time_ms: 637.323\n",
      "    load_throughput: 54885.115\n",
      "    load_time_ms: 18.22\n",
      "    sample_throughput: 49.009\n",
      "    sample_time_ms: 20404.308\n",
      "    update_time_ms: 2.172\n",
      "  timestamp: 1633792214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1106.28</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            406.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 407.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 124\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.548745033476088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011176474038571834\n",
      "          policy_loss: -0.06997227987481489\n",
      "          total_loss: -0.09518907190197044\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00023573140060761943\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89230769230768\n",
      "    ram_util_percent: 71.16538461538462\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660290781940818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.556422531346467\n",
      "    mean_inference_ms: 1.708814342912097\n",
      "    mean_raw_obs_processing_ms: 1.2923174926955636\n",
      "  time_since_restore: 1124.9982478618622\n",
      "  time_this_iter_s: 18.717966556549072\n",
      "  time_total_s: 1124.9982478618622\n",
      "  timers:\n",
      "    learn_throughput: 1567.757\n",
      "    learn_time_ms: 637.854\n",
      "    load_throughput: 55236.979\n",
      "    load_time_ms: 18.104\n",
      "    sample_throughput: 49.144\n",
      "    sample_time_ms: 20348.401\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1633792233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">            1125</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            407.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-10-53\n",
      "  done: false\n",
      "  episode_len_mean: 408.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 126\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.537495909796821\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010339025065230465\n",
      "          policy_loss: -0.11536694425675605\n",
      "          total_loss: -0.14054883534295692\n",
      "          vf_explained_var: -0.46721717715263367\n",
      "          vf_loss: 0.00016075499297585338\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.30714285714286\n",
      "    ram_util_percent: 70.74285714285712\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660503049435228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.479020902661365\n",
      "    mean_inference_ms: 1.7089156787389508\n",
      "    mean_raw_obs_processing_ms: 1.318878687588251\n",
      "  time_since_restore: 1144.3116261959076\n",
      "  time_this_iter_s: 19.31337833404541\n",
      "  time_total_s: 1144.3116261959076\n",
      "  timers:\n",
      "    learn_throughput: 1567.904\n",
      "    learn_time_ms: 637.794\n",
      "    load_throughput: 54764.006\n",
      "    load_time_ms: 18.26\n",
      "    sample_throughput: 49.212\n",
      "    sample_time_ms: 20320.043\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1633792253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1144.31</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            408.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 409.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 129\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.568503173192342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008678344828305734\n",
      "          policy_loss: -0.05346477698120806\n",
      "          total_loss: -0.07894170992076396\n",
      "          vf_explained_var: -0.6834637522697449\n",
      "          vf_loss: 0.00018097769162624092\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48620689655173\n",
      "    ram_util_percent: 70.53448275862068\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03660875625686795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.38378130333862\n",
      "    mean_inference_ms: 1.7091242560752382\n",
      "    mean_raw_obs_processing_ms: 1.3442331530808005\n",
      "  time_since_restore: 1164.5949878692627\n",
      "  time_this_iter_s: 20.283361673355103\n",
      "  time_total_s: 1164.5949878692627\n",
      "  timers:\n",
      "    learn_throughput: 1569.557\n",
      "    learn_time_ms: 637.122\n",
      "    load_throughput: 54837.396\n",
      "    load_time_ms: 18.236\n",
      "    sample_throughput: 48.839\n",
      "    sample_time_ms: 20475.472\n",
      "    update_time_ms: 2.176\n",
      "  timestamp: 1633792273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1164.59</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            409.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 409.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 131\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5898052242067124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011046766479427008\n",
      "          policy_loss: -0.01696582076450189\n",
      "          total_loss: -0.04254196733236313\n",
      "          vf_explained_var: -0.9628962874412537\n",
      "          vf_loss: 0.00028738560075806973\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.590322580645164\n",
      "    ram_util_percent: 70.49677419354839\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036610663041294594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.32636569416973\n",
      "    mean_inference_ms: 1.7092535680533205\n",
      "    mean_raw_obs_processing_ms: 1.3422422335406992\n",
      "  time_since_restore: 1186.1931102275848\n",
      "  time_this_iter_s: 21.598122358322144\n",
      "  time_total_s: 1186.1931102275848\n",
      "  timers:\n",
      "    learn_throughput: 1571.258\n",
      "    learn_time_ms: 636.433\n",
      "    load_throughput: 54719.423\n",
      "    load_time_ms: 18.275\n",
      "    sample_throughput: 48.425\n",
      "    sample_time_ms: 20650.525\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633792294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1186.19</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            409.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 409.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 134\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.565952028168572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013012808667153344\n",
      "          policy_loss: -0.0630023223761883\n",
      "          total_loss: -0.08847338776621554\n",
      "          vf_explained_var: -0.8353697061538696\n",
      "          vf_loss: 0.0001477896134828269\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68064516129032\n",
      "    ram_util_percent: 70.51612903225805\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03661224238644342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.248399760384846\n",
      "    mean_inference_ms: 1.7094085537504689\n",
      "    mean_raw_obs_processing_ms: 1.3414731895864198\n",
      "  time_since_restore: 1208.205328464508\n",
      "  time_this_iter_s: 22.012218236923218\n",
      "  time_total_s: 1208.205328464508\n",
      "  timers:\n",
      "    learn_throughput: 1574.006\n",
      "    learn_time_ms: 635.322\n",
      "    load_throughput: 54596.201\n",
      "    load_time_ms: 18.316\n",
      "    sample_throughput: 48.04\n",
      "    sample_time_ms: 20816.124\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1633792316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1208.21</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            409.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 410.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 136\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.568463924196031\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008118602245237158\n",
      "          policy_loss: -0.04163514744076464\n",
      "          total_loss: -0.06713986458877723\n",
      "          vf_explained_var: -0.7187998294830322\n",
      "          vf_loss: 0.00015455209375229767\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.50967741935484\n",
      "    ram_util_percent: 70.65161290322582\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036612780223725154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.197828520902068\n",
      "    mean_inference_ms: 1.70948490169571\n",
      "    mean_raw_obs_processing_ms: 1.3422039917790196\n",
      "  time_since_restore: 1229.9206433296204\n",
      "  time_this_iter_s: 21.715314865112305\n",
      "  time_total_s: 1229.9206433296204\n",
      "  timers:\n",
      "    learn_throughput: 1577.992\n",
      "    learn_time_ms: 633.717\n",
      "    load_throughput: 54509.921\n",
      "    load_time_ms: 18.345\n",
      "    sample_throughput: 47.363\n",
      "    sample_time_ms: 21113.723\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633792338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1229.92</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            410.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-12-41\n",
      "  done: false\n",
      "  episode_len_mean: 411.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 139\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.44428149594201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011814540112029783\n",
      "          policy_loss: -0.07932253811094496\n",
      "          total_loss: -0.10358499478962686\n",
      "          vf_explained_var: -0.8732729554176331\n",
      "          vf_loss: 0.00014343679524447199\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.571875\n",
      "    ram_util_percent: 70.75625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036613612440101174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.12426612059306\n",
      "    mean_inference_ms: 1.7095877793280168\n",
      "    mean_raw_obs_processing_ms: 1.34420343168514\n",
      "  time_since_restore: 1252.430599451065\n",
      "  time_this_iter_s: 22.509956121444702\n",
      "  time_total_s: 1252.430599451065\n",
      "  timers:\n",
      "    learn_throughput: 1575.883\n",
      "    learn_time_ms: 634.565\n",
      "    load_throughput: 53599.0\n",
      "    load_time_ms: 18.657\n",
      "    sample_throughput: 46.676\n",
      "    sample_time_ms: 21424.13\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633792361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1252.43</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             411.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 411.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 141\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.450995087623596\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013789148584357038\n",
      "          policy_loss: -0.033108876335124174\n",
      "          total_loss: -0.057358519256942804\n",
      "          vf_explained_var: -0.9963732957839966\n",
      "          vf_loss: 0.00021721665883281578\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46333333333334\n",
      "    ram_util_percent: 70.90000000000002\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03661460537282371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.077159283434245\n",
      "    mean_inference_ms: 1.7096416434882427\n",
      "    mean_raw_obs_processing_ms: 1.3469221056575447\n",
      "  time_since_restore: 1273.485461473465\n",
      "  time_this_iter_s: 21.054862022399902\n",
      "  time_total_s: 1273.485461473465\n",
      "  timers:\n",
      "    learn_throughput: 1575.577\n",
      "    learn_time_ms: 634.688\n",
      "    load_throughput: 53535.718\n",
      "    load_time_ms: 18.679\n",
      "    sample_throughput: 46.166\n",
      "    sample_time_ms: 21661.131\n",
      "    update_time_ms: 2.32\n",
      "  timestamp: 1633792382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1273.49</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            411.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 413.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 144\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.440985533926222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014690368664478207\n",
      "          policy_loss: 0.03679350276167194\n",
      "          total_loss: 0.012578236766987376\n",
      "          vf_explained_var: 0.12073888629674911\n",
      "          vf_loss: 0.00014868082410733526\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42258064516128\n",
      "    ram_util_percent: 70.9193548387097\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036615809430072116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.007868106151616\n",
      "    mean_inference_ms: 1.7097143218808142\n",
      "    mean_raw_obs_processing_ms: 1.3515743322446965\n",
      "  time_since_restore: 1294.685786485672\n",
      "  time_this_iter_s: 21.20032501220703\n",
      "  time_total_s: 1294.685786485672\n",
      "  timers:\n",
      "    learn_throughput: 1578.055\n",
      "    learn_time_ms: 633.692\n",
      "    load_throughput: 52864.136\n",
      "    load_time_ms: 18.916\n",
      "    sample_throughput: 49.72\n",
      "    sample_time_ms: 20112.815\n",
      "    update_time_ms: 2.317\n",
      "  timestamp: 1633792403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1294.69</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            413.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 413.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.05\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 147\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4882962067921954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00976045560474071\n",
      "          policy_loss: -0.06795769615305794\n",
      "          total_loss: -0.09262587701280912\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00018427841261857086\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.665625\n",
      "    ram_util_percent: 70.99375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03661711689052418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.941790512159226\n",
      "    mean_inference_ms: 1.709783781457091\n",
      "    mean_raw_obs_processing_ms: 1.357315093386245\n",
      "  time_since_restore: 1317.0494871139526\n",
      "  time_this_iter_s: 22.36370062828064\n",
      "  time_total_s: 1317.0494871139526\n",
      "  timers:\n",
      "    learn_throughput: 1576.309\n",
      "    learn_time_ms: 634.393\n",
      "    load_throughput: 52961.393\n",
      "    load_time_ms: 18.882\n",
      "    sample_throughput: 48.977\n",
      "    sample_time_ms: 20417.867\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1633792425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1317.05</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">   -0.05</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            413.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 413.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 149\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.445694571071201\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012380136403269364\n",
      "          policy_loss: -0.022762408687008753\n",
      "          total_loss: -0.04701974878294601\n",
      "          vf_explained_var: -0.6646129488945007\n",
      "          vf_loss: 0.00016092160335069315\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.766666666666666\n",
      "    ram_util_percent: 71.11818181818181\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03661818629262055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.90055953835026\n",
      "    mean_inference_ms: 1.7098307188326844\n",
      "    mean_raw_obs_processing_ms: 1.3620516914417338\n",
      "  time_since_restore: 1340.5778224468231\n",
      "  time_this_iter_s: 23.528335332870483\n",
      "  time_total_s: 1340.5778224468231\n",
      "  timers:\n",
      "    learn_throughput: 1577.282\n",
      "    learn_time_ms: 634.002\n",
      "    load_throughput: 52499.806\n",
      "    load_time_ms: 19.048\n",
      "    sample_throughput: 47.849\n",
      "    sample_time_ms: 20899.072\n",
      "    update_time_ms: 2.314\n",
      "  timestamp: 1633792449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1340.58</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            413.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-14-48\n",
      "  done: false\n",
      "  episode_len_mean: 413.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 152\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.476668259832594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009733152845673863\n",
      "          policy_loss: 0.05123751349747181\n",
      "          total_loss: 0.02658142652362585\n",
      "          vf_explained_var: -0.7113504409790039\n",
      "          vf_loss: 8.017727060279058e-05\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.762499999999996\n",
      "    ram_util_percent: 71.28928571428571\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03661985829041121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.840119034747477\n",
      "    mean_inference_ms: 1.7098943862946976\n",
      "    mean_raw_obs_processing_ms: 1.3775343548404553\n",
      "  time_since_restore: 1379.6130394935608\n",
      "  time_this_iter_s: 39.03521704673767\n",
      "  time_total_s: 1379.6130394935608\n",
      "  timers:\n",
      "    learn_throughput: 1576.795\n",
      "    learn_time_ms: 634.198\n",
      "    load_throughput: 52718.621\n",
      "    load_time_ms: 18.969\n",
      "    sample_throughput: 43.723\n",
      "    sample_time_ms: 22871.148\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1633792488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1379.61</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            413.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-15-09\n",
      "  done: false\n",
      "  episode_len_mean: 413.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 155\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4018918063905503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011113185592119863\n",
      "          policy_loss: 0.019222861611180836\n",
      "          total_loss: -0.00461765651901563\n",
      "          vf_explained_var: -0.9632623195648193\n",
      "          vf_loss: 0.00014367224175657612\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60666666666667\n",
      "    ram_util_percent: 71.42000000000002\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036621543319320715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78145789371851\n",
      "    mean_inference_ms: 1.709953744995406\n",
      "    mean_raw_obs_processing_ms: 1.3935690234615172\n",
      "  time_since_restore: 1400.6372730731964\n",
      "  time_this_iter_s: 21.02423357963562\n",
      "  time_total_s: 1400.6372730731964\n",
      "  timers:\n",
      "    learn_throughput: 1574.385\n",
      "    learn_time_ms: 635.169\n",
      "    load_throughput: 52783.771\n",
      "    load_time_ms: 18.945\n",
      "    sample_throughput: 43.584\n",
      "    sample_time_ms: 22944.37\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1633792509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1400.64</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            413.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-15-30\n",
      "  done: false\n",
      "  episode_len_mean: 414.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 157\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3947931740019057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007799091856864098\n",
      "          policy_loss: -0.025943030334181254\n",
      "          total_loss: -0.04977190949850612\n",
      "          vf_explained_var: -0.25405943393707275\n",
      "          vf_loss: 9.468218793497524e-05\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.793103448275865\n",
      "    ram_util_percent: 71.03103448275864\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662271706556139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.743574962548145\n",
      "    mean_inference_ms: 1.7099916446469046\n",
      "    mean_raw_obs_processing_ms: 1.404455909377465\n",
      "  time_since_restore: 1421.302302122116\n",
      "  time_this_iter_s: 20.665029048919678\n",
      "  time_total_s: 1421.302302122116\n",
      "  timers:\n",
      "    learn_throughput: 1573.605\n",
      "    learn_time_ms: 635.484\n",
      "    load_throughput: 53197.774\n",
      "    load_time_ms: 18.798\n",
      "    sample_throughput: 43.762\n",
      "    sample_time_ms: 22850.888\n",
      "    update_time_ms: 2.289\n",
      "  timestamp: 1633792530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          1421.3</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            414.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 415.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 160\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.356024185816447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010662043698883478\n",
      "          policy_loss: -0.16776170945829816\n",
      "          total_loss: -0.19111866686079237\n",
      "          vf_explained_var: -0.9624335169792175\n",
      "          vf_loss: 0.00016996428716487975\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58333333333334\n",
      "    ram_util_percent: 70.87000000000003\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662424597850075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.68722922966946\n",
      "    mean_inference_ms: 1.7100388469471275\n",
      "    mean_raw_obs_processing_ms: 1.4072575309366762\n",
      "  time_since_restore: 1442.0858581066132\n",
      "  time_this_iter_s: 20.78355598449707\n",
      "  time_total_s: 1442.0858581066132\n",
      "  timers:\n",
      "    learn_throughput: 1572.183\n",
      "    learn_time_ms: 636.058\n",
      "    load_throughput: 53471.494\n",
      "    load_time_ms: 18.702\n",
      "    sample_throughput: 43.999\n",
      "    sample_time_ms: 22727.544\n",
      "    update_time_ms: 2.281\n",
      "  timestamp: 1633792551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1442.09</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            415.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 416.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 162\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.373092034127977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010722877058513101\n",
      "          policy_loss: -0.052918620738718246\n",
      "          total_loss: -0.07646425397445758\n",
      "          vf_explained_var: -0.7299266457557678\n",
      "          vf_loss: 0.00015177706102096838\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59666666666667\n",
      "    ram_util_percent: 70.86000000000003\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662505085601869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.649908194227557\n",
      "    mean_inference_ms: 1.7100621772010804\n",
      "    mean_raw_obs_processing_ms: 1.4048108665864298\n",
      "  time_since_restore: 1462.8008453845978\n",
      "  time_this_iter_s: 20.71498727798462\n",
      "  time_total_s: 1462.8008453845978\n",
      "  timers:\n",
      "    learn_throughput: 1570.804\n",
      "    learn_time_ms: 636.617\n",
      "    load_throughput: 53664.286\n",
      "    load_time_ms: 18.634\n",
      "    sample_throughput: 44.195\n",
      "    sample_time_ms: 22627.043\n",
      "    update_time_ms: 2.276\n",
      "  timestamp: 1633792571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">          1462.8</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-16-31\n",
      "  done: false\n",
      "  episode_len_mean: 416.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 164\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3141723367902967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010417282812420374\n",
      "          policy_loss: -0.05574496856166257\n",
      "          total_loss: -0.07869509605483876\n",
      "          vf_explained_var: -0.9849169254302979\n",
      "          vf_loss: 0.00015904367816498658\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58214285714286\n",
      "    ram_util_percent: 70.89285714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662582441048481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.61271060999856\n",
      "    mean_inference_ms: 1.7100800571074444\n",
      "    mean_raw_obs_processing_ms: 1.402551333699005\n",
      "  time_since_restore: 1482.3216662406921\n",
      "  time_this_iter_s: 19.52082085609436\n",
      "  time_total_s: 1482.3216662406921\n",
      "  timers:\n",
      "    learn_throughput: 1571.313\n",
      "    learn_time_ms: 636.41\n",
      "    load_throughput: 54395.327\n",
      "    load_time_ms: 18.384\n",
      "    sample_throughput: 44.786\n",
      "    sample_time_ms: 22328.597\n",
      "    update_time_ms: 2.262\n",
      "  timestamp: 1633792591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1482.32</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             416.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 417.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 167\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2617956585354273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010373712015154999\n",
      "          policy_loss: -0.011618935420281357\n",
      "          total_loss: -0.033861441537737846\n",
      "          vf_explained_var: -0.35834434628486633\n",
      "          vf_loss: 0.00034303156583822175\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.74615384615384\n",
      "    ram_util_percent: 70.94615384615386\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662728865482272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.558504123972945\n",
      "    mean_inference_ms: 1.710105768362637\n",
      "    mean_raw_obs_processing_ms: 1.4007271146601576\n",
      "  time_since_restore: 1500.4866976737976\n",
      "  time_this_iter_s: 18.16503143310547\n",
      "  time_total_s: 1500.4866976737976\n",
      "  timers:\n",
      "    learn_throughput: 1573.347\n",
      "    learn_time_ms: 635.588\n",
      "    load_throughput: 54483.368\n",
      "    load_time_ms: 18.354\n",
      "    sample_throughput: 45.371\n",
      "    sample_time_ms: 22040.694\n",
      "    update_time_ms: 2.012\n",
      "  timestamp: 1633792609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1500.49</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            417.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 418.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 169\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.350656252437168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009669149609951688\n",
      "          policy_loss: -0.00442705609732204\n",
      "          total_loss: -0.027732924703094693\n",
      "          vf_explained_var: -0.8002883791923523\n",
      "          vf_loss: 0.00017047646577136928\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.95\n",
      "    ram_util_percent: 71.03846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662826199891964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.522305450779335\n",
      "    mean_inference_ms: 1.7101206804317692\n",
      "    mean_raw_obs_processing_ms: 1.399797135249916\n",
      "  time_since_restore: 1519.1712272167206\n",
      "  time_this_iter_s: 18.684529542922974\n",
      "  time_total_s: 1519.1712272167206\n",
      "  timers:\n",
      "    learn_throughput: 1573.405\n",
      "    learn_time_ms: 635.564\n",
      "    load_throughput: 55024.874\n",
      "    load_time_ms: 18.174\n",
      "    sample_throughput: 45.894\n",
      "    sample_time_ms: 21789.316\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1633792628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1519.17</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            418.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 419.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 172\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.322822682062785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01229618591567632\n",
      "          policy_loss: 0.0028223560088210635\n",
      "          total_loss: -0.020190064857403437\n",
      "          vf_explained_var: -0.15828213095664978\n",
      "          vf_loss: 0.00017737925073662254\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.33103448275861\n",
      "    ram_util_percent: 71.06896551724137\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662968096473235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.471665261170866\n",
      "    mean_inference_ms: 1.71014283702652\n",
      "    mean_raw_obs_processing_ms: 1.3997079442606348\n",
      "  time_since_restore: 1539.0600621700287\n",
      "  time_this_iter_s: 19.888834953308105\n",
      "  time_total_s: 1539.0600621700287\n",
      "  timers:\n",
      "    learn_throughput: 1575.122\n",
      "    learn_time_ms: 634.872\n",
      "    load_throughput: 54763.148\n",
      "    load_time_ms: 18.26\n",
      "    sample_throughput: 46.42\n",
      "    sample_time_ms: 21542.425\n",
      "    update_time_ms: 2.01\n",
      "  timestamp: 1633792648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1539.06</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 419.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 174\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2676279253429836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008206836681420013\n",
      "          policy_loss: -0.1172503982981046\n",
      "          total_loss: -0.1396757670574718\n",
      "          vf_explained_var: -0.4591027498245239\n",
      "          vf_loss: 0.0002252635423954214\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84615384615384\n",
      "    ram_util_percent: 71.11153846153846\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036630558582903275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.438266887988583\n",
      "    mean_inference_ms: 1.710155941124571\n",
      "    mean_raw_obs_processing_ms: 1.3998621018537716\n",
      "  time_since_restore: 1557.6537647247314\n",
      "  time_this_iter_s: 18.59370255470276\n",
      "  time_total_s: 1557.6537647247314\n",
      "  timers:\n",
      "    learn_throughput: 1575.134\n",
      "    learn_time_ms: 634.867\n",
      "    load_throughput: 55157.729\n",
      "    load_time_ms: 18.13\n",
      "    sample_throughput: 47.508\n",
      "    sample_time_ms: 21049.166\n",
      "    update_time_ms: 2.005\n",
      "  timestamp: 1633792666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1557.65</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-18-06\n",
      "  done: false\n",
      "  episode_len_mean: 419.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 176\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2047211541069878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010229904727048912\n",
      "          policy_loss: 0.0034676404462920295\n",
      "          total_loss: -0.018393161023656526\n",
      "          vf_explained_var: -0.5247876644134521\n",
      "          vf_loss: 0.00015444278566671224\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.47857142857142\n",
      "    ram_util_percent: 71.19642857142858\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663141948263944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.407260265507134\n",
      "    mean_inference_ms: 1.7101731260601154\n",
      "    mean_raw_obs_processing_ms: 1.4004888823462875\n",
      "  time_since_restore: 1577.2326924800873\n",
      "  time_this_iter_s: 19.578927755355835\n",
      "  time_total_s: 1577.2326924800873\n",
      "  timers:\n",
      "    learn_throughput: 1574.997\n",
      "    learn_time_ms: 634.922\n",
      "    load_throughput: 54804.507\n",
      "    load_time_ms: 18.247\n",
      "    sample_throughput: 52.347\n",
      "    sample_time_ms: 19103.335\n",
      "    update_time_ms: 2.012\n",
      "  timestamp: 1633792686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1577.23</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 419.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 178\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3613383611043295\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013229968307831068\n",
      "          policy_loss: -0.0364862362957663\n",
      "          total_loss: -0.059746835670537415\n",
      "          vf_explained_var: 0.20639580488204956\n",
      "          vf_loss: 0.0003114413454669476\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.78846153846155\n",
      "    ram_util_percent: 71.31923076923077\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036632197843011534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.37776321020131\n",
      "    mean_inference_ms: 1.710189954371579\n",
      "    mean_raw_obs_processing_ms: 1.4015494655013387\n",
      "  time_since_restore: 1595.1115791797638\n",
      "  time_this_iter_s: 17.878886699676514\n",
      "  time_total_s: 1595.1115791797638\n",
      "  timers:\n",
      "    learn_throughput: 1572.851\n",
      "    learn_time_ms: 635.788\n",
      "    load_throughput: 55076.897\n",
      "    load_time_ms: 18.156\n",
      "    sample_throughput: 53.225\n",
      "    sample_time_ms: 18787.99\n",
      "    update_time_ms: 2.02\n",
      "  timestamp: 1633792704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1595.11</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 419.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 181\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.227650725841522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013172459274642535\n",
      "          policy_loss: 0.018484296484125987\n",
      "          total_loss: -0.0033384895159138572\n",
      "          vf_explained_var: -0.4088258445262909\n",
      "          vf_loss: 0.0004125569647941221\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.739583333333336\n",
      "    ram_util_percent: 71.37083333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663326301866489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.33604974741873\n",
      "    mean_inference_ms: 1.71021879107254\n",
      "    mean_raw_obs_processing_ms: 1.41070996338959\n",
      "  time_since_restore: 1629.175330877304\n",
      "  time_this_iter_s: 34.06375169754028\n",
      "  time_total_s: 1629.175330877304\n",
      "  timers:\n",
      "    learn_throughput: 1570.637\n",
      "    learn_time_ms: 636.684\n",
      "    load_throughput: 54974.821\n",
      "    load_time_ms: 18.19\n",
      "    sample_throughput: 49.685\n",
      "    sample_time_ms: 20126.91\n",
      "    update_time_ms: 2.016\n",
      "  timestamp: 1633792738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1629.18</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 419.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 183\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.222700990570916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010573073047468403\n",
      "          policy_loss: -0.03350950446393755\n",
      "          total_loss: -0.055550455550352734\n",
      "          vf_explained_var: -0.3208636939525604\n",
      "          vf_loss: 0.00015301865730887383\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.06923076923078\n",
      "    ram_util_percent: 71.23846153846154\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036633955739161436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.309376255422357\n",
      "    mean_inference_ms: 1.7102345087849584\n",
      "    mean_raw_obs_processing_ms: 1.4171036505751688\n",
      "  time_since_restore: 1647.422967672348\n",
      "  time_this_iter_s: 18.247636795043945\n",
      "  time_total_s: 1647.422967672348\n",
      "  timers:\n",
      "    learn_throughput: 1569.758\n",
      "    learn_time_ms: 637.041\n",
      "    load_throughput: 54665.864\n",
      "    load_time_ms: 18.293\n",
      "    sample_throughput: 50.32\n",
      "    sample_time_ms: 19872.853\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633792756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1647.42</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-19-34\n",
      "  done: false\n",
      "  episode_len_mean: 420.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 185\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.068968031141493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01313095026674826\n",
      "          policy_loss: -0.05126176240543524\n",
      "          total_loss: -0.0717457238998678\n",
      "          vf_explained_var: -0.9452893733978271\n",
      "          vf_loss: 0.0001646859015535382\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46153846153846\n",
      "    ram_util_percent: 70.91538461538462\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036634632246678955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.28465628956451\n",
      "    mean_inference_ms: 1.7102562818348874\n",
      "    mean_raw_obs_processing_ms: 1.4237570855724857\n",
      "  time_since_restore: 1665.372895002365\n",
      "  time_this_iter_s: 17.94992733001709\n",
      "  time_total_s: 1665.372895002365\n",
      "  timers:\n",
      "    learn_throughput: 1569.337\n",
      "    learn_time_ms: 637.212\n",
      "    load_throughput: 54841.626\n",
      "    load_time_ms: 18.234\n",
      "    sample_throughput: 51.03\n",
      "    sample_time_ms: 19596.227\n",
      "    update_time_ms: 2.046\n",
      "  timestamp: 1633792774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1665.37</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            420.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 421.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 187\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2922282377878824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007853721615853735\n",
      "          policy_loss: -0.1362877584165997\n",
      "          total_loss: -0.15896299655238788\n",
      "          vf_explained_var: -0.5141886472702026\n",
      "          vf_loss: 0.00022250121967065043\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.75769230769231\n",
      "    ram_util_percent: 70.58076923076922\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663515313643745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.260709673214343\n",
      "    mean_inference_ms: 1.7102766933779836\n",
      "    mean_raw_obs_processing_ms: 1.4306185962254534\n",
      "  time_since_restore: 1683.240478515625\n",
      "  time_this_iter_s: 17.867583513259888\n",
      "  time_total_s: 1683.240478515625\n",
      "  timers:\n",
      "    learn_throughput: 1568.642\n",
      "    learn_time_ms: 637.494\n",
      "    load_throughput: 54572.049\n",
      "    load_time_ms: 18.324\n",
      "    sample_throughput: 51.465\n",
      "    sample_time_ms: 19430.521\n",
      "    update_time_ms: 2.049\n",
      "  timestamp: 1633792792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1683.24</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            421.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 421.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 190\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1543350325690374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012730479458931162\n",
      "          policy_loss: -0.020438182312581273\n",
      "          total_loss: -0.041578258035911454\n",
      "          vf_explained_var: -0.19125837087631226\n",
      "          vf_loss: 0.0003634898181189783\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86428571428571\n",
      "    ram_util_percent: 70.45357142857144\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663584379934269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.22655994113888\n",
      "    mean_inference_ms: 1.7103112000463694\n",
      "    mean_raw_obs_processing_ms: 1.4316696554876824\n",
      "  time_since_restore: 1703.0884628295898\n",
      "  time_this_iter_s: 19.847984313964844\n",
      "  time_total_s: 1703.0884628295898\n",
      "  timers:\n",
      "    learn_throughput: 1568.477\n",
      "    learn_time_ms: 637.561\n",
      "    load_throughput: 54335.923\n",
      "    load_time_ms: 18.404\n",
      "    sample_throughput: 51.024\n",
      "    sample_time_ms: 19598.684\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633792812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1703.09</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            421.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 420.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 192\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.144740276866489\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012368283452986075\n",
      "          policy_loss: 0.08496271040704516\n",
      "          total_loss: 0.06399087210496267\n",
      "          vf_explained_var: 0.06855780631303787\n",
      "          vf_loss: 0.0004369158144982066\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93333333333334\n",
      "    ram_util_percent: 70.47916666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036636032033557704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.205922316385095\n",
      "    mean_inference_ms: 1.7103335440236622\n",
      "    mean_raw_obs_processing_ms: 1.429893283096266\n",
      "  time_since_restore: 1720.272227525711\n",
      "  time_this_iter_s: 17.183764696121216\n",
      "  time_total_s: 1720.272227525711\n",
      "  timers:\n",
      "    learn_throughput: 1567.426\n",
      "    learn_time_ms: 637.989\n",
      "    load_throughput: 57647.799\n",
      "    load_time_ms: 17.347\n",
      "    sample_throughput: 51.416\n",
      "    sample_time_ms: 19449.245\n",
      "    update_time_ms: 2.043\n",
      "  timestamp: 1633792829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1720.27</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            420.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 419.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 195\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.131298944685194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009044370943518985\n",
      "          policy_loss: -0.017834035803874333\n",
      "          total_loss: -0.038912065492735966\n",
      "          vf_explained_var: -0.39087867736816406\n",
      "          vf_loss: 0.00020669826578038433\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6551724137931\n",
      "    ram_util_percent: 70.50689655172413\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036636337204674976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.17976906989115\n",
      "    mean_inference_ms: 1.7103795301864664\n",
      "    mean_raw_obs_processing_ms: 1.4278196333003856\n",
      "  time_since_restore: 1740.4996047019958\n",
      "  time_this_iter_s: 20.22737717628479\n",
      "  time_total_s: 1740.4996047019958\n",
      "  timers:\n",
      "    learn_throughput: 1567.187\n",
      "    learn_time_ms: 638.086\n",
      "    load_throughput: 56795.985\n",
      "    load_time_ms: 17.607\n",
      "    sample_throughput: 51.328\n",
      "    sample_time_ms: 19482.724\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1633792849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">          1740.5</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 416.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 198\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.092074059115516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011408008858639088\n",
      "          policy_loss: -0.009047204587194655\n",
      "          total_loss: -0.02970229900545544\n",
      "          vf_explained_var: -0.6451937556266785\n",
      "          vf_loss: 0.0002299951481594083\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.74285714285714\n",
      "    ram_util_percent: 70.66071428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663668578420013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.1583871770853\n",
      "    mean_inference_ms: 1.7104354207667891\n",
      "    mean_raw_obs_processing_ms: 1.4266611090562125\n",
      "  time_since_restore: 1760.1781160831451\n",
      "  time_this_iter_s: 19.678511381149292\n",
      "  time_total_s: 1760.1781160831451\n",
      "  timers:\n",
      "    learn_throughput: 1564.531\n",
      "    learn_time_ms: 639.169\n",
      "    load_throughput: 56984.032\n",
      "    load_time_ms: 17.549\n",
      "    sample_throughput: 51.046\n",
      "    sample_time_ms: 19590.18\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633792869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1760.18</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 416.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 200\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.20894040134218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012000200045933933\n",
      "          policy_loss: -0.054567246552970676\n",
      "          total_loss: -0.07632506241401037\n",
      "          vf_explained_var: -0.6225420832633972\n",
      "          vf_loss: 0.00029408343155713134\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61428571428571\n",
      "    ram_util_percent: 70.75\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663699951630039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.145619538966745\n",
      "    mean_inference_ms: 1.710473692751092\n",
      "    mean_raw_obs_processing_ms: 1.4263308450413357\n",
      "  time_since_restore: 1779.4497878551483\n",
      "  time_this_iter_s: 19.271671772003174\n",
      "  time_total_s: 1779.4497878551483\n",
      "  timers:\n",
      "    learn_throughput: 1564.817\n",
      "    learn_time_ms: 639.052\n",
      "    load_throughput: 57383.586\n",
      "    load_time_ms: 17.427\n",
      "    sample_throughput: 51.126\n",
      "    sample_time_ms: 19559.68\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1633792888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1779.45</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-21-49\n",
      "  done: false\n",
      "  episode_len_mean: 412.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 203\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.145531921916538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01408960051274768\n",
      "          policy_loss: -0.0038984853360388015\n",
      "          total_loss: -0.0248043538381656\n",
      "          vf_explained_var: -0.12973123788833618\n",
      "          vf_loss: 0.000505420951715981\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82333333333334\n",
      "    ram_util_percent: 70.83333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663767944097573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.128103326862885\n",
      "    mean_inference_ms: 1.710531432970802\n",
      "    mean_raw_obs_processing_ms: 1.4264499297225512\n",
      "  time_since_restore: 1800.631222486496\n",
      "  time_this_iter_s: 21.181434631347656\n",
      "  time_total_s: 1800.631222486496\n",
      "  timers:\n",
      "    learn_throughput: 1567.096\n",
      "    learn_time_ms: 638.123\n",
      "    load_throughput: 57089.439\n",
      "    load_time_ms: 17.516\n",
      "    sample_throughput: 50.275\n",
      "    sample_time_ms: 19890.783\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633792909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1800.63</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             412.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 410.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 206\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1604588508605955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01253829248973309\n",
      "          policy_loss: -0.02995159129301707\n",
      "          total_loss: -0.05132724651032024\n",
      "          vf_explained_var: -0.4248601794242859\n",
      "          vf_loss: 0.00018975006185226247\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84516129032258\n",
      "    ram_util_percent: 70.9225806451613\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663824201101929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.11301491125511\n",
      "    mean_inference_ms: 1.7105898539808442\n",
      "    mean_raw_obs_processing_ms: 1.4272648221624669\n",
      "  time_since_restore: 1822.4007675647736\n",
      "  time_this_iter_s: 21.769545078277588\n",
      "  time_total_s: 1822.4007675647736\n",
      "  timers:\n",
      "    learn_throughput: 1569.804\n",
      "    learn_time_ms: 637.022\n",
      "    load_throughput: 56371.341\n",
      "    load_time_ms: 17.74\n",
      "    sample_throughput: 53.584\n",
      "    sample_time_ms: 18662.269\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633792931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">          1822.4</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            410.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 407.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 209\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.085181172688802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014703799319222425\n",
      "          policy_loss: -0.022553058423929743\n",
      "          total_loss: -0.04317752064930068\n",
      "          vf_explained_var: -0.9895347356796265\n",
      "          vf_loss: 0.00018139938264438469\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.669696969696965\n",
      "    ram_util_percent: 71.04848484848485\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663872109444229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.099742170877725\n",
      "    mean_inference_ms: 1.7106416662483355\n",
      "    mean_raw_obs_processing_ms: 1.4287309750442778\n",
      "  time_since_restore: 1845.4748482704163\n",
      "  time_this_iter_s: 23.0740807056427\n",
      "  time_total_s: 1845.4748482704163\n",
      "  timers:\n",
      "    learn_throughput: 1572.911\n",
      "    learn_time_ms: 635.764\n",
      "    load_throughput: 57352.121\n",
      "    load_time_ms: 17.436\n",
      "    sample_throughput: 52.229\n",
      "    sample_time_ms: 19146.503\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633792954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1845.47</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             407.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 405.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 212\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0610658393965826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014617790289179035\n",
      "          policy_loss: -0.04428210761398077\n",
      "          total_loss: -0.06463632869223754\n",
      "          vf_explained_var: -0.5778048038482666\n",
      "          vf_loss: 0.0002107569845166937\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.04074074074074\n",
      "    ram_util_percent: 71.19074074074076\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663896509488979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.087858936594987\n",
      "    mean_inference_ms: 1.7106839907345575\n",
      "    mean_raw_obs_processing_ms: 1.4366257934092834\n",
      "  time_since_restore: 1883.2083113193512\n",
      "  time_this_iter_s: 37.73346304893494\n",
      "  time_total_s: 1883.2083113193512\n",
      "  timers:\n",
      "    learn_throughput: 1566.278\n",
      "    learn_time_ms: 638.456\n",
      "    load_throughput: 56933.291\n",
      "    load_time_ms: 17.564\n",
      "    sample_throughput: 47.344\n",
      "    sample_time_ms: 21122.04\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1633792992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1883.21</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            405.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 403.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 214\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1642228921254474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012809769908622126\n",
      "          policy_loss: -0.029669082464857235\n",
      "          total_loss: -0.05111959754592842\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001516846006981925\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.17931034482758\n",
      "    ram_util_percent: 71.34482758620689\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663916411430599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08049320703539\n",
      "    mean_inference_ms: 1.7107124564898768\n",
      "    mean_raw_obs_processing_ms: 1.4419652932040736\n",
      "  time_since_restore: 1903.3405902385712\n",
      "  time_this_iter_s: 20.13227891921997\n",
      "  time_total_s: 1903.3405902385712\n",
      "  timers:\n",
      "    learn_throughput: 1569.792\n",
      "    learn_time_ms: 637.027\n",
      "    load_throughput: 56732.606\n",
      "    load_time_ms: 17.627\n",
      "    sample_throughput: 46.839\n",
      "    sample_time_ms: 21349.89\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633793012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1903.34</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            403.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 217\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1976837317148843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014452591019874422\n",
      "          policy_loss: -0.026298945893843968\n",
      "          total_loss: -0.04806816904909081\n",
      "          vf_explained_var: -0.8758071660995483\n",
      "          vf_loss: 0.00016244910447211523\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65\n",
      "    ram_util_percent: 71.04666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663969689938179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.070945738546705\n",
      "    mean_inference_ms: 1.7107548205349932\n",
      "    mean_raw_obs_processing_ms: 1.450328236765833\n",
      "  time_since_restore: 1924.2054479122162\n",
      "  time_this_iter_s: 20.86485767364502\n",
      "  time_total_s: 1924.2054479122162\n",
      "  timers:\n",
      "    learn_throughput: 1572.36\n",
      "    learn_time_ms: 635.987\n",
      "    load_throughput: 56847.561\n",
      "    load_time_ms: 17.591\n",
      "    sample_throughput: 46.614\n",
      "    sample_time_ms: 21452.666\n",
      "    update_time_ms: 2.036\n",
      "  timestamp: 1633793033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1924.21</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-24-13\n",
      "  done: false\n",
      "  episode_len_mean: 399.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 220\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3218068467246162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010264455288937905\n",
      "          policy_loss: -0.04211341780092981\n",
      "          total_loss: -0.06512165856030253\n",
      "          vf_explained_var: -0.24384267628192902\n",
      "          vf_loss: 0.0001777518809022796\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.939285714285724\n",
      "    ram_util_percent: 70.875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036640217623521425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.062451077122294\n",
      "    mean_inference_ms: 1.7107931010429513\n",
      "    mean_raw_obs_processing_ms: 1.4485582279533926\n",
      "  time_since_restore: 1943.9766826629639\n",
      "  time_this_iter_s: 19.77123475074768\n",
      "  time_total_s: 1943.9766826629639\n",
      "  timers:\n",
      "    learn_throughput: 1573.781\n",
      "    learn_time_ms: 635.412\n",
      "    load_throughput: 54143.456\n",
      "    load_time_ms: 18.469\n",
      "    sample_throughput: 46.059\n",
      "    sample_time_ms: 21711.098\n",
      "    update_time_ms: 2.041\n",
      "  timestamp: 1633793053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1943.98</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-24-32\n",
      "  done: false\n",
      "  episode_len_mean: 398.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 222\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.132879991001553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013087787573906147\n",
      "          policy_loss: -0.06141128871175978\n",
      "          total_loss: -0.0825905793863866\n",
      "          vf_explained_var: -0.7009994983673096\n",
      "          vf_loss: 0.00010860855505193791\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.814814814814824\n",
      "    ram_util_percent: 70.84444444444445\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036640501841571284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.057068309681497\n",
      "    mean_inference_ms: 1.7108152922407407\n",
      "    mean_raw_obs_processing_ms: 1.4476354521857915\n",
      "  time_since_restore: 1963.0542907714844\n",
      "  time_this_iter_s: 19.077608108520508\n",
      "  time_total_s: 1963.0542907714844\n",
      "  timers:\n",
      "    learn_throughput: 1572.875\n",
      "    learn_time_ms: 635.779\n",
      "    load_throughput: 54869.319\n",
      "    load_time_ms: 18.225\n",
      "    sample_throughput: 46.305\n",
      "    sample_time_ms: 21595.984\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1633793072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1963.05</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 397.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 224\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2208752420213487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009187790080201274\n",
      "          policy_loss: -0.06493970654490921\n",
      "          total_loss: -0.08706755585347613\n",
      "          vf_explained_var: -0.16950510442256927\n",
      "          vf_loss: 5.2189372945576905e-05\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65199999999999\n",
      "    ram_util_percent: 70.884\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664076291048915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.05172748290646\n",
      "    mean_inference_ms: 1.7108372797965763\n",
      "    mean_raw_obs_processing_ms: 1.4469621678053175\n",
      "  time_since_restore: 1980.0901367664337\n",
      "  time_this_iter_s: 17.03584599494934\n",
      "  time_total_s: 1980.0901367664337\n",
      "  timers:\n",
      "    learn_throughput: 1576.539\n",
      "    learn_time_ms: 634.301\n",
      "    load_throughput: 57677.844\n",
      "    load_time_ms: 17.338\n",
      "    sample_throughput: 46.873\n",
      "    sample_time_ms: 21334.084\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633793089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1980.09</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            397.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 393.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 228\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0539027015368143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012483861289335095\n",
      "          policy_loss: -0.07421425059437751\n",
      "          total_loss: -0.09460934851732519\n",
      "          vf_explained_var: -0.9015882015228271\n",
      "          vf_loss: 0.00010491509771478983\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.714285714285715\n",
      "    ram_util_percent: 70.93142857142858\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664112343112382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.044043510816355\n",
      "    mean_inference_ms: 1.7108802502268747\n",
      "    mean_raw_obs_processing_ms: 1.446586809285165\n",
      "  time_since_restore: 2004.628322839737\n",
      "  time_this_iter_s: 24.538186073303223\n",
      "  time_total_s: 2004.628322839737\n",
      "  timers:\n",
      "    learn_throughput: 1577.546\n",
      "    learn_time_ms: 633.896\n",
      "    load_throughput: 57869.55\n",
      "    load_time_ms: 17.28\n",
      "    sample_throughput: 45.743\n",
      "    sample_time_ms: 21861.203\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633793114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         2004.63</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 392.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 231\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.036001083585951\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01327695459893437\n",
      "          policy_loss: -0.08798767721487416\n",
      "          total_loss: -0.10813127648499277\n",
      "          vf_explained_var: -0.7907484173774719\n",
      "          vf_loss: 0.00017491960938463712\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.790625\n",
      "    ram_util_percent: 71.05\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664140767308858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.038835836081503\n",
      "    mean_inference_ms: 1.710913138409018\n",
      "    mean_raw_obs_processing_ms: 1.4467357464963606\n",
      "  time_since_restore: 2027.5466136932373\n",
      "  time_this_iter_s: 22.918290853500366\n",
      "  time_total_s: 2027.5466136932373\n",
      "  timers:\n",
      "    learn_throughput: 1577.304\n",
      "    learn_time_ms: 633.993\n",
      "    load_throughput: 57820.887\n",
      "    load_time_ms: 17.295\n",
      "    sample_throughput: 45.383\n",
      "    sample_time_ms: 22034.797\n",
      "    update_time_ms: 2.042\n",
      "  timestamp: 1633793137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2027.55</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 390.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 234\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.018157302008735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012414521139848908\n",
      "          policy_loss: -0.1238494448363781\n",
      "          total_loss: -0.14390124074286884\n",
      "          vf_explained_var: -0.9863114356994629\n",
      "          vf_loss: 9.098194221021712e-05\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.765714285714274\n",
      "    ram_util_percent: 71.11142857142856\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664188226644204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.03396154761607\n",
      "    mean_inference_ms: 1.7109461929606817\n",
      "    mean_raw_obs_processing_ms: 1.447401959930842\n",
      "  time_since_restore: 2051.886631011963\n",
      "  time_this_iter_s: 24.340017318725586\n",
      "  time_total_s: 2051.886631011963\n",
      "  timers:\n",
      "    learn_throughput: 1576.228\n",
      "    learn_time_ms: 634.426\n",
      "    load_throughput: 58466.733\n",
      "    load_time_ms: 17.104\n",
      "    sample_throughput: 44.86\n",
      "    sample_time_ms: 22291.603\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1633793161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2051.89</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 389.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 237\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9552664372656081\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012794139624027916\n",
      "          policy_loss: -0.0011359384076462852\n",
      "          total_loss: -0.020543079761167368\n",
      "          vf_explained_var: -0.7407287359237671\n",
      "          vf_loss: 0.00010554151663705448\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8774193548387\n",
      "    ram_util_percent: 71.25161290322582\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664230339928873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.028555348043568\n",
      "    mean_inference_ms: 1.7109743932617705\n",
      "    mean_raw_obs_processing_ms: 1.4485475627054032\n",
      "  time_since_restore: 2073.4196212291718\n",
      "  time_this_iter_s: 21.532990217208862\n",
      "  time_total_s: 2073.4196212291718\n",
      "  timers:\n",
      "    learn_throughput: 1574.486\n",
      "    learn_time_ms: 635.128\n",
      "    load_throughput: 57175.823\n",
      "    load_time_ms: 17.49\n",
      "    sample_throughput: 45.174\n",
      "    sample_time_ms: 22136.395\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1633793182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2073.42</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               389</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 388.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 239\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7679659022225274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008365173872079475\n",
      "          policy_loss: -0.08689721963471836\n",
      "          total_loss: -0.10439469309316741\n",
      "          vf_explained_var: -0.6693298816680908\n",
      "          vf_loss: 0.0001560434637617113\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57999999999999\n",
      "    ram_util_percent: 71.30333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036642599113824746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.024658718222643\n",
      "    mean_inference_ms: 1.7109925933136418\n",
      "    mean_raw_obs_processing_ms: 1.4493173012660534\n",
      "  time_since_restore: 2094.4716489315033\n",
      "  time_this_iter_s: 21.052027702331543\n",
      "  time_total_s: 2094.4716489315033\n",
      "  timers:\n",
      "    learn_throughput: 1579.956\n",
      "    learn_time_ms: 632.929\n",
      "    load_throughput: 57376.364\n",
      "    load_time_ms: 17.429\n",
      "    sample_throughput: 48.851\n",
      "    sample_time_ms: 20470.516\n",
      "    update_time_ms: 2.035\n",
      "  timestamp: 1633793204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2094.47</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            388.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 387.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 242\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6911805550257364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010995666854934426\n",
      "          policy_loss: -0.05204412074138721\n",
      "          total_loss: -0.06883679913977782\n",
      "          vf_explained_var: -0.8292525410652161\n",
      "          vf_loss: 8.476491863499784e-05\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.85849056603774\n",
      "    ram_util_percent: 71.34339622641512\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664295628469841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.01847299299897\n",
      "    mean_inference_ms: 1.7110113850312942\n",
      "    mean_raw_obs_processing_ms: 1.456286476139191\n",
      "  time_since_restore: 2131.9762239456177\n",
      "  time_this_iter_s: 37.50457501411438\n",
      "  time_total_s: 2131.9762239456177\n",
      "  timers:\n",
      "    learn_throughput: 1581.036\n",
      "    learn_time_ms: 632.497\n",
      "    load_throughput: 57300.409\n",
      "    load_time_ms: 17.452\n",
      "    sample_throughput: 45.029\n",
      "    sample_time_ms: 22208.123\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633793241\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2131.98</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 386.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 245\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8293482489056057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019676955611824712\n",
      "          policy_loss: 0.006517901395757993\n",
      "          total_loss: -0.011538899482952223\n",
      "          vf_explained_var: -0.19867034256458282\n",
      "          vf_loss: 0.0001751914404546066\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.609677419354846\n",
      "    ram_util_percent: 71.12903225806453\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664314770278248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.012198202280263\n",
      "    mean_inference_ms: 1.7110241736809315\n",
      "    mean_raw_obs_processing_ms: 1.4634062603982658\n",
      "  time_since_restore: 2153.528077840805\n",
      "  time_this_iter_s: 21.551853895187378\n",
      "  time_total_s: 2153.528077840805\n",
      "  timers:\n",
      "    learn_throughput: 1579.511\n",
      "    learn_time_ms: 633.107\n",
      "    load_throughput: 57321.709\n",
      "    load_time_ms: 17.445\n",
      "    sample_throughput: 44.891\n",
      "    sample_time_ms: 22276.201\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633793263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2153.53</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 386.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 248\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8327135192023383\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012965373910576124\n",
      "          policy_loss: -0.03325154644747575\n",
      "          total_loss: -0.0514050196028418\n",
      "          vf_explained_var: -0.7593660354614258\n",
      "          vf_loss: 0.00013314343377714976\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76129032258065\n",
      "    ram_util_percent: 70.69032258064513\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664312921108245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.00531558979365\n",
      "    mean_inference_ms: 1.7110310352785583\n",
      "    mean_raw_obs_processing_ms: 1.4706656095720536\n",
      "  time_since_restore: 2175.2388005256653\n",
      "  time_this_iter_s: 21.71072268486023\n",
      "  time_total_s: 2175.2388005256653\n",
      "  timers:\n",
      "    learn_throughput: 1580.652\n",
      "    learn_time_ms: 632.65\n",
      "    load_throughput: 57113.305\n",
      "    load_time_ms: 17.509\n",
      "    sample_throughput: 44.503\n",
      "    sample_time_ms: 22470.534\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1633793284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         2175.24</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-28-27\n",
      "  done: false\n",
      "  episode_len_mean: 386.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 250\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5878170715437996\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008914101773062594\n",
      "          policy_loss: -0.002964423348506292\n",
      "          total_loss: -0.018678380880090925\n",
      "          vf_explained_var: -0.8305172324180603\n",
      "          vf_loss: 0.00013635670184157788\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7969696969697\n",
      "    ram_util_percent: 70.54545454545455\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036642992758451835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.000375291592572\n",
      "    mean_inference_ms: 1.711031172512859\n",
      "    mean_raw_obs_processing_ms: 1.4729614279177685\n",
      "  time_since_restore: 2198.188763141632\n",
      "  time_this_iter_s: 22.949962615966797\n",
      "  time_total_s: 2198.188763141632\n",
      "  timers:\n",
      "    learn_throughput: 1579.824\n",
      "    learn_time_ms: 632.982\n",
      "    load_throughput: 57328.996\n",
      "    load_time_ms: 17.443\n",
      "    sample_throughput: 43.749\n",
      "    sample_time_ms: 22857.531\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633793307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2198.19</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             386.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 384.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 254\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6444857319196065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012037227804348167\n",
      "          policy_loss: 0.003391495512591468\n",
      "          total_loss: -0.012842449214723375\n",
      "          vf_explained_var: -0.11352115124464035\n",
      "          vf_loss: 0.0001732926424463383\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71212121212121\n",
      "    ram_util_percent: 70.5\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664259047491246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.990955996746024\n",
      "    mean_inference_ms: 1.711032314759496\n",
      "    mean_raw_obs_processing_ms: 1.4724298322037515\n",
      "  time_since_restore: 2221.3127171993256\n",
      "  time_this_iter_s: 23.12395405769348\n",
      "  time_total_s: 2221.3127171993256\n",
      "  timers:\n",
      "    learn_throughput: 1578.316\n",
      "    learn_time_ms: 633.587\n",
      "    load_throughput: 54470.985\n",
      "    load_time_ms: 18.358\n",
      "    sample_throughput: 42.617\n",
      "    sample_time_ms: 23464.822\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633793330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2221.31</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-29-14\n",
      "  done: false\n",
      "  episode_len_mean: 381.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 257\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7386402593718635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009906910731072966\n",
      "          policy_loss: -0.07387184573130476\n",
      "          total_loss: -0.09116117652091715\n",
      "          vf_explained_var: -0.37203165888786316\n",
      "          vf_loss: 6.611256057011713e-05\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56764705882353\n",
      "    ram_util_percent: 70.62647058823528\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664212099188955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.98482351668433\n",
      "    mean_inference_ms: 1.711037151646295\n",
      "    mean_raw_obs_processing_ms: 1.4723787401609925\n",
      "  time_since_restore: 2245.014448404312\n",
      "  time_this_iter_s: 23.701731204986572\n",
      "  time_total_s: 2245.014448404312\n",
      "  timers:\n",
      "    learn_throughput: 1580.629\n",
      "    learn_time_ms: 632.66\n",
      "    load_throughput: 53765.063\n",
      "    load_time_ms: 18.599\n",
      "    sample_throughput: 42.768\n",
      "    sample_time_ms: 23381.873\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633793354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2245.01</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            381.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 380.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 259\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8421046296755472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008907931771490555\n",
      "          policy_loss: -0.046502353913254205\n",
      "          total_loss: -0.06464895498421457\n",
      "          vf_explained_var: 0.12953370809555054\n",
      "          vf_loss: 0.0002466090466542583\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.628125\n",
      "    ram_util_percent: 70.728125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036641755128427975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.98115397899838\n",
      "    mean_inference_ms: 1.7110421741536421\n",
      "    mean_raw_obs_processing_ms: 1.4726187213162065\n",
      "  time_since_restore: 2267.5130050182343\n",
      "  time_this_iter_s: 22.49855661392212\n",
      "  time_total_s: 2267.5130050182343\n",
      "  timers:\n",
      "    learn_throughput: 1578.57\n",
      "    learn_time_ms: 633.485\n",
      "    load_throughput: 53672.252\n",
      "    load_time_ms: 18.632\n",
      "    sample_throughput: 42.847\n",
      "    sample_time_ms: 23339.009\n",
      "    update_time_ms: 2.101\n",
      "  timestamp: 1633793377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2267.51</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            380.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 379.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 262\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.767840900686052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009497732423488486\n",
      "          policy_loss: -0.01907120785779423\n",
      "          total_loss: -0.03662237206266986\n",
      "          vf_explained_var: -0.781218945980072\n",
      "          vf_loss: 9.7562176759109e-05\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.4774193548387\n",
      "    ram_util_percent: 70.83870967741937\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664117453262671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.975958604394247\n",
      "    mean_inference_ms: 1.7110507577570246\n",
      "    mean_raw_obs_processing_ms: 1.4729961860393659\n",
      "  time_since_restore: 2289.1271386146545\n",
      "  time_this_iter_s: 21.614133596420288\n",
      "  time_total_s: 2289.1271386146545\n",
      "  timers:\n",
      "    learn_throughput: 1577.68\n",
      "    learn_time_ms: 633.842\n",
      "    load_throughput: 53721.886\n",
      "    load_time_ms: 18.614\n",
      "    sample_throughput: 43.354\n",
      "    sample_time_ms: 23066.073\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1633793398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2289.13</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            379.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 378.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 265\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7825364258554246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012381315634496678\n",
      "          policy_loss: -0.030191282348500357\n",
      "          total_loss: -0.04785696988304456\n",
      "          vf_explained_var: -0.7201476097106934\n",
      "          vf_loss: 0.00012098655048854804\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.636666666666656\n",
      "    ram_util_percent: 70.93333333333335\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664044020210555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.971882530383283\n",
      "    mean_inference_ms: 1.711062476473302\n",
      "    mean_raw_obs_processing_ms: 1.4739711581746193\n",
      "  time_since_restore: 2310.1923828125\n",
      "  time_this_iter_s: 21.06524419784546\n",
      "  time_total_s: 2310.1923828125\n",
      "  timers:\n",
      "    learn_throughput: 1576.929\n",
      "    learn_time_ms: 634.144\n",
      "    load_throughput: 54400.266\n",
      "    load_time_ms: 18.382\n",
      "    sample_throughput: 43.442\n",
      "    sample_time_ms: 23019.222\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1633793419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2310.19</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            378.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 376.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 268\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6513494624031915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009680362485223071\n",
      "          policy_loss: -0.1530752672917313\n",
      "          total_loss: -0.16948179747495387\n",
      "          vf_explained_var: -0.5819435119628906\n",
      "          vf_loss: 7.671130948033856e-05\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63225806451613\n",
      "    ram_util_percent: 70.93548387096774\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663977959463598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.969187314311043\n",
      "    mean_inference_ms: 1.71107851441049\n",
      "    mean_raw_obs_processing_ms: 1.475142247253379\n",
      "  time_since_restore: 2331.928179502487\n",
      "  time_this_iter_s: 21.735796689987183\n",
      "  time_total_s: 2331.928179502487\n",
      "  timers:\n",
      "    learn_throughput: 1578.552\n",
      "    learn_time_ms: 633.492\n",
      "    load_throughput: 54101.552\n",
      "    load_time_ms: 18.484\n",
      "    sample_throughput: 43.312\n",
      "    sample_time_ms: 23088.131\n",
      "    update_time_ms: 2.122\n",
      "  timestamp: 1633793441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2331.93</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            376.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 375.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 270\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.850080414613088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01373861713234332\n",
      "          policy_loss: -0.014936741731233067\n",
      "          total_loss: -0.0332754915787114\n",
      "          vf_explained_var: -0.7555676102638245\n",
      "          vf_loss: 0.00011912077582868127\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.988679245283016\n",
      "    ram_util_percent: 71.0377358490566\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663931019497362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.967882218661344\n",
      "    mean_inference_ms: 1.7110910424700432\n",
      "    mean_raw_obs_processing_ms: 1.4792170312955437\n",
      "  time_since_restore: 2369.0735783576965\n",
      "  time_this_iter_s: 37.14539885520935\n",
      "  time_total_s: 2369.0735783576965\n",
      "  timers:\n",
      "    learn_throughput: 1576.385\n",
      "    learn_time_ms: 634.363\n",
      "    load_throughput: 54019.68\n",
      "    load_time_ms: 18.512\n",
      "    sample_throughput: 43.381\n",
      "    sample_time_ms: 23051.347\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633793478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2369.07</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            375.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-31-41\n",
      "  done: false\n",
      "  episode_len_mean: 374.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 273\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8536574376953974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017217361761223966\n",
      "          policy_loss: -0.023000009854634604\n",
      "          total_loss: -0.04140841778781679\n",
      "          vf_explained_var: -0.39312833547592163\n",
      "          vf_loss: 7.436117719205666e-05\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.2060606060606\n",
      "    ram_util_percent: 71.25454545454546\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036638703099592426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.967096294307797\n",
      "    mean_inference_ms: 1.711113665823448\n",
      "    mean_raw_obs_processing_ms: 1.4853650956284061\n",
      "  time_since_restore: 2392.2057299613953\n",
      "  time_this_iter_s: 23.13215160369873\n",
      "  time_total_s: 2392.2057299613953\n",
      "  timers:\n",
      "    learn_throughput: 1579.525\n",
      "    learn_time_ms: 633.102\n",
      "    load_throughput: 54111.115\n",
      "    load_time_ms: 18.48\n",
      "    sample_throughput: 43.084\n",
      "    sample_time_ms: 23210.671\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633793501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2392.21</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            374.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 372.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 276\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7910836338996887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01889405420721554\n",
      "          policy_loss: -0.09380883549650511\n",
      "          total_loss: -0.11142532399131191\n",
      "          vf_explained_var: 0.13104136288166046\n",
      "          vf_loss: 0.0002353029630386219\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68333333333332\n",
      "    ram_util_percent: 71.27333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663815904904076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.967295876924968\n",
      "    mean_inference_ms: 1.7111391457365657\n",
      "    mean_raw_obs_processing_ms: 1.4918032007686646\n",
      "  time_since_restore: 2413.1434273719788\n",
      "  time_this_iter_s: 20.937697410583496\n",
      "  time_total_s: 2413.1434273719788\n",
      "  timers:\n",
      "    learn_throughput: 1576.565\n",
      "    learn_time_ms: 634.29\n",
      "    load_throughput: 53647.744\n",
      "    load_time_ms: 18.64\n",
      "    sample_throughput: 43.23\n",
      "    sample_time_ms: 23132.017\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633793522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2413.14</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            372.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 371.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 278\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.783830592367384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013123122481440373\n",
      "          policy_loss: 0.04222708625925912\n",
      "          total_loss: 0.02459688815805647\n",
      "          vf_explained_var: 0.4861195981502533\n",
      "          vf_loss: 0.00016709815447231652\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68076923076923\n",
      "    ram_util_percent: 71.00384615384615\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663784982856835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.96788430765015\n",
      "    mean_inference_ms: 1.711156362059915\n",
      "    mean_raw_obs_processing_ms: 1.4962726023206283\n",
      "  time_since_restore: 2431.4896643161774\n",
      "  time_this_iter_s: 18.34623694419861\n",
      "  time_total_s: 2431.4896643161774\n",
      "  timers:\n",
      "    learn_throughput: 1575.933\n",
      "    learn_time_ms: 634.545\n",
      "    load_throughput: 53786.299\n",
      "    load_time_ms: 18.592\n",
      "    sample_throughput: 44.108\n",
      "    sample_time_ms: 22671.417\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633793541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2431.49</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            371.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 371.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 280\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.577880981233385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01044267798827369\n",
      "          policy_loss: -0.022050994137922924\n",
      "          total_loss: -0.03763782911830478\n",
      "          vf_explained_var: 0.3900274932384491\n",
      "          vf_loss: 0.0001593417252782577\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56206896551724\n",
      "    ram_util_percent: 70.8793103448276\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663756821137915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.969492557957345\n",
      "    mean_inference_ms: 1.7111757723088599\n",
      "    mean_raw_obs_processing_ms: 1.4961314835772237\n",
      "  time_since_restore: 2451.9779059886932\n",
      "  time_this_iter_s: 20.48824167251587\n",
      "  time_total_s: 2451.9779059886932\n",
      "  timers:\n",
      "    learn_throughput: 1578.533\n",
      "    learn_time_ms: 633.5\n",
      "    load_throughput: 53540.228\n",
      "    load_time_ms: 18.678\n",
      "    sample_throughput: 44.625\n",
      "    sample_time_ms: 22408.77\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1633793561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2451.98</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            371.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 370.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 283\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7972912020153469\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011088996919618961\n",
      "          policy_loss: -0.06277121160593298\n",
      "          total_loss: -0.08060483659307162\n",
      "          vf_explained_var: -0.32611486315727234\n",
      "          vf_loss: 0.00010463290479189406\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.387096774193544\n",
      "    ram_util_percent: 70.82903225806453\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036637100929904506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.972691309854\n",
      "    mean_inference_ms: 1.711207159598997\n",
      "    mean_raw_obs_processing_ms: 1.495926621098778\n",
      "  time_since_restore: 2473.2876510620117\n",
      "  time_this_iter_s: 21.30974507331848\n",
      "  time_total_s: 2473.2876510620117\n",
      "  timers:\n",
      "    learn_throughput: 1574.946\n",
      "    learn_time_ms: 634.943\n",
      "    load_throughput: 54304.125\n",
      "    load_time_ms: 18.415\n",
      "    sample_throughput: 45.109\n",
      "    sample_time_ms: 22168.415\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1633793583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2473.29</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             370.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 367.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 286\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.812442782190111\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012047056402498965\n",
      "          policy_loss: -0.05149554614391592\n",
      "          total_loss: -0.06939894441101287\n",
      "          vf_explained_var: -0.8611239194869995\n",
      "          vf_loss: 0.00018338238334965557\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.349999999999994\n",
      "    ram_util_percent: 70.9\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663672637275705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.977946206532383\n",
      "    mean_inference_ms: 1.7112447802352428\n",
      "    mean_raw_obs_processing_ms: 1.4962779184220787\n",
      "  time_since_restore: 2495.3913073539734\n",
      "  time_this_iter_s: 22.10365629196167\n",
      "  time_total_s: 2495.3913073539734\n",
      "  timers:\n",
      "    learn_throughput: 1577.664\n",
      "    learn_time_ms: 633.848\n",
      "    load_throughput: 53914.827\n",
      "    load_time_ms: 18.548\n",
      "    sample_throughput: 45.188\n",
      "    sample_time_ms: 22129.869\n",
      "    update_time_ms: 2.102\n",
      "  timestamp: 1633793605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2495.39</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            367.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 366.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 289\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8001066274113124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01568635908124784\n",
      "          policy_loss: -0.023022836446762084\n",
      "          total_loss: -0.04089832752943039\n",
      "          vf_explained_var: -0.35505276918411255\n",
      "          vf_loss: 7.65580587742281e-05\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46969696969697\n",
      "    ram_util_percent: 70.96969696969697\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036636421888458365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.98483959665321\n",
      "    mean_inference_ms: 1.7112862286631787\n",
      "    mean_raw_obs_processing_ms: 1.4969709036217844\n",
      "  time_since_restore: 2519.048943042755\n",
      "  time_this_iter_s: 23.65763568878174\n",
      "  time_total_s: 2519.048943042755\n",
      "  timers:\n",
      "    learn_throughput: 1578.002\n",
      "    learn_time_ms: 633.713\n",
      "    load_throughput: 53957.691\n",
      "    load_time_ms: 18.533\n",
      "    sample_throughput: 44.774\n",
      "    sample_time_ms: 22334.355\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1633793628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2519.05</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 366.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 291\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8290798664093018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012283302001311548\n",
      "          policy_loss: -0.027702542518575988\n",
      "          total_loss: -0.045839288954933485\n",
      "          vf_explained_var: -0.2868104875087738\n",
      "          vf_loss: 0.00011566814504880717\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.548387096774185\n",
      "    ram_util_percent: 71.04193548387094\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663619001814723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.990095911068302\n",
      "    mean_inference_ms: 1.7113160081649783\n",
      "    mean_raw_obs_processing_ms: 1.4974781972553892\n",
      "  time_since_restore: 2540.8531546592712\n",
      "  time_this_iter_s: 21.804211616516113\n",
      "  time_total_s: 2540.8531546592712\n",
      "  timers:\n",
      "    learn_throughput: 1576.743\n",
      "    learn_time_ms: 634.219\n",
      "    load_throughput: 53373.919\n",
      "    load_time_ms: 18.736\n",
      "    sample_throughput: 44.628\n",
      "    sample_time_ms: 22407.531\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1633793650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2540.85</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 364.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7376277168591818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013446460725021175\n",
      "          policy_loss: -0.022694475907418462\n",
      "          total_loss: -0.03989302048252689\n",
      "          vf_explained_var: -0.2901662290096283\n",
      "          vf_loss: 0.00013571128009870235\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42258064516129\n",
      "    ram_util_percent: 71.17741935483869\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663581438137101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.998862617336435\n",
      "    mean_inference_ms: 1.711364194844596\n",
      "    mean_raw_obs_processing_ms: 1.498474822383549\n",
      "  time_since_restore: 2562.3703911304474\n",
      "  time_this_iter_s: 21.517236471176147\n",
      "  time_total_s: 2562.3703911304474\n",
      "  timers:\n",
      "    learn_throughput: 1577.608\n",
      "    learn_time_ms: 633.871\n",
      "    load_throughput: 53659.412\n",
      "    load_time_ms: 18.636\n",
      "    sample_throughput: 44.67\n",
      "    sample_time_ms: 22386.14\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1633793672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         2562.37</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            364.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 364.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 296\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7116985771391127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010736354608310981\n",
      "          policy_loss: -0.033125686479939354\n",
      "          total_loss: -0.050035156806310016\n",
      "          vf_explained_var: -0.11760305613279343\n",
      "          vf_loss: 0.0001739647797270057\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.483870967741936\n",
      "    ram_util_percent: 71.21935483870969\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663551104105963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.004952143386603\n",
      "    mean_inference_ms: 1.7113968937545843\n",
      "    mean_raw_obs_processing_ms: 1.4991751708654704\n",
      "  time_since_restore: 2583.8339745998383\n",
      "  time_this_iter_s: 21.46358346939087\n",
      "  time_total_s: 2583.8339745998383\n",
      "  timers:\n",
      "    learn_throughput: 1580.21\n",
      "    learn_time_ms: 632.827\n",
      "    load_throughput: 53398.109\n",
      "    load_time_ms: 18.727\n",
      "    sample_throughput: 48.034\n",
      "    sample_time_ms: 20818.799\n",
      "    update_time_ms: 2.188\n",
      "  timestamp: 1633793693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         2583.83</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            364.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-35-16\n",
      "  done: false\n",
      "  episode_len_mean: 363.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 299\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9859497692849901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011517933201717253\n",
      "          policy_loss: 0.021807295415136548\n",
      "          total_loss: 0.0021080551462041006\n",
      "          vf_explained_var: -0.366049587726593\n",
      "          vf_loss: 0.00012426550270498005\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5\n",
      "    ram_util_percent: 71.321875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663504838334166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.014815916208843\n",
      "    mean_inference_ms: 1.7114499802692245\n",
      "    mean_raw_obs_processing_ms: 1.500288266380279\n",
      "  time_since_restore: 2606.3447852134705\n",
      "  time_this_iter_s: 22.510810613632202\n",
      "  time_total_s: 2606.3447852134705\n",
      "  timers:\n",
      "    learn_throughput: 1577.588\n",
      "    learn_time_ms: 633.879\n",
      "    load_throughput: 53915.382\n",
      "    load_time_ms: 18.548\n",
      "    sample_throughput: 48.179\n",
      "    sample_time_ms: 20755.792\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633793716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2606.34</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            363.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 363.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 302\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0555977132585315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014334023512064399\n",
      "          policy_loss: -0.05325395663579305\n",
      "          total_loss: -0.07370016343063779\n",
      "          vf_explained_var: -0.00569114601239562\n",
      "          vf_loss: 6.49747134755469e-05\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.54074074074074\n",
      "    ram_util_percent: 71.33518518518518\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036634534540852665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.024744892492066\n",
      "    mean_inference_ms: 1.7115051956586365\n",
      "    mean_raw_obs_processing_ms: 1.5059647656926496\n",
      "  time_since_restore: 2643.9289536476135\n",
      "  time_this_iter_s: 37.584168434143066\n",
      "  time_total_s: 2643.9289536476135\n",
      "  timers:\n",
      "    learn_throughput: 1577.559\n",
      "    learn_time_ms: 633.891\n",
      "    load_throughput: 54386.581\n",
      "    load_time_ms: 18.387\n",
      "    sample_throughput: 44.602\n",
      "    sample_time_ms: 22420.613\n",
      "    update_time_ms: 2.187\n",
      "  timestamp: 1633793753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         2643.93</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            363.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-36-14\n",
      "  done: false\n",
      "  episode_len_mean: 364.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 305\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8326853275299073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011523359025088503\n",
      "          policy_loss: -0.05013928305771616\n",
      "          total_loss: -0.06831182833347056\n",
      "          vf_explained_var: 0.6495673060417175\n",
      "          vf_loss: 0.0001182972627753366\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.13928571428573\n",
      "    ram_util_percent: 71.19285714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036634101406676764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.034119589099195\n",
      "    mean_inference_ms: 1.7115612906034299\n",
      "    mean_raw_obs_processing_ms: 1.5117273805699645\n",
      "  time_since_restore: 2664.1029880046844\n",
      "  time_this_iter_s: 20.174034357070923\n",
      "  time_total_s: 2664.1029880046844\n",
      "  timers:\n",
      "    learn_throughput: 1579.44\n",
      "    learn_time_ms: 633.136\n",
      "    load_throughput: 54161.774\n",
      "    load_time_ms: 18.463\n",
      "    sample_throughput: 44.24\n",
      "    sample_time_ms: 22604.099\n",
      "    update_time_ms: 2.181\n",
      "  timestamp: 1633793774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          2664.1</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            364.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-36-32\n",
      "  done: false\n",
      "  episode_len_mean: 365.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 307\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8204902092615762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011032090435762025\n",
      "          policy_loss: -0.06634370336929957\n",
      "          total_loss: -0.08433025843567318\n",
      "          vf_explained_var: -0.2554301917552948\n",
      "          vf_loss: 0.00018387056164404688\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7962962962963\n",
      "    ram_util_percent: 71.12962962962962\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663381213054685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.039628939115527\n",
      "    mean_inference_ms: 1.7115969998388685\n",
      "    mean_raw_obs_processing_ms: 1.5155714722200702\n",
      "  time_since_restore: 2682.8671905994415\n",
      "  time_this_iter_s: 18.76420259475708\n",
      "  time_total_s: 2682.8671905994415\n",
      "  timers:\n",
      "    learn_throughput: 1575.703\n",
      "    learn_time_ms: 634.637\n",
      "    load_throughput: 54075.117\n",
      "    load_time_ms: 18.493\n",
      "    sample_throughput: 44.583\n",
      "    sample_time_ms: 22430.219\n",
      "    update_time_ms: 2.152\n",
      "  timestamp: 1633793792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         2682.87</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-36-52\n",
      "  done: false\n",
      "  episode_len_mean: 367.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 309\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.865172513326009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014239083378372493\n",
      "          policy_loss: -0.04283749444617165\n",
      "          total_loss: -0.061368013421694435\n",
      "          vf_explained_var: -0.3394540846347809\n",
      "          vf_loss: 7.670775790352814e-05\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5448275862069\n",
      "    ram_util_percent: 71.01034482758621\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663342710260419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.044714417222384\n",
      "    mean_inference_ms: 1.711632233892239\n",
      "    mean_raw_obs_processing_ms: 1.5193251197367863\n",
      "  time_since_restore: 2702.895772457123\n",
      "  time_this_iter_s: 20.028581857681274\n",
      "  time_total_s: 2702.895772457123\n",
      "  timers:\n",
      "    learn_throughput: 1577.102\n",
      "    learn_time_ms: 634.074\n",
      "    load_throughput: 53857.505\n",
      "    load_time_ms: 18.568\n",
      "    sample_throughput: 44.838\n",
      "    sample_time_ms: 22302.588\n",
      "    update_time_ms: 2.148\n",
      "  timestamp: 1633793812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">          2702.9</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             367.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 367.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 312\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8787393556700813\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017748501943292656\n",
      "          policy_loss: -0.0848937615338299\n",
      "          total_loss: -0.10352958809170458\n",
      "          vf_explained_var: 0.26031503081321716\n",
      "          vf_loss: 9.610301264425894e-05\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55517241379311\n",
      "    ram_util_percent: 70.98965517241379\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663288398192004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.052016439123257\n",
      "    mean_inference_ms: 1.7116853546622466\n",
      "    mean_raw_obs_processing_ms: 1.5191938270395808\n",
      "  time_since_restore: 2722.9803109169006\n",
      "  time_this_iter_s: 20.084538459777832\n",
      "  time_total_s: 2722.9803109169006\n",
      "  timers:\n",
      "    learn_throughput: 1574.394\n",
      "    learn_time_ms: 635.165\n",
      "    load_throughput: 54279.037\n",
      "    load_time_ms: 18.423\n",
      "    sample_throughput: 45.249\n",
      "    sample_time_ms: 22099.749\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1633793833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         2722.98</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             367.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-37-31\n",
      "  done: false\n",
      "  episode_len_mean: 369.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 314\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8407539168993632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009673379771754673\n",
      "          policy_loss: -0.008780675753951073\n",
      "          total_loss: -0.027023128461506632\n",
      "          vf_explained_var: -0.9223127365112305\n",
      "          vf_loss: 0.00013485642634299843\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.47692307692308\n",
      "    ram_util_percent: 70.99230769230769\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366325042725899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.05666578844595\n",
      "    mean_inference_ms: 1.7117197112328768\n",
      "    mean_raw_obs_processing_ms: 1.5192036125429849\n",
      "  time_since_restore: 2741.8484902381897\n",
      "  time_this_iter_s: 18.868179321289062\n",
      "  time_total_s: 2741.8484902381897\n",
      "  timers:\n",
      "    learn_throughput: 1576.217\n",
      "    learn_time_ms: 634.431\n",
      "    load_throughput: 54223.882\n",
      "    load_time_ms: 18.442\n",
      "    sample_throughput: 46.25\n",
      "    sample_time_ms: 21621.515\n",
      "    update_time_ms: 2.133\n",
      "  timestamp: 1633793851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         2741.85</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            369.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-37-52\n",
      "  done: false\n",
      "  episode_len_mean: 370.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 317\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8460662802060446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01257858032751339\n",
      "          policy_loss: -0.05717751387920644\n",
      "          total_loss: -0.07553661006192366\n",
      "          vf_explained_var: -0.5585795640945435\n",
      "          vf_loss: 6.225654619306119e-05\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58620689655173\n",
      "    ram_util_percent: 71.08620689655172\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663186444579201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.063380068279535\n",
      "    mean_inference_ms: 1.7117707649342124\n",
      "    mean_raw_obs_processing_ms: 1.5193569613047608\n",
      "  time_since_restore: 2762.0195693969727\n",
      "  time_this_iter_s: 20.17107915878296\n",
      "  time_total_s: 2762.0195693969727\n",
      "  timers:\n",
      "    learn_throughput: 1577.968\n",
      "    learn_time_ms: 633.726\n",
      "    load_throughput: 54549.905\n",
      "    load_time_ms: 18.332\n",
      "    sample_throughput: 46.6\n",
      "    sample_time_ms: 21459.018\n",
      "    update_time_ms: 2.134\n",
      "  timestamp: 1633793872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         2762.02</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            370.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 370.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 319\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7865031878153483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007803438262245364\n",
      "          policy_loss: -0.0356166075501177\n",
      "          total_loss: -0.05338488680620988\n",
      "          vf_explained_var: -0.6565423607826233\n",
      "          vf_loss: 7.23670399464835e-05\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.17333333333333\n",
      "    ram_util_percent: 71.47999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663146015036107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.068011609461642\n",
      "    mean_inference_ms: 1.7118057899996024\n",
      "    mean_raw_obs_processing_ms: 1.5195415385207418\n",
      "  time_since_restore: 2782.6673414707184\n",
      "  time_this_iter_s: 20.647772073745728\n",
      "  time_total_s: 2782.6673414707184\n",
      "  timers:\n",
      "    learn_throughput: 1574.31\n",
      "    learn_time_ms: 635.199\n",
      "    load_throughput: 54376.287\n",
      "    load_time_ms: 18.39\n",
      "    sample_throughput: 46.793\n",
      "    sample_time_ms: 21370.54\n",
      "    update_time_ms: 2.129\n",
      "  timestamp: 1633793892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         2782.67</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            370.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 372.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 322\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7721057150099013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012674869804865372\n",
      "          policy_loss: -0.024156102538108827\n",
      "          total_loss: -0.04176635301361482\n",
      "          vf_explained_var: -0.7801542282104492\n",
      "          vf_loss: 7.119434940250358e-05\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61600000000001\n",
      "    ram_util_percent: 71.49600000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663084403531914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.07455125620714\n",
      "    mean_inference_ms: 1.7118573647687745\n",
      "    mean_raw_obs_processing_ms: 1.5198084378914336\n",
      "  time_since_restore: 2800.378059387207\n",
      "  time_this_iter_s: 17.710717916488647\n",
      "  time_total_s: 2800.378059387207\n",
      "  timers:\n",
      "    learn_throughput: 1571.384\n",
      "    learn_time_ms: 636.382\n",
      "    load_throughput: 55002.36\n",
      "    load_time_ms: 18.181\n",
      "    sample_throughput: 47.632\n",
      "    sample_time_ms: 20994.374\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1633793910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         2800.38</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            372.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-38-48\n",
      "  done: false\n",
      "  episode_len_mean: 373.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 324\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.706884549723731\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01377384018160917\n",
      "          policy_loss: -0.018559640376932092\n",
      "          total_loss: -0.03548823700596889\n",
      "          vf_explained_var: -0.40142303705215454\n",
      "          vf_loss: 9.720278810871403e-05\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.44230769230769\n",
      "    ram_util_percent: 71.45384615384616\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663040185100883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.079340946885498\n",
      "    mean_inference_ms: 1.7118942904619177\n",
      "    mean_raw_obs_processing_ms: 1.5201737135304927\n",
      "  time_since_restore: 2818.384789943695\n",
      "  time_this_iter_s: 18.006730556488037\n",
      "  time_total_s: 2818.384789943695\n",
      "  timers:\n",
      "    learn_throughput: 1571.547\n",
      "    learn_time_ms: 636.316\n",
      "    load_throughput: 54367.97\n",
      "    load_time_ms: 18.393\n",
      "    sample_throughput: 48.676\n",
      "    sample_time_ms: 20543.811\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633793928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         2818.38</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            373.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-39-06\n",
      "  done: false\n",
      "  episode_len_mean: 375.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 326\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.701506245136261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006229889589380989\n",
      "          policy_loss: -0.12651148330834178\n",
      "          total_loss: -0.14347387908233536\n",
      "          vf_explained_var: -0.7449097633361816\n",
      "          vf_loss: 3.319905988771805e-05\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.576\n",
      "    ram_util_percent: 71.424\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366299435303491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.082821359127976\n",
      "    mean_inference_ms: 1.71192753786099\n",
      "    mean_raw_obs_processing_ms: 1.5206071262752214\n",
      "  time_since_restore: 2836.265661239624\n",
      "  time_this_iter_s: 17.880871295928955\n",
      "  time_total_s: 2836.265661239624\n",
      "  timers:\n",
      "    learn_throughput: 1573.841\n",
      "    learn_time_ms: 635.388\n",
      "    load_throughput: 54315.588\n",
      "    load_time_ms: 18.411\n",
      "    sample_throughput: 53.838\n",
      "    sample_time_ms: 18574.389\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1633793946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         2836.27</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            375.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-39-23\n",
      "  done: false\n",
      "  episode_len_mean: 379.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 328\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.758418122927348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01919576378608914\n",
      "          policy_loss: -0.037031456993685825\n",
      "          total_loss: -0.05436470707257589\n",
      "          vf_explained_var: -0.19999630749225616\n",
      "          vf_loss: 0.00019094325669155094\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.47083333333333\n",
      "    ram_util_percent: 71.47916666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662944192605368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.085686096162334\n",
      "    mean_inference_ms: 1.7119611018542895\n",
      "    mean_raw_obs_processing_ms: 1.5208338545225366\n",
      "  time_since_restore: 2852.901185274124\n",
      "  time_this_iter_s: 16.635524034500122\n",
      "  time_total_s: 2852.901185274124\n",
      "  timers:\n",
      "    learn_throughput: 1572.662\n",
      "    learn_time_ms: 635.865\n",
      "    load_throughput: 57911.499\n",
      "    load_time_ms: 17.268\n",
      "    sample_throughput: 54.881\n",
      "    sample_time_ms: 18221.196\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633793963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">          2852.9</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            379.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 382.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 330\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6920328974723815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009369207974555508\n",
      "          policy_loss: -0.04999643042683601\n",
      "          total_loss: -0.06686906011568175\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 1.8420632871614848e-05\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.26938775510204\n",
      "    ram_util_percent: 71.49795918367347\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036629014347446774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08753846576795\n",
      "    mean_inference_ms: 1.711993112965799\n",
      "    mean_raw_obs_processing_ms: 1.523731455656819\n",
      "  time_since_restore: 2887.299010515213\n",
      "  time_this_iter_s: 34.39782524108887\n",
      "  time_total_s: 2887.299010515213\n",
      "  timers:\n",
      "    learn_throughput: 1573.441\n",
      "    learn_time_ms: 635.55\n",
      "    load_throughput: 58044.456\n",
      "    load_time_ms: 17.228\n",
      "    sample_throughput: 50.544\n",
      "    sample_time_ms: 19784.882\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633793997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">          2887.3</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            382.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 383.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 332\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8284900890456306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013465660202957015\n",
      "          policy_loss: -0.09154598149988387\n",
      "          total_loss: -0.10974542639321751\n",
      "          vf_explained_var: -0.34725236892700195\n",
      "          vf_loss: 4.337211398200856e-05\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.31153846153846\n",
      "    ram_util_percent: 71.33846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036628636194897826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08863030932181\n",
      "    mean_inference_ms: 1.7120248922369108\n",
      "    mean_raw_obs_processing_ms: 1.5265447871977402\n",
      "  time_since_restore: 2905.546457529068\n",
      "  time_this_iter_s: 18.24744701385498\n",
      "  time_total_s: 2905.546457529068\n",
      "  timers:\n",
      "    learn_throughput: 1573.142\n",
      "    learn_time_ms: 635.671\n",
      "    load_throughput: 57701.411\n",
      "    load_time_ms: 17.331\n",
      "    sample_throughput: 51.003\n",
      "    sample_time_ms: 19606.522\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633794015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         2905.55</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            383.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-40-34\n",
      "  done: false\n",
      "  episode_len_mean: 387.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 335\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8153907537460328\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01255238447409472\n",
      "          policy_loss: -0.08129978084729778\n",
      "          total_loss: -0.09931124887532658\n",
      "          vf_explained_var: -0.41688263416290283\n",
      "          vf_loss: 0.00010321313770368255\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.596296296296295\n",
      "    ram_util_percent: 71.19259259259259\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662807608804268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.089112903782393\n",
      "    mean_inference_ms: 1.7120701795230897\n",
      "    mean_raw_obs_processing_ms: 1.5307830260495996\n",
      "  time_since_restore: 2924.168366909027\n",
      "  time_this_iter_s: 18.621909379959106\n",
      "  time_total_s: 2924.168366909027\n",
      "  timers:\n",
      "    learn_throughput: 1573.871\n",
      "    learn_time_ms: 635.376\n",
      "    load_throughput: 58281.501\n",
      "    load_time_ms: 17.158\n",
      "    sample_throughput: 51.386\n",
      "    sample_time_ms: 19460.714\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633794034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         2924.17</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-40-51\n",
      "  done: false\n",
      "  episode_len_mean: 389.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 337\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6173755168914794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007534533749591274\n",
      "          policy_loss: -0.01598330649236838\n",
      "          total_loss: -0.03211005094150702\n",
      "          vf_explained_var: -0.7716098427772522\n",
      "          vf_loss: 2.3465727594561437e-05\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73333333333334\n",
      "    ram_util_percent: 71.04166666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662768388596408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.088760508205105\n",
      "    mean_inference_ms: 1.7120992408303488\n",
      "    mean_raw_obs_processing_ms: 1.5335723324659534\n",
      "  time_since_restore: 2941.0897946357727\n",
      "  time_this_iter_s: 16.921427726745605\n",
      "  time_total_s: 2941.0897946357727\n",
      "  timers:\n",
      "    learn_throughput: 1575.074\n",
      "    learn_time_ms: 634.891\n",
      "    load_throughput: 62335.649\n",
      "    load_time_ms: 16.042\n",
      "    sample_throughput: 51.9\n",
      "    sample_time_ms: 19267.675\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633794051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         2941.09</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            389.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-41-09\n",
      "  done: false\n",
      "  episode_len_mean: 391.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 339\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5414422154426575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008661707204541595\n",
      "          policy_loss: 0.031900685611698364\n",
      "          total_loss: 0.01654376449684302\n",
      "          vf_explained_var: -0.2890508472919464\n",
      "          vf_loss: 3.0432890768376333e-05\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.526923076923076\n",
      "    ram_util_percent: 70.97307692307693\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662724661061457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.087930800607065\n",
      "    mean_inference_ms: 1.7121259398395086\n",
      "    mean_raw_obs_processing_ms: 1.5364179677245218\n",
      "  time_since_restore: 2959.095991373062\n",
      "  time_this_iter_s: 18.00619673728943\n",
      "  time_total_s: 2959.095991373062\n",
      "  timers:\n",
      "    learn_throughput: 1573.865\n",
      "    learn_time_ms: 635.379\n",
      "    load_throughput: 62082.561\n",
      "    load_time_ms: 16.108\n",
      "    sample_throughput: 52.492\n",
      "    sample_time_ms: 19050.625\n",
      "    update_time_ms: 2.07\n",
      "  timestamp: 1633794069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">          2959.1</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-41-25\n",
      "  done: false\n",
      "  episode_len_mean: 394.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 341\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5865398526191712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012245398290807922\n",
      "          policy_loss: -0.020347752918799717\n",
      "          total_loss: -0.03612953548630079\n",
      "          vf_explained_var: -0.9949418902397156\n",
      "          vf_loss: 4.534708417243868e-05\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57272727272727\n",
      "    ram_util_percent: 70.95454545454548\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036626767056555154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08647369523864\n",
      "    mean_inference_ms: 1.71214982739134\n",
      "    mean_raw_obs_processing_ms: 1.535735948688208\n",
      "  time_since_restore: 2974.722900867462\n",
      "  time_this_iter_s: 15.626909494400024\n",
      "  time_total_s: 2974.722900867462\n",
      "  timers:\n",
      "    learn_throughput: 1577.966\n",
      "    learn_time_ms: 633.727\n",
      "    load_throughput: 68709.408\n",
      "    load_time_ms: 14.554\n",
      "    sample_throughput: 53.903\n",
      "    sample_time_ms: 18551.759\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1633794085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         2974.72</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            394.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 397.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 343\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4914623022079467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011774320318205255\n",
      "          policy_loss: -0.05299000889062881\n",
      "          total_loss: -0.06784188449382782\n",
      "          vf_explained_var: -0.595230758190155\n",
      "          vf_loss: 2.5955356381422865e-05\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71363636363637\n",
      "    ram_util_percent: 70.98181818181818\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662624275371974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.084146667745603\n",
      "    mean_inference_ms: 1.7121712581961719\n",
      "    mean_raw_obs_processing_ms: 1.5349919460762687\n",
      "  time_since_restore: 2989.8758158683777\n",
      "  time_this_iter_s: 15.152915000915527\n",
      "  time_total_s: 2989.8758158683777\n",
      "  timers:\n",
      "    learn_throughput: 1578.928\n",
      "    learn_time_ms: 633.341\n",
      "    load_throughput: 76220.577\n",
      "    load_time_ms: 13.12\n",
      "    sample_throughput: 54.651\n",
      "    sample_time_ms: 18297.813\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633794100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         2989.88</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             397.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 398.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 345\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6333648774358962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011320103149151153\n",
      "          policy_loss: -0.038493360641101995\n",
      "          total_loss: -0.05469473865297106\n",
      "          vf_explained_var: -0.9975253343582153\n",
      "          vf_loss: 9.689767979984431e-05\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62307692307692\n",
      "    ram_util_percent: 70.99230769230769\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036625728964851154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.08136937937995\n",
      "    mean_inference_ms: 1.7121915173044957\n",
      "    mean_raw_obs_processing_ms: 1.5341892989524337\n",
      "  time_since_restore: 3007.8942267894745\n",
      "  time_this_iter_s: 18.0184109210968\n",
      "  time_total_s: 3007.8942267894745\n",
      "  timers:\n",
      "    learn_throughput: 1578.326\n",
      "    learn_time_ms: 633.583\n",
      "    load_throughput: 76993.471\n",
      "    load_time_ms: 12.988\n",
      "    sample_throughput: 54.653\n",
      "    sample_time_ms: 18297.292\n",
      "    update_time_ms: 3.627\n",
      "  timestamp: 1633794118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3007.89</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 402.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 347\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.681359346707662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009965763381794012\n",
      "          policy_loss: -0.017087004085381826\n",
      "          total_loss: -0.03370050274663501\n",
      "          vf_explained_var: -0.4577208161354065\n",
      "          vf_loss: 0.0001689473056709782\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93333333333334\n",
      "    ram_util_percent: 71.06190476190474\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662522958905378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.077574409816958\n",
      "    mean_inference_ms: 1.7122089708775445\n",
      "    mean_raw_obs_processing_ms: 1.5334685168372537\n",
      "  time_since_restore: 3022.789344549179\n",
      "  time_this_iter_s: 14.89511775970459\n",
      "  time_total_s: 3022.789344549179\n",
      "  timers:\n",
      "    learn_throughput: 1578.248\n",
      "    learn_time_ms: 633.614\n",
      "    load_throughput: 86958.678\n",
      "    load_time_ms: 11.5\n",
      "    sample_throughput: 55.555\n",
      "    sample_time_ms: 18000.153\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1633794133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3022.79</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            402.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-42-30\n",
      "  done: false\n",
      "  episode_len_mean: 405.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 349\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7921497344970703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011677368271096306\n",
      "          policy_loss: -0.05902641365925471\n",
      "          total_loss: -0.07681522427333726\n",
      "          vf_explained_var: -0.6875420212745667\n",
      "          vf_loss: 9.619647320909684e-05\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.449999999999996\n",
      "    ram_util_percent: 71.12916666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036624699811103424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.07307611632406\n",
      "    mean_inference_ms: 1.7122246871535083\n",
      "    mean_raw_obs_processing_ms: 1.5326938050933785\n",
      "  time_since_restore: 3039.89829993248\n",
      "  time_this_iter_s: 17.10895538330078\n",
      "  time_total_s: 3039.89829993248\n",
      "  timers:\n",
      "    learn_throughput: 1578.616\n",
      "    learn_time_ms: 633.466\n",
      "    load_throughput: 85732.588\n",
      "    load_time_ms: 11.664\n",
      "    sample_throughput: 55.41\n",
      "    sample_time_ms: 18047.424\n",
      "    update_time_ms: 3.617\n",
      "  timestamp: 1633794150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">          3039.9</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            405.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 407.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 351\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8604309135013157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013970266695958605\n",
      "          policy_loss: -0.0037713455657164257\n",
      "          total_loss: -0.022255683773093754\n",
      "          vf_explained_var: -0.37314996123313904\n",
      "          vf_loss: 7.631733733433066e-05\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63703703703704\n",
      "    ram_util_percent: 71.21481481481482\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662418914538461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.067900285507445\n",
      "    mean_inference_ms: 1.712238311581707\n",
      "    mean_raw_obs_processing_ms: 1.5319992970819698\n",
      "  time_since_restore: 3058.8673191070557\n",
      "  time_this_iter_s: 18.969019174575806\n",
      "  time_total_s: 3058.8673191070557\n",
      "  timers:\n",
      "    learn_throughput: 1580.117\n",
      "    learn_time_ms: 632.864\n",
      "    load_throughput: 85504.858\n",
      "    load_time_ms: 11.695\n",
      "    sample_throughput: 60.587\n",
      "    sample_time_ms: 16505.114\n",
      "    update_time_ms: 3.633\n",
      "  timestamp: 1633794169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3058.87</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            407.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-43-06\n",
      "  done: false\n",
      "  episode_len_mean: 410.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 353\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8361974080403647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017136713543779554\n",
      "          policy_loss: 0.0013350907299253675\n",
      "          total_loss: -0.01692278716299269\n",
      "          vf_explained_var: -0.4603888690471649\n",
      "          vf_loss: 5.054493354287438e-05\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.523999999999994\n",
      "    ram_util_percent: 71.27199999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366236673408741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.062076570882592\n",
      "    mean_inference_ms: 1.7122503778418174\n",
      "    mean_raw_obs_processing_ms: 1.5312468776589319\n",
      "  time_since_restore: 3076.3257122039795\n",
      "  time_this_iter_s: 17.458393096923828\n",
      "  time_total_s: 3076.3257122039795\n",
      "  timers:\n",
      "    learn_throughput: 1582.025\n",
      "    learn_time_ms: 632.101\n",
      "    load_throughput: 91801.981\n",
      "    load_time_ms: 10.893\n",
      "    sample_throughput: 60.872\n",
      "    sample_time_ms: 16427.799\n",
      "    update_time_ms: 3.626\n",
      "  timestamp: 1633794186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3076.33</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             410.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-43-27\n",
      "  done: false\n",
      "  episode_len_mean: 412.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 356\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6499021490414938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013122924522419687\n",
      "          policy_loss: -0.02071201710237397\n",
      "          total_loss: -0.03714002850982878\n",
      "          vf_explained_var: -0.9070490598678589\n",
      "          vf_loss: 3.0000694136510397e-05\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.453333333333326\n",
      "    ram_util_percent: 71.32333333333332\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662300028787674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.05280448261071\n",
      "    mean_inference_ms: 1.7122666315282498\n",
      "    mean_raw_obs_processing_ms: 1.5301070967518136\n",
      "  time_since_restore: 3096.9626820087433\n",
      "  time_this_iter_s: 20.636969804763794\n",
      "  time_total_s: 3096.9626820087433\n",
      "  timers:\n",
      "    learn_throughput: 1584.775\n",
      "    learn_time_ms: 631.004\n",
      "    load_throughput: 90378.9\n",
      "    load_time_ms: 11.065\n",
      "    sample_throughput: 60.131\n",
      "    sample_time_ms: 16630.245\n",
      "    update_time_ms: 3.62\n",
      "  timestamp: 1633794207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3096.96</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            412.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 415.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 358\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8649657514360216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009826726906860926\n",
      "          policy_loss: 0.008032245292431779\n",
      "          total_loss: -0.010546838968164392\n",
      "          vf_explained_var: -0.6501753926277161\n",
      "          vf_loss: 3.986589949312878e-05\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67391304347826\n",
      "    ram_util_percent: 71.4217391304348\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662262385085127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.04568616787826\n",
      "    mean_inference_ms: 1.7122749166064033\n",
      "    mean_raw_obs_processing_ms: 1.5293827122957588\n",
      "  time_since_restore: 3113.239690065384\n",
      "  time_this_iter_s: 16.277008056640625\n",
      "  time_total_s: 3113.239690065384\n",
      "  timers:\n",
      "    learn_throughput: 1583.199\n",
      "    learn_time_ms: 631.632\n",
      "    load_throughput: 89702.191\n",
      "    load_time_ms: 11.148\n",
      "    sample_throughput: 60.368\n",
      "    sample_time_ms: 16565.051\n",
      "    update_time_ms: 3.64\n",
      "  timestamp: 1633794223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3113.24</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            415.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 416.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 360\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7603941096199884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007807088123719956\n",
      "          policy_loss: -0.019516170935498345\n",
      "          total_loss: -0.03701341272228294\n",
      "          vf_explained_var: -0.7546815872192383\n",
      "          vf_loss: 8.230394665184172e-05\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.39019607843137\n",
      "    ram_util_percent: 71.49215686274509\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662230583520673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.03812147215228\n",
      "    mean_inference_ms: 1.7122821429531994\n",
      "    mean_raw_obs_processing_ms: 1.531045054277388\n",
      "  time_since_restore: 3148.9793531894684\n",
      "  time_this_iter_s: 35.73966312408447\n",
      "  time_total_s: 3148.9793531894684\n",
      "  timers:\n",
      "    learn_throughput: 1583.486\n",
      "    learn_time_ms: 631.518\n",
      "    load_throughput: 90093.524\n",
      "    load_time_ms: 11.1\n",
      "    sample_throughput: 54.53\n",
      "    sample_time_ms: 18338.568\n",
      "    update_time_ms: 3.644\n",
      "  timestamp: 1633794259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3148.98</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 417.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 363\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7661957555347019\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015255312019640973\n",
      "          policy_loss: -0.07881278209388257\n",
      "          total_loss: -0.096345206308696\n",
      "          vf_explained_var: -0.862668514251709\n",
      "          vf_loss: 8.186075691709346e-05\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.31666666666668\n",
      "    ram_util_percent: 71.51333333333332\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662188207077134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.02671798724284\n",
      "    mean_inference_ms: 1.712292879400266\n",
      "    mean_raw_obs_processing_ms: 1.533567380219645\n",
      "  time_since_restore: 3169.8471252918243\n",
      "  time_this_iter_s: 20.867772102355957\n",
      "  time_total_s: 3169.8471252918243\n",
      "  timers:\n",
      "    learn_throughput: 1578.948\n",
      "    learn_time_ms: 633.333\n",
      "    load_throughput: 81006.149\n",
      "    load_time_ms: 12.345\n",
      "    sample_throughput: 53.023\n",
      "    sample_time_ms: 18859.579\n",
      "    update_time_ms: 3.647\n",
      "  timestamp: 1633794280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3169.85</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            417.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-45-01\n",
      "  done: false\n",
      "  episode_len_mean: 416.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 365\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8395878394444785\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012834783448459557\n",
      "          policy_loss: 0.04706991596354378\n",
      "          total_loss: 0.02874672462542852\n",
      "          vf_explained_var: -0.22183769941329956\n",
      "          vf_loss: 3.257529336527417e-05\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.88275862068967\n",
      "    ram_util_percent: 71.51034482758621\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366216642341724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.01913097267319\n",
      "    mean_inference_ms: 1.7123005820737802\n",
      "    mean_raw_obs_processing_ms: 1.5352219793435689\n",
      "  time_since_restore: 3190.593764781952\n",
      "  time_this_iter_s: 20.746639490127563\n",
      "  time_total_s: 3190.593764781952\n",
      "  timers:\n",
      "    learn_throughput: 1578.114\n",
      "    learn_time_ms: 633.668\n",
      "    load_throughput: 72205.038\n",
      "    load_time_ms: 13.849\n",
      "    sample_throughput: 51.501\n",
      "    sample_time_ms: 19417.121\n",
      "    update_time_ms: 3.642\n",
      "  timestamp: 1633794301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3190.59</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-45-19\n",
      "  done: false\n",
      "  episode_len_mean: 419.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 368\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7943525049421523\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011324297128755647\n",
      "          policy_loss: -0.11432907755176226\n",
      "          total_loss: -0.13219742162360085\n",
      "          vf_explained_var: -0.5835985541343689\n",
      "          vf_loss: 3.9791867614743144e-05\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62592592592593\n",
      "    ram_util_percent: 71.22962962962963\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662130344059475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.00707603071666\n",
      "    mean_inference_ms: 1.7123107431661555\n",
      "    mean_raw_obs_processing_ms: 1.5377868958176115\n",
      "  time_since_restore: 3208.9951536655426\n",
      "  time_this_iter_s: 18.4013888835907\n",
      "  time_total_s: 3208.9951536655426\n",
      "  timers:\n",
      "    learn_throughput: 1581.878\n",
      "    learn_time_ms: 632.16\n",
      "    load_throughput: 71560.133\n",
      "    load_time_ms: 13.974\n",
      "    sample_throughput: 51.392\n",
      "    sample_time_ms: 19458.39\n",
      "    update_time_ms: 2.058\n",
      "  timestamp: 1633794319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">            3209</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            419.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-45-37\n",
      "  done: false\n",
      "  episode_len_mean: 420.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 370\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.901957803302341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013539563082045486\n",
      "          policy_loss: -0.025310181495216157\n",
      "          total_loss: -0.0442489404645231\n",
      "          vf_explained_var: -0.6028187274932861\n",
      "          vf_loss: 3.850575184413982e-05\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.628\n",
      "    ram_util_percent: 71.064\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662110779142489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.99877637505207\n",
      "    mean_inference_ms: 1.7123170408068398\n",
      "    mean_raw_obs_processing_ms: 1.536434960979717\n",
      "  time_since_restore: 3227.0092022418976\n",
      "  time_this_iter_s: 18.01404857635498\n",
      "  time_total_s: 3227.0092022418976\n",
      "  timers:\n",
      "    learn_throughput: 1577.088\n",
      "    learn_time_ms: 634.08\n",
      "    load_throughput: 64610.953\n",
      "    load_time_ms: 15.477\n",
      "    sample_throughput: 50.59\n",
      "    sample_time_ms: 19766.871\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633794337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3227.01</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            420.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 422.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 372\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9494694550832112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010802580123622403\n",
      "          policy_loss: -0.02103431856052743\n",
      "          total_loss: -0.04042585372096962\n",
      "          vf_explained_var: -0.3876015841960907\n",
      "          vf_loss: 6.940374363087662e-05\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.05862068965517\n",
      "    ram_util_percent: 71.15172413793103\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366209514401328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.989973402024397\n",
      "    mean_inference_ms: 1.7123226893981047\n",
      "    mean_raw_obs_processing_ms: 1.5351627720977976\n",
      "  time_since_restore: 3246.903088569641\n",
      "  time_this_iter_s: 19.89388632774353\n",
      "  time_total_s: 3246.903088569641\n",
      "  timers:\n",
      "    learn_throughput: 1570.179\n",
      "    learn_time_ms: 636.87\n",
      "    load_throughput: 63392.827\n",
      "    load_time_ms: 15.775\n",
      "    sample_throughput: 49.894\n",
      "    sample_time_ms: 20042.312\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633794357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">          3246.9</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            422.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 422.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 374\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8981078770425586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013768645203554798\n",
      "          policy_loss: -0.03779507097270754\n",
      "          total_loss: -0.05668814861112171\n",
      "          vf_explained_var: -0.8739738464355469\n",
      "          vf_loss: 4.497481147862143e-05\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.99615384615385\n",
      "    ram_util_percent: 71.25\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036620937877627296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.98085836170849\n",
      "    mean_inference_ms: 1.7123311006957727\n",
      "    mean_raw_obs_processing_ms: 1.5338387421370552\n",
      "  time_since_restore: 3264.988428592682\n",
      "  time_this_iter_s: 18.08534002304077\n",
      "  time_total_s: 3264.988428592682\n",
      "  timers:\n",
      "    learn_throughput: 1567.091\n",
      "    learn_time_ms: 638.125\n",
      "    load_throughput: 65143.612\n",
      "    load_time_ms: 15.351\n",
      "    sample_throughput: 50.117\n",
      "    sample_time_ms: 19953.14\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1633794375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3264.99</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            422.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-46-36\n",
      "  done: false\n",
      "  episode_len_mean: 425.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 377\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8362424241171942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014466262844621427\n",
      "          policy_loss: 0.05306520644161436\n",
      "          total_loss: 0.03481763758593136\n",
      "          vf_explained_var: -0.40361425280570984\n",
      "          vf_loss: 6.964868840037121e-05\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.282758620689656\n",
      "    ram_util_percent: 71.56896551724138\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662146643434777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.96738117882252\n",
      "    mean_inference_ms: 1.712356085811529\n",
      "    mean_raw_obs_processing_ms: 1.5319144007433534\n",
      "  time_since_restore: 3285.4234807491302\n",
      "  time_this_iter_s: 20.435052156448364\n",
      "  time_total_s: 3285.4234807491302\n",
      "  timers:\n",
      "    learn_throughput: 1555.381\n",
      "    learn_time_ms: 642.929\n",
      "    load_throughput: 62923.306\n",
      "    load_time_ms: 15.892\n",
      "    sample_throughput: 49.394\n",
      "    sample_time_ms: 20245.421\n",
      "    update_time_ms: 2.111\n",
      "  timestamp: 1633794396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3285.42</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            425.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 425.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 379\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031249999999999993\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7096639805369906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023649211565082626\n",
      "          policy_loss: -0.049505221678151023\n",
      "          total_loss: -0.06622781107823054\n",
      "          vf_explained_var: 0.44114363193511963\n",
      "          vf_loss: 0.0003001481033505924\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.967857142857156\n",
      "    ram_util_percent: 72.01071428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036622138296753565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.95857321471506\n",
      "    mean_inference_ms: 1.7123837336795698\n",
      "    mean_raw_obs_processing_ms: 1.5307553261533533\n",
      "  time_since_restore: 3304.8367731571198\n",
      "  time_this_iter_s: 19.413292407989502\n",
      "  time_total_s: 3304.8367731571198\n",
      "  timers:\n",
      "    learn_throughput: 1543.224\n",
      "    learn_time_ms: 647.994\n",
      "    load_throughput: 64279.536\n",
      "    load_time_ms: 15.557\n",
      "    sample_throughput: 49.706\n",
      "    sample_time_ms: 20118.307\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1633794415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         3304.84</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            425.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-47-15\n",
      "  done: false\n",
      "  episode_len_mean: 425.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 381\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0046875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.739018232292599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013229834675820484\n",
      "          policy_loss: -0.017593701556324958\n",
      "          total_loss: -0.03473846498462889\n",
      "          vf_explained_var: 0.4414299428462982\n",
      "          vf_loss: 0.0001834028633311391\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.54642857142857\n",
      "    ram_util_percent: 72.04642857142856\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662307116558254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.949683049510377\n",
      "    mean_inference_ms: 1.7124184153583542\n",
      "    mean_raw_obs_processing_ms: 1.5296713397861958\n",
      "  time_since_restore: 3324.8562185764313\n",
      "  time_this_iter_s: 20.019445419311523\n",
      "  time_total_s: 3324.8562185764313\n",
      "  timers:\n",
      "    learn_throughput: 1525.622\n",
      "    learn_time_ms: 655.47\n",
      "    load_throughput: 60275.892\n",
      "    load_time_ms: 16.59\n",
      "    sample_throughput: 48.819\n",
      "    sample_time_ms: 20484.035\n",
      "    update_time_ms: 2.141\n",
      "  timestamp: 1633794435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3324.86</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            425.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 427.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 384\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0046875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8286021153132122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02177658351164015\n",
      "          policy_loss: -0.10996218191252814\n",
      "          total_loss: -0.12794882667561372\n",
      "          vf_explained_var: -0.6375928521156311\n",
      "          vf_loss: 0.0001972984261657176\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.720000000000006\n",
      "    ram_util_percent: 72.19000000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036624993737805075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.93623635143872\n",
      "    mean_inference_ms: 1.712482579127946\n",
      "    mean_raw_obs_processing_ms: 1.5280995091406522\n",
      "  time_since_restore: 3345.718773126602\n",
      "  time_this_iter_s: 20.8625545501709\n",
      "  time_total_s: 3345.718773126602\n",
      "  timers:\n",
      "    learn_throughput: 1518.152\n",
      "    learn_time_ms: 658.696\n",
      "    load_throughput: 60795.737\n",
      "    load_time_ms: 16.449\n",
      "    sample_throughput: 52.65\n",
      "    sample_time_ms: 18993.214\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1633794456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         3345.72</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            427.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 427.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 386\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.74776664574941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014180529856419098\n",
      "          policy_loss: -0.016376285751660665\n",
      "          total_loss: -0.03356868492232429\n",
      "          vf_explained_var: 0.013668937608599663\n",
      "          vf_loss: 0.0001855577149803543\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.98620689655172\n",
      "    ram_util_percent: 73.44827586206898\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662663247366218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.927059608954824\n",
      "    mean_inference_ms: 1.7125345744039329\n",
      "    mean_raw_obs_processing_ms: 1.527042289638619\n",
      "  time_since_restore: 3365.6996669769287\n",
      "  time_this_iter_s: 19.980893850326538\n",
      "  time_total_s: 3365.6996669769287\n",
      "  timers:\n",
      "    learn_throughput: 1513.523\n",
      "    learn_time_ms: 660.71\n",
      "    load_throughput: 60682.359\n",
      "    load_time_ms: 16.479\n",
      "    sample_throughput: 52.903\n",
      "    sample_time_ms: 18902.451\n",
      "    update_time_ms: 2.187\n",
      "  timestamp: 1633794476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">          3365.7</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            427.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 430.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 388\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.693778465853797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016152374500295667\n",
      "          policy_loss: 0.08793003658453623\n",
      "          total_loss: 0.0712440470026599\n",
      "          vf_explained_var: -0.19891567528247833\n",
      "          vf_loss: 0.00013822480442387232\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.636\n",
      "    ram_util_percent: 73.236\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03662842752394717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.917067727393906\n",
      "    mean_inference_ms: 1.7125889552105162\n",
      "    mean_raw_obs_processing_ms: 1.5260534582617296\n",
      "  time_since_restore: 3383.6016964912415\n",
      "  time_this_iter_s: 17.902029514312744\n",
      "  time_total_s: 3383.6016964912415\n",
      "  timers:\n",
      "    learn_throughput: 1512.721\n",
      "    learn_time_ms: 661.06\n",
      "    load_throughput: 60367.157\n",
      "    load_time_ms: 16.565\n",
      "    sample_throughput: 53.713\n",
      "    sample_time_ms: 18617.527\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633794494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">          3383.6</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            430.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 431.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 391\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.625627417034573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012867015969018682\n",
      "          policy_loss: -0.05234737214114931\n",
      "          total_loss: -0.06831738568014568\n",
      "          vf_explained_var: -0.07953639328479767\n",
      "          vf_loss: 0.00019579178745819567\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.425925925925924\n",
      "    ram_util_percent: 73.16296296296295\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036631218261343766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.90203872401672\n",
      "    mean_inference_ms: 1.7126733651533763\n",
      "    mean_raw_obs_processing_ms: 1.5276951671741683\n",
      "  time_since_restore: 3421.1507337093353\n",
      "  time_this_iter_s: 37.54903721809387\n",
      "  time_total_s: 3421.1507337093353\n",
      "  timers:\n",
      "    learn_throughput: 1506.2\n",
      "    learn_time_ms: 663.923\n",
      "    load_throughput: 61143.508\n",
      "    load_time_ms: 16.355\n",
      "    sample_throughput: 48.71\n",
      "    sample_time_ms: 20529.667\n",
      "    update_time_ms: 2.193\n",
      "  timestamp: 1633794531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3421.15</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            431.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-49-07\n",
      "  done: false\n",
      "  episode_len_mean: 434.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 392\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6213497069146898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015573782303806204\n",
      "          policy_loss: -0.02213223667608367\n",
      "          total_loss: -0.038134766577018635\n",
      "          vf_explained_var: 0.4515998959541321\n",
      "          vf_loss: 0.00010146290078409948\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.03636363636364\n",
      "    ram_util_percent: 73.09999999999997\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663216709171806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.896638558838223\n",
      "    mean_inference_ms: 1.7127018063782011\n",
      "    mean_raw_obs_processing_ms: 1.5283054239606824\n",
      "  time_since_restore: 3436.677479505539\n",
      "  time_this_iter_s: 15.526745796203613\n",
      "  time_total_s: 3436.677479505539\n",
      "  timers:\n",
      "    learn_throughput: 1507.145\n",
      "    learn_time_ms: 663.506\n",
      "    load_throughput: 67223.844\n",
      "    load_time_ms: 14.876\n",
      "    sample_throughput: 49.303\n",
      "    sample_time_ms: 20282.822\n",
      "    update_time_ms: 2.192\n",
      "  timestamp: 1633794547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3436.68</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            434.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-49-23\n",
      "  done: false\n",
      "  episode_len_mean: 436.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 394\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.725774387518565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009961585871992766\n",
      "          policy_loss: -0.0059776428673002455\n",
      "          total_loss: -0.023052721553378636\n",
      "          vf_explained_var: -0.20437787473201752\n",
      "          vf_loss: 0.0001126219497463252\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30434782608697\n",
      "    ram_util_percent: 73.05652173913043\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663411671333734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.885352880092075\n",
      "    mean_inference_ms: 1.7127600146674928\n",
      "    mean_raw_obs_processing_ms: 1.5293504758852068\n",
      "  time_since_restore: 3453.022030353546\n",
      "  time_this_iter_s: 16.344550848007202\n",
      "  time_total_s: 3453.022030353546\n",
      "  timers:\n",
      "    learn_throughput: 1510.976\n",
      "    learn_time_ms: 661.824\n",
      "    load_throughput: 67407.615\n",
      "    load_time_ms: 14.835\n",
      "    sample_throughput: 50.177\n",
      "    sample_time_ms: 19929.609\n",
      "    update_time_ms: 2.178\n",
      "  timestamp: 1633794563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         3453.02</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            436.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 439.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 396\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.751977511246999\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010332417021993173\n",
      "          policy_loss: -0.019874205854203967\n",
      "          total_loss: -0.037221237934297983\n",
      "          vf_explained_var: -0.2323797047138214\n",
      "          vf_loss: 0.00010009466511999361\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.415384615384625\n",
      "    ram_util_percent: 72.84615384615384\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036636167627641936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.873640473989262\n",
      "    mean_inference_ms: 1.712820341536417\n",
      "    mean_raw_obs_processing_ms: 1.5304516441778855\n",
      "  time_since_restore: 3470.7962341308594\n",
      "  time_this_iter_s: 17.774203777313232\n",
      "  time_total_s: 3470.7962341308594\n",
      "  timers:\n",
      "    learn_throughput: 1511.375\n",
      "    learn_time_ms: 661.649\n",
      "    load_throughput: 66453.684\n",
      "    load_time_ms: 15.048\n",
      "    sample_throughput: 50.255\n",
      "    sample_time_ms: 19898.427\n",
      "    update_time_ms: 2.185\n",
      "  timestamp: 1633794581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">          3470.8</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             439.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 439.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 399\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5572167409790887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009943035979777647\n",
      "          policy_loss: -0.097049946586291\n",
      "          total_loss: -0.1124244189924664\n",
      "          vf_explained_var: 0.08306393027305603\n",
      "          vf_loss: 0.0001277814698470239\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.310344827586206\n",
      "    ram_util_percent: 72.46896551724139\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03663924470091012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.855679035920808\n",
      "    mean_inference_ms: 1.712910519072897\n",
      "    mean_raw_obs_processing_ms: 1.5321840017593351\n",
      "  time_since_restore: 3491.268041372299\n",
      "  time_this_iter_s: 20.47180724143982\n",
      "  time_total_s: 3491.268041372299\n",
      "  timers:\n",
      "    learn_throughput: 1517.415\n",
      "    learn_time_ms: 659.016\n",
      "    load_throughput: 66463.161\n",
      "    load_time_ms: 15.046\n",
      "    sample_throughput: 50.239\n",
      "    sample_time_ms: 19904.747\n",
      "    update_time_ms: 2.152\n",
      "  timestamp: 1633794602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         3491.27</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            439.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 442.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 401\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6814451376597086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01282932728663811\n",
      "          policy_loss: -0.018714004506667454\n",
      "          total_loss: -0.03534285310241911\n",
      "          vf_explained_var: -0.06814315915107727\n",
      "          vf_loss: 9.539447015009097e-05\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.23749999999999\n",
      "    ram_util_percent: 72.44166666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664132703398987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.843293902463596\n",
      "    mean_inference_ms: 1.7129714123924642\n",
      "    mean_raw_obs_processing_ms: 1.5305379563334622\n",
      "  time_since_restore: 3507.8631987571716\n",
      "  time_this_iter_s: 16.595157384872437\n",
      "  time_total_s: 3507.8631987571716\n",
      "  timers:\n",
      "    learn_throughput: 1526.572\n",
      "    learn_time_ms: 655.062\n",
      "    load_throughput: 69938.637\n",
      "    load_time_ms: 14.298\n",
      "    sample_throughput: 50.948\n",
      "    sample_time_ms: 19627.665\n",
      "    update_time_ms: 2.154\n",
      "  timestamp: 1633794618\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         3507.86</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            442.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-50-39\n",
      "  done: false\n",
      "  episode_len_mean: 443.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 404\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.825882093111674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011143645650693618\n",
      "          policy_loss: -0.11490615208943684\n",
      "          total_loss: -0.1330046541781889\n",
      "          vf_explained_var: -0.5930744409561157\n",
      "          vf_loss: 8.19611264079059e-05\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.11333333333334\n",
      "    ram_util_percent: 72.44333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664452127285053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.824985393382303\n",
      "    mean_inference_ms: 1.7130650877125155\n",
      "    mean_raw_obs_processing_ms: 1.5280615083559383\n",
      "  time_since_restore: 3528.9857981204987\n",
      "  time_this_iter_s: 21.122599363327026\n",
      "  time_total_s: 3528.9857981204987\n",
      "  timers:\n",
      "    learn_throughput: 1538.673\n",
      "    learn_time_ms: 649.911\n",
      "    load_throughput: 70488.476\n",
      "    load_time_ms: 14.187\n",
      "    sample_throughput: 50.65\n",
      "    sample_time_ms: 19743.275\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1633794639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         3528.99</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            443.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 444.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 406\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7923869927724203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009966689498827976\n",
      "          policy_loss: -0.12689614875449073\n",
      "          total_loss: -0.1446469325158331\n",
      "          vf_explained_var: -0.2971605658531189\n",
      "          vf_loss: 0.00010301042780661697\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.056000000000004\n",
      "    ram_util_percent: 72.488\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03664672738345807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.81264498613955\n",
      "    mean_inference_ms: 1.7131294821103302\n",
      "    mean_raw_obs_processing_ms: 1.5264437807885565\n",
      "  time_since_restore: 3546.4061138629913\n",
      "  time_this_iter_s: 17.420315742492676\n",
      "  time_total_s: 3546.4061138629913\n",
      "  timers:\n",
      "    learn_throughput: 1543.591\n",
      "    learn_time_ms: 647.84\n",
      "    load_throughput: 70858.946\n",
      "    load_time_ms: 14.113\n",
      "    sample_throughput: 51.543\n",
      "    sample_time_ms: 19401.223\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633794657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         3546.41</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            444.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-51-17\n",
      "  done: false\n",
      "  episode_len_mean: 444.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 408\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5385098391109042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008619839009273277\n",
      "          policy_loss: -0.0011613750623332129\n",
      "          total_loss: -0.016426207704676524\n",
      "          vf_explained_var: -0.7853887677192688\n",
      "          vf_loss: 5.9658109421434347e-05\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.425000000000004\n",
      "    ram_util_percent: 72.58928571428571\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036649033472496345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.800510120453893\n",
      "    mean_inference_ms: 1.7131958701146366\n",
      "    mean_raw_obs_processing_ms: 1.524901078177686\n",
      "  time_since_restore: 3566.435361623764\n",
      "  time_this_iter_s: 20.029247760772705\n",
      "  time_total_s: 3566.435361623764\n",
      "  timers:\n",
      "    learn_throughput: 1547.267\n",
      "    learn_time_ms: 646.301\n",
      "    load_throughput: 71240.106\n",
      "    load_time_ms: 14.037\n",
      "    sample_throughput: 51.527\n",
      "    sample_time_ms: 19407.332\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1633794677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         3566.44</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            444.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 445.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 410\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.535850497086843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006012002329302943\n",
      "          policy_loss: 0.04789287398258845\n",
      "          total_loss: 0.032641065493226054\n",
      "          vf_explained_var: 0.13416266441345215\n",
      "          vf_loss: 6.44253898321444e-05\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.82\n",
      "    ram_util_percent: 72.04400000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03665138996399136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.788123236311208\n",
      "    mean_inference_ms: 1.7132637721027648\n",
      "    mean_raw_obs_processing_ms: 1.5234279280829486\n",
      "  time_since_restore: 3583.520810842514\n",
      "  time_this_iter_s: 17.08544921875\n",
      "  time_total_s: 3583.520810842514\n",
      "  timers:\n",
      "    learn_throughput: 1549.043\n",
      "    learn_time_ms: 645.56\n",
      "    load_throughput: 75617.189\n",
      "    load_time_ms: 13.225\n",
      "    sample_throughput: 51.74\n",
      "    sample_time_ms: 19327.258\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1633794694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         3583.52</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            445.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-51-50\n",
      "  done: false\n",
      "  episode_len_mean: 448.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 412\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7391629020373027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010421116207564888\n",
      "          policy_loss: 0.0345521023703946\n",
      "          total_loss: 0.017314767589171727\n",
      "          vf_explained_var: -0.6565656065940857\n",
      "          vf_loss: 8.101793876752102e-05\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.608695652173914\n",
      "    ram_util_percent: 71.87391304347828\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366536965018644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.775305653282288\n",
      "    mean_inference_ms: 1.713330598082361\n",
      "    mean_raw_obs_processing_ms: 1.5219092194043313\n",
      "  time_since_restore: 3599.5425980091095\n",
      "  time_this_iter_s: 16.02178716659546\n",
      "  time_total_s: 3599.5425980091095\n",
      "  timers:\n",
      "    learn_throughput: 1552.733\n",
      "    learn_time_ms: 644.026\n",
      "    load_throughput: 78170.27\n",
      "    load_time_ms: 12.793\n",
      "    sample_throughput: 58.219\n",
      "    sample_time_ms: 17176.46\n",
      "    update_time_ms: 2.415\n",
      "  timestamp: 1633794710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         3599.54</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 450.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 414\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8319886631435818\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01183500885165382\n",
      "          policy_loss: -0.0225369052340587\n",
      "          total_loss: -0.04068126243849595\n",
      "          vf_explained_var: -0.7707067728042603\n",
      "          vf_loss: 9.231282229949203e-05\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.686363636363645\n",
      "    ram_util_percent: 71.89545454545457\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036656032498077085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.762308069182115\n",
      "    mean_inference_ms: 1.713396685026455\n",
      "    mean_raw_obs_processing_ms: 1.5204613353191854\n",
      "  time_since_restore: 3615.5832209587097\n",
      "  time_this_iter_s: 16.04062294960022\n",
      "  time_total_s: 3615.5832209587097\n",
      "  timers:\n",
      "    learn_throughput: 1553.708\n",
      "    learn_time_ms: 643.622\n",
      "    load_throughput: 76013.65\n",
      "    load_time_ms: 13.156\n",
      "    sample_throughput: 58.045\n",
      "    sample_time_ms: 17227.907\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1633794726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         3615.58</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 454.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 416\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8517103287908765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011208531386588581\n",
      "          policy_loss: -0.054147018192129005\n",
      "          total_loss: -0.07246706676152018\n",
      "          vf_explained_var: -0.31547775864601135\n",
      "          vf_loss: 0.00011824649148669171\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.631818181818176\n",
      "    ram_util_percent: 71.97727272727273\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03665829602549537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.748857329667143\n",
      "    mean_inference_ms: 1.7134604217167484\n",
      "    mean_raw_obs_processing_ms: 1.5190785115469094\n",
      "  time_since_restore: 3630.9967708587646\n",
      "  time_this_iter_s: 15.413549900054932\n",
      "  time_total_s: 3630.9967708587646\n",
      "  timers:\n",
      "    learn_throughput: 1559.073\n",
      "    learn_time_ms: 641.407\n",
      "    load_throughput: 81095.267\n",
      "    load_time_ms: 12.331\n",
      "    sample_throughput: 58.35\n",
      "    sample_time_ms: 17137.886\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1633794741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">            3631</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 456.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 418\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.788584005832672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015299777691546446\n",
      "          policy_loss: 0.07511610312180386\n",
      "          total_loss: 0.057415446804629434\n",
      "          vf_explained_var: -0.37481170892715454\n",
      "          vf_loss: 7.76073724814018e-05\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.339285714285715\n",
      "    ram_util_percent: 72.26785714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036660715055790986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.735266460021403\n",
      "    mean_inference_ms: 1.7135298038447901\n",
      "    mean_raw_obs_processing_ms: 1.5176536540097363\n",
      "  time_since_restore: 3649.9309470653534\n",
      "  time_this_iter_s: 18.934176206588745\n",
      "  time_total_s: 3649.9309470653534\n",
      "  timers:\n",
      "    learn_throughput: 1544.225\n",
      "    learn_time_ms: 647.574\n",
      "    load_throughput: 81901.124\n",
      "    load_time_ms: 12.21\n",
      "    sample_throughput: 57.978\n",
      "    sample_time_ms: 17247.855\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1633794760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         3649.93</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 457.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 420\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7548754453659057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009984861282650767\n",
      "          policy_loss: -0.05710701665116681\n",
      "          total_loss: -0.07444356671637958\n",
      "          vf_explained_var: -0.34712979197502136\n",
      "          vf_loss: 0.00014199813295918932\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.134\n",
      "    ram_util_percent: 72.132\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03666316375960358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.721734342710793\n",
      "    mean_inference_ms: 1.7135994825690801\n",
      "    mean_raw_obs_processing_ms: 1.5182390834154973\n",
      "  time_since_restore: 3685.1197814941406\n",
      "  time_this_iter_s: 35.18883442878723\n",
      "  time_total_s: 3685.1197814941406\n",
      "  timers:\n",
      "    learn_throughput: 1546.051\n",
      "    learn_time_ms: 646.809\n",
      "    load_throughput: 82350.571\n",
      "    load_time_ms: 12.143\n",
      "    sample_throughput: 53.418\n",
      "    sample_time_ms: 18720.405\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1633794796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         3685.12</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 456.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 422\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.847973652680715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01467317256248137\n",
      "          policy_loss: -0.08390911892056466\n",
      "          total_loss: -0.10222439956333902\n",
      "          vf_explained_var: -0.5263182520866394\n",
      "          vf_loss: 6.128392947680873e-05\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.975862068965526\n",
      "    ram_util_percent: 71.99310344827585\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03666563483811856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.708457118392445\n",
      "    mean_inference_ms: 1.713671226928903\n",
      "    mean_raw_obs_processing_ms: 1.518769700473668\n",
      "  time_since_restore: 3705.333970308304\n",
      "  time_this_iter_s: 20.214188814163208\n",
      "  time_total_s: 3705.333970308304\n",
      "  timers:\n",
      "    learn_throughput: 1545.906\n",
      "    learn_time_ms: 646.87\n",
      "    load_throughput: 76160.925\n",
      "    load_time_ms: 13.13\n",
      "    sample_throughput: 52.407\n",
      "    sample_time_ms: 19081.251\n",
      "    update_time_ms: 2.389\n",
      "  timestamp: 1633794816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         3705.33</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-53-52\n",
      "  done: false\n",
      "  episode_len_mean: 457.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 424\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8092558966742622\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013311439725762418\n",
      "          policy_loss: -0.036515207340319954\n",
      "          total_loss: -0.05439736379517449\n",
      "          vf_explained_var: -0.6523586511611938\n",
      "          vf_loss: 0.00011680338448059047\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89130434782608\n",
      "    ram_util_percent: 71.68260869565215\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03666811490914473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.695187036314305\n",
      "    mean_inference_ms: 1.7137425180843946\n",
      "    mean_raw_obs_processing_ms: 1.5193524363407451\n",
      "  time_since_restore: 3721.793117046356\n",
      "  time_this_iter_s: 16.459146738052368\n",
      "  time_total_s: 3721.793117046356\n",
      "  timers:\n",
      "    learn_throughput: 1541.75\n",
      "    learn_time_ms: 648.614\n",
      "    load_throughput: 82391.66\n",
      "    load_time_ms: 12.137\n",
      "    sample_throughput: 53.723\n",
      "    sample_time_ms: 18614.11\n",
      "    update_time_ms: 2.404\n",
      "  timestamp: 1633794832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         3721.79</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 457.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 426\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9243861979908414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011750534626063174\n",
      "          policy_loss: -0.0996553124446008\n",
      "          total_loss: -0.11876446501248412\n",
      "          vf_explained_var: -0.6604120135307312\n",
      "          vf_loss: 5.208969333681226e-05\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.276\n",
      "    ram_util_percent: 71.408\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036670709452742735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.682051956026477\n",
      "    mean_inference_ms: 1.7138155176457988\n",
      "    mean_raw_obs_processing_ms: 1.5199870958729962\n",
      "  time_since_restore: 3739.224757909775\n",
      "  time_this_iter_s: 17.43164086341858\n",
      "  time_total_s: 3739.224757909775\n",
      "  timers:\n",
      "    learn_throughput: 1545.291\n",
      "    learn_time_ms: 647.127\n",
      "    load_throughput: 84717.326\n",
      "    load_time_ms: 11.804\n",
      "    sample_throughput: 53.715\n",
      "    sample_time_ms: 18616.807\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1633794850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         3739.22</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 455.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 429\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.722788589530521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01223290547581573\n",
      "          policy_loss: -0.08586668074131013\n",
      "          total_loss: -0.1029220373266273\n",
      "          vf_explained_var: -0.42344191670417786\n",
      "          vf_loss: 8.651717164159183e-05\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.865517241379315\n",
      "    ram_util_percent: 71.16896551724136\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667462733577681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.663517305017447\n",
      "    mean_inference_ms: 1.7139239646538673\n",
      "    mean_raw_obs_processing_ms: 1.51982357848166\n",
      "  time_since_restore: 3759.2566883563995\n",
      "  time_this_iter_s: 20.031930446624756\n",
      "  time_total_s: 3759.2566883563995\n",
      "  timers:\n",
      "    learn_throughput: 1542.452\n",
      "    learn_time_ms: 648.318\n",
      "    load_throughput: 84219.928\n",
      "    load_time_ms: 11.874\n",
      "    sample_throughput: 53.717\n",
      "    sample_time_ms: 18616.151\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633794870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         3759.26</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 456.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 431\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7943379534615411\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012706840385051689\n",
      "          policy_loss: -0.05834294077422884\n",
      "          total_loss: -0.07604973643190331\n",
      "          vf_explained_var: -0.25377359986305237\n",
      "          vf_loss: 0.0001472363881475758\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.972\n",
      "    ram_util_percent: 71.044\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03667719349257266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.651426884231476\n",
      "    mean_inference_ms: 1.7139946464062563\n",
      "    mean_raw_obs_processing_ms: 1.5180753636845248\n",
      "  time_since_restore: 3777.1384098529816\n",
      "  time_this_iter_s: 17.88172149658203\n",
      "  time_total_s: 3777.1384098529816\n",
      "  timers:\n",
      "    learn_throughput: 1537.283\n",
      "    learn_time_ms: 650.499\n",
      "    load_throughput: 75881.081\n",
      "    load_time_ms: 13.179\n",
      "    sample_throughput: 53.498\n",
      "    sample_time_ms: 18692.289\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633794888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         3777.14</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 457.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 433\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8575461811489529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012299908275673854\n",
      "          policy_loss: 0.007101403352701001\n",
      "          total_loss: -0.011161862417227691\n",
      "          vf_explained_var: -0.9188305139541626\n",
      "          vf_loss: 0.000225712879263382\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10833333333333\n",
      "    ram_util_percent: 71.03333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036679724765158316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.639323022296967\n",
      "    mean_inference_ms: 1.7140644622281525\n",
      "    mean_raw_obs_processing_ms: 1.5163912069791474\n",
      "  time_since_restore: 3794.0813806056976\n",
      "  time_this_iter_s: 16.942970752716064\n",
      "  time_total_s: 3794.0813806056976\n",
      "  timers:\n",
      "    learn_throughput: 1536.958\n",
      "    learn_time_ms: 650.636\n",
      "    load_throughput: 78669.505\n",
      "    load_time_ms: 12.711\n",
      "    sample_throughput: 53.235\n",
      "    sample_time_ms: 18784.749\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633794905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         3794.08</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-55-22\n",
      "  done: false\n",
      "  episode_len_mean: 459.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 435\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.80266066259808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018623159569355997\n",
      "          policy_loss: 0.06449938639998436\n",
      "          total_loss: 0.04679934423830774\n",
      "          vf_explained_var: -0.40643310546875\n",
      "          vf_loss: 0.00019561828473039591\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.895999999999994\n",
      "    ram_util_percent: 71.05999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036682277503784014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.627072585611483\n",
      "    mean_inference_ms: 1.7141339312014328\n",
      "    mean_raw_obs_processing_ms: 1.5146623925055696\n",
      "  time_since_restore: 3811.40984082222\n",
      "  time_this_iter_s: 17.328460216522217\n",
      "  time_total_s: 3811.40984082222\n",
      "  timers:\n",
      "    learn_throughput: 1533.544\n",
      "    learn_time_ms: 652.084\n",
      "    load_throughput: 76410.671\n",
      "    load_time_ms: 13.087\n",
      "    sample_throughput: 52.877\n",
      "    sample_time_ms: 18911.705\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633794922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         3811.41</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-55-40\n",
      "  done: false\n",
      "  episode_len_mean: 459.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 437\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.880177546872033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0137817495562827\n",
      "          policy_loss: 0.0903697226403488\n",
      "          total_loss: 0.07175502739846706\n",
      "          vf_explained_var: -0.3466048836708069\n",
      "          vf_loss: 9.017756207564768e-05\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.13846153846154\n",
      "    ram_util_percent: 71.09615384615384\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03668483043328872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.61512751500019\n",
      "    mean_inference_ms: 1.7142044453079728\n",
      "    mean_raw_obs_processing_ms: 1.5129981865984825\n",
      "  time_since_restore: 3829.2610926628113\n",
      "  time_this_iter_s: 17.85125184059143\n",
      "  time_total_s: 3829.2610926628113\n",
      "  timers:\n",
      "    learn_throughput: 1532.415\n",
      "    learn_time_ms: 652.565\n",
      "    load_throughput: 69180.744\n",
      "    load_time_ms: 14.455\n",
      "    sample_throughput: 52.21\n",
      "    sample_time_ms: 19153.527\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633794940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         3829.26</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 459.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 439\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.838924511273702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015592778969715285\n",
      "          policy_loss: -0.058856046779288185\n",
      "          total_loss: -0.07689541975657145\n",
      "          vf_explained_var: -0.5610204935073853\n",
      "          vf_loss: 0.00024023449227065106\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.01538461538461\n",
      "    ram_util_percent: 71.16153846153846\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036687407167660085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.603423589433035\n",
      "    mean_inference_ms: 1.7142752820851297\n",
      "    mean_raw_obs_processing_ms: 1.5113972519120324\n",
      "  time_since_restore: 3848.000578403473\n",
      "  time_this_iter_s: 18.73948574066162\n",
      "  time_total_s: 3848.000578403473\n",
      "  timers:\n",
      "    learn_throughput: 1548.503\n",
      "    learn_time_ms: 645.785\n",
      "    load_throughput: 68806.344\n",
      "    load_time_ms: 14.534\n",
      "    sample_throughput: 52.245\n",
      "    sample_time_ms: 19140.758\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633794959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">            3848</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-56-18\n",
      "  done: false\n",
      "  episode_len_mean: 457.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 441\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9115799877378676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014550910382156938\n",
      "          policy_loss: -0.00980608906182978\n",
      "          total_loss: -0.028743295412924554\n",
      "          vf_explained_var: 0.049948256462812424\n",
      "          vf_loss: 7.628214698343072e-05\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.489285714285714\n",
      "    ram_util_percent: 71.26071428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03669003712916498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.592377150244896\n",
      "    mean_inference_ms: 1.7143487950501908\n",
      "    mean_raw_obs_processing_ms: 1.509855552535151\n",
      "  time_since_restore: 3867.4566972255707\n",
      "  time_this_iter_s: 19.45611882209778\n",
      "  time_total_s: 3867.4566972255707\n",
      "  timers:\n",
      "    learn_throughput: 1541.23\n",
      "    learn_time_ms: 648.832\n",
      "    load_throughput: 67872.182\n",
      "    load_time_ms: 14.734\n",
      "    sample_throughput: 56.934\n",
      "    sample_time_ms: 17564.228\n",
      "    update_time_ms: 2.104\n",
      "  timestamp: 1633794978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         3867.46</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-56-38\n",
      "  done: false\n",
      "  episode_len_mean: 454.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 444\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8073137799898784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01565423222849876\n",
      "          policy_loss: -0.0478058911446068\n",
      "          total_loss: -0.06571422471768326\n",
      "          vf_explained_var: -0.7788659930229187\n",
      "          vf_loss: 5.47347349109057e-05\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.23448275862068\n",
      "    ram_util_percent: 71.25172413793103\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036694066374809514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.577197742334917\n",
      "    mean_inference_ms: 1.714463795607633\n",
      "    mean_raw_obs_processing_ms: 1.5077397618242854\n",
      "  time_since_restore: 3887.7744042873383\n",
      "  time_this_iter_s: 20.317707061767578\n",
      "  time_total_s: 3887.7744042873383\n",
      "  timers:\n",
      "    learn_throughput: 1542.303\n",
      "    learn_time_ms: 648.381\n",
      "    load_throughput: 67979.105\n",
      "    load_time_ms: 14.71\n",
      "    sample_throughput: 56.899\n",
      "    sample_time_ms: 17575.035\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1633794998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         3887.77</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 454.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 446\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9186894999610054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012003642594751776\n",
      "          policy_loss: -0.010013075338469611\n",
      "          total_loss: -0.029084038568867578\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 3.1534892403012943e-05\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73076923076922\n",
      "    ram_util_percent: 71.28461538461538\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036696758296704796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.56748521195609\n",
      "    mean_inference_ms: 1.7145421801518355\n",
      "    mean_raw_obs_processing_ms: 1.5064200654778068\n",
      "  time_since_restore: 3905.6664481163025\n",
      "  time_this_iter_s: 17.892043828964233\n",
      "  time_total_s: 3905.6664481163025\n",
      "  timers:\n",
      "    learn_throughput: 1549.533\n",
      "    learn_time_ms: 645.356\n",
      "    load_throughput: 63934.345\n",
      "    load_time_ms: 15.641\n",
      "    sample_throughput: 56.432\n",
      "    sample_time_ms: 17720.471\n",
      "    update_time_ms: 2.098\n",
      "  timestamp: 1633795016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         3905.67</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 454.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 448\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9337972124417624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012142003035460727\n",
      "          policy_loss: 0.018203683156106206\n",
      "          total_loss: -0.0009889257864819632\n",
      "          vf_explained_var: -0.3166123032569885\n",
      "          vf_loss: 5.99898013307312e-05\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10833333333334\n",
      "    ram_util_percent: 71.40833333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0366994616724601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.558111382341366\n",
      "    mean_inference_ms: 1.7146219275210826\n",
      "    mean_raw_obs_processing_ms: 1.5051560067289051\n",
      "  time_since_restore: 3922.906112909317\n",
      "  time_this_iter_s: 17.239664793014526\n",
      "  time_total_s: 3922.906112909317\n",
      "  timers:\n",
      "    learn_throughput: 1549.541\n",
      "    learn_time_ms: 645.352\n",
      "    load_throughput: 64406.076\n",
      "    load_time_ms: 15.526\n",
      "    sample_throughput: 56.492\n",
      "    sample_time_ms: 17701.631\n",
      "    update_time_ms: 2.097\n",
      "  timestamp: 1633795034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         3922.91</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-57-49\n",
      "  done: false\n",
      "  episode_len_mean: 454.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 450\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9650402267773945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011807143940073397\n",
      "          policy_loss: 0.013204002214802637\n",
      "          total_loss: -0.006303379695034689\n",
      "          vf_explained_var: -0.9823881387710571\n",
      "          vf_loss: 6.000098490201506e-05\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84705882352941\n",
      "    ram_util_percent: 71.36274509803923\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670224522452123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.54891684293814\n",
      "    mean_inference_ms: 1.7147038186595465\n",
      "    mean_raw_obs_processing_ms: 1.5057446769977827\n",
      "  time_since_restore: 3958.1568336486816\n",
      "  time_this_iter_s: 35.250720739364624\n",
      "  time_total_s: 3958.1568336486816\n",
      "  timers:\n",
      "    learn_throughput: 1550.013\n",
      "    learn_time_ms: 645.156\n",
      "    load_throughput: 66583.758\n",
      "    load_time_ms: 15.019\n",
      "    sample_throughput: 52.018\n",
      "    sample_time_ms: 19224.237\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633795069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         3958.16</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 453.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 453\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8275651031070286\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013252879178256401\n",
      "          policy_loss: -0.010444658663537767\n",
      "          total_loss: -0.028571220487356185\n",
      "          vf_explained_var: -0.39025846123695374\n",
      "          vf_loss: 5.590448818111326e-05\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.96206896551724\n",
      "    ram_util_percent: 71.53448275862067\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670649412652905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.535786097472112\n",
      "    mean_inference_ms: 1.7148299159671347\n",
      "    mean_raw_obs_processing_ms: 1.5067447115754402\n",
      "  time_since_restore: 3978.785815477371\n",
      "  time_this_iter_s: 20.628981828689575\n",
      "  time_total_s: 3978.785815477371\n",
      "  timers:\n",
      "    learn_throughput: 1552.954\n",
      "    learn_time_ms: 643.934\n",
      "    load_throughput: 69335.476\n",
      "    load_time_ms: 14.423\n",
      "    sample_throughput: 51.28\n",
      "    sample_time_ms: 19500.789\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633795090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         3978.79</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-58-31\n",
      "  done: false\n",
      "  episode_len_mean: 453.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 455\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.793597850534651\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010402537974359008\n",
      "          policy_loss: -0.01315511961778005\n",
      "          total_loss: -0.030948703156577217\n",
      "          vf_explained_var: -0.7997217178344727\n",
      "          vf_loss: 6.925149666333002e-05\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.08064516129031\n",
      "    ram_util_percent: 71.4935483870968\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03670931361732529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.527294092910996\n",
      "    mean_inference_ms: 1.7149138123613437\n",
      "    mean_raw_obs_processing_ms: 1.5075115839606894\n",
      "  time_since_restore: 4000.2544927597046\n",
      "  time_this_iter_s: 21.468677282333374\n",
      "  time_total_s: 4000.2544927597046\n",
      "  timers:\n",
      "    learn_throughput: 1553.732\n",
      "    learn_time_ms: 643.612\n",
      "    load_throughput: 64341.658\n",
      "    load_time_ms: 15.542\n",
      "    sample_throughput: 50.119\n",
      "    sample_time_ms: 19952.566\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633795111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         4000.25</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 451.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8975350115034315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01028902084395013\n",
      "          policy_loss: -0.0183300761712922\n",
      "          total_loss: -0.037192206664217846\n",
      "          vf_explained_var: -0.7973954081535339\n",
      "          vf_loss: 4.087438673094665e-05\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.22222222222223\n",
      "    ram_util_percent: 71.14814814814815\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671354074235079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.515009151564556\n",
      "    mean_inference_ms: 1.7150414379684693\n",
      "    mean_raw_obs_processing_ms: 1.5086355005517935\n",
      "  time_since_restore: 4019.544198513031\n",
      "  time_this_iter_s: 19.289705753326416\n",
      "  time_total_s: 4019.544198513031\n",
      "  timers:\n",
      "    learn_throughput: 1555.912\n",
      "    learn_time_ms: 642.71\n",
      "    load_throughput: 61856.504\n",
      "    load_time_ms: 16.166\n",
      "    sample_throughput: 49.63\n",
      "    sample_time_ms: 20148.936\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633795130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4019.54</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            451.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-59-11\n",
      "  done: false\n",
      "  episode_len_mean: 450.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 460\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8148484839333427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010514813804473189\n",
      "          policy_loss: -0.05359478315545453\n",
      "          total_loss: -0.0715994567092922\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 6.987672786635812e-05\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.42758620689656\n",
      "    ram_util_percent: 70.97931034482761\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03671635957270584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.507228820818668\n",
      "    mean_inference_ms: 1.7151280488502334\n",
      "    mean_raw_obs_processing_ms: 1.507176521421722\n",
      "  time_since_restore: 4039.8759622573853\n",
      "  time_this_iter_s: 20.331763744354248\n",
      "  time_total_s: 4039.8759622573853\n",
      "  timers:\n",
      "    learn_throughput: 1554.467\n",
      "    learn_time_ms: 643.307\n",
      "    load_throughput: 61772.328\n",
      "    load_time_ms: 16.188\n",
      "    sample_throughput: 49.028\n",
      "    sample_time_ms: 20396.462\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633795151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4039.88</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 450.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 463\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9763655185699462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012080154158094509\n",
      "          policy_loss: 0.013567764477597343\n",
      "          total_loss: -0.0060616585115591684\n",
      "          vf_explained_var: -0.9344716668128967\n",
      "          vf_loss: 4.9290281559579956e-05\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.18\n",
      "    ram_util_percent: 71.09999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367206302687576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.49556276609906\n",
      "    mean_inference_ms: 1.715258553478093\n",
      "    mean_raw_obs_processing_ms: 1.505066798151072\n",
      "  time_since_restore: 4060.56285905838\n",
      "  time_this_iter_s: 20.686896800994873\n",
      "  time_total_s: 4060.56285905838\n",
      "  timers:\n",
      "    learn_throughput: 1551.349\n",
      "    learn_time_ms: 644.6\n",
      "    load_throughput: 62184.084\n",
      "    load_time_ms: 16.081\n",
      "    sample_throughput: 48.567\n",
      "    sample_time_ms: 20590.004\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633795171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4060.56</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             450.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_15-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 451.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 465\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9186081714100307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01375816345514459\n",
      "          policy_loss: -0.05015413934985797\n",
      "          total_loss: -0.0692057217988703\n",
      "          vf_explained_var: -0.5928470492362976\n",
      "          vf_loss: 3.776312116517349e-05\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05517241379311\n",
      "    ram_util_percent: 71.2103448275862\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672342947380309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.487767673312984\n",
      "    mean_inference_ms: 1.7153441706435928\n",
      "    mean_raw_obs_processing_ms: 1.5037130882771652\n",
      "  time_since_restore: 4080.9051880836487\n",
      "  time_this_iter_s: 20.342329025268555\n",
      "  time_total_s: 4080.9051880836487\n",
      "  timers:\n",
      "    learn_throughput: 1555.315\n",
      "    learn_time_ms: 642.957\n",
      "    load_throughput: 62080.631\n",
      "    load_time_ms: 16.108\n",
      "    sample_throughput: 48.355\n",
      "    sample_time_ms: 20680.259\n",
      "    update_time_ms: 2.056\n",
      "  timestamp: 1633795192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4080.91</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            451.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-00-11\n",
      "  done: false\n",
      "  episode_len_mean: 450.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 468\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9620353976885478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01192755827399919\n",
      "          policy_loss: -0.049437999228636426\n",
      "          total_loss: -0.06893666850195991\n",
      "          vf_explained_var: -0.3386099636554718\n",
      "          vf_loss: 3.7819638580711196e-05\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.37407407407406\n",
      "    ram_util_percent: 71.27407407407408\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036727679199898604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.476326785971608\n",
      "    mean_inference_ms: 1.7154733840941987\n",
      "    mean_raw_obs_processing_ms: 1.501758509102294\n",
      "  time_since_restore: 4099.983461856842\n",
      "  time_this_iter_s: 19.07827377319336\n",
      "  time_total_s: 4099.983461856842\n",
      "  timers:\n",
      "    learn_throughput: 1554.226\n",
      "    learn_time_ms: 643.407\n",
      "    load_throughput: 62352.422\n",
      "    load_time_ms: 16.038\n",
      "    sample_throughput: 48.648\n",
      "    sample_time_ms: 20555.946\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1633795211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4099.98</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 450.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 470\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8591241425938076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012349788850769549\n",
      "          policy_loss: -0.015357382016049491\n",
      "          total_loss: -0.033782269722885554\n",
      "          vf_explained_var: -0.9895790815353394\n",
      "          vf_loss: 7.951850630989712e-05\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86785714285715\n",
      "    ram_util_percent: 71.30357142857142\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367304847142041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.46891496160478\n",
      "    mean_inference_ms: 1.715559471844906\n",
      "    mean_raw_obs_processing_ms: 1.5005057119592293\n",
      "  time_since_restore: 4119.013169527054\n",
      "  time_this_iter_s: 19.029707670211792\n",
      "  time_total_s: 4119.013169527054\n",
      "  timers:\n",
      "    learn_throughput: 1554.269\n",
      "    learn_time_ms: 643.389\n",
      "    load_throughput: 62242.589\n",
      "    load_time_ms: 16.066\n",
      "    sample_throughput: 48.38\n",
      "    sample_time_ms: 20669.698\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633795230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4119.01</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 449.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 473\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9668074131011963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009599139227996902\n",
      "          policy_loss: -0.029499180366595587\n",
      "          total_loss: -0.0490721279134353\n",
      "          vf_explained_var: -0.3950995206832886\n",
      "          vf_loss: 2.76343274890678e-05\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.21785714285715\n",
      "    ram_util_percent: 71.40357142857144\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036734563507699096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.458074625789813\n",
      "    mean_inference_ms: 1.715686705239745\n",
      "    mean_raw_obs_processing_ms: 1.498793731814665\n",
      "  time_since_restore: 4139.272165060043\n",
      "  time_this_iter_s: 20.258995532989502\n",
      "  time_total_s: 4139.272165060043\n",
      "  timers:\n",
      "    learn_throughput: 1548.975\n",
      "    learn_time_ms: 645.588\n",
      "    load_throughput: 62390.542\n",
      "    load_time_ms: 16.028\n",
      "    sample_throughput: 47.688\n",
      "    sample_time_ms: 20969.473\n",
      "    update_time_ms: 2.058\n",
      "  timestamp: 1633795250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4139.27</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            449.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 448.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 475\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6505668110317655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007326594807790106\n",
      "          policy_loss: -0.05473397992965248\n",
      "          total_loss: -0.07115211563391818\n",
      "          vf_explained_var: -0.984559953212738\n",
      "          vf_loss: 3.601860656393304e-05\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.08571428571428\n",
      "    ram_util_percent: 71.50357142857142\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673703424286314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.450971583114253\n",
      "    mean_inference_ms: 1.715765934323751\n",
      "    mean_raw_obs_processing_ms: 1.4977271352386965\n",
      "  time_since_restore: 4158.507968425751\n",
      "  time_this_iter_s: 19.235803365707397\n",
      "  time_total_s: 4158.507968425751\n",
      "  timers:\n",
      "    learn_throughput: 1553.726\n",
      "    learn_time_ms: 643.614\n",
      "    load_throughput: 60117.187\n",
      "    load_time_ms: 16.634\n",
      "    sample_throughput: 51.628\n",
      "    sample_time_ms: 19369.333\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633795269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4158.51</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 447.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 477\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8726422203911675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010965560353430506\n",
      "          policy_loss: -0.026792161787549656\n",
      "          total_loss: -0.04536733025064071\n",
      "          vf_explained_var: -0.6205481886863708\n",
      "          vf_loss: 7.414971411587127e-05\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.093103448275855\n",
      "    ram_util_percent: 71.54482758620689\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673932760802801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.44391521059962\n",
      "    mean_inference_ms: 1.715841572436713\n",
      "    mean_raw_obs_processing_ms: 1.4966149753869156\n",
      "  time_since_restore: 4178.910747289658\n",
      "  time_this_iter_s: 20.40277886390686\n",
      "  time_total_s: 4178.910747289658\n",
      "  timers:\n",
      "    learn_throughput: 1551.849\n",
      "    learn_time_ms: 644.392\n",
      "    load_throughput: 60515.048\n",
      "    load_time_ms: 16.525\n",
      "    sample_throughput: 51.69\n",
      "    sample_time_ms: 19346.027\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633795290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4178.91</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            447.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 445.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 480\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.908802690770891\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012214284082175019\n",
      "          policy_loss: -0.13064831764333779\n",
      "          total_loss: -0.14961094717598625\n",
      "          vf_explained_var: -0.40630102157592773\n",
      "          vf_loss: 3.951840442621485e-05\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.35490196078431\n",
      "    ram_util_percent: 71.66862745098038\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036742174960548726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.43336815851965\n",
      "    mean_inference_ms: 1.715937161737353\n",
      "    mean_raw_obs_processing_ms: 1.4976435097631393\n",
      "  time_since_restore: 4214.433552742004\n",
      "  time_this_iter_s: 35.5228054523468\n",
      "  time_total_s: 4214.433552742004\n",
      "  timers:\n",
      "    learn_throughput: 1550.444\n",
      "    learn_time_ms: 644.976\n",
      "    load_throughput: 61188.643\n",
      "    load_time_ms: 16.343\n",
      "    sample_throughput: 48.19\n",
      "    sample_time_ms: 20751.031\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633795325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4214.43</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            445.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-02-24\n",
      "  done: false\n",
      "  episode_len_mean: 446.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 482\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8810038447380066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013066297823560764\n",
      "          policy_loss: 0.046045792433950634\n",
      "          total_loss: 0.02741672131750319\n",
      "          vf_explained_var: -0.5966601371765137\n",
      "          vf_loss: 8.909592595753363e-05\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.20000000000001\n",
      "    ram_util_percent: 71.5923076923077\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036743750175633895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.426213539779916\n",
      "    mean_inference_ms: 1.715993200489318\n",
      "    mean_raw_obs_processing_ms: 1.4983907473512357\n",
      "  time_since_restore: 4233.107218980789\n",
      "  time_this_iter_s: 18.67366623878479\n",
      "  time_total_s: 4233.107218980789\n",
      "  timers:\n",
      "    learn_throughput: 1553.184\n",
      "    learn_time_ms: 643.839\n",
      "    load_throughput: 61231.967\n",
      "    load_time_ms: 16.331\n",
      "    sample_throughput: 48.331\n",
      "    sample_time_ms: 20690.613\n",
      "    update_time_ms: 2.057\n",
      "  timestamp: 1633795344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         4233.11</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            446.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-02-43\n",
      "  done: false\n",
      "  episode_len_mean: 446.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 485\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9032908797264099\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01522631794510614\n",
      "          policy_loss: -0.030869161296221945\n",
      "          total_loss: -0.049744763142532775\n",
      "          vf_explained_var: -0.4988439679145813\n",
      "          vf_loss: 5.024745979527425e-05\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.08928571428572\n",
      "    ram_util_percent: 71.45714285714287\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036745633421227726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.41534448501336\n",
      "    mean_inference_ms: 1.7160667249315822\n",
      "    mean_raw_obs_processing_ms: 1.499519407445086\n",
      "  time_since_restore: 4252.256071805954\n",
      "  time_this_iter_s: 19.148852825164795\n",
      "  time_total_s: 4252.256071805954\n",
      "  timers:\n",
      "    learn_throughput: 1552.69\n",
      "    learn_time_ms: 644.044\n",
      "    load_throughput: 61106.807\n",
      "    load_time_ms: 16.365\n",
      "    sample_throughput: 48.61\n",
      "    sample_time_ms: 20572.074\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633795363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         4252.26</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            446.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-03-03\n",
      "  done: false\n",
      "  episode_len_mean: 445.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 487\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.957079197300805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012715363089695755\n",
      "          policy_loss: 0.09060555953118536\n",
      "          total_loss: 0.07117613212515911\n",
      "          vf_explained_var: -0.204570010304451\n",
      "          vf_loss: 5.195948294082579e-05\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.98571428571428\n",
      "    ram_util_percent: 71.39285714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036746603097762134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.40825112826799\n",
      "    mean_inference_ms: 1.7161077182223614\n",
      "    mean_raw_obs_processing_ms: 1.500332830411167\n",
      "  time_since_restore: 4271.823200464249\n",
      "  time_this_iter_s: 19.567128658294678\n",
      "  time_total_s: 4271.823200464249\n",
      "  timers:\n",
      "    learn_throughput: 1556.25\n",
      "    learn_time_ms: 642.57\n",
      "    load_throughput: 60384.365\n",
      "    load_time_ms: 16.561\n",
      "    sample_throughput: 48.872\n",
      "    sample_time_ms: 20461.41\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633795383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4271.82</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            445.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 444.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 489\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8512922949261135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011847801668042064\n",
      "          policy_loss: -0.02051788709229893\n",
      "          total_loss: -0.03886494098438157\n",
      "          vf_explained_var: -0.26549702882766724\n",
      "          vf_loss: 8.256656777424116e-05\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.34814814814814\n",
      "    ram_util_percent: 71.28888888888889\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674749804150146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.401165959326537\n",
      "    mean_inference_ms: 1.7161473450503328\n",
      "    mean_raw_obs_processing_ms: 1.500133743154587\n",
      "  time_since_restore: 4290.762860059738\n",
      "  time_this_iter_s: 18.939659595489502\n",
      "  time_total_s: 4290.762860059738\n",
      "  timers:\n",
      "    learn_throughput: 1561.827\n",
      "    learn_time_ms: 640.276\n",
      "    load_throughput: 61182.127\n",
      "    load_time_ms: 16.345\n",
      "    sample_throughput: 49.204\n",
      "    sample_time_ms: 20323.631\n",
      "    update_time_ms: 2.041\n",
      "  timestamp: 1633795402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4290.76</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            444.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 443.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 492\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9618671668900385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014527583903066018\n",
      "          policy_loss: -0.09666345096710656\n",
      "          total_loss: -0.11609283280041482\n",
      "          vf_explained_var: -0.6560520529747009\n",
      "          vf_loss: 8.714151196424307e-05\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.26153846153846\n",
      "    ram_util_percent: 71.26923076923077\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036748853157501954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.39053327228234\n",
      "    mean_inference_ms: 1.716204698006635\n",
      "    mean_raw_obs_processing_ms: 1.4982862552647456\n",
      "  time_since_restore: 4309.162706851959\n",
      "  time_this_iter_s: 18.39984679222107\n",
      "  time_total_s: 4309.162706851959\n",
      "  timers:\n",
      "    learn_throughput: 1555.476\n",
      "    learn_time_ms: 642.89\n",
      "    load_throughput: 62866.342\n",
      "    load_time_ms: 15.907\n",
      "    sample_throughput: 49.374\n",
      "    sample_time_ms: 20253.61\n",
      "    update_time_ms: 2.043\n",
      "  timestamp: 1633795420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4309.16</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            443.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-03-59\n",
      "  done: false\n",
      "  episode_len_mean: 442.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 494\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.925492795308431\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01478767075092959\n",
      "          policy_loss: -0.06537467887004217\n",
      "          total_loss: -0.08438829324311681\n",
      "          vf_explained_var: -0.5453851819038391\n",
      "          vf_loss: 0.00013733800644533605\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.955555555555556\n",
      "    ram_util_percent: 71.28148148148149\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036749704512935626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.38421413942359\n",
      "    mean_inference_ms: 1.7162416519600436\n",
      "    mean_raw_obs_processing_ms: 1.4972188653305696\n",
      "  time_since_restore: 4327.924647808075\n",
      "  time_this_iter_s: 18.761940956115723\n",
      "  time_total_s: 4327.924647808075\n",
      "  timers:\n",
      "    learn_throughput: 1559.82\n",
      "    learn_time_ms: 641.099\n",
      "    load_throughput: 62461.62\n",
      "    load_time_ms: 16.01\n",
      "    sample_throughput: 49.435\n",
      "    sample_time_ms: 20228.534\n",
      "    update_time_ms: 2.037\n",
      "  timestamp: 1633795439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         4327.92</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            442.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 439.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 496\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9356845604048836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01269431132127118\n",
      "          policy_loss: -0.06970117050740454\n",
      "          total_loss: -0.0889098082565599\n",
      "          vf_explained_var: -0.828281819820404\n",
      "          vf_loss: 5.895011009771325e-05\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.38928571428572\n",
      "    ram_util_percent: 71.37142857142858\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675051517994606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.378193476430138\n",
      "    mean_inference_ms: 1.7162774910900993\n",
      "    mean_raw_obs_processing_ms: 1.496193860319188\n",
      "  time_since_restore: 4347.78239274025\n",
      "  time_this_iter_s: 19.857744932174683\n",
      "  time_total_s: 4347.78239274025\n",
      "  timers:\n",
      "    learn_throughput: 1563.898\n",
      "    learn_time_ms: 639.428\n",
      "    load_throughput: 58652.571\n",
      "    load_time_ms: 17.05\n",
      "    sample_throughput: 49.532\n",
      "    sample_time_ms: 20189.039\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633795459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         4347.78</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            439.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 440.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 499\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.007031250000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.71362028254403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0262111921348394\n",
      "          policy_loss: -0.06246107150283125\n",
      "          total_loss: -0.07933907645444076\n",
      "          vf_explained_var: -0.276409387588501\n",
      "          vf_loss: 7.389838000461976e-05\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.44137931034482\n",
      "    ram_util_percent: 71.42758620689655\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036751792972598125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.369171637808325\n",
      "    mean_inference_ms: 1.7163301895627692\n",
      "    mean_raw_obs_processing_ms: 1.494717060573619\n",
      "  time_since_restore: 4368.070283651352\n",
      "  time_this_iter_s: 20.287890911102295\n",
      "  time_total_s: 4368.070283651352\n",
      "  timers:\n",
      "    learn_throughput: 1563.079\n",
      "    learn_time_ms: 639.763\n",
      "    load_throughput: 58908.019\n",
      "    load_time_ms: 16.976\n",
      "    sample_throughput: 49.276\n",
      "    sample_time_ms: 20294.014\n",
      "    update_time_ms: 2.036\n",
      "  timestamp: 1633795479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         4368.07</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            440.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 438.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 501\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.010546874999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9530021945635478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.041150077852422424\n",
      "          policy_loss: 0.09366988895667924\n",
      "          total_loss: 0.07463632623354594\n",
      "          vf_explained_var: 0.055012691766023636\n",
      "          vf_loss: 6.245384582952183e-05\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.285185185185185\n",
      "    ram_util_percent: 71.53333333333332\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675268199012599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.363458735634904\n",
      "    mean_inference_ms: 1.7163645320400895\n",
      "    mean_raw_obs_processing_ms: 1.4937732116366091\n",
      "  time_since_restore: 4386.547934770584\n",
      "  time_this_iter_s: 18.477651119232178\n",
      "  time_total_s: 4386.547934770584\n",
      "  timers:\n",
      "    learn_throughput: 1560.499\n",
      "    learn_time_ms: 640.821\n",
      "    load_throughput: 58591.694\n",
      "    load_time_ms: 17.067\n",
      "    sample_throughput: 49.75\n",
      "    sample_time_ms: 20100.362\n",
      "    update_time_ms: 2.012\n",
      "  timestamp: 1633795498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         4386.55</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            438.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 439.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 504\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.908736448817783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013632740585534325\n",
      "          policy_loss: -0.04407530828482575\n",
      "          total_loss: -0.0628822839508454\n",
      "          vf_explained_var: -0.4630299508571625\n",
      "          vf_loss: 6.471304792891412e-05\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.957142857142856\n",
      "    ram_util_percent: 71.61785714285712\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036753912218708086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.35468259114783\n",
      "    mean_inference_ms: 1.716412274738911\n",
      "    mean_raw_obs_processing_ms: 1.4924160044042\n",
      "  time_since_restore: 4406.158056735992\n",
      "  time_this_iter_s: 19.610121965408325\n",
      "  time_total_s: 4406.158056735992\n",
      "  timers:\n",
      "    learn_throughput: 1560.529\n",
      "    learn_time_ms: 640.808\n",
      "    load_throughput: 57720.865\n",
      "    load_time_ms: 17.325\n",
      "    sample_throughput: 54.028\n",
      "    sample_time_ms: 18508.862\n",
      "    update_time_ms: 2.014\n",
      "  timestamp: 1633795517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         4406.16</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            439.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-05-35\n",
      "  done: false\n",
      "  episode_len_mean: 438.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 506\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.824567511346605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013725137726524405\n",
      "          policy_loss: -0.020400709576076932\n",
      "          total_loss: -0.03834414436585373\n",
      "          vf_explained_var: -0.7310506105422974\n",
      "          vf_loss: 8.510387762928278e-05\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.04\n",
      "    ram_util_percent: 71.676\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036754652867662906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.348968928982227\n",
      "    mean_inference_ms: 1.7164417504381857\n",
      "    mean_raw_obs_processing_ms: 1.4915508750345612\n",
      "  time_since_restore: 4423.962689399719\n",
      "  time_this_iter_s: 17.804632663726807\n",
      "  time_total_s: 4423.962689399719\n",
      "  timers:\n",
      "    learn_throughput: 1559.349\n",
      "    learn_time_ms: 641.293\n",
      "    load_throughput: 56902.781\n",
      "    load_time_ms: 17.574\n",
      "    sample_throughput: 54.285\n",
      "    sample_time_ms: 18421.209\n",
      "    update_time_ms: 2.022\n",
      "  timestamp: 1633795535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         4423.96</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             438.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 440.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 508\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8376974238289727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013792689425340784\n",
      "          policy_loss: -0.032322968294223146\n",
      "          total_loss: -0.05040117700894674\n",
      "          vf_explained_var: -0.6159078478813171\n",
      "          vf_loss: 8.055993550644618e-05\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.525\n",
      "    ram_util_percent: 71.75833333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036755336293480705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.342979209653006\n",
      "    mean_inference_ms: 1.716468616840912\n",
      "    mean_raw_obs_processing_ms: 1.4907217484564972\n",
      "  time_since_restore: 4440.691944122314\n",
      "  time_this_iter_s: 16.729254722595215\n",
      "  time_total_s: 4440.691944122314\n",
      "  timers:\n",
      "    learn_throughput: 1562.278\n",
      "    learn_time_ms: 640.091\n",
      "    load_throughput: 60428.211\n",
      "    load_time_ms: 16.549\n",
      "    sample_throughput: 55.001\n",
      "    sample_time_ms: 18181.479\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1633795552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         4440.69</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            440.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-06-31\n",
      "  done: false\n",
      "  episode_len_mean: 437.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 511\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8281964911354913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010926332658646625\n",
      "          policy_loss: 0.032652597253521286\n",
      "          total_loss: 0.014651364212234815\n",
      "          vf_explained_var: -0.9475390911102295\n",
      "          vf_loss: 0.00010787382117188018\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.275\n",
      "    ram_util_percent: 71.79285714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675635276715308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.33506510999074\n",
      "    mean_inference_ms: 1.716507796457512\n",
      "    mean_raw_obs_processing_ms: 1.4920498117799585\n",
      "  time_since_restore: 4479.626378774643\n",
      "  time_this_iter_s: 38.93443465232849\n",
      "  time_total_s: 4479.626378774643\n",
      "  timers:\n",
      "    learn_throughput: 1563.116\n",
      "    learn_time_ms: 639.748\n",
      "    load_throughput: 60096.428\n",
      "    load_time_ms: 16.64\n",
      "    sample_throughput: 49.706\n",
      "    sample_time_ms: 20118.437\n",
      "    update_time_ms: 2.027\n",
      "  timestamp: 1633795591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         4479.63</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            437.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-06-46\n",
      "  done: false\n",
      "  episode_len_mean: 436.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 513\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.899515328142378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013533151925082334\n",
      "          policy_loss: -0.026091988301939435\n",
      "          total_loss: -0.0447930588076512\n",
      "          vf_explained_var: -0.028051989153027534\n",
      "          vf_loss: 7.998727077291631e-05\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.71904761904763\n",
      "    ram_util_percent: 71.72380952380952\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036757029384883406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.329913556861385\n",
      "    mean_inference_ms: 1.7165348904574569\n",
      "    mean_raw_obs_processing_ms: 1.4929887512028275\n",
      "  time_since_restore: 4494.798829078674\n",
      "  time_this_iter_s: 15.172450304031372\n",
      "  time_total_s: 4494.798829078674\n",
      "  timers:\n",
      "    learn_throughput: 1562.335\n",
      "    learn_time_ms: 640.067\n",
      "    load_throughput: 65177.321\n",
      "    load_time_ms: 15.343\n",
      "    sample_throughput: 50.652\n",
      "    sample_time_ms: 19742.712\n",
      "    update_time_ms: 2.037\n",
      "  timestamp: 1633795606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">          4494.8</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             436.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 435.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 515\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6737702581617566\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016867509069497987\n",
      "          policy_loss: -0.092787891253829\n",
      "          total_loss: -0.1088213471074899\n",
      "          vf_explained_var: -0.011959163472056389\n",
      "          vf_loss: 0.0004373979337591057\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68260869565218\n",
      "    ram_util_percent: 71.6304347826087\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675772274258567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.324919163962186\n",
      "    mean_inference_ms: 1.7165624907703005\n",
      "    mean_raw_obs_processing_ms: 1.4939537657473099\n",
      "  time_since_restore: 4510.847069740295\n",
      "  time_this_iter_s: 16.048240661621094\n",
      "  time_total_s: 4510.847069740295\n",
      "  timers:\n",
      "    learn_throughput: 1565.289\n",
      "    learn_time_ms: 638.86\n",
      "    load_throughput: 67349.166\n",
      "    load_time_ms: 14.848\n",
      "    sample_throughput: 51.258\n",
      "    sample_time_ms: 19509.271\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1633795622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         4510.85</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            435.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 433.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 517\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7782210999064976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013125885712024714\n",
      "          policy_loss: -0.06261317332585653\n",
      "          total_loss: -0.07989625169171227\n",
      "          vf_explained_var: -0.3616238534450531\n",
      "          vf_loss: 0.0002914754394926907\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.97777777777778\n",
      "    ram_util_percent: 71.30740740740741\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675837896251227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.320193138976258\n",
      "    mean_inference_ms: 1.7165879021813897\n",
      "    mean_raw_obs_processing_ms: 1.4949441017848613\n",
      "  time_since_restore: 4529.8370769023895\n",
      "  time_this_iter_s: 18.990007162094116\n",
      "  time_total_s: 4529.8370769023895\n",
      "  timers:\n",
      "    learn_throughput: 1561.158\n",
      "    learn_time_ms: 640.55\n",
      "    load_throughput: 66794.343\n",
      "    load_time_ms: 14.971\n",
      "    sample_throughput: 51.203\n",
      "    sample_time_ms: 19530.25\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1633795641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         4529.84</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            433.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 432.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 519\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0012553228272334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01489411732658449\n",
      "          policy_loss: -0.05782570911364423\n",
      "          total_loss: -0.07752601237346729\n",
      "          vf_explained_var: -0.6579868197441101\n",
      "          vf_loss: 7.66209567599516e-05\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.729166666666664\n",
      "    ram_util_percent: 71.10833333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675892996296084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.315322285753535\n",
      "    mean_inference_ms: 1.7166093277890873\n",
      "    mean_raw_obs_processing_ms: 1.494985701716887\n",
      "  time_since_restore: 4546.277937412262\n",
      "  time_this_iter_s: 16.440860509872437\n",
      "  time_total_s: 4546.277937412262\n",
      "  timers:\n",
      "    learn_throughput: 1561.14\n",
      "    learn_time_ms: 640.557\n",
      "    load_throughput: 73867.96\n",
      "    load_time_ms: 13.538\n",
      "    sample_throughput: 52.111\n",
      "    sample_time_ms: 19189.973\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633795658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         4546.28</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             432.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-07-59\n",
      "  done: false\n",
      "  episode_len_mean: 432.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 522\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.935628314812978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013739181160423422\n",
      "          policy_loss: -0.0329279338940978\n",
      "          total_loss: -0.0519528969294495\n",
      "          vf_explained_var: -0.8538214564323425\n",
      "          vf_loss: 0.0001139624515619491\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.809999999999995\n",
      "    ram_util_percent: 71.04666666666665\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036759712073412325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.3083051306043\n",
      "    mean_inference_ms: 1.7166404827230022\n",
      "    mean_raw_obs_processing_ms: 1.4936777503197436\n",
      "  time_since_restore: 4567.298398017883\n",
      "  time_this_iter_s: 21.020460605621338\n",
      "  time_total_s: 4567.298398017883\n",
      "  timers:\n",
      "    learn_throughput: 1561.813\n",
      "    learn_time_ms: 640.281\n",
      "    load_throughput: 73168.376\n",
      "    load_time_ms: 13.667\n",
      "    sample_throughput: 51.912\n",
      "    sample_time_ms: 19263.386\n",
      "    update_time_ms: 2.033\n",
      "  timestamp: 1633795679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          4567.3</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            432.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-08-17\n",
      "  done: false\n",
      "  episode_len_mean: 430.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 524\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.990524819162157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011085138549099585\n",
      "          policy_loss: -0.030693195511897406\n",
      "          total_loss: -0.05036857809043593\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 5.449542889740163e-05\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.91923076923077\n",
      "    ram_util_percent: 71.11923076923077\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676021152724682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.30389365572328\n",
      "    mean_inference_ms: 1.7166604822973295\n",
      "    mean_raw_obs_processing_ms: 1.4928938704234553\n",
      "  time_since_restore: 4585.5517592430115\n",
      "  time_this_iter_s: 18.253361225128174\n",
      "  time_total_s: 4585.5517592430115\n",
      "  timers:\n",
      "    learn_throughput: 1567.321\n",
      "    learn_time_ms: 638.031\n",
      "    load_throughput: 72982.495\n",
      "    load_time_ms: 13.702\n",
      "    sample_throughput: 51.967\n",
      "    sample_time_ms: 19243.166\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633795697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         4585.55</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            430.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 429.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 526\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8947503023677401\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012881879474905588\n",
      "          policy_loss: -0.019387205503880976\n",
      "          total_loss: -0.03806681192169587\n",
      "          vf_explained_var: 0.001984884263947606\n",
      "          vf_loss: 6.410322215136452e-05\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.896428571428565\n",
      "    ram_util_percent: 71.18214285714286\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676064708545367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.299752683108817\n",
      "    mean_inference_ms: 1.7166791952205904\n",
      "    mean_raw_obs_processing_ms: 1.4921453934998397\n",
      "  time_since_restore: 4605.078731060028\n",
      "  time_this_iter_s: 19.5269718170166\n",
      "  time_total_s: 4605.078731060028\n",
      "  timers:\n",
      "    learn_throughput: 1568.793\n",
      "    learn_time_ms: 637.433\n",
      "    load_throughput: 71570.757\n",
      "    load_time_ms: 13.972\n",
      "    sample_throughput: 51.988\n",
      "    sample_time_ms: 19235.176\n",
      "    update_time_ms: 2.044\n",
      "  timestamp: 1633795716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         4605.08</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            429.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 428.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 529\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.625099515914917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010200033994875766\n",
      "          policy_loss: 0.022160373917884297\n",
      "          total_loss: 0.00616724801560243\n",
      "          vf_explained_var: -0.7567110061645508\n",
      "          vf_loss: 9.650202199635613e-05\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.775\n",
      "    ram_util_percent: 71.24285714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676126750444837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.2935533054979\n",
      "    mean_inference_ms: 1.7167068654398039\n",
      "    mean_raw_obs_processing_ms: 1.4910727751008706\n",
      "  time_since_restore: 4624.960558414459\n",
      "  time_this_iter_s: 19.881827354431152\n",
      "  time_total_s: 4624.960558414459\n",
      "  timers:\n",
      "    learn_throughput: 1570.228\n",
      "    learn_time_ms: 636.85\n",
      "    load_throughput: 72093.092\n",
      "    load_time_ms: 13.871\n",
      "    sample_throughput: 51.431\n",
      "    sample_time_ms: 19443.586\n",
      "    update_time_ms: 2.046\n",
      "  timestamp: 1633795736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         4624.96</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            428.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 428.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 531\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8308948742018805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011424716259657231\n",
      "          policy_loss: -0.045813803974952966\n",
      "          total_loss: -0.06380494141744243\n",
      "          vf_explained_var: -0.44819504022598267\n",
      "          vf_loss: 0.0001370710246394285\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.78461538461538\n",
      "    ram_util_percent: 71.33846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676167251138194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.2894972412186\n",
      "    mean_inference_ms: 1.7167248658488312\n",
      "    mean_raw_obs_processing_ms: 1.4903912311771876\n",
      "  time_since_restore: 4642.937178611755\n",
      "  time_this_iter_s: 17.976620197296143\n",
      "  time_total_s: 4642.937178611755\n",
      "  timers:\n",
      "    learn_throughput: 1569.346\n",
      "    learn_time_ms: 637.208\n",
      "    load_throughput: 64681.198\n",
      "    load_time_ms: 15.46\n",
      "    sample_throughput: 51.108\n",
      "    sample_time_ms: 19566.376\n",
      "    update_time_ms: 2.041\n",
      "  timestamp: 1633795754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         4642.94</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            428.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-09-32\n",
      "  done: false\n",
      "  episode_len_mean: 427.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 533\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6862361431121826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01477821370933964\n",
      "          policy_loss: -0.06493104952904913\n",
      "          total_loss: -0.08139310379823049\n",
      "          vf_explained_var: -0.2737819254398346\n",
      "          vf_loss: 0.00016651137092493526\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.88\n",
      "    ram_util_percent: 71.38000000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676208289411681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.285568756415554\n",
      "    mean_inference_ms: 1.7167425106468728\n",
      "    mean_raw_obs_processing_ms: 1.4897422760559451\n",
      "  time_since_restore: 4660.349852323532\n",
      "  time_this_iter_s: 17.412673711776733\n",
      "  time_total_s: 4660.349852323532\n",
      "  timers:\n",
      "    learn_throughput: 1569.806\n",
      "    learn_time_ms: 637.021\n",
      "    load_throughput: 67089.971\n",
      "    load_time_ms: 14.905\n",
      "    sample_throughput: 57.422\n",
      "    sample_time_ms: 17414.945\n",
      "    update_time_ms: 2.049\n",
      "  timestamp: 1633795772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         4660.35</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            427.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 427.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 535\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9686066296365525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010414632139687423\n",
      "          policy_loss: -0.032650830017195806\n",
      "          total_loss: -0.05211217651764552\n",
      "          vf_explained_var: -0.3459118604660034\n",
      "          vf_loss: 5.995550994460549e-05\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.743478260869566\n",
      "    ram_util_percent: 71.40434782608699\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676243398455734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.28165337004399\n",
      "    mean_inference_ms: 1.7167596901534166\n",
      "    mean_raw_obs_processing_ms: 1.4891254512394578\n",
      "  time_since_restore: 4676.953391075134\n",
      "  time_this_iter_s: 16.603538751602173\n",
      "  time_total_s: 4676.953391075134\n",
      "  timers:\n",
      "    learn_throughput: 1572.49\n",
      "    learn_time_ms: 635.934\n",
      "    load_throughput: 65283.638\n",
      "    load_time_ms: 15.318\n",
      "    sample_throughput: 56.952\n",
      "    sample_time_ms: 17558.709\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633795788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         4676.95</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            427.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-10-07\n",
      "  done: false\n",
      "  episode_len_mean: 426.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 538\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9995272623168097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014543055886723583\n",
      "          policy_loss: -0.055380827519628736\n",
      "          total_loss: -0.07505179146925609\n",
      "          vf_explained_var: -0.8462936282157898\n",
      "          vf_loss: 9.423310200670838e-05\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8962962962963\n",
      "    ram_util_percent: 71.5074074074074\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676296238407324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.276067123497775\n",
      "    mean_inference_ms: 1.716785732490244\n",
      "    mean_raw_obs_processing_ms: 1.488325548752749\n",
      "  time_since_restore: 4695.8018271923065\n",
      "  time_this_iter_s: 18.84843611717224\n",
      "  time_total_s: 4695.8018271923065\n",
      "  timers:\n",
      "    learn_throughput: 1574.088\n",
      "    learn_time_ms: 635.289\n",
      "    load_throughput: 60882.485\n",
      "    load_time_ms: 16.425\n",
      "    sample_throughput: 56.059\n",
      "    sample_time_ms: 17838.262\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633795807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">          4695.8</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            426.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 426.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 540\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7686997916963365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014248342771627698\n",
      "          policy_loss: 0.017180033152302106\n",
      "          total_loss: -0.00013157942642768225\n",
      "          vf_explained_var: -0.3619594871997833\n",
      "          vf_loss: 0.00014997643148591226\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.989795918367346\n",
      "    ram_util_percent: 71.60612244897959\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676326958112511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.272257067681913\n",
      "    mean_inference_ms: 1.7168026550005948\n",
      "    mean_raw_obs_processing_ms: 1.4893352841754106\n",
      "  time_since_restore: 4729.954324960709\n",
      "  time_this_iter_s: 34.1524977684021\n",
      "  time_total_s: 4729.954324960709\n",
      "  timers:\n",
      "    learn_throughput: 1574.36\n",
      "    learn_time_ms: 635.179\n",
      "    load_throughput: 61228.928\n",
      "    load_time_ms: 16.332\n",
      "    sample_throughput: 51.667\n",
      "    sample_time_ms: 19354.688\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633795841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         4729.95</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            426.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 428.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 542\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6837827801704406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013211126697478745\n",
      "          policy_loss: -0.0749515804151694\n",
      "          total_loss: -0.0912856554819478\n",
      "          vf_explained_var: -0.5007671117782593\n",
      "          vf_loss: 0.00029475263808207173\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.27777777777778\n",
      "    ram_util_percent: 71.64074074074074\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676357007695347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.268358246381368\n",
      "    mean_inference_ms: 1.7168189455700358\n",
      "    mean_raw_obs_processing_ms: 1.4903687452479937\n",
      "  time_since_restore: 4748.63445019722\n",
      "  time_this_iter_s: 18.68012523651123\n",
      "  time_total_s: 4748.63445019722\n",
      "  timers:\n",
      "    learn_throughput: 1573.014\n",
      "    learn_time_ms: 635.722\n",
      "    load_throughput: 55788.45\n",
      "    load_time_ms: 17.925\n",
      "    sample_throughput: 51.082\n",
      "    sample_time_ms: 19576.527\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633795860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         4748.63</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            428.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 429.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 544\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6586836046642728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012308723102367056\n",
      "          policy_loss: -0.04938880507316854\n",
      "          total_loss: -0.06572578003009161\n",
      "          vf_explained_var: -0.5495082139968872\n",
      "          vf_loss: 5.5131603625745306e-05\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77692307692307\n",
      "    ram_util_percent: 71.75769230769231\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676386755351813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.264307603961495\n",
      "    mean_inference_ms: 1.716834860026771\n",
      "    mean_raw_obs_processing_ms: 1.4913518841645856\n",
      "  time_since_restore: 4766.878089427948\n",
      "  time_this_iter_s: 18.24363923072815\n",
      "  time_total_s: 4766.878089427948\n",
      "  timers:\n",
      "    learn_throughput: 1573.153\n",
      "    learn_time_ms: 635.666\n",
      "    load_throughput: 55529.424\n",
      "    load_time_ms: 18.008\n",
      "    sample_throughput: 51.817\n",
      "    sample_time_ms: 19298.804\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633795878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         4766.88</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            429.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 429.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 546\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7198659817377726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011945381975273487\n",
      "          policy_loss: -0.05836928677227762\n",
      "          total_loss: -0.0752888101670477\n",
      "          vf_explained_var: -0.10501791536808014\n",
      "          vf_loss: 9.015772674400877e-05\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8423076923077\n",
      "    ram_util_percent: 71.56153846153846\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676413519046837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.26037499765156\n",
      "    mean_inference_ms: 1.7168501250598878\n",
      "    mean_raw_obs_processing_ms: 1.4923586316569504\n",
      "  time_since_restore: 4785.425929069519\n",
      "  time_this_iter_s: 18.547839641571045\n",
      "  time_total_s: 4785.425929069519\n",
      "  timers:\n",
      "    learn_throughput: 1574.13\n",
      "    learn_time_ms: 635.272\n",
      "    load_throughput: 55582.333\n",
      "    load_time_ms: 17.991\n",
      "    sample_throughput: 51.737\n",
      "    sample_time_ms: 19328.669\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1633795897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         4785.43</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            429.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-11-54\n",
      "  done: false\n",
      "  episode_len_mean: 428.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 548\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8481740262773303\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007091720365131475\n",
      "          policy_loss: -0.10759815014898777\n",
      "          total_loss: -0.12590023221241103\n",
      "          vf_explained_var: -0.600493848323822\n",
      "          vf_loss: 6.746653299261298e-05\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.523999999999994\n",
      "    ram_util_percent: 71.29599999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676438445982532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.256484799365964\n",
      "    mean_inference_ms: 1.7168645693287494\n",
      "    mean_raw_obs_processing_ms: 1.4933879149432983\n",
      "  time_since_restore: 4802.320522069931\n",
      "  time_this_iter_s: 16.894593000411987\n",
      "  time_total_s: 4802.320522069931\n",
      "  timers:\n",
      "    learn_throughput: 1573.421\n",
      "    learn_time_ms: 635.558\n",
      "    load_throughput: 60571.853\n",
      "    load_time_ms: 16.509\n",
      "    sample_throughput: 52.448\n",
      "    sample_time_ms: 19066.622\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633795914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         4802.32</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            428.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 430.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 550\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.683923966354794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012372305996344016\n",
      "          policy_loss: -0.06641222101946671\n",
      "          total_loss: -0.08294580082098643\n",
      "          vf_explained_var: -0.6006527543067932\n",
      "          vf_loss: 0.00010992533199088131\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.044\n",
      "    ram_util_percent: 71.22800000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676457108173962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.25256680269165\n",
      "    mean_inference_ms: 1.7168769344408568\n",
      "    mean_raw_obs_processing_ms: 1.4926399592343949\n",
      "  time_since_restore: 4819.954313516617\n",
      "  time_this_iter_s: 17.63379144668579\n",
      "  time_total_s: 4819.954313516617\n",
      "  timers:\n",
      "    learn_throughput: 1573.076\n",
      "    learn_time_ms: 635.697\n",
      "    load_throughput: 65012.044\n",
      "    load_time_ms: 15.382\n",
      "    sample_throughput: 53.071\n",
      "    sample_time_ms: 18842.797\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633795932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         4819.95</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            430.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-12-29\n",
      "  done: false\n",
      "  episode_len_mean: 431.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 552\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8264898419380189\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014963401044627586\n",
      "          policy_loss: 0.032465206252204046\n",
      "          total_loss: 0.01449615690443251\n",
      "          vf_explained_var: -0.08694285899400711\n",
      "          vf_loss: 5.911979747502806e-05\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.711999999999996\n",
      "    ram_util_percent: 71.22800000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367646896606029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.248401336501043\n",
      "    mean_inference_ms: 1.7168871859071726\n",
      "    mean_raw_obs_processing_ms: 1.4919203461075181\n",
      "  time_since_restore: 4837.654123306274\n",
      "  time_this_iter_s: 17.699809789657593\n",
      "  time_total_s: 4837.654123306274\n",
      "  timers:\n",
      "    learn_throughput: 1571.143\n",
      "    learn_time_ms: 636.479\n",
      "    load_throughput: 66838.409\n",
      "    load_time_ms: 14.961\n",
      "    sample_throughput: 53.15\n",
      "    sample_time_ms: 18814.747\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633795949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         4837.65</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            431.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 432.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 554\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9843426439497205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019343838680107143\n",
      "          policy_loss: 0.037546875948707266\n",
      "          total_loss: 0.018110471467177074\n",
      "          vf_explained_var: -0.3980475962162018\n",
      "          vf_loss: 0.00010099571211160057\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.684\n",
      "    ram_util_percent: 71.252\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764786611558346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.243957189266954\n",
      "    mean_inference_ms: 1.71689681020737\n",
      "    mean_raw_obs_processing_ms: 1.4911588125199273\n",
      "  time_since_restore: 4855.089046955109\n",
      "  time_this_iter_s: 17.43492364883423\n",
      "  time_total_s: 4855.089046955109\n",
      "  timers:\n",
      "    learn_throughput: 1568.794\n",
      "    learn_time_ms: 637.432\n",
      "    load_throughput: 69226.417\n",
      "    load_time_ms: 14.445\n",
      "    sample_throughput: 53.145\n",
      "    sample_time_ms: 18816.54\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633795967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         4855.09</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            432.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 435.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 556\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.959283471107483\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01198246124267866\n",
      "          policy_loss: 0.003483214146561093\n",
      "          total_loss: -0.015784028006924522\n",
      "          vf_explained_var: -0.6542240381240845\n",
      "          vf_loss: 0.0001360283668165923\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52916666666667\n",
      "    ram_util_percent: 71.31666666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676486995281924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.239223371915813\n",
      "    mean_inference_ms: 1.7169059251016596\n",
      "    mean_raw_obs_processing_ms: 1.490427036217643\n",
      "  time_since_restore: 4871.969727516174\n",
      "  time_this_iter_s: 16.880680561065674\n",
      "  time_total_s: 4871.969727516174\n",
      "  timers:\n",
      "    learn_throughput: 1565.895\n",
      "    learn_time_ms: 638.613\n",
      "    load_throughput: 69302.825\n",
      "    load_time_ms: 14.429\n",
      "    sample_throughput: 53.07\n",
      "    sample_time_ms: 18843.107\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1633795984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         4871.97</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            435.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 438.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 558\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6633772015571595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014409007967616677\n",
      "          policy_loss: -0.10240708059734768\n",
      "          total_loss: -0.11870223585930136\n",
      "          vf_explained_var: -0.04194872826337814\n",
      "          vf_loss: 0.00011065762494884741\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.85909090909091\n",
      "    ram_util_percent: 71.37727272727275\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764936150088454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.23412976857484\n",
      "    mean_inference_ms: 1.7169143856395703\n",
      "    mean_raw_obs_processing_ms: 1.489652710966051\n",
      "  time_since_restore: 4887.015229940414\n",
      "  time_this_iter_s: 15.045502424240112\n",
      "  time_total_s: 4887.015229940414\n",
      "  timers:\n",
      "    learn_throughput: 1571.487\n",
      "    learn_time_ms: 636.34\n",
      "    load_throughput: 77224.537\n",
      "    load_time_ms: 12.949\n",
      "    sample_throughput: 54.152\n",
      "    sample_time_ms: 18466.552\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633795999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         4887.02</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            438.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 440.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 560\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8211749500698513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01135435053430967\n",
      "          policy_loss: -0.03158805337217119\n",
      "          total_loss: -0.049454546595613165\n",
      "          vf_explained_var: -0.1791807860136032\n",
      "          vf_loss: 0.0001656276557898511\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.728\n",
      "    ram_util_percent: 71.416\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764938187409724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.22884185906055\n",
      "    mean_inference_ms: 1.7169214293463244\n",
      "    mean_raw_obs_processing_ms: 1.4889099786903484\n",
      "  time_since_restore: 4904.924268722534\n",
      "  time_this_iter_s: 17.90903878211975\n",
      "  time_total_s: 4904.924268722534\n",
      "  timers:\n",
      "    learn_throughput: 1570.224\n",
      "    learn_time_ms: 636.852\n",
      "    load_throughput: 77574.179\n",
      "    load_time_ms: 12.891\n",
      "    sample_throughput: 59.376\n",
      "    sample_time_ms: 16841.792\n",
      "    update_time_ms: 2.057\n",
      "  timestamp: 1633796017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         4904.92</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            440.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-13-54\n",
      "  done: false\n",
      "  episode_len_mean: 443.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 562\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8964116507106357\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010179654798137743\n",
      "          policy_loss: -0.04079553716712528\n",
      "          total_loss: -0.059486648150616224\n",
      "          vf_explained_var: -0.7886021733283997\n",
      "          vf_loss: 0.00011195928859201054\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.833333333333336\n",
      "    ram_util_percent: 71.45416666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676483309993189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.223258167109442\n",
      "    mean_inference_ms: 1.7169270134324277\n",
      "    mean_raw_obs_processing_ms: 1.4881948511176804\n",
      "  time_since_restore: 4921.973402500153\n",
      "  time_this_iter_s: 17.049133777618408\n",
      "  time_total_s: 4921.973402500153\n",
      "  timers:\n",
      "    learn_throughput: 1574.453\n",
      "    learn_time_ms: 635.141\n",
      "    load_throughput: 88441.899\n",
      "    load_time_ms: 11.307\n",
      "    sample_throughput: 59.945\n",
      "    sample_time_ms: 16681.958\n",
      "    update_time_ms: 2.055\n",
      "  timestamp: 1633796034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         4921.97</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            443.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 444.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 563\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0000445697042677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010301567281865716\n",
      "          policy_loss: -0.006456945091485977\n",
      "          total_loss: -0.026162286475300788\n",
      "          vf_explained_var: -0.4707740247249603\n",
      "          vf_loss: 0.0001321286986947396\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67619047619047\n",
      "    ram_util_percent: 71.54761904761902\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367647579098843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.220240829547986\n",
      "    mean_inference_ms: 1.7169293763241091\n",
      "    mean_raw_obs_processing_ms: 1.4877802940639941\n",
      "  time_since_restore: 4936.22296333313\n",
      "  time_this_iter_s: 14.249560832977295\n",
      "  time_total_s: 4936.22296333313\n",
      "  timers:\n",
      "    learn_throughput: 1575.479\n",
      "    learn_time_ms: 634.728\n",
      "    load_throughput: 102275.901\n",
      "    load_time_ms: 9.777\n",
      "    sample_throughput: 61.408\n",
      "    sample_time_ms: 16284.476\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633796048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         4936.22</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            444.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 448.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 565\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9848913894759284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011491020693791161\n",
      "          policy_loss: 0.019860043418076304\n",
      "          total_loss: 0.0002928300450245539\n",
      "          vf_explained_var: -0.831053614616394\n",
      "          vf_loss: 9.990866464148793e-05\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.762499999999996\n",
      "    ram_util_percent: 71.55416666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676460827733324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.21391188578552\n",
      "    mean_inference_ms: 1.7169337093761907\n",
      "    mean_raw_obs_processing_ms: 1.4869815423835444\n",
      "  time_since_restore: 4952.837420225143\n",
      "  time_this_iter_s: 16.61445689201355\n",
      "  time_total_s: 4952.837420225143\n",
      "  timers:\n",
      "    learn_throughput: 1575.593\n",
      "    learn_time_ms: 634.682\n",
      "    load_throughput: 114858.313\n",
      "    load_time_ms: 8.706\n",
      "    sample_throughput: 62.142\n",
      "    sample_time_ms: 16092.188\n",
      "    update_time_ms: 2.152\n",
      "  timestamp: 1633796065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         4952.84</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            448.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 450.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 567\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9888832714822557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014234767506450445\n",
      "          policy_loss: -0.0050998510585890874\n",
      "          total_loss: -0.02463931060499615\n",
      "          vf_explained_var: -0.5854783654212952\n",
      "          vf_loss: 0.00012417510442901402\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.95652173913043\n",
      "    ram_util_percent: 71.57391304347823\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676443814937033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.207382305101692\n",
      "    mean_inference_ms: 1.7169383779495706\n",
      "    mean_raw_obs_processing_ms: 1.4862094822644494\n",
      "  time_since_restore: 4968.98579287529\n",
      "  time_this_iter_s: 16.148372650146484\n",
      "  time_total_s: 4968.98579287529\n",
      "  timers:\n",
      "    learn_throughput: 1572.164\n",
      "    learn_time_ms: 636.066\n",
      "    load_throughput: 114396.557\n",
      "    load_time_ms: 8.742\n",
      "    sample_throughput: 62.437\n",
      "    sample_time_ms: 16016.132\n",
      "    update_time_ms: 2.161\n",
      "  timestamp: 1633796081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         4968.99</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 451.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 570\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.807257138358222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011858586611617422\n",
      "          policy_loss: -0.010942975514464909\n",
      "          total_loss: -0.02863820923699273\n",
      "          vf_explained_var: -0.2641127407550812\n",
      "          vf_loss: 0.0001897329207470951\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.36734693877551\n",
      "    ram_util_percent: 71.64489795918367\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367642222964615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.197555146100193\n",
      "    mean_inference_ms: 1.7169463063191264\n",
      "    mean_raw_obs_processing_ms: 1.4870339535511534\n",
      "  time_since_restore: 5003.579070329666\n",
      "  time_this_iter_s: 34.59327745437622\n",
      "  time_total_s: 5003.579070329666\n",
      "  timers:\n",
      "    learn_throughput: 1573.021\n",
      "    learn_time_ms: 635.719\n",
      "    load_throughput: 100837.705\n",
      "    load_time_ms: 9.917\n",
      "    sample_throughput: 56.461\n",
      "    sample_time_ms: 17711.275\n",
      "    update_time_ms: 2.157\n",
      "  timestamp: 1633796115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         5003.58</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             451.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 453.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 571\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9124950488408408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013398751812314636\n",
      "          policy_loss: 0.08626539980371793\n",
      "          total_loss: 0.06743445263968574\n",
      "          vf_explained_var: -0.29301780462265015\n",
      "          vf_loss: 8.202952562391551e-05\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.98799999999999\n",
      "    ram_util_percent: 71.732\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764152713332214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.194196864587227\n",
      "    mean_inference_ms: 1.7169484681203242\n",
      "    mean_raw_obs_processing_ms: 1.4873405847431211\n",
      "  time_since_restore: 5021.157093048096\n",
      "  time_this_iter_s: 17.578022718429565\n",
      "  time_total_s: 5021.157093048096\n",
      "  timers:\n",
      "    learn_throughput: 1573.512\n",
      "    learn_time_ms: 635.521\n",
      "    load_throughput: 100874.81\n",
      "    load_time_ms: 9.913\n",
      "    sample_throughput: 56.499\n",
      "    sample_time_ms: 17699.292\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1633796133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         5021.16</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 455.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 574\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.926882070965237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011953601670470625\n",
      "          policy_loss: -0.020512397297554545\n",
      "          total_loss: -0.03940101845396889\n",
      "          vf_explained_var: -0.7818788290023804\n",
      "          vf_loss: 0.00019109069891985403\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.880769230769225\n",
      "    ram_util_percent: 71.78076923076924\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676392025133564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.183986299754068\n",
      "    mean_inference_ms: 1.716955238399702\n",
      "    mean_raw_obs_processing_ms: 1.4881638030024675\n",
      "  time_since_restore: 5039.39506649971\n",
      "  time_this_iter_s: 18.23797345161438\n",
      "  time_total_s: 5039.39506649971\n",
      "  timers:\n",
      "    learn_throughput: 1574.9\n",
      "    learn_time_ms: 634.961\n",
      "    load_throughput: 91757.798\n",
      "    load_time_ms: 10.898\n",
      "    sample_throughput: 56.246\n",
      "    sample_time_ms: 17779.165\n",
      "    update_time_ms: 2.162\n",
      "  timestamp: 1633796151\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">          5039.4</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 454.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 576\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6141456524531046\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011919216662088615\n",
      "          policy_loss: -0.027601496875286104\n",
      "          total_loss: -0.04319681508673562\n",
      "          vf_explained_var: 0.3551444113254547\n",
      "          vf_loss: 0.0003575706508450417\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9896551724138\n",
      "    ram_util_percent: 71.6655172413793\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676374464990627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.177264433497385\n",
      "    mean_inference_ms: 1.7169587484308135\n",
      "    mean_raw_obs_processing_ms: 1.4887588256905306\n",
      "  time_since_restore: 5059.744081735611\n",
      "  time_this_iter_s: 20.34901523590088\n",
      "  time_total_s: 5059.744081735611\n",
      "  timers:\n",
      "    learn_throughput: 1573.525\n",
      "    learn_time_ms: 635.516\n",
      "    load_throughput: 82815.115\n",
      "    load_time_ms: 12.075\n",
      "    sample_throughput: 55.175\n",
      "    sample_time_ms: 18124.245\n",
      "    update_time_ms: 2.163\n",
      "  timestamp: 1633796172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         5059.74</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            454.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-16-30\n",
      "  done: false\n",
      "  episode_len_mean: 456.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 578\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7173328889740838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010478448108125744\n",
      "          policy_loss: -0.09799484900302358\n",
      "          total_loss: -0.11480062819189496\n",
      "          vf_explained_var: -0.6579020619392395\n",
      "          vf_loss: 0.0002017762047924205\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6\n",
      "    ram_util_percent: 71.4851851851852\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367635371801876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.17051502917153\n",
      "    mean_inference_ms: 1.716961712945448\n",
      "    mean_raw_obs_processing_ms: 1.4885290975890495\n",
      "  time_since_restore: 5078.5709233284\n",
      "  time_this_iter_s: 18.826841592788696\n",
      "  time_total_s: 5078.5709233284\n",
      "  timers:\n",
      "    learn_throughput: 1568.599\n",
      "    learn_time_ms: 637.512\n",
      "    load_throughput: 73713.603\n",
      "    load_time_ms: 13.566\n",
      "    sample_throughput: 54.057\n",
      "    sample_time_ms: 18498.894\n",
      "    update_time_ms: 2.174\n",
      "  timestamp: 1633796190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         5078.57</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-16-48\n",
      "  done: false\n",
      "  episode_len_mean: 458.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 580\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9210947632789612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0085365788523605\n",
      "          policy_loss: -0.08037451584306028\n",
      "          total_loss: -0.09931073095649481\n",
      "          vf_explained_var: -0.4049414396286011\n",
      "          vf_loss: 0.0001396774156698181\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60000000000001\n",
      "    ram_util_percent: 71.38846153846156\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036763314562815855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.1636904543523\n",
      "    mean_inference_ms: 1.7169637128741788\n",
      "    mean_raw_obs_processing_ms: 1.487409605007661\n",
      "  time_since_restore: 5096.496201515198\n",
      "  time_this_iter_s: 17.925278186798096\n",
      "  time_total_s: 5096.496201515198\n",
      "  timers:\n",
      "    learn_throughput: 1572.506\n",
      "    learn_time_ms: 635.927\n",
      "    load_throughput: 73045.665\n",
      "    load_time_ms: 13.69\n",
      "    sample_throughput: 54.048\n",
      "    sample_time_ms: 18501.988\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1633796208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">          5096.5</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             458.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 459.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 582\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.930101290014055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01845555813031368\n",
      "          policy_loss: -0.0029569020908739832\n",
      "          total_loss: -0.021752453222870827\n",
      "          vf_explained_var: -0.06204622983932495\n",
      "          vf_loss: 0.0002134907090900621\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87391304347826\n",
      "    ram_util_percent: 71.37826086956522\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676306715941238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.156737699896354\n",
      "    mean_inference_ms: 1.7169641429520324\n",
      "    mean_raw_obs_processing_ms: 1.4863211015937758\n",
      "  time_since_restore: 5112.943797588348\n",
      "  time_this_iter_s: 16.447596073150635\n",
      "  time_total_s: 5112.943797588348\n",
      "  timers:\n",
      "    learn_throughput: 1572.295\n",
      "    learn_time_ms: 636.013\n",
      "    load_throughput: 73217.806\n",
      "    load_time_ms: 13.658\n",
      "    sample_throughput: 54.225\n",
      "    sample_time_ms: 18441.798\n",
      "    update_time_ms: 2.169\n",
      "  timestamp: 1633796225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         5112.94</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 461.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 584\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7892523010571797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011269390636909155\n",
      "          policy_loss: 0.0070603083198269205\n",
      "          total_loss: -0.010473277895814842\n",
      "          vf_explained_var: -0.814426839351654\n",
      "          vf_loss: 0.00018064902574729382\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48\n",
      "    ram_util_percent: 71.4\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676281969962867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.149673671670193\n",
      "    mean_inference_ms: 1.716963139254264\n",
      "    mean_raw_obs_processing_ms: 1.4852626446120885\n",
      "  time_since_restore: 5130.168365716934\n",
      "  time_this_iter_s: 17.224568128585815\n",
      "  time_total_s: 5130.168365716934\n",
      "  timers:\n",
      "    learn_throughput: 1572.743\n",
      "    learn_time_ms: 635.832\n",
      "    load_throughput: 70867.447\n",
      "    load_time_ms: 14.111\n",
      "    sample_throughput: 53.365\n",
      "    sample_time_ms: 18739.048\n",
      "    update_time_ms: 2.157\n",
      "  timestamp: 1633796242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         5130.17</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-17-43\n",
      "  done: false\n",
      "  episode_len_mean: 461.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 587\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8941306233406068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013181490270750117\n",
      "          policy_loss: 0.0027231626626518035\n",
      "          total_loss: -0.01572588843603929\n",
      "          vf_explained_var: -0.388994425535202\n",
      "          vf_loss: 0.00028372164346769245\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61724137931034\n",
      "    ram_util_percent: 71.48275862068965\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367623797532434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.13920955395754\n",
      "    mean_inference_ms: 1.7169611106955285\n",
      "    mean_raw_obs_processing_ms: 1.483656226657741\n",
      "  time_since_restore: 5150.558876514435\n",
      "  time_this_iter_s: 20.39051079750061\n",
      "  time_total_s: 5150.558876514435\n",
      "  timers:\n",
      "    learn_throughput: 1572.775\n",
      "    learn_time_ms: 635.819\n",
      "    load_throughput: 66094.499\n",
      "    load_time_ms: 15.13\n",
      "    sample_throughput: 52.313\n",
      "    sample_time_ms: 19115.694\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633796263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         5150.56</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            461.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 462.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 589\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6787773013114928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010514450858991293\n",
      "          policy_loss: -0.044815718341204853\n",
      "          total_loss: -0.061266300744480556\n",
      "          vf_explained_var: -0.6657524704933167\n",
      "          vf_loss: 0.00017084803338447173\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.064285714285724\n",
      "    ram_util_percent: 71.56071428571427\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036762054891828405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.13232904800292\n",
      "    mean_inference_ms: 1.7169585410632402\n",
      "    mean_raw_obs_processing_ms: 1.4826612926223937\n",
      "  time_since_restore: 5170.017199277878\n",
      "  time_this_iter_s: 19.458322763442993\n",
      "  time_total_s: 5170.017199277878\n",
      "  timers:\n",
      "    learn_throughput: 1570.844\n",
      "    learn_time_ms: 636.601\n",
      "    load_throughput: 62137.745\n",
      "    load_time_ms: 16.093\n",
      "    sample_throughput: 51.427\n",
      "    sample_time_ms: 19444.974\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633796282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         5170.02</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            462.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-18-20\n",
      "  done: false\n",
      "  episode_len_mean: 463.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 591\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8257441692882115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012670416957400477\n",
      "          policy_loss: -0.05912998773985439\n",
      "          total_loss: -0.07701842474440733\n",
      "          vf_explained_var: -0.9775105118751526\n",
      "          vf_loss: 0.00016855713515219071\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.973076923076924\n",
      "    ram_util_percent: 71.92692307692307\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676180281462815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.125496211168183\n",
      "    mean_inference_ms: 1.7169602013212062\n",
      "    mean_raw_obs_processing_ms: 1.4816936280139024\n",
      "  time_since_restore: 5188.3392741680145\n",
      "  time_this_iter_s: 18.32207489013672\n",
      "  time_total_s: 5188.3392741680145\n",
      "  timers:\n",
      "    learn_throughput: 1567.238\n",
      "    learn_time_ms: 638.065\n",
      "    load_throughput: 65466.748\n",
      "    load_time_ms: 15.275\n",
      "    sample_throughput: 56.126\n",
      "    sample_time_ms: 17817.198\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633796300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         5188.34</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 463.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 593\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0227650350994533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01746968537259202\n",
      "          policy_loss: -0.013811878114938735\n",
      "          total_loss: -0.033625616298781504\n",
      "          vf_explained_var: -0.9616786241531372\n",
      "          vf_loss: 0.000137537366713837\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.96896551724139\n",
      "    ram_util_percent: 72.01724137931035\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761647321562375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.118806736338858\n",
      "    mean_inference_ms: 1.7169640704628488\n",
      "    mean_raw_obs_processing_ms: 1.4806890032316613\n",
      "  time_since_restore: 5209.0407881736755\n",
      "  time_this_iter_s: 20.70151400566101\n",
      "  time_total_s: 5209.0407881736755\n",
      "  timers:\n",
      "    learn_throughput: 1565.246\n",
      "    learn_time_ms: 638.877\n",
      "    load_throughput: 65891.609\n",
      "    load_time_ms: 15.176\n",
      "    sample_throughput: 55.161\n",
      "    sample_time_ms: 18128.835\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1633796321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         5209.04</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 463.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 595\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.875073258082072\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019585124289761384\n",
      "          policy_loss: -0.07849961833821403\n",
      "          total_loss: -0.09678131143252054\n",
      "          vf_explained_var: -0.1378510743379593\n",
      "          vf_loss: 0.0001591964236771067\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24814814814815\n",
      "    ram_util_percent: 71.84444444444445\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761523788775216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.11207425648711\n",
      "    mean_inference_ms: 1.7169681746411158\n",
      "    mean_raw_obs_processing_ms: 1.4797130963326957\n",
      "  time_since_restore: 5227.354167222977\n",
      "  time_this_iter_s: 18.313379049301147\n",
      "  time_total_s: 5227.354167222977\n",
      "  timers:\n",
      "    learn_throughput: 1562.732\n",
      "    learn_time_ms: 639.905\n",
      "    load_throughput: 68657.671\n",
      "    load_time_ms: 14.565\n",
      "    sample_throughput: 55.139\n",
      "    sample_time_ms: 18135.942\n",
      "    update_time_ms: 2.056\n",
      "  timestamp: 1633796339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         5227.35</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            463.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 468.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 597\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6604021112124125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018759352482174644\n",
      "          policy_loss: -0.001651905911664168\n",
      "          total_loss: -0.017927670064899655\n",
      "          vf_explained_var: -0.705698549747467\n",
      "          vf_loss: 3.148078686131663e-05\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.27619047619048\n",
      "    ram_util_percent: 71.90000000000002\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761374919318836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.104958659636942\n",
      "    mean_inference_ms: 1.7169722429790346\n",
      "    mean_raw_obs_processing_ms: 1.4787699397649174\n",
      "  time_since_restore: 5242.3425126075745\n",
      "  time_this_iter_s: 14.988345384597778\n",
      "  time_total_s: 5242.3425126075745\n",
      "  timers:\n",
      "    learn_throughput: 1563.776\n",
      "    learn_time_ms: 639.478\n",
      "    load_throughput: 76739.064\n",
      "    load_time_ms: 13.031\n",
      "    sample_throughput: 56.812\n",
      "    sample_time_ms: 17601.839\n",
      "    update_time_ms: 2.06\n",
      "  timestamp: 1633796354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         5242.34</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             468.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 471.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 599\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.975137264198727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01357084645410901\n",
      "          policy_loss: -0.026706447783443662\n",
      "          total_loss: -0.04607828340182702\n",
      "          vf_explained_var: -0.5030237436294556\n",
      "          vf_loss: 0.00016484317485365965\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.144444444444446\n",
      "    ram_util_percent: 71.92962962962964\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761220009116025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.09777182744364\n",
      "    mean_inference_ms: 1.716977217307705\n",
      "    mean_raw_obs_processing_ms: 1.4777893028442\n",
      "  time_since_restore: 5261.236017942429\n",
      "  time_this_iter_s: 18.893505334854126\n",
      "  time_total_s: 5261.236017942429\n",
      "  timers:\n",
      "    learn_throughput: 1560.275\n",
      "    learn_time_ms: 640.913\n",
      "    load_throughput: 77145.28\n",
      "    load_time_ms: 12.963\n",
      "    sample_throughput: 56.795\n",
      "    sample_time_ms: 17607.143\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633796373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         5261.24</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            471.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 473.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 601\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0158203125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7718210445510016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023429944168441193\n",
      "          policy_loss: -0.06452230083652669\n",
      "          total_loss: -0.08176983080597387\n",
      "          vf_explained_var: -0.6326974630355835\n",
      "          vf_loss: 0.00010001325651450719\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.06041666666667\n",
      "    ram_util_percent: 71.91458333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676105581852696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.090495336813724\n",
      "    mean_inference_ms: 1.7169828318921885\n",
      "    mean_raw_obs_processing_ms: 1.4781871597110479\n",
      "  time_since_restore: 5295.058588504791\n",
      "  time_this_iter_s: 33.82257056236267\n",
      "  time_total_s: 5295.058588504791\n",
      "  timers:\n",
      "    learn_throughput: 1551.307\n",
      "    learn_time_ms: 644.618\n",
      "    load_throughput: 81823.474\n",
      "    load_time_ms: 12.221\n",
      "    sample_throughput: 52.1\n",
      "    sample_time_ms: 19193.879\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633796407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         5295.06</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            473.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-20-25\n",
      "  done: false\n",
      "  episode_len_mean: 475.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 603\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.85129351483451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012499798116251236\n",
      "          policy_loss: -0.07407031978170077\n",
      "          total_loss: -0.09217831128173404\n",
      "          vf_explained_var: -0.66136634349823\n",
      "          vf_loss: 0.00010831794109738742\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.35769230769232\n",
      "    ram_util_percent: 71.78076923076922\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367609018380943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0831402479299\n",
      "    mean_inference_ms: 1.7169891851685628\n",
      "    mean_raw_obs_processing_ms: 1.4786052242286747\n",
      "  time_since_restore: 5313.219349384308\n",
      "  time_this_iter_s: 18.1607608795166\n",
      "  time_total_s: 5313.219349384308\n",
      "  timers:\n",
      "    learn_throughput: 1550.701\n",
      "    learn_time_ms: 644.87\n",
      "    load_throughput: 76582.555\n",
      "    load_time_ms: 13.058\n",
      "    sample_throughput: 51.642\n",
      "    sample_time_ms: 19364.097\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633796425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         5313.22</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               475</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 476.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 604\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8340713964568245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010968696092820135\n",
      "          policy_loss: -0.035929204440779156\n",
      "          total_loss: -0.053958894312381746\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 5.0733081338370945e-05\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.291304347826085\n",
      "    ram_util_percent: 71.50434782608698\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676084301934188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.079327835440303\n",
      "    mean_inference_ms: 1.7169929062009297\n",
      "    mean_raw_obs_processing_ms: 1.478761588885222\n",
      "  time_since_restore: 5329.271716117859\n",
      "  time_this_iter_s: 16.052366733551025\n",
      "  time_total_s: 5329.271716117859\n",
      "  timers:\n",
      "    learn_throughput: 1547.032\n",
      "    learn_time_ms: 646.399\n",
      "    load_throughput: 74413.799\n",
      "    load_time_ms: 13.438\n",
      "    sample_throughput: 51.962\n",
      "    sample_time_ms: 19244.944\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633796441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         5329.27</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            476.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-20-59\n",
      "  done: false\n",
      "  episode_len_mean: 480.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 606\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7396918058395385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010613461461680017\n",
      "          policy_loss: -0.016869356917838255\n",
      "          total_loss: -0.03391709617442555\n",
      "          vf_explained_var: -0.390960693359375\n",
      "          vf_loss: 9.73168492540329e-05\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.657692307692315\n",
      "    ram_util_percent: 71.18846153846154\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676073370319472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.071767827790577\n",
      "    mean_inference_ms: 1.7170017370751502\n",
      "    mean_raw_obs_processing_ms: 1.4790997096828193\n",
      "  time_since_restore: 5347.093714952469\n",
      "  time_this_iter_s: 17.821998834609985\n",
      "  time_total_s: 5347.093714952469\n",
      "  timers:\n",
      "    learn_throughput: 1547.233\n",
      "    learn_time_ms: 646.315\n",
      "    load_throughput: 78540.753\n",
      "    load_time_ms: 12.732\n",
      "    sample_throughput: 52.662\n",
      "    sample_time_ms: 18988.913\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633796459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         5347.09</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            480.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-21-18\n",
      "  done: false\n",
      "  episode_len_mean: 480.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 608\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8266860418849522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010669259931353479\n",
      "          policy_loss: 0.02681907872772879\n",
      "          total_loss: 0.008869834989309312\n",
      "          vf_explained_var: -0.8796023726463318\n",
      "          vf_loss: 6.442685864587677e-05\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.396153846153844\n",
      "    ram_util_percent: 71.09615384615384\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036760643970931144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.064435463992396\n",
      "    mean_inference_ms: 1.7170130394401841\n",
      "    mean_raw_obs_processing_ms: 1.4794628725017043\n",
      "  time_since_restore: 5365.775145530701\n",
      "  time_this_iter_s: 18.68143057823181\n",
      "  time_total_s: 5365.775145530701\n",
      "  timers:\n",
      "    learn_throughput: 1549.891\n",
      "    learn_time_ms: 645.207\n",
      "    load_throughput: 78056.072\n",
      "    load_time_ms: 12.811\n",
      "    sample_throughput: 52.876\n",
      "    sample_time_ms: 18912.241\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633796478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         5365.78</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            480.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-21-38\n",
      "  done: false\n",
      "  episode_len_mean: 482.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 610\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8467036339971754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011791060501681066\n",
      "          policy_loss: -0.026248664243353738\n",
      "          total_loss: -0.044330182692242995\n",
      "          vf_explained_var: -0.9443789124488831\n",
      "          vf_loss: 0.00010570882582720111\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.2551724137931\n",
      "    ram_util_percent: 71.06896551724137\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036760566685640754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.056977484731167\n",
      "    mean_inference_ms: 1.717025236507247\n",
      "    mean_raw_obs_processing_ms: 1.4782233064362398\n",
      "  time_since_restore: 5386.071273803711\n",
      "  time_this_iter_s: 20.296128273010254\n",
      "  time_total_s: 5386.071273803711\n",
      "  timers:\n",
      "    learn_throughput: 1545.273\n",
      "    learn_time_ms: 647.135\n",
      "    load_throughput: 75221.065\n",
      "    load_time_ms: 13.294\n",
      "    sample_throughput: 52.336\n",
      "    sample_time_ms: 19107.231\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1633796498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         5386.07</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            482.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-21-59\n",
      "  done: false\n",
      "  episode_len_mean: 481.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 613\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8704504635598924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01216972918058664\n",
      "          policy_loss: 0.02073248161209954\n",
      "          total_loss: 0.0024448777238527935\n",
      "          vf_explained_var: -0.9103450179100037\n",
      "          vf_loss: 0.0001281067369240595\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67999999999999\n",
      "    ram_util_percent: 71.1133333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676054153918609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.046346453150587\n",
      "    mean_inference_ms: 1.7170461421722252\n",
      "    mean_raw_obs_processing_ms: 1.4763479762207745\n",
      "  time_since_restore: 5406.6336805820465\n",
      "  time_this_iter_s: 20.56240677833557\n",
      "  time_total_s: 5406.6336805820465\n",
      "  timers:\n",
      "    learn_throughput: 1546.193\n",
      "    learn_time_ms: 646.75\n",
      "    load_throughput: 75631.779\n",
      "    load_time_ms: 13.222\n",
      "    sample_throughput: 52.373\n",
      "    sample_time_ms: 19093.776\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633796519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         5406.63</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            481.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 479.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 615\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7749973297119142\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013103815599628671\n",
      "          policy_loss: -0.03803403170572387\n",
      "          total_loss: -0.055308235519462165\n",
      "          vf_explained_var: -0.8457498550415039\n",
      "          vf_loss: 0.00016481039937288087\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.263333333333335\n",
      "    ram_util_percent: 71.14999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676057627822476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.039896218721445\n",
      "    mean_inference_ms: 1.7170620491578052\n",
      "    mean_raw_obs_processing_ms: 1.4751710499331079\n",
      "  time_since_restore: 5427.924049854279\n",
      "  time_this_iter_s: 21.290369272232056\n",
      "  time_total_s: 5427.924049854279\n",
      "  timers:\n",
      "    learn_throughput: 1546.495\n",
      "    learn_time_ms: 646.623\n",
      "    load_throughput: 73374.07\n",
      "    load_time_ms: 13.629\n",
      "    sample_throughput: 51.57\n",
      "    sample_time_ms: 19391.2\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633796540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         5427.92</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            479.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 480.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 617\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.814703851275974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013214241662799017\n",
      "          policy_loss: -0.044037393977244696\n",
      "          total_loss: -0.06168481748965052\n",
      "          vf_explained_var: -0.9153910279273987\n",
      "          vf_loss: 0.0001860343471182407\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.903225806451616\n",
      "    ram_util_percent: 71.1548387096774\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676063953354202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.03369067454591\n",
      "    mean_inference_ms: 1.7170791814213129\n",
      "    mean_raw_obs_processing_ms: 1.4740236078802043\n",
      "  time_since_restore: 5449.652039051056\n",
      "  time_this_iter_s: 21.727989196777344\n",
      "  time_total_s: 5449.652039051056\n",
      "  timers:\n",
      "    learn_throughput: 1545.243\n",
      "    learn_time_ms: 647.147\n",
      "    load_throughput: 66366.515\n",
      "    load_time_ms: 15.068\n",
      "    sample_throughput: 49.843\n",
      "    sample_time_ms: 20063.197\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1633796562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         5449.65</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            480.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 479.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 619\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9321239405208164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01139663114373418\n",
      "          policy_loss: -0.024019997235801484\n",
      "          total_loss: -0.042958527968989475\n",
      "          vf_explained_var: -0.9998120069503784\n",
      "          vf_loss: 0.00011225933097496939\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.00344827586206\n",
      "    ram_util_percent: 71.23103448275862\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676073758483353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.027868075784124\n",
      "    mean_inference_ms: 1.7170986750116963\n",
      "    mean_raw_obs_processing_ms: 1.472905178975845\n",
      "  time_since_restore: 5470.120350122452\n",
      "  time_this_iter_s: 20.468311071395874\n",
      "  time_total_s: 5470.120350122452\n",
      "  timers:\n",
      "    learn_throughput: 1552.612\n",
      "    learn_time_ms: 644.076\n",
      "    load_throughput: 66683.477\n",
      "    load_time_ms: 14.996\n",
      "    sample_throughput: 49.447\n",
      "    sample_time_ms: 20223.801\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1633796582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         5470.12</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            479.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 480.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 622\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8186872826682197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014604899241264391\n",
      "          policy_loss: -0.022814901607731978\n",
      "          total_loss: -0.04056578775246938\n",
      "          vf_explained_var: -0.46842190623283386\n",
      "          vf_loss: 8.940717430555701e-05\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 71.29354838709679\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676095945608197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.019175684280626\n",
      "    mean_inference_ms: 1.717129423855525\n",
      "    mean_raw_obs_processing_ms: 1.471269119816466\n",
      "  time_since_restore: 5491.5230350494385\n",
      "  time_this_iter_s: 21.402684926986694\n",
      "  time_total_s: 5491.5230350494385\n",
      "  timers:\n",
      "    learn_throughput: 1556.767\n",
      "    learn_time_ms: 642.357\n",
      "    load_throughput: 64472.308\n",
      "    load_time_ms: 15.511\n",
      "    sample_throughput: 52.679\n",
      "    sample_time_ms: 18983.0\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633796604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         5491.52</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            480.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 479.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 624\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9640670352511935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010019505226687405\n",
      "          policy_loss: -0.023202566926678023\n",
      "          total_loss: -0.042518485875593294\n",
      "          vf_explained_var: -0.6959994435310364\n",
      "          vf_loss: 8.698698167993118e-05\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.039393939393946\n",
      "    ram_util_percent: 71.42727272727274\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676120097709184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0137750261067\n",
      "    mean_inference_ms: 1.717151689266759\n",
      "    mean_raw_obs_processing_ms: 1.4702060232581784\n",
      "  time_since_restore: 5514.411504030228\n",
      "  time_this_iter_s: 22.888468980789185\n",
      "  time_total_s: 5514.411504030228\n",
      "  timers:\n",
      "    learn_throughput: 1554.574\n",
      "    learn_time_ms: 643.263\n",
      "    load_throughput: 64487.077\n",
      "    load_time_ms: 15.507\n",
      "    sample_throughput: 51.401\n",
      "    sample_time_ms: 19454.869\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1633796627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         5514.41</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            479.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 479.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 626\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6982089890374077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009346295050301616\n",
      "          policy_loss: 0.07660186969571643\n",
      "          total_loss: 0.0599007124081254\n",
      "          vf_explained_var: 0.06599447131156921\n",
      "          vf_loss: 5.914197940405251e-05\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.46071428571428\n",
      "    ram_util_percent: 71.45714285714284\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761499404437334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.008430231462356\n",
      "    mean_inference_ms: 1.7171757031533883\n",
      "    mean_raw_obs_processing_ms: 1.4691703422430726\n",
      "  time_since_restore: 5534.413494586945\n",
      "  time_this_iter_s: 20.00199055671692\n",
      "  time_total_s: 5534.413494586945\n",
      "  timers:\n",
      "    learn_throughput: 1552.493\n",
      "    learn_time_ms: 644.125\n",
      "    load_throughput: 62123.664\n",
      "    load_time_ms: 16.097\n",
      "    sample_throughput: 50.382\n",
      "    sample_time_ms: 19848.393\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633796647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         5534.41</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            479.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 481.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 629\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8562223792076111\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014250371689814934\n",
      "          policy_loss: -0.07390292642845048\n",
      "          total_loss: -0.09204037876592742\n",
      "          vf_explained_var: -0.5268504023551941\n",
      "          vf_loss: 8.66014271398146e-05\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.28387096774193\n",
      "    ram_util_percent: 71.52903225806452\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036762023086030986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.000624289230323\n",
      "    mean_inference_ms: 1.7172134069950793\n",
      "    mean_raw_obs_processing_ms: 1.4676581286426198\n",
      "  time_since_restore: 5556.008690834045\n",
      "  time_this_iter_s: 21.59519624710083\n",
      "  time_total_s: 5556.008690834045\n",
      "  timers:\n",
      "    learn_throughput: 1542.026\n",
      "    learn_time_ms: 648.498\n",
      "    load_throughput: 59792.722\n",
      "    load_time_ms: 16.724\n",
      "    sample_throughput: 49.454\n",
      "    sample_time_ms: 20220.695\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633796668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         5556.01</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            481.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-25-07\n",
      "  done: false\n",
      "  episode_len_mean: 479.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 631\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.797497652636634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013398607141390784\n",
      "          policy_loss: 0.0030636784310142198\n",
      "          total_loss: -0.014551288696626823\n",
      "          vf_explained_var: -0.5980709791183472\n",
      "          vf_loss: 4.205331780313928e-05\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.033928571428575\n",
      "    ram_util_percent: 71.60535714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367623859097018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.995806244114448\n",
      "    mean_inference_ms: 1.717239572984825\n",
      "    mean_raw_obs_processing_ms: 1.467910368037643\n",
      "  time_since_restore: 5595.06481385231\n",
      "  time_this_iter_s: 39.05612301826477\n",
      "  time_total_s: 5595.06481385231\n",
      "  timers:\n",
      "    learn_throughput: 1543.219\n",
      "    learn_time_ms: 647.996\n",
      "    load_throughput: 62293.247\n",
      "    load_time_ms: 16.053\n",
      "    sample_throughput: 44.925\n",
      "    sample_time_ms: 22259.333\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633796707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         5595.06</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            479.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-25-26\n",
      "  done: false\n",
      "  episode_len_mean: 480.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 633\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02373046875000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7132478793462118\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024335060967988653\n",
      "          policy_loss: 0.04420688268211153\n",
      "          total_loss: 0.08323970387379329\n",
      "          vf_explained_var: -0.6282535791397095\n",
      "          vf_loss: 0.05558781295419774\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.63703703703704\n",
      "    ram_util_percent: 71.83703703703704\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036762752454448704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.991145874090957\n",
      "    mean_inference_ms: 1.7172672500327584\n",
      "    mean_raw_obs_processing_ms: 1.4681839495065667\n",
      "  time_since_restore: 5613.935968875885\n",
      "  time_this_iter_s: 18.87115502357483\n",
      "  time_total_s: 5613.935968875885\n",
      "  timers:\n",
      "    learn_throughput: 1543.237\n",
      "    learn_time_ms: 647.988\n",
      "    load_throughput: 63059.723\n",
      "    load_time_ms: 15.858\n",
      "    sample_throughput: 45.214\n",
      "    sample_time_ms: 22117.026\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1633796726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         5613.94</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            480.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 478.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 636\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7244366142484877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012335136623984057\n",
      "          policy_loss: 0.08892283729381031\n",
      "          total_loss: 0.07406869812144173\n",
      "          vf_explained_var: 0.19350437819957733\n",
      "          vf_loss: 0.0019511487904108234\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.16428571428571\n",
      "    ram_util_percent: 71.66071428571428\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036763346165521925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.984639935335537\n",
      "    mean_inference_ms: 1.7173102517096266\n",
      "    mean_raw_obs_processing_ms: 1.4686870375038013\n",
      "  time_since_restore: 5633.683933734894\n",
      "  time_this_iter_s: 19.74796485900879\n",
      "  time_total_s: 5633.683933734894\n",
      "  timers:\n",
      "    learn_throughput: 1545.221\n",
      "    learn_time_ms: 647.157\n",
      "    load_throughput: 63268.42\n",
      "    load_time_ms: 15.806\n",
      "    sample_throughput: 45.379\n",
      "    sample_time_ms: 22036.474\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633796746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         5633.68</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            478.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-26-05\n",
      "  done: false\n",
      "  episode_len_mean: 478.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 638\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.728412351343367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01262425012308515\n",
      "          policy_loss: -0.005568623377217187\n",
      "          total_loss: -0.02194612692627642\n",
      "          vf_explained_var: 0.1400679498910904\n",
      "          vf_loss: 0.00045724712318689047\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.15357142857143\n",
      "    ram_util_percent: 71.41785714285713\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036763732683067485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.980341741370193\n",
      "    mean_inference_ms: 1.7173390466916845\n",
      "    mean_raw_obs_processing_ms: 1.4690011356485184\n",
      "  time_since_restore: 5652.9853756427765\n",
      "  time_this_iter_s: 19.30144190788269\n",
      "  time_total_s: 5652.9853756427765\n",
      "  timers:\n",
      "    learn_throughput: 1546.135\n",
      "    learn_time_ms: 646.774\n",
      "    load_throughput: 63187.403\n",
      "    load_time_ms: 15.826\n",
      "    sample_throughput: 45.792\n",
      "    sample_time_ms: 21837.967\n",
      "    update_time_ms: 2.092\n",
      "  timestamp: 1633796765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         5652.99</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            478.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 478.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 640\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7393496751785278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011187083015541353\n",
      "          policy_loss: -0.08218586602144771\n",
      "          total_loss: -0.09884938854310248\n",
      "          vf_explained_var: -0.3382240831851959\n",
      "          vf_loss: 0.00033176631542220197\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.286206896551725\n",
      "    ram_util_percent: 71.48965517241379\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764223612659075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.976304124783304\n",
      "    mean_inference_ms: 1.717371085558642\n",
      "    mean_raw_obs_processing_ms: 1.4678473562332874\n",
      "  time_since_restore: 5673.388715028763\n",
      "  time_this_iter_s: 20.403339385986328\n",
      "  time_total_s: 5673.388715028763\n",
      "  timers:\n",
      "    learn_throughput: 1542.805\n",
      "    learn_time_ms: 648.17\n",
      "    load_throughput: 63604.413\n",
      "    load_time_ms: 15.722\n",
      "    sample_throughput: 46.074\n",
      "    sample_time_ms: 21704.239\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633796786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         5673.39</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">               478</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-26-49\n",
      "  done: false\n",
      "  episode_len_mean: 474.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 643\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7620482206344605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01481723559862276\n",
      "          policy_loss: 0.015559048453966776\n",
      "          total_loss: -0.0010980096128251817\n",
      "          vf_explained_var: -0.041552748531103134\n",
      "          vf_loss: 0.0004359915165372917\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.334375\n",
      "    ram_util_percent: 71.459375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036765039960317936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.970811608090127\n",
      "    mean_inference_ms: 1.7174232000013072\n",
      "    mean_raw_obs_processing_ms: 1.4662174221268318\n",
      "  time_since_restore: 5696.206814050674\n",
      "  time_this_iter_s: 22.81809902191162\n",
      "  time_total_s: 5696.206814050674\n",
      "  timers:\n",
      "    learn_throughput: 1542.159\n",
      "    learn_time_ms: 648.441\n",
      "    load_throughput: 63288.468\n",
      "    load_time_ms: 15.801\n",
      "    sample_throughput: 45.581\n",
      "    sample_time_ms: 21938.881\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633796809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         5696.21</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            474.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 475.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 645\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7365757279925875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00897100876041039\n",
      "          policy_loss: -0.059762602051099144\n",
      "          total_loss: -0.07659109731515248\n",
      "          vf_explained_var: -0.21614906191825867\n",
      "          vf_loss: 0.0002179306336782045\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9037037037037\n",
      "    ram_util_percent: 71.3111111111111\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676557370992962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.967219663510846\n",
      "    mean_inference_ms: 1.7174583473443903\n",
      "    mean_raw_obs_processing_ms: 1.46517604798873\n",
      "  time_since_restore: 5714.807457208633\n",
      "  time_this_iter_s: 18.600643157958984\n",
      "  time_total_s: 5714.807457208633\n",
      "  timers:\n",
      "    learn_throughput: 1544.166\n",
      "    learn_time_ms: 647.599\n",
      "    load_throughput: 63285.222\n",
      "    load_time_ms: 15.801\n",
      "    sample_throughput: 46.169\n",
      "    sample_time_ms: 21659.527\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633796827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         5714.81</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            475.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 474.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 647\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8452466527620952\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009709999286284014\n",
      "          policy_loss: 0.12298727904756863\n",
      "          total_loss: 0.10508299767971038\n",
      "          vf_explained_var: -0.17100222408771515\n",
      "          vf_loss: 0.00020255139243090524\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.16\n",
      "    ram_util_percent: 71.44666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367661299628808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.96392892069333\n",
      "    mean_inference_ms: 1.7174945149312961\n",
      "    mean_raw_obs_processing_ms: 1.4641593238416184\n",
      "  time_since_restore: 5736.105328798294\n",
      "  time_this_iter_s: 21.297871589660645\n",
      "  time_total_s: 5736.105328798294\n",
      "  timers:\n",
      "    learn_throughput: 1544.339\n",
      "    learn_time_ms: 647.526\n",
      "    load_throughput: 62319.07\n",
      "    load_time_ms: 16.046\n",
      "    sample_throughput: 46.511\n",
      "    sample_time_ms: 21500.31\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633796849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         5736.11</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            474.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-27-49\n",
      "  done: false\n",
      "  episode_len_mean: 471.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 650\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7554155548413595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012772061303358465\n",
      "          policy_loss: -0.023366337662769687\n",
      "          total_loss: -0.04025607160809967\n",
      "          vf_explained_var: -0.8708781599998474\n",
      "          vf_loss: 0.0002097934578260821\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.07931034482759\n",
      "    ram_util_percent: 71.58620689655172\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676696040012663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.959442923211096\n",
      "    mean_inference_ms: 1.7175510396733389\n",
      "    mean_raw_obs_processing_ms: 1.4627051804405702\n",
      "  time_since_restore: 5756.511843442917\n",
      "  time_this_iter_s: 20.406514644622803\n",
      "  time_total_s: 5756.511843442917\n",
      "  timers:\n",
      "    learn_throughput: 1548.089\n",
      "    learn_time_ms: 645.958\n",
      "    load_throughput: 61990.621\n",
      "    load_time_ms: 16.131\n",
      "    sample_throughput: 46.42\n",
      "    sample_time_ms: 21542.242\n",
      "    update_time_ms: 2.06\n",
      "  timestamp: 1633796869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         5756.51</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             471.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 471.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 652\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8080695046318902\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017150033173899615\n",
      "          policy_loss: 0.06845188699662685\n",
      "          total_loss: 0.051112689036462045\n",
      "          vf_explained_var: -0.260018527507782\n",
      "          vf_loss: 0.00013102957673254422\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30344827586207\n",
      "    ram_util_percent: 71.68620689655172\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676757656477843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.956736314269957\n",
      "    mean_inference_ms: 1.7175900968169509\n",
      "    mean_raw_obs_processing_ms: 1.4617988030102702\n",
      "  time_since_restore: 5776.785382509232\n",
      "  time_this_iter_s: 20.273539066314697\n",
      "  time_total_s: 5776.785382509232\n",
      "  timers:\n",
      "    learn_throughput: 1558.259\n",
      "    learn_time_ms: 641.742\n",
      "    load_throughput: 62002.625\n",
      "    load_time_ms: 16.128\n",
      "    sample_throughput: 46.698\n",
      "    sample_time_ms: 21414.324\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1633796889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         5776.79</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             471.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 469.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 654\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7958305120468139\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011305348599463055\n",
      "          policy_loss: -0.12138333171606064\n",
      "          total_loss: -0.13877547399865256\n",
      "          vf_explained_var: -0.016216658055782318\n",
      "          vf_loss: 0.00016374305972324994\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.29333333333334\n",
      "    ram_util_percent: 71.65666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676820646722753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95432751894601\n",
      "    mean_inference_ms: 1.7176299326019966\n",
      "    mean_raw_obs_processing_ms: 1.4609152579641116\n",
      "  time_since_restore: 5797.792702913284\n",
      "  time_this_iter_s: 21.007320404052734\n",
      "  time_total_s: 5797.792702913284\n",
      "  timers:\n",
      "    learn_throughput: 1557.163\n",
      "    learn_time_ms: 642.193\n",
      "    load_throughput: 60290.362\n",
      "    load_time_ms: 16.586\n",
      "    sample_throughput: 50.998\n",
      "    sample_time_ms: 19608.555\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633796910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         5797.79</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            469.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-28-50\n",
      "  done: false\n",
      "  episode_len_mean: 466.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 657\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6834415780173408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014886358411112985\n",
      "          policy_loss: 0.014326235486401452\n",
      "          total_loss: -0.0018543654017978244\n",
      "          vf_explained_var: -0.2555370032787323\n",
      "          vf_loss: 0.00012391946472942235\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.771428571428565\n",
      "    ram_util_percent: 71.71428571428572\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676917774436663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95127046579471\n",
      "    mean_inference_ms: 1.7176911024949397\n",
      "    mean_raw_obs_processing_ms: 1.4596842460799513\n",
      "  time_since_restore: 5817.394183397293\n",
      "  time_this_iter_s: 19.60148048400879\n",
      "  time_total_s: 5817.394183397293\n",
      "  timers:\n",
      "    learn_throughput: 1561.172\n",
      "    learn_time_ms: 640.544\n",
      "    load_throughput: 59276.997\n",
      "    load_time_ms: 16.87\n",
      "    sample_throughput: 50.805\n",
      "    sample_time_ms: 19682.974\n",
      "    update_time_ms: 2.021\n",
      "  timestamp: 1633796930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         5817.39</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            466.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 465.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 659\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8486768060260348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010528970024260904\n",
      "          policy_loss: -0.006537346293528875\n",
      "          total_loss: -0.024509285390377045\n",
      "          vf_explained_var: -0.9886908531188965\n",
      "          vf_loss: 0.00014003914345974206\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.19333333333334\n",
      "    ram_util_percent: 71.8\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676985131930551\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.949646746000067\n",
      "    mean_inference_ms: 1.7177327231337625\n",
      "    mean_raw_obs_processing_ms: 1.4589045057690606\n",
      "  time_since_restore: 5838.04092335701\n",
      "  time_this_iter_s: 20.646739959716797\n",
      "  time_total_s: 5838.04092335701\n",
      "  timers:\n",
      "    learn_throughput: 1561.114\n",
      "    learn_time_ms: 640.568\n",
      "    load_throughput: 58575.247\n",
      "    load_time_ms: 17.072\n",
      "    sample_throughput: 50.575\n",
      "    sample_time_ms: 19772.628\n",
      "    update_time_ms: 2.02\n",
      "  timestamp: 1633796951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         5838.04</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            465.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 463.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 661\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7387938804096645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01177585477584061\n",
      "          policy_loss: -0.05750272509952386\n",
      "          total_loss: -0.07438385751512315\n",
      "          vf_explained_var: -0.024929529055953026\n",
      "          vf_loss: 8.763698691230578e-05\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.411320754716975\n",
      "    ram_util_percent: 71.85283018867926\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677056249962435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.948266654852606\n",
      "    mean_inference_ms: 1.7177746439954302\n",
      "    mean_raw_obs_processing_ms: 1.4593247799581592\n",
      "  time_since_restore: 5875.0354816913605\n",
      "  time_this_iter_s: 36.994558334350586\n",
      "  time_total_s: 5875.0354816913605\n",
      "  timers:\n",
      "    learn_throughput: 1553.726\n",
      "    learn_time_ms: 643.614\n",
      "    load_throughput: 59762.137\n",
      "    load_time_ms: 16.733\n",
      "    sample_throughput: 46.427\n",
      "    sample_time_ms: 21539.219\n",
      "    update_time_ms: 2.012\n",
      "  timestamp: 1633796988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         5875.04</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            463.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 459.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 664\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8147431320614285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01411173323675854\n",
      "          policy_loss: -0.0948322772151894\n",
      "          total_loss: -0.11234094810982545\n",
      "          vf_explained_var: -0.1074923500418663\n",
      "          vf_loss: 0.0001364429907375274\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.75172413793103\n",
      "    ram_util_percent: 71.88275862068967\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036771725740034795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.94700134731376\n",
      "    mean_inference_ms: 1.7178395630892842\n",
      "    mean_raw_obs_processing_ms: 1.4600724470289383\n",
      "  time_since_restore: 5895.277618646622\n",
      "  time_this_iter_s: 20.24213695526123\n",
      "  time_total_s: 5895.277618646622\n",
      "  timers:\n",
      "    learn_throughput: 1557.963\n",
      "    learn_time_ms: 641.864\n",
      "    load_throughput: 59974.662\n",
      "    load_time_ms: 16.674\n",
      "    sample_throughput: 46.458\n",
      "    sample_time_ms: 21524.889\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1633797008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         5895.28</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            459.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-30-28\n",
      "  done: false\n",
      "  episode_len_mean: 456.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 666\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.722153079509735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011041415348436818\n",
      "          policy_loss: 0.011536445944673485\n",
      "          total_loss: -0.005205408359567325\n",
      "          vf_explained_var: -0.3267260789871216\n",
      "          vf_loss: 8.664750732552622e-05\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30714285714286\n",
      "    ram_util_percent: 71.83214285714287\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677252515644103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.946621121055617\n",
      "    mean_inference_ms: 1.7178831498004499\n",
      "    mean_raw_obs_processing_ms: 1.4606447996678338\n",
      "  time_since_restore: 5914.877571344376\n",
      "  time_this_iter_s: 19.599952697753906\n",
      "  time_total_s: 5914.877571344376\n",
      "  timers:\n",
      "    learn_throughput: 1557.282\n",
      "    learn_time_ms: 642.145\n",
      "    load_throughput: 60086.614\n",
      "    load_time_ms: 16.643\n",
      "    sample_throughput: 47.163\n",
      "    sample_time_ms: 21202.838\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1633797028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         5914.88</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            456.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 454.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 668\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7528382963604396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01737678968215819\n",
      "          policy_loss: -0.05491688855820232\n",
      "          total_loss: -0.07176703363656997\n",
      "          vf_explained_var: -0.6383150815963745\n",
      "          vf_loss: 5.969776304684476e-05\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.165517241379305\n",
      "    ram_util_percent: 71.60344827586206\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677331984819623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.946475650019647\n",
      "    mean_inference_ms: 1.7179267588782345\n",
      "    mean_raw_obs_processing_ms: 1.4605650385015878\n",
      "  time_since_restore: 5935.190766096115\n",
      "  time_this_iter_s: 20.313194751739502\n",
      "  time_total_s: 5935.190766096115\n",
      "  timers:\n",
      "    learn_throughput: 1554.617\n",
      "    learn_time_ms: 643.245\n",
      "    load_throughput: 60015.768\n",
      "    load_time_ms: 16.662\n",
      "    sample_throughput: 46.788\n",
      "    sample_time_ms: 21372.983\n",
      "    update_time_ms: 2.013\n",
      "  timestamp: 1633797048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         5935.19</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            454.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 451.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 671\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5851265867551168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010724277062000543\n",
      "          policy_loss: -0.059333109358946486\n",
      "          total_loss: -0.0746040197296275\n",
      "          vf_explained_var: -0.3869033753871918\n",
      "          vf_loss: 0.0001986176361646762\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.212500000000006\n",
      "    ram_util_percent: 71.55\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677453433058209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.946713570679286\n",
      "    mean_inference_ms: 1.717993523531092\n",
      "    mean_raw_obs_processing_ms: 1.4594488649010642\n",
      "  time_since_restore: 5957.8361032009125\n",
      "  time_this_iter_s: 22.645337104797363\n",
      "  time_total_s: 5957.8361032009125\n",
      "  timers:\n",
      "    learn_throughput: 1551.062\n",
      "    learn_time_ms: 644.719\n",
      "    load_throughput: 60720.046\n",
      "    load_time_ms: 16.469\n",
      "    sample_throughput: 46.498\n",
      "    sample_time_ms: 21506.365\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633797071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         5957.84</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            451.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-31-32\n",
      "  done: false\n",
      "  episode_len_mean: 450.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 674\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.731181553999583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012573915526452062\n",
      "          policy_loss: -0.02904113084077835\n",
      "          total_loss: -0.04579896421896087\n",
      "          vf_explained_var: -0.9581840634346008\n",
      "          vf_loss: 0.00010640633022881552\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.056666666666665\n",
      "    ram_util_percent: 71.54666666666665\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036775769565927104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.947436751703286\n",
      "    mean_inference_ms: 1.7180602609152635\n",
      "    mean_raw_obs_processing_ms: 1.4584809701821977\n",
      "  time_since_restore: 5978.965205907822\n",
      "  time_this_iter_s: 21.12910270690918\n",
      "  time_total_s: 5978.965205907822\n",
      "  timers:\n",
      "    learn_throughput: 1544.329\n",
      "    learn_time_ms: 647.53\n",
      "    load_throughput: 60027.192\n",
      "    load_time_ms: 16.659\n",
      "    sample_throughput: 46.349\n",
      "    sample_time_ms: 21575.614\n",
      "    update_time_ms: 2.089\n",
      "  timestamp: 1633797092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         5978.97</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            450.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-31-54\n",
      "  done: false\n",
      "  episode_len_mean: 449.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 676\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6568456994162666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009635644906846885\n",
      "          policy_loss: -0.1073537777695391\n",
      "          total_loss: -0.12345456158121428\n",
      "          vf_explained_var: -0.38142597675323486\n",
      "          vf_loss: 0.00012468326905137574\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24062500000001\n",
      "    ram_util_percent: 71.59375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677657943832537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.948028395489654\n",
      "    mean_inference_ms: 1.7181052665269982\n",
      "    mean_raw_obs_processing_ms: 1.4578554609597376\n",
      "  time_since_restore: 6001.096422433853\n",
      "  time_this_iter_s: 22.131216526031494\n",
      "  time_total_s: 6001.096422433853\n",
      "  timers:\n",
      "    learn_throughput: 1534.121\n",
      "    learn_time_ms: 651.839\n",
      "    load_throughput: 59990.274\n",
      "    load_time_ms: 16.669\n",
      "    sample_throughput: 45.962\n",
      "    sample_time_ms: 21756.973\n",
      "    update_time_ms: 2.101\n",
      "  timestamp: 1633797114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">          6001.1</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            449.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 447.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 679\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7492978201972114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011039843676925109\n",
      "          policy_loss: 0.1176256155802144\n",
      "          total_loss: 0.10058641185363133\n",
      "          vf_explained_var: -0.9877239465713501\n",
      "          vf_loss: 6.080282098789919e-05\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.116666666666674\n",
      "    ram_util_percent: 71.66333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036777836238940084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.949266546064212\n",
      "    mean_inference_ms: 1.7181736247473014\n",
      "    mean_raw_obs_processing_ms: 1.4570065316019358\n",
      "  time_since_restore: 6022.600078582764\n",
      "  time_this_iter_s: 21.503656148910522\n",
      "  time_total_s: 6022.600078582764\n",
      "  timers:\n",
      "    learn_throughput: 1535.06\n",
      "    learn_time_ms: 651.441\n",
      "    load_throughput: 59341.322\n",
      "    load_time_ms: 16.852\n",
      "    sample_throughput: 45.857\n",
      "    sample_time_ms: 21806.805\n",
      "    update_time_ms: 2.118\n",
      "  timestamp: 1633797135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">          6022.6</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            447.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 446.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 681\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.855860353840722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013795856859170305\n",
      "          policy_loss: 0.028222518745395872\n",
      "          total_loss: 0.01019872741566764\n",
      "          vf_explained_var: -0.8579778075218201\n",
      "          vf_loss: 4.373816289242111e-05\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.49032258064516\n",
      "    ram_util_percent: 71.74193548387096\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677867690733517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.950416333194095\n",
      "    mean_inference_ms: 1.7182198053319024\n",
      "    mean_raw_obs_processing_ms: 1.4564776119938343\n",
      "  time_since_restore: 6043.790566205978\n",
      "  time_this_iter_s: 21.19048762321472\n",
      "  time_total_s: 6043.790566205978\n",
      "  timers:\n",
      "    learn_throughput: 1538.316\n",
      "    learn_time_ms: 650.061\n",
      "    load_throughput: 58450.763\n",
      "    load_time_ms: 17.108\n",
      "    sample_throughput: 45.523\n",
      "    sample_time_ms: 21966.821\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1633797157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         6043.79</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            446.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-32-55\n",
      "  done: false\n",
      "  episode_len_mean: 445.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 683\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7283424933751423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015786839815288674\n",
      "          policy_loss: -0.07100525663958655\n",
      "          total_loss: -0.08766738325357437\n",
      "          vf_explained_var: -0.3510124683380127\n",
      "          vf_loss: 5.9356018725035634e-05\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65384615384615\n",
      "    ram_util_percent: 71.79999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677951846239467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95169088884885\n",
      "    mean_inference_ms: 1.7182658587828223\n",
      "    mean_raw_obs_processing_ms: 1.4559664244490498\n",
      "  time_since_restore: 6061.999823331833\n",
      "  time_this_iter_s: 18.209257125854492\n",
      "  time_total_s: 6061.999823331833\n",
      "  timers:\n",
      "    learn_throughput: 1539.867\n",
      "    learn_time_ms: 649.407\n",
      "    load_throughput: 58477.819\n",
      "    load_time_ms: 17.101\n",
      "    sample_throughput: 46.033\n",
      "    sample_time_ms: 21723.74\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1633797175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">            6062</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            445.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 444.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 685\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7076087766223484\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014158188912150196\n",
      "          policy_loss: -0.08117862459686068\n",
      "          total_loss: -0.09764857391516367\n",
      "          vf_explained_var: -0.5592575073242188\n",
      "          vf_loss: 0.00010216710240153285\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.627586206896545\n",
      "    ram_util_percent: 71.83793103448276\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036780374315916055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95306696260202\n",
      "    mean_inference_ms: 1.718311723963398\n",
      "    mean_raw_obs_processing_ms: 1.455471038657471\n",
      "  time_since_restore: 6082.184063673019\n",
      "  time_this_iter_s: 20.184240341186523\n",
      "  time_total_s: 6082.184063673019\n",
      "  timers:\n",
      "    learn_throughput: 1548.894\n",
      "    learn_time_ms: 645.622\n",
      "    load_throughput: 56594.051\n",
      "    load_time_ms: 17.67\n",
      "    sample_throughput: 49.885\n",
      "    sample_time_ms: 20045.926\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1633797195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         6082.18</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            444.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-33-31\n",
      "  done: false\n",
      "  episode_len_mean: 446.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 687\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7014295975367228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012774296720608388\n",
      "          policy_loss: -0.04314078804519442\n",
      "          total_loss: -0.059676288151078755\n",
      "          vf_explained_var: -0.23752649128437042\n",
      "          vf_loss: 2.4084395797924825e-05\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.79565217391305\n",
      "    ram_util_percent: 71.90434782608699\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678121596610904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.954192807943635\n",
      "    mean_inference_ms: 1.7183570629107578\n",
      "    mean_raw_obs_processing_ms: 1.4549398090618644\n",
      "  time_since_restore: 6098.607975244522\n",
      "  time_this_iter_s: 16.423911571502686\n",
      "  time_total_s: 6098.607975244522\n",
      "  timers:\n",
      "    learn_throughput: 1551.871\n",
      "    learn_time_ms: 644.384\n",
      "    load_throughput: 59649.185\n",
      "    load_time_ms: 16.765\n",
      "    sample_throughput: 50.848\n",
      "    sample_time_ms: 19666.274\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1633797211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         6098.61</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            446.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 447.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 689\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.559748281372918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00950505151358092\n",
      "          policy_loss: -0.011258359936376413\n",
      "          total_loss: -0.026468278591831526\n",
      "          vf_explained_var: -0.9774453043937683\n",
      "          vf_loss: 4.922655368015209e-05\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.864000000000004\n",
      "    ram_util_percent: 72.012\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036782047699517414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95519340189331\n",
      "    mean_inference_ms: 1.7184014277801805\n",
      "    mean_raw_obs_processing_ms: 1.4544264682848433\n",
      "  time_since_restore: 6116.230165719986\n",
      "  time_this_iter_s: 17.622190475463867\n",
      "  time_total_s: 6116.230165719986\n",
      "  timers:\n",
      "    learn_throughput: 1552.181\n",
      "    learn_time_ms: 644.255\n",
      "    load_throughput: 59271.72\n",
      "    load_time_ms: 16.871\n",
      "    sample_throughput: 51.365\n",
      "    sample_time_ms: 19468.502\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1633797229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         6116.23</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            447.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 449.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 691\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5481203979916043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01158375347171693\n",
      "          policy_loss: 0.006999898350073232\n",
      "          total_loss: -0.008024490169352956\n",
      "          vf_explained_var: -0.999451220035553\n",
      "          vf_loss: 4.4482677326919255e-05\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.39777777777778\n",
      "    ram_util_percent: 71.91333333333336\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678276905293905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.955969805992236\n",
      "    mean_inference_ms: 1.718440599080164\n",
      "    mean_raw_obs_processing_ms: 1.4550635526253977\n",
      "  time_since_restore: 6147.66517162323\n",
      "  time_this_iter_s: 31.43500590324402\n",
      "  time_total_s: 6147.66517162323\n",
      "  timers:\n",
      "    learn_throughput: 1553.736\n",
      "    learn_time_ms: 643.61\n",
      "    load_throughput: 58691.802\n",
      "    load_time_ms: 17.038\n",
      "    sample_throughput: 48.588\n",
      "    sample_time_ms: 20581.16\n",
      "    update_time_ms: 2.121\n",
      "  timestamp: 1633797261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         6147.67</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            449.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 452.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 693\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03559570312499999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8427776045269437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020478435631687948\n",
      "          policy_loss: -0.004388086270127031\n",
      "          total_loss: -0.02204256947669718\n",
      "          vf_explained_var: -0.6586599349975586\n",
      "          vf_loss: 4.434577882016634e-05\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.586363636363636\n",
      "    ram_util_percent: 71.84545454545456\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678339906633792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.956355939385883\n",
      "    mean_inference_ms: 1.718476376857921\n",
      "    mean_raw_obs_processing_ms: 1.4557131589476586\n",
      "  time_since_restore: 6162.754962682724\n",
      "  time_this_iter_s: 15.089791059494019\n",
      "  time_total_s: 6162.754962682724\n",
      "  timers:\n",
      "    learn_throughput: 1560.857\n",
      "    learn_time_ms: 640.674\n",
      "    load_throughput: 63459.295\n",
      "    load_time_ms: 15.758\n",
      "    sample_throughput: 50.429\n",
      "    sample_time_ms: 19829.873\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1633797276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         6162.75</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            452.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-34-54\n",
      "  done: false\n",
      "  episode_len_mean: 453.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 695\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9032308671209548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0114858402513737\n",
      "          policy_loss: -0.027899293932649824\n",
      "          total_loss: -0.04624516152673298\n",
      "          vf_explained_var: -0.21140684187412262\n",
      "          vf_loss: 7.317312786957094e-05\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.457692307692305\n",
      "    ram_util_percent: 71.78846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678398908378558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.956745565379162\n",
      "    mean_inference_ms: 1.7185112713107644\n",
      "    mean_raw_obs_processing_ms: 1.4563771985694798\n",
      "  time_since_restore: 6180.9541392326355\n",
      "  time_this_iter_s: 18.1991765499115\n",
      "  time_total_s: 6180.9541392326355\n",
      "  timers:\n",
      "    learn_throughput: 1560.739\n",
      "    learn_time_ms: 640.722\n",
      "    load_throughput: 63289.614\n",
      "    load_time_ms: 15.8\n",
      "    sample_throughput: 51.186\n",
      "    sample_time_ms: 19536.755\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633797294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         6180.95</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            453.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-35-11\n",
      "  done: false\n",
      "  episode_len_mean: 451.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 696\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9167507237858243\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010992712475730797\n",
      "          policy_loss: -0.05215156342213353\n",
      "          total_loss: -0.0706325536283354\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.957492030581408e-05\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.132\n",
      "    ram_util_percent: 71.78399999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678437085324182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.957052874032932\n",
      "    mean_inference_ms: 1.718530834676464\n",
      "    mean_raw_obs_processing_ms: 1.456712415688542\n",
      "  time_since_restore: 6198.485365390778\n",
      "  time_this_iter_s: 17.53122615814209\n",
      "  time_total_s: 6198.485365390778\n",
      "  timers:\n",
      "    learn_throughput: 1564.227\n",
      "    learn_time_ms: 639.293\n",
      "    load_throughput: 67332.3\n",
      "    load_time_ms: 14.852\n",
      "    sample_throughput: 52.413\n",
      "    sample_time_ms: 19079.203\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633797311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         6198.49</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            451.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-35-28\n",
      "  done: false\n",
      "  episode_len_mean: 452.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 698\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9173453278011745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010351811161008268\n",
      "          policy_loss: 0.03496697054555019\n",
      "          total_loss: 0.016443775708062783\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.753849541690821e-05\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.869565217391305\n",
      "    ram_util_percent: 71.39130434782611\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678517174776855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.957463079998835\n",
      "    mean_inference_ms: 1.718570682576136\n",
      "    mean_raw_obs_processing_ms: 1.4573451694726154\n",
      "  time_since_restore: 6214.618924617767\n",
      "  time_this_iter_s: 16.133559226989746\n",
      "  time_total_s: 6214.618924617767\n",
      "  timers:\n",
      "    learn_throughput: 1563.642\n",
      "    learn_time_ms: 639.533\n",
      "    load_throughput: 72195.716\n",
      "    load_time_ms: 13.851\n",
      "    sample_throughput: 53.929\n",
      "    sample_time_ms: 18542.964\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633797328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         6214.62</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            452.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-35-46\n",
      "  done: false\n",
      "  episode_len_mean: 453.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 700\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9617960625224644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013825898219485013\n",
      "          policy_loss: -0.05271392501890659\n",
      "          total_loss: -0.07143373141686121\n",
      "          vf_explained_var: -0.9336583614349365\n",
      "          vf_loss: 0.00015993973534528374\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77307692307693\n",
      "    ram_util_percent: 71.13076923076922\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036785937976012414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.957929606409913\n",
      "    mean_inference_ms: 1.7186093062060386\n",
      "    mean_raw_obs_processing_ms: 1.4573159360978003\n",
      "  time_since_restore: 6232.909537792206\n",
      "  time_this_iter_s: 18.290613174438477\n",
      "  time_total_s: 6232.909537792206\n",
      "  timers:\n",
      "    learn_throughput: 1563.584\n",
      "    learn_time_ms: 639.556\n",
      "    load_throughput: 72939.724\n",
      "    load_time_ms: 13.71\n",
      "    sample_throughput: 54.785\n",
      "    sample_time_ms: 18253.084\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633797346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         6232.91</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            453.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 452.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 702\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9745875385072496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013399820029412663\n",
      "          policy_loss: -0.0830832451581955\n",
      "          total_loss: -0.10200343529383342\n",
      "          vf_explained_var: -0.981566846370697\n",
      "          vf_loss: 0.00011022228657869467\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.775\n",
      "    ram_util_percent: 71.06666666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678669597597856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95839465435056\n",
      "    mean_inference_ms: 1.7186465494970844\n",
      "    mean_raw_obs_processing_ms: 1.4566270456467323\n",
      "  time_since_restore: 6249.908070325851\n",
      "  time_this_iter_s: 16.99853253364563\n",
      "  time_total_s: 6249.908070325851\n",
      "  timers:\n",
      "    learn_throughput: 1563.513\n",
      "    learn_time_ms: 639.585\n",
      "    load_throughput: 77352.002\n",
      "    load_time_ms: 12.928\n",
      "    sample_throughput: 55.15\n",
      "    sample_time_ms: 18132.376\n",
      "    update_time_ms: 2.455\n",
      "  timestamp: 1633797363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         6249.91</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            452.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-36-20\n",
      "  done: false\n",
      "  episode_len_mean: 451.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 704\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.012256047460768\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008312773395254899\n",
      "          policy_loss: 0.007238453295495775\n",
      "          total_loss: -0.012338242638442251\n",
      "          vf_explained_var: -0.8250820636749268\n",
      "          vf_loss: 0.0001020150218310947\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.587999999999994\n",
      "    ram_util_percent: 71.25999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678746290510327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.958881779129168\n",
      "    mean_inference_ms: 1.7186842377155198\n",
      "    mean_raw_obs_processing_ms: 1.4559563542242513\n",
      "  time_since_restore: 6266.947522878647\n",
      "  time_this_iter_s: 17.03945255279541\n",
      "  time_total_s: 6266.947522878647\n",
      "  timers:\n",
      "    learn_throughput: 1564.003\n",
      "    learn_time_ms: 639.385\n",
      "    load_throughput: 80750.381\n",
      "    load_time_ms: 12.384\n",
      "    sample_throughput: 56.121\n",
      "    sample_time_ms: 17818.623\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1633797380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         6266.95</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            451.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 450.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 706\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9707373552852208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009798552062986686\n",
      "          policy_loss: -0.031744682581888305\n",
      "          total_loss: -0.050830450902382536\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.842664730967954e-05\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.08695652173914\n",
      "    ram_util_percent: 71.40869565217393\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678825195052807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.959419774074828\n",
      "    mean_inference_ms: 1.7187217034549283\n",
      "    mean_raw_obs_processing_ms: 1.4553540373198048\n",
      "  time_since_restore: 6283.253701686859\n",
      "  time_this_iter_s: 16.30617880821228\n",
      "  time_total_s: 6283.253701686859\n",
      "  timers:\n",
      "    learn_throughput: 1556.397\n",
      "    learn_time_ms: 642.51\n",
      "    load_throughput: 77967.417\n",
      "    load_time_ms: 12.826\n",
      "    sample_throughput: 56.169\n",
      "    sample_time_ms: 17803.276\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1633797396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         6283.25</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            450.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 451.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 708\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9727089073922899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01978148977403406\n",
      "          policy_loss: -0.07683501508500841\n",
      "          total_loss: -0.09541928426673014\n",
      "          vf_explained_var: -0.8778991103172302\n",
      "          vf_loss: 8.66140964313268e-05\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.3076923076923\n",
      "    ram_util_percent: 71.71923076923076\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678913113121118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.959939780920163\n",
      "    mean_inference_ms: 1.7187611408377677\n",
      "    mean_raw_obs_processing_ms: 1.4547679029928904\n",
      "  time_since_restore: 6301.571940422058\n",
      "  time_this_iter_s: 18.318238735198975\n",
      "  time_total_s: 6301.571940422058\n",
      "  timers:\n",
      "    learn_throughput: 1551.621\n",
      "    learn_time_ms: 644.487\n",
      "    load_throughput: 78436.471\n",
      "    load_time_ms: 12.749\n",
      "    sample_throughput: 55.957\n",
      "    sample_time_ms: 17870.997\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1633797415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         6301.57</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            451.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 451.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 710\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0706976426972283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008591346567838906\n",
      "          policy_loss: 0.009848197259836726\n",
      "          total_loss: -0.010328922586308586\n",
      "          vf_explained_var: -0.9260637164115906\n",
      "          vf_loss: 7.112966316829746e-05\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.388000000000005\n",
      "    ram_util_percent: 71.584\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679001964311399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.96027962199498\n",
      "    mean_inference_ms: 1.718799955653423\n",
      "    mean_raw_obs_processing_ms: 1.4541988558498848\n",
      "  time_since_restore: 6319.257771492004\n",
      "  time_this_iter_s: 17.68583106994629\n",
      "  time_total_s: 6319.257771492004\n",
      "  timers:\n",
      "    learn_throughput: 1553.771\n",
      "    learn_time_ms: 643.595\n",
      "    load_throughput: 79525.234\n",
      "    load_time_ms: 12.575\n",
      "    sample_throughput: 60.616\n",
      "    sample_time_ms: 16497.161\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1633797432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         6319.26</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            451.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 452.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 712\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9375900758637323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013694150860870459\n",
      "          policy_loss: -0.086257214181953\n",
      "          total_loss: -0.10479407029019462\n",
      "          vf_explained_var: 0.007767815142869949\n",
      "          vf_loss: 0.00010786232370365824\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.516000000000005\n",
      "    ram_util_percent: 71.648\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679082640354246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.960417668158644\n",
      "    mean_inference_ms: 1.7188364588835185\n",
      "    mean_raw_obs_processing_ms: 1.4536437044302957\n",
      "  time_since_restore: 6336.854399681091\n",
      "  time_this_iter_s: 17.596628189086914\n",
      "  time_total_s: 6336.854399681091\n",
      "  timers:\n",
      "    learn_throughput: 1552.585\n",
      "    learn_time_ms: 644.087\n",
      "    load_throughput: 71989.77\n",
      "    load_time_ms: 13.891\n",
      "    sample_throughput: 59.716\n",
      "    sample_time_ms: 16746.061\n",
      "    update_time_ms: 2.444\n",
      "  timestamp: 1633797450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         6336.85</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            452.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 453.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 714\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.858858315149943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01209507076755102\n",
      "          policy_loss: -0.1058311296833886\n",
      "          total_loss: -0.12364907827642228\n",
      "          vf_explained_var: 0.22974848747253418\n",
      "          vf_loss: 0.00012483273409695053\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29230769230769\n",
      "    ram_util_percent: 71.73076923076923\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036791595669951285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.96037476085958\n",
      "    mean_inference_ms: 1.718871436609026\n",
      "    mean_raw_obs_processing_ms: 1.4530546180301784\n",
      "  time_since_restore: 6354.617282629013\n",
      "  time_this_iter_s: 17.762882947921753\n",
      "  time_total_s: 6354.617282629013\n",
      "  timers:\n",
      "    learn_throughput: 1554.878\n",
      "    learn_time_ms: 643.137\n",
      "    load_throughput: 79591.332\n",
      "    load_time_ms: 12.564\n",
      "    sample_throughput: 59.863\n",
      "    sample_time_ms: 16704.734\n",
      "    update_time_ms: 2.423\n",
      "  timestamp: 1633797468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         6354.62</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             453.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 454.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 716\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.814637593428294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012118946867964218\n",
      "          policy_loss: -0.10113286905818515\n",
      "          total_loss: -0.11852783610423406\n",
      "          vf_explained_var: 0.04719946160912514\n",
      "          vf_loss: 0.00010433178605227214\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.66538461538461\n",
      "    ram_util_percent: 72.07307692307693\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679239827786958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.960092417327793\n",
      "    mean_inference_ms: 1.7189071036218109\n",
      "    mean_raw_obs_processing_ms: 1.4524820381853727\n",
      "  time_since_restore: 6372.802142620087\n",
      "  time_this_iter_s: 18.18485999107361\n",
      "  time_total_s: 6372.802142620087\n",
      "  timers:\n",
      "    learn_throughput: 1560.565\n",
      "    learn_time_ms: 640.793\n",
      "    load_throughput: 74844.692\n",
      "    load_time_ms: 13.361\n",
      "    sample_throughput: 59.624\n",
      "    sample_time_ms: 16771.644\n",
      "    update_time_ms: 2.438\n",
      "  timestamp: 1633797486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">          6372.8</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            454.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-38-23\n",
      "  done: false\n",
      "  episode_len_mean: 456.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 718\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.953978799449073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014986673398901276\n",
      "          policy_loss: -0.046617322911818825\n",
      "          total_loss: -0.06524943419628673\n",
      "          vf_explained_var: -0.20591531693935394\n",
      "          vf_loss: 0.00010748043866139293\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.12799999999999\n",
      "    ram_util_percent: 72.08\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679316649500883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95955379243673\n",
      "    mean_inference_ms: 1.718940743663066\n",
      "    mean_raw_obs_processing_ms: 1.4519258272046\n",
      "  time_since_restore: 6390.176633358002\n",
      "  time_this_iter_s: 17.37449073791504\n",
      "  time_total_s: 6390.176633358002\n",
      "  timers:\n",
      "    learn_throughput: 1562.545\n",
      "    learn_time_ms: 639.981\n",
      "    load_throughput: 74598.294\n",
      "    load_time_ms: 13.405\n",
      "    sample_throughput: 59.184\n",
      "    sample_time_ms: 16896.513\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1633797503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         6390.18</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            456.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-38-58\n",
      "  done: false\n",
      "  episode_len_mean: 455.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 720\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0710043165418837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011052629149492076\n",
      "          policy_loss: 0.015666315683888064\n",
      "          total_loss: -0.004418338504102495\n",
      "          vf_explained_var: -0.7763226628303528\n",
      "          vf_loss: 3.524892603713346e-05\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.298\n",
      "    ram_util_percent: 72.13199999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679387213855115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95882738738322\n",
      "    mean_inference_ms: 1.7189720915499311\n",
      "    mean_raw_obs_processing_ms: 1.452474425494997\n",
      "  time_since_restore: 6425.200931072235\n",
      "  time_this_iter_s: 35.0242977142334\n",
      "  time_total_s: 6425.200931072235\n",
      "  timers:\n",
      "    learn_throughput: 1564.355\n",
      "    learn_time_ms: 639.241\n",
      "    load_throughput: 74895.477\n",
      "    load_time_ms: 13.352\n",
      "    sample_throughput: 53.848\n",
      "    sample_time_ms: 18570.685\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1633797538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">          6425.2</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            455.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 456.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 722\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.892886209487915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010784519324771619\n",
      "          policy_loss: -0.01769907642155886\n",
      "          total_loss: -0.03591440547671583\n",
      "          vf_explained_var: -0.9062246680259705\n",
      "          vf_loss: 0.00013770655655712794\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.11071428571428\n",
      "    ram_util_percent: 72.07142857142856\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679452551635705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95805687731754\n",
      "    mean_inference_ms: 1.7190022651514383\n",
      "    mean_raw_obs_processing_ms: 1.4529854995332279\n",
      "  time_since_restore: 6445.047547578812\n",
      "  time_this_iter_s: 19.846616506576538\n",
      "  time_total_s: 6445.047547578812\n",
      "  timers:\n",
      "    learn_throughput: 1564.05\n",
      "    learn_time_ms: 639.366\n",
      "    load_throughput: 71025.855\n",
      "    load_time_ms: 14.079\n",
      "    sample_throughput: 53.036\n",
      "    sample_time_ms: 18854.999\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633797558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         6445.05</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            456.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 458.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 724\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8786634617381626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0137945026638109\n",
      "          policy_loss: -0.07666832841932773\n",
      "          total_loss: -0.09460805476539665\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011036982282853892\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.542307692307695\n",
      "    ram_util_percent: 72.03846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036795097079904374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.956943419572\n",
      "    mean_inference_ms: 1.7190304160131091\n",
      "    mean_raw_obs_processing_ms: 1.453509750967698\n",
      "  time_since_restore: 6463.134118556976\n",
      "  time_this_iter_s: 18.086570978164673\n",
      "  time_total_s: 6463.134118556976\n",
      "  timers:\n",
      "    learn_throughput: 1559.846\n",
      "    learn_time_ms: 641.089\n",
      "    load_throughput: 68905.138\n",
      "    load_time_ms: 14.513\n",
      "    sample_throughput: 52.749\n",
      "    sample_time_ms: 18957.559\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633797576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         6463.13</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            458.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 459.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 726\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8688854058583577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009018062193285084\n",
      "          policy_loss: -0.05332535128626559\n",
      "          total_loss: -0.07138010267582205\n",
      "          vf_explained_var: -0.7777732610702515\n",
      "          vf_loss: 0.00015259553850531423\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46666666666667\n",
      "    ram_util_percent: 71.74166666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679560351486285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.955621698563558\n",
      "    mean_inference_ms: 1.7190567924549542\n",
      "    mean_raw_obs_processing_ms: 1.4540462121380688\n",
      "  time_since_restore: 6479.847232103348\n",
      "  time_this_iter_s: 16.71311354637146\n",
      "  time_total_s: 6479.847232103348\n",
      "  timers:\n",
      "    learn_throughput: 1565.502\n",
      "    learn_time_ms: 638.773\n",
      "    load_throughput: 71179.657\n",
      "    load_time_ms: 14.049\n",
      "    sample_throughput: 52.629\n",
      "    sample_time_ms: 19000.997\n",
      "    update_time_ms: 2.092\n",
      "  timestamp: 1633797593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         6479.85</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             459.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 460.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 728\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9119883073700799\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010759755325401767\n",
      "          policy_loss: -0.0014909416230188476\n",
      "          total_loss: -0.0199398181711634\n",
      "          vf_explained_var: -0.9938287734985352\n",
      "          vf_loss: 9.650435030784542e-05\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 71.58800000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679605122089489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.95405492855981\n",
      "    mean_inference_ms: 1.7190815821620289\n",
      "    mean_raw_obs_processing_ms: 1.4545923206902784\n",
      "  time_since_restore: 6497.788004875183\n",
      "  time_this_iter_s: 17.940772771835327\n",
      "  time_total_s: 6497.788004875183\n",
      "  timers:\n",
      "    learn_throughput: 1567.303\n",
      "    learn_time_ms: 638.039\n",
      "    load_throughput: 75023.011\n",
      "    load_time_ms: 13.329\n",
      "    sample_throughput: 52.73\n",
      "    sample_time_ms: 18964.71\n",
      "    update_time_ms: 2.091\n",
      "  timestamp: 1633797611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         6497.79</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             460.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 461.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 731\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8250386118888855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015063881229252176\n",
      "          policy_loss: -0.010006088163289758\n",
      "          total_loss: -0.0273088240582082\n",
      "          vf_explained_var: -0.38388511538505554\n",
      "          vf_loss: 0.00014333534313158857\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65000000000001\n",
      "    ram_util_percent: 71.33214285714287\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036796687415553996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.951503838184465\n",
      "    mean_inference_ms: 1.7191176688302745\n",
      "    mean_raw_obs_processing_ms: 1.4541526580611523\n",
      "  time_since_restore: 6517.509291410446\n",
      "  time_this_iter_s: 19.72128653526306\n",
      "  time_total_s: 6517.509291410446\n",
      "  timers:\n",
      "    learn_throughput: 1567.188\n",
      "    learn_time_ms: 638.086\n",
      "    load_throughput: 75244.275\n",
      "    load_time_ms: 13.29\n",
      "    sample_throughput: 52.17\n",
      "    sample_time_ms: 19168.236\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633797631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         6517.51</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            461.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 459.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 733\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8078969730271233\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008672383001423018\n",
      "          policy_loss: 0.019778422721558147\n",
      "          total_loss: 0.0022620033472776414\n",
      "          vf_explained_var: -0.8444816470146179\n",
      "          vf_loss: 9.950058766763605e-05\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.332258064516125\n",
      "    ram_util_percent: 71.21612903225807\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367970863529074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.949909592629226\n",
      "    mean_inference_ms: 1.7191397824682964\n",
      "    mean_raw_obs_processing_ms: 1.453495783909125\n",
      "  time_since_restore: 6538.928075790405\n",
      "  time_this_iter_s: 21.418784379959106\n",
      "  time_total_s: 6538.928075790405\n",
      "  timers:\n",
      "    learn_throughput: 1565.886\n",
      "    learn_time_ms: 638.616\n",
      "    load_throughput: 74284.51\n",
      "    load_time_ms: 13.462\n",
      "    sample_throughput: 51.152\n",
      "    sample_time_ms: 19549.768\n",
      "    update_time_ms: 2.1\n",
      "  timestamp: 1633797652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         6538.93</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 459.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 736\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8225158267550998\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011513678870263987\n",
      "          policy_loss: -0.05336193175365527\n",
      "          total_loss: -0.07088964593907197\n",
      "          vf_explained_var: -0.8531293272972107\n",
      "          vf_loss: 8.268946096197598e-05\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.667857142857144\n",
      "    ram_util_percent: 71.30714285714285\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367976503436069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.947524222961555\n",
      "    mean_inference_ms: 1.7191716579873715\n",
      "    mean_raw_obs_processing_ms: 1.4525352130770686\n",
      "  time_since_restore: 6558.6880440711975\n",
      "  time_this_iter_s: 19.759968280792236\n",
      "  time_total_s: 6558.6880440711975\n",
      "  timers:\n",
      "    learn_throughput: 1571.391\n",
      "    learn_time_ms: 636.379\n",
      "    load_throughput: 69840.813\n",
      "    load_time_ms: 14.318\n",
      "    sample_throughput: 50.631\n",
      "    sample_time_ms: 19750.896\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633797672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         6558.69</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 459.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 738\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6180223517947727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011773700177002105\n",
      "          policy_loss: -0.010470791533589364\n",
      "          total_loss: -0.025952495779428215\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 6.98776940731073e-05\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.01724137931033\n",
      "    ram_util_percent: 71.31724137931036\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036798098960496774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.9459805608821\n",
      "    mean_inference_ms: 1.7191943982942246\n",
      "    mean_raw_obs_processing_ms: 1.4519117436399107\n",
      "  time_since_restore: 6578.646842718124\n",
      "  time_this_iter_s: 19.95879864692688\n",
      "  time_total_s: 6578.646842718124\n",
      "  timers:\n",
      "    learn_throughput: 1570.747\n",
      "    learn_time_ms: 636.64\n",
      "    load_throughput: 68937.075\n",
      "    load_time_ms: 14.506\n",
      "    sample_throughput: 50.181\n",
      "    sample_time_ms: 19927.837\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633797692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         6578.65</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 460.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 740\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7328697774145339\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012911480996882574\n",
      "          policy_loss: 0.03079516070170535\n",
      "          total_loss: 0.01422031716340118\n",
      "          vf_explained_var: -0.6976348757743835\n",
      "          vf_loss: 6.446364334098892e-05\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.160000000000004\n",
      "    ram_util_percent: 70.95333333333335\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679845982148602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.94447489786714\n",
      "    mean_inference_ms: 1.719214019012062\n",
      "    mean_raw_obs_processing_ms: 1.4513042241903316\n",
      "  time_since_restore: 6599.658158779144\n",
      "  time_this_iter_s: 21.011316061019897\n",
      "  time_total_s: 6599.658158779144\n",
      "  timers:\n",
      "    learn_throughput: 1568.326\n",
      "    learn_time_ms: 637.623\n",
      "    load_throughput: 64636.741\n",
      "    load_time_ms: 15.471\n",
      "    sample_throughput: 49.286\n",
      "    sample_time_ms: 20289.538\n",
      "    update_time_ms: 2.07\n",
      "  timestamp: 1633797713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         6599.66</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            460.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 459.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 743\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7019898242420621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011437999446750346\n",
      "          policy_loss: -0.06413892577919696\n",
      "          total_loss: -0.08039840206296908\n",
      "          vf_explained_var: 0.03042825683951378\n",
      "          vf_loss: 0.00014970522477395005\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.618181818181824\n",
      "    ram_util_percent: 71.0\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679892648183045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.942207185908742\n",
      "    mean_inference_ms: 1.7192403830997836\n",
      "    mean_raw_obs_processing_ms: 1.4504147615922678\n",
      "  time_since_restore: 6622.815988302231\n",
      "  time_this_iter_s: 23.157829523086548\n",
      "  time_total_s: 6622.815988302231\n",
      "  timers:\n",
      "    learn_throughput: 1563.365\n",
      "    learn_time_ms: 639.646\n",
      "    load_throughput: 64780.598\n",
      "    load_time_ms: 15.437\n",
      "    sample_throughput: 52.354\n",
      "    sample_time_ms: 19100.889\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633797736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         6622.82</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-42-35\n",
      "  done: false\n",
      "  episode_len_mean: 459.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 745\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8106394131978354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012775761868370338\n",
      "          policy_loss: -0.03679121434688568\n",
      "          total_loss: -0.05413456575738059\n",
      "          vf_explained_var: -0.813411295413971\n",
      "          vf_loss: 8.090197677827544e-05\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77777777777777\n",
      "    ram_util_percent: 71.07777777777777\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679924014031135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.940731041733006\n",
      "    mean_inference_ms: 1.719258014489005\n",
      "    mean_raw_obs_processing_ms: 1.4498371287517966\n",
      "  time_since_restore: 6641.762764215469\n",
      "  time_this_iter_s: 18.946775913238525\n",
      "  time_total_s: 6641.762764215469\n",
      "  timers:\n",
      "    learn_throughput: 1560.609\n",
      "    learn_time_ms: 640.775\n",
      "    load_throughput: 65060.852\n",
      "    load_time_ms: 15.37\n",
      "    sample_throughput: 52.608\n",
      "    sample_time_ms: 19008.461\n",
      "    update_time_ms: 2.296\n",
      "  timestamp: 1633797755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         6641.76</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            459.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 457.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 748\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.70987649096383\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013107429979837172\n",
      "          policy_loss: 0.014050489105284215\n",
      "          total_loss: -0.0022569985853301156\n",
      "          vf_explained_var: -0.5145699977874756\n",
      "          vf_loss: 9.142638009507209e-05\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.25\n",
      "    ram_util_percent: 71.12647058823529\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799687320642566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93867798911356\n",
      "    mean_inference_ms: 1.7192844594200398\n",
      "    mean_raw_obs_processing_ms: 1.4490414691092597\n",
      "  time_since_restore: 6665.37591958046\n",
      "  time_this_iter_s: 23.613155364990234\n",
      "  time_total_s: 6665.37591958046\n",
      "  timers:\n",
      "    learn_throughput: 1551.904\n",
      "    learn_time_ms: 644.37\n",
      "    load_throughput: 67718.22\n",
      "    load_time_ms: 14.767\n",
      "    sample_throughput: 51.13\n",
      "    sample_time_ms: 19558.156\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1633797779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         6665.38</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            457.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-43-37\n",
      "  done: false\n",
      "  episode_len_mean: 456.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 751\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7284247345394559\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013209143916497901\n",
      "          policy_loss: -0.13813441192938222\n",
      "          total_loss: -0.15460560843348503\n",
      "          vf_explained_var: -0.36697447299957275\n",
      "          vf_loss: 0.00010776696395219511\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.09818181818182\n",
      "    ram_util_percent: 71.21454545454547\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036800109912356695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.9367554215867\n",
      "    mean_inference_ms: 1.7193106700425722\n",
      "    mean_raw_obs_processing_ms: 1.4498219569614732\n",
      "  time_since_restore: 6703.868750095367\n",
      "  time_this_iter_s: 38.49283051490784\n",
      "  time_total_s: 6703.868750095367\n",
      "  timers:\n",
      "    learn_throughput: 1552.389\n",
      "    learn_time_ms: 644.169\n",
      "    load_throughput: 63413.146\n",
      "    load_time_ms: 15.77\n",
      "    sample_throughput: 46.008\n",
      "    sample_time_ms: 21735.343\n",
      "    update_time_ms: 2.29\n",
      "  timestamp: 1633797817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         6703.87</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-43-57\n",
      "  done: false\n",
      "  episode_len_mean: 456.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 753\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7019508944617376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015568354904261858\n",
      "          policy_loss: -0.00888200087679757\n",
      "          total_loss: -0.024967182344860502\n",
      "          vf_explained_var: -0.8374032974243164\n",
      "          vf_loss: 0.0001030801407371958\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.34444444444444\n",
      "    ram_util_percent: 71.23333333333333\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036800368684309595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.935377234598004\n",
      "    mean_inference_ms: 1.7193275717944851\n",
      "    mean_raw_obs_processing_ms: 1.450369720919977\n",
      "  time_since_restore: 6723.219326734543\n",
      "  time_this_iter_s: 19.350576639175415\n",
      "  time_total_s: 6723.219326734543\n",
      "  timers:\n",
      "    learn_throughput: 1547.073\n",
      "    learn_time_ms: 646.382\n",
      "    load_throughput: 62510.026\n",
      "    load_time_ms: 15.997\n",
      "    sample_throughput: 45.717\n",
      "    sample_time_ms: 21873.806\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1633797837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         6723.22</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 456.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 755\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7718248393800524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011216896122065853\n",
      "          policy_loss: -0.021051576361060143\n",
      "          total_loss: -0.038102251001530224\n",
      "          vf_explained_var: -0.010777494870126247\n",
      "          vf_loss: 6.865991381346248e-05\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.435483870967744\n",
      "    ram_util_percent: 71.08064516129032\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680063403675375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.934089392688783\n",
      "    mean_inference_ms: 1.7193442797431318\n",
      "    mean_raw_obs_processing_ms: 1.4509276723787374\n",
      "  time_since_restore: 6745.006627559662\n",
      "  time_this_iter_s: 21.78730082511902\n",
      "  time_total_s: 6745.006627559662\n",
      "  timers:\n",
      "    learn_throughput: 1547.87\n",
      "    learn_time_ms: 646.049\n",
      "    load_throughput: 61679.399\n",
      "    load_time_ms: 16.213\n",
      "    sample_throughput: 45.289\n",
      "    sample_time_ms: 22080.528\n",
      "    update_time_ms: 2.32\n",
      "  timestamp: 1633797858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         6745.01</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            456.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 455.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 758\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7727350698577033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011715167351462118\n",
      "          policy_loss: -0.07810282574759589\n",
      "          total_loss: -0.09512314059668117\n",
      "          vf_explained_var: -0.8986191749572754\n",
      "          vf_loss: 8.152331328245864e-05\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76129032258064\n",
      "    ram_util_percent: 70.82580645161289\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801063674706945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93227023859296\n",
      "    mean_inference_ms: 1.7193686925244662\n",
      "    mean_raw_obs_processing_ms: 1.4517589330697986\n",
      "  time_since_restore: 6766.155209064484\n",
      "  time_this_iter_s: 21.148581504821777\n",
      "  time_total_s: 6766.155209064484\n",
      "  timers:\n",
      "    learn_throughput: 1549.069\n",
      "    learn_time_ms: 645.549\n",
      "    load_throughput: 61958.936\n",
      "    load_time_ms: 16.14\n",
      "    sample_throughput: 45.343\n",
      "    sample_time_ms: 22054.048\n",
      "    update_time_ms: 2.325\n",
      "  timestamp: 1633797880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         6766.16</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            455.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-45-03\n",
      "  done: false\n",
      "  episode_len_mean: 453.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 761\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5438359220822653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018872071712492142\n",
      "          policy_loss: -0.0520926124519772\n",
      "          total_loss: -0.06640105686253972\n",
      "          vf_explained_var: -0.71479332447052\n",
      "          vf_loss: 0.00012226932121848222\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.121875\n",
      "    ram_util_percent: 70.66874999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801531692593044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93063569169908\n",
      "    mean_inference_ms: 1.7193930429439732\n",
      "    mean_raw_obs_processing_ms: 1.4514741371384017\n",
      "  time_since_restore: 6789.161603689194\n",
      "  time_this_iter_s: 23.006394624710083\n",
      "  time_total_s: 6789.161603689194\n",
      "  timers:\n",
      "    learn_throughput: 1548.878\n",
      "    learn_time_ms: 645.629\n",
      "    load_throughput: 61575.99\n",
      "    load_time_ms: 16.24\n",
      "    sample_throughput: 44.686\n",
      "    sample_time_ms: 22378.488\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1633797903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         6789.16</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            453.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 451.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 763\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6831847508748372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009383824917860275\n",
      "          policy_loss: -0.022440208908584384\n",
      "          total_loss: -0.0387130012942685\n",
      "          vf_explained_var: 0.013840978965163231\n",
      "          vf_loss: 5.801812029757356e-05\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.09142857142857\n",
      "    ram_util_percent: 70.69714285714285\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801887637124955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.929754403348273\n",
      "    mean_inference_ms: 1.7194093946754105\n",
      "    mean_raw_obs_processing_ms: 1.4509321008607015\n",
      "  time_since_restore: 6813.144105911255\n",
      "  time_this_iter_s: 23.982502222061157\n",
      "  time_total_s: 6813.144105911255\n",
      "  timers:\n",
      "    learn_throughput: 1543.587\n",
      "    learn_time_ms: 647.842\n",
      "    load_throughput: 60927.943\n",
      "    load_time_ms: 16.413\n",
      "    sample_throughput: 43.901\n",
      "    sample_time_ms: 22778.477\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1633797927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         6813.14</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            451.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-45-49\n",
      "  done: false\n",
      "  episode_len_mean: 450.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 766\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6334055529700384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009415611819106958\n",
      "          policy_loss: 0.015458198967907164\n",
      "          total_loss: -0.0003105983138084412\n",
      "          vf_explained_var: -0.2669447064399719\n",
      "          vf_loss: 6.252632467496571e-05\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.034375\n",
      "    ram_util_percent: 70.703125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680243162921273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.92869371700902\n",
      "    mean_inference_ms: 1.7194338549790518\n",
      "    mean_raw_obs_processing_ms: 1.450094884718946\n",
      "  time_since_restore: 6835.561186552048\n",
      "  time_this_iter_s: 22.417080640792847\n",
      "  time_total_s: 6835.561186552048\n",
      "  timers:\n",
      "    learn_throughput: 1547.231\n",
      "    learn_time_ms: 646.316\n",
      "    load_throughput: 60205.292\n",
      "    load_time_ms: 16.61\n",
      "    sample_throughput: 43.629\n",
      "    sample_time_ms: 22920.403\n",
      "    update_time_ms: 2.347\n",
      "  timestamp: 1633797949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         6835.56</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            450.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-46-14\n",
      "  done: false\n",
      "  episode_len_mean: 447.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 769\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6767453948656719\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010776647401841128\n",
      "          policy_loss: -0.010698595891396205\n",
      "          total_loss: -0.026834073000484043\n",
      "          vf_explained_var: -0.43363139033317566\n",
      "          vf_loss: 5.657218435872993e-05\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.402777777777786\n",
      "    ram_util_percent: 70.81388888888891\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680299953531737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.92797967120514\n",
      "    mean_inference_ms: 1.719457011314413\n",
      "    mean_raw_obs_processing_ms: 1.4493699597391698\n",
      "  time_since_restore: 6860.897714138031\n",
      "  time_this_iter_s: 25.336527585983276\n",
      "  time_total_s: 6860.897714138031\n",
      "  timers:\n",
      "    learn_throughput: 1544.987\n",
      "    learn_time_ms: 647.255\n",
      "    load_throughput: 60020.234\n",
      "    load_time_ms: 16.661\n",
      "    sample_throughput: 43.22\n",
      "    sample_time_ms: 23137.236\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1633797974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">          6860.9</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            447.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-46-38\n",
      "  done: false\n",
      "  episode_len_mean: 447.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 772\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.580103537771437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010908979604162046\n",
      "          policy_loss: -0.07820711396634579\n",
      "          total_loss: -0.09336852836940024\n",
      "          vf_explained_var: -0.33095476031303406\n",
      "          vf_loss: 5.7152732491279794e-05\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.02058823529412\n",
      "    ram_util_percent: 70.96470588235293\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680357925357443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.927367136451334\n",
      "    mean_inference_ms: 1.7194795965913778\n",
      "    mean_raw_obs_processing_ms: 1.4486616769690994\n",
      "  time_since_restore: 6884.44628739357\n",
      "  time_this_iter_s: 23.54857325553894\n",
      "  time_total_s: 6884.44628739357\n",
      "  timers:\n",
      "    learn_throughput: 1546.616\n",
      "    learn_time_ms: 646.573\n",
      "    load_throughput: 60575.527\n",
      "    load_time_ms: 16.508\n",
      "    sample_throughput: 42.373\n",
      "    sample_time_ms: 23599.667\n",
      "    update_time_ms: 2.145\n",
      "  timestamp: 1633797998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         6884.45</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            447.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 444.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 775\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3230177972051833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009524982773916798\n",
      "          policy_loss: -0.051907190183798475\n",
      "          total_loss: -0.06455279762546222\n",
      "          vf_explained_var: -0.8375434875488281\n",
      "          vf_loss: 7.599767510934423e-05\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82631578947369\n",
      "    ram_util_percent: 71.06842105263156\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680419502667133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.927223549466966\n",
      "    mean_inference_ms: 1.7195024823259475\n",
      "    mean_raw_obs_processing_ms: 1.4479734482442137\n",
      "  time_since_restore: 6911.30167388916\n",
      "  time_this_iter_s: 26.85538649559021\n",
      "  time_total_s: 6911.30167388916\n",
      "  timers:\n",
      "    learn_throughput: 1559.745\n",
      "    learn_time_ms: 641.131\n",
      "    load_throughput: 58037.147\n",
      "    load_time_ms: 17.23\n",
      "    sample_throughput: 41.791\n",
      "    sample_time_ms: 23928.588\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1633798025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">          6911.3</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            444.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 442.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 778\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5451772967974344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008378211389676002\n",
      "          policy_loss: -0.04547617998388079\n",
      "          total_loss: -0.060433640744951035\n",
      "          vf_explained_var: -0.24075357615947723\n",
      "          vf_loss: 4.696949981735088e-05\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.636842105263156\n",
      "    ram_util_percent: 71.13947368421051\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680480333690868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.927440069201285\n",
      "    mean_inference_ms: 1.7195248007116737\n",
      "    mean_raw_obs_processing_ms: 1.4473513267476141\n",
      "  time_since_restore: 6938.27827835083\n",
      "  time_this_iter_s: 26.976604461669922\n",
      "  time_total_s: 6938.27827835083\n",
      "  timers:\n",
      "    learn_throughput: 1555.21\n",
      "    learn_time_ms: 643.0\n",
      "    load_throughput: 58114.425\n",
      "    load_time_ms: 17.207\n",
      "    sample_throughput: 43.908\n",
      "    sample_time_ms: 22775.125\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1633798052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         6938.28</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            442.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-48-15\n",
      "  done: false\n",
      "  episode_len_mean: 439.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 781\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5293036381403604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010768486440259375\n",
      "          policy_loss: -0.038611617187658945\n",
      "          total_loss: -0.05324660340944926\n",
      "          vf_explained_var: -0.13399766385555267\n",
      "          vf_loss: 8.307968265499867e-05\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.73870967741935\n",
      "    ram_util_percent: 71.24677419354838\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680544533413107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.928128436697165\n",
      "    mean_inference_ms: 1.7195481498123797\n",
      "    mean_raw_obs_processing_ms: 1.4482413565377394\n",
      "  time_since_restore: 6981.65106844902\n",
      "  time_this_iter_s: 43.37279009819031\n",
      "  time_total_s: 6981.65106844902\n",
      "  timers:\n",
      "    learn_throughput: 1562.936\n",
      "    learn_time_ms: 639.822\n",
      "    load_throughput: 56276.268\n",
      "    load_time_ms: 17.769\n",
      "    sample_throughput: 39.714\n",
      "    sample_time_ms: 25180.036\n",
      "    update_time_ms: 2.124\n",
      "  timestamp: 1633798095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         6981.65</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            439.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 432.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 785\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8011309994591607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01287440254576274\n",
      "          policy_loss: -0.03617890212270949\n",
      "          total_loss: -0.05342979903022448\n",
      "          vf_explained_var: -0.17443878948688507\n",
      "          vf_loss: 7.3001251217243e-05\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81025641025641\n",
      "    ram_util_percent: 71.39487179487182\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680634143160631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.930016220893027\n",
      "    mean_inference_ms: 1.7195820622453502\n",
      "    mean_raw_obs_processing_ms: 1.4496021430306407\n",
      "  time_since_restore: 7008.799593925476\n",
      "  time_this_iter_s: 27.14852547645569\n",
      "  time_total_s: 7008.799593925476\n",
      "  timers:\n",
      "    learn_throughput: 1560.051\n",
      "    learn_time_ms: 641.005\n",
      "    load_throughput: 56410.462\n",
      "    load_time_ms: 17.727\n",
      "    sample_throughput: 38.888\n",
      "    sample_time_ms: 25715.005\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1633798122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">          7008.8</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            432.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 426.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 788\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0533935546875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7644010278913709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025272642036807645\n",
      "          policy_loss: 0.10050454470846389\n",
      "          total_loss: 0.8327462977833218\n",
      "          vf_explained_var: -0.22227753698825836\n",
      "          vf_loss: 0.7485363642840336\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8952380952381\n",
      "    ram_util_percent: 71.06190476190477\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680705737220631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.932654270091426\n",
      "    mean_inference_ms: 1.7196102013361558\n",
      "    mean_raw_obs_processing_ms: 1.4507496530551531\n",
      "  time_since_restore: 7038.107108831406\n",
      "  time_this_iter_s: 29.307514905929565\n",
      "  time_total_s: 7038.107108831406\n",
      "  timers:\n",
      "    learn_throughput: 1560.726\n",
      "    learn_time_ms: 640.727\n",
      "    load_throughput: 56007.467\n",
      "    load_time_ms: 17.855\n",
      "    sample_throughput: 37.692\n",
      "    sample_time_ms: 26531.083\n",
      "    update_time_ms: 2.109\n",
      "  timestamp: 1633798152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         7038.11</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            426.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 414.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 792\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7016897320747375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012589154932623003\n",
      "          policy_loss: 0.0024159006774425507\n",
      "          total_loss: 0.04011598461204105\n",
      "          vf_explained_var: 0.6665397882461548\n",
      "          vf_loss: 0.053708711887399355\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.673809523809524\n",
      "    ram_util_percent: 70.96904761904763\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680800846180698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.93812057809047\n",
      "    mean_inference_ms: 1.7196511972151485\n",
      "    mean_raw_obs_processing_ms: 1.4507195542551126\n",
      "  time_since_restore: 7067.3353435993195\n",
      "  time_this_iter_s: 29.22823476791382\n",
      "  time_total_s: 7067.3353435993195\n",
      "  timers:\n",
      "    learn_throughput: 1556.907\n",
      "    learn_time_ms: 642.299\n",
      "    load_throughput: 55303.11\n",
      "    load_time_ms: 18.082\n",
      "    sample_throughput: 36.83\n",
      "    sample_time_ms: 27151.467\n",
      "    update_time_ms: 2.109\n",
      "  timestamp: 1633798181\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         7067.34</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            414.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-50-05\n",
      "  done: false\n",
      "  episode_len_mean: 409.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 795\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7112171451250713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018101400954903292\n",
      "          policy_loss: 0.012076890551381642\n",
      "          total_loss: 0.014435683108038373\n",
      "          vf_explained_var: 0.5923033952713013\n",
      "          vf_loss: 0.018021220082624093\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.79117647058823\n",
      "    ram_util_percent: 70.96764705882353\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680871298008119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.943170078556335\n",
      "    mean_inference_ms: 1.7196826173129358\n",
      "    mean_raw_obs_processing_ms: 1.4503805760704298\n",
      "  time_since_restore: 7091.481083154678\n",
      "  time_this_iter_s: 24.145739555358887\n",
      "  time_total_s: 7091.481083154678\n",
      "  timers:\n",
      "    learn_throughput: 1563.708\n",
      "    learn_time_ms: 639.505\n",
      "    load_throughput: 55441.784\n",
      "    load_time_ms: 18.037\n",
      "    sample_throughput: 36.804\n",
      "    sample_time_ms: 27170.618\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1633798205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         7091.48</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               409</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-50-28\n",
      "  done: false\n",
      "  episode_len_mean: 402.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 798\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.811668544345432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011428134168643527\n",
      "          policy_loss: 0.05664947662088606\n",
      "          total_loss: 0.04234723705384466\n",
      "          vf_explained_var: 0.609605073928833\n",
      "          vf_loss: 0.0028991638631042506\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.078125\n",
      "    ram_util_percent: 71.10625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680904618297077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.94895925929632\n",
      "    mean_inference_ms: 1.7197057877147035\n",
      "    mean_raw_obs_processing_ms: 1.4501955360744063\n",
      "  time_since_restore: 7114.04295539856\n",
      "  time_this_iter_s: 22.561872243881226\n",
      "  time_total_s: 7114.04295539856\n",
      "  timers:\n",
      "    learn_throughput: 1564.014\n",
      "    learn_time_ms: 639.381\n",
      "    load_throughput: 55916.374\n",
      "    load_time_ms: 17.884\n",
      "    sample_throughput: 36.784\n",
      "    sample_time_ms: 27185.371\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1633798228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         7114.04</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            402.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 800\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6919780810674032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013548782062962781\n",
      "          policy_loss: -0.07610088586807251\n",
      "          total_loss: -0.08827258911397722\n",
      "          vf_explained_var: 0.4713405668735504\n",
      "          vf_loss: 0.003662950047550516\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76451612903225\n",
      "    ram_util_percent: 71.1290322580645\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809269019592035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.953095729962982\n",
      "    mean_inference_ms: 1.7197207929596368\n",
      "    mean_raw_obs_processing_ms: 1.4501120466446635\n",
      "  time_since_restore: 7135.754219055176\n",
      "  time_this_iter_s: 21.71126365661621\n",
      "  time_total_s: 7135.754219055176\n",
      "  timers:\n",
      "    learn_throughput: 1569.692\n",
      "    learn_time_ms: 637.068\n",
      "    load_throughput: 55354.346\n",
      "    load_time_ms: 18.065\n",
      "    sample_throughput: 37.279\n",
      "    sample_time_ms: 26825.018\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633798250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         7135.75</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            400.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 396.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 803\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.877640950679779\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01478136614849856\n",
      "          policy_loss: -0.03425285890698433\n",
      "          total_loss: -0.04723798113150729\n",
      "          vf_explained_var: -0.13186298310756683\n",
      "          vf_loss: 0.004607442857619996\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81515151515151\n",
      "    ram_util_percent: 71.21212121212122\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809579447862534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.959927614369303\n",
      "    mean_inference_ms: 1.7197436912395625\n",
      "    mean_raw_obs_processing_ms: 1.4500482659543\n",
      "  time_since_restore: 7158.8499619960785\n",
      "  time_this_iter_s: 23.09574294090271\n",
      "  time_total_s: 7158.8499619960785\n",
      "  timers:\n",
      "    learn_throughput: 1567.451\n",
      "    learn_time_ms: 637.978\n",
      "    load_throughput: 54339.795\n",
      "    load_time_ms: 18.403\n",
      "    sample_throughput: 37.343\n",
      "    sample_time_ms: 26778.46\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633798273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         7158.85</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            396.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 390.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 806\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.98858503235711\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014029443976735766\n",
      "          policy_loss: 0.07664484447903103\n",
      "          total_loss: 0.05845772481213014\n",
      "          vf_explained_var: -0.10808488726615906\n",
      "          vf_loss: 0.0005751064726306746\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.688235294117646\n",
      "    ram_util_percent: 71.28235294117648\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680979285062527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.96746032356545\n",
      "    mean_inference_ms: 1.7197643042698514\n",
      "    mean_raw_obs_processing_ms: 1.4500433668464456\n",
      "  time_since_restore: 7182.238390922546\n",
      "  time_this_iter_s: 23.388428926467896\n",
      "  time_total_s: 7182.238390922546\n",
      "  timers:\n",
      "    learn_throughput: 1568.059\n",
      "    learn_time_ms: 637.731\n",
      "    load_throughput: 53825.85\n",
      "    load_time_ms: 18.578\n",
      "    sample_throughput: 37.833\n",
      "    sample_time_ms: 26431.856\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633798296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         7182.24</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            390.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 386.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 809\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1070228695869444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009043319073911284\n",
      "          policy_loss: -0.055516593960217304\n",
      "          total_loss: -0.07551488234765\n",
      "          vf_explained_var: 0.03521757572889328\n",
      "          vf_loss: 0.00034765801441002014\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.66875\n",
      "    ram_util_percent: 71.290625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809771950564445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.975531349369476\n",
      "    mean_inference_ms: 1.7197787615327016\n",
      "    mean_raw_obs_processing_ms: 1.4501377257208585\n",
      "  time_since_restore: 7204.79581451416\n",
      "  time_this_iter_s: 22.55742359161377\n",
      "  time_total_s: 7204.79581451416\n",
      "  timers:\n",
      "    learn_throughput: 1571.482\n",
      "    learn_time_ms: 636.342\n",
      "    load_throughput: 53734.894\n",
      "    load_time_ms: 18.61\n",
      "    sample_throughput: 38.474\n",
      "    sample_time_ms: 25991.29\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633798319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">          7204.8</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            386.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-52-41\n",
      "  done: false\n",
      "  episode_len_mean: 381.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.18\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 812\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1309199081526864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015461524833132304\n",
      "          policy_loss: -0.06487698952356974\n",
      "          total_loss: -0.08444144460890028\n",
      "          vf_explained_var: -0.3971020579338074\n",
      "          vf_loss: 0.0005064258631642184\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.380327868852454\n",
      "    ram_util_percent: 71.41311475409836\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809726543863656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.9843413137812\n",
      "    mean_inference_ms: 1.719793600189224\n",
      "    mean_raw_obs_processing_ms: 1.4517707949103325\n",
      "  time_since_restore: 7247.192887306213\n",
      "  time_this_iter_s: 42.39707279205322\n",
      "  time_total_s: 7247.192887306213\n",
      "  timers:\n",
      "    learn_throughput: 1569.395\n",
      "    learn_time_ms: 637.188\n",
      "    load_throughput: 52831.575\n",
      "    load_time_ms: 18.928\n",
      "    sample_throughput: 38.621\n",
      "    sample_time_ms: 25892.531\n",
      "    update_time_ms: 2.07\n",
      "  timestamp: 1633798361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         7247.19</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">   -0.18</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            381.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 376.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 815\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1337700499428642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006028769374647207\n",
      "          policy_loss: 0.07811647049254841\n",
      "          total_loss: 0.05773023437294695\n",
      "          vf_explained_var: -0.4006154239177704\n",
      "          vf_loss: 0.0004686181510300634\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71081081081081\n",
      "    ram_util_percent: 71.39459459459462\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680971055899294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.993958959145843\n",
      "    mean_inference_ms: 1.719810468563734\n",
      "    mean_raw_obs_processing_ms: 1.4534985268033518\n",
      "  time_since_restore: 7273.128489494324\n",
      "  time_this_iter_s: 25.93560218811035\n",
      "  time_total_s: 7273.128489494324\n",
      "  timers:\n",
      "    learn_throughput: 1572.583\n",
      "    learn_time_ms: 635.897\n",
      "    load_throughput: 52381.329\n",
      "    load_time_ms: 19.091\n",
      "    sample_throughput: 38.801\n",
      "    sample_time_ms: 25772.37\n",
      "    update_time_ms: 2.057\n",
      "  timestamp: 1633798387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         7273.13</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            376.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 370.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 818\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.112147238519457\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013170849344049135\n",
      "          policy_loss: 0.00674780516160859\n",
      "          total_loss: -0.012903203773829672\n",
      "          vf_explained_var: -0.5385060906410217\n",
      "          vf_loss: 0.000415606731419555\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55405405405405\n",
      "    ram_util_percent: 71.03783783783787\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368096149187635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.0043374283992\n",
      "    mean_inference_ms: 1.7198269079167492\n",
      "    mean_raw_obs_processing_ms: 1.4552763577313401\n",
      "  time_since_restore: 7299.134582042694\n",
      "  time_this_iter_s: 26.00609254837036\n",
      "  time_total_s: 7299.134582042694\n",
      "  timers:\n",
      "    learn_throughput: 1571.412\n",
      "    learn_time_ms: 636.37\n",
      "    load_throughput: 52265.34\n",
      "    load_time_ms: 19.133\n",
      "    sample_throughput: 39.306\n",
      "    sample_time_ms: 25441.699\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633798413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         7299.13</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            370.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-53-55\n",
      "  done: false\n",
      "  episode_len_mean: 367.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 821\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08009033203125002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.949219905005561\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02279236946720112\n",
      "          policy_loss: -0.016482206227050888\n",
      "          total_loss: -0.03355724811553955\n",
      "          vf_explained_var: 0.48643842339515686\n",
      "          vf_loss: 0.0005917081333690374\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73125\n",
      "    ram_util_percent: 70.821875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680954525050352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.015113510028065\n",
      "    mean_inference_ms: 1.7198451700672612\n",
      "    mean_raw_obs_processing_ms: 1.4555125924808123\n",
      "  time_since_restore: 7321.498854398727\n",
      "  time_this_iter_s: 22.364272356033325\n",
      "  time_total_s: 7321.498854398727\n",
      "  timers:\n",
      "    learn_throughput: 1577.685\n",
      "    learn_time_ms: 633.84\n",
      "    load_throughput: 52104.709\n",
      "    load_time_ms: 19.192\n",
      "    sample_throughput: 40.391\n",
      "    sample_time_ms: 24757.789\n",
      "    update_time_ms: 2.058\n",
      "  timestamp: 1633798435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">          7321.5</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            367.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 363.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 824\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.016873694790734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017606238014743664\n",
      "          policy_loss: -0.05361244926850001\n",
      "          total_loss: -0.07130191938744651\n",
      "          vf_explained_var: 0.031995102763175964\n",
      "          vf_loss: 0.0003641334111711735\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.22285714285715\n",
      "    ram_util_percent: 70.83428571428573\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809555265762406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.026349575974255\n",
      "    mean_inference_ms: 1.7198650184401125\n",
      "    mean_raw_obs_processing_ms: 1.455804112241162\n",
      "  time_since_restore: 7345.92217373848\n",
      "  time_this_iter_s: 24.423319339752197\n",
      "  time_total_s: 7345.92217373848\n",
      "  timers:\n",
      "    learn_throughput: 1576.63\n",
      "    learn_time_ms: 634.264\n",
      "    load_throughput: 52228.894\n",
      "    load_time_ms: 19.146\n",
      "    sample_throughput: 40.347\n",
      "    sample_time_ms: 24785.173\n",
      "    update_time_ms: 2.043\n",
      "  timestamp: 1633798460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         7345.92</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            363.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-54-44\n",
      "  done: false\n",
      "  episode_len_mean: 358.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 827\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2164789226320054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010196326210882962\n",
      "          policy_loss: -0.07993090682559544\n",
      "          total_loss: -0.10056796752744251\n",
      "          vf_explained_var: -0.42219871282577515\n",
      "          vf_loss: 0.00030278749893315965\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86764705882353\n",
      "    ram_util_percent: 70.80882352941178\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809635673661155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.03828873818\n",
      "    mean_inference_ms: 1.7198864880181017\n",
      "    mean_raw_obs_processing_ms: 1.4561910932014785\n",
      "  time_since_restore: 7369.780356884003\n",
      "  time_this_iter_s: 23.85818314552307\n",
      "  time_total_s: 7369.780356884003\n",
      "  timers:\n",
      "    learn_throughput: 1578.124\n",
      "    learn_time_ms: 633.664\n",
      "    load_throughput: 51588.869\n",
      "    load_time_ms: 19.384\n",
      "    sample_throughput: 40.136\n",
      "    sample_time_ms: 24915.169\n",
      "    update_time_ms: 2.042\n",
      "  timestamp: 1633798484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         7369.78</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            358.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 355.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 830\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.170193174150255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009337318338038338\n",
      "          policy_loss: 0.017877722365988627\n",
      "          total_loss: -0.002281004356013404\n",
      "          vf_explained_var: 0.32238876819610596\n",
      "          vf_loss: 0.00042146317868577577\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.8\n",
      "    ram_util_percent: 70.93437500000002\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680975587024799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.050571023109697\n",
      "    mean_inference_ms: 1.7199087829651925\n",
      "    mean_raw_obs_processing_ms: 1.4566284472838698\n",
      "  time_since_restore: 7392.680598258972\n",
      "  time_this_iter_s: 22.900241374969482\n",
      "  time_total_s: 7392.680598258972\n",
      "  timers:\n",
      "    learn_throughput: 1576.612\n",
      "    learn_time_ms: 634.271\n",
      "    load_throughput: 51287.336\n",
      "    load_time_ms: 19.498\n",
      "    sample_throughput: 39.947\n",
      "    sample_time_ms: 25033.364\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1633798507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         7392.68</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">   -0.17</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            355.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 353.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 832\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.219187765651279\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011465926813252123\n",
      "          policy_loss: 0.03399006575345993\n",
      "          total_loss: 0.24398261573579577\n",
      "          vf_explained_var: -0.5526103973388672\n",
      "          vf_loss: 0.23080696068807607\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.982352941176465\n",
      "    ram_util_percent: 71.0764705882353\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368098220383249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.058923363887647\n",
      "    mean_inference_ms: 1.7199240665238849\n",
      "    mean_raw_obs_processing_ms: 1.456913167175694\n",
      "  time_since_restore: 7416.612678766251\n",
      "  time_this_iter_s: 23.932080507278442\n",
      "  time_total_s: 7416.612678766251\n",
      "  timers:\n",
      "    learn_throughput: 1576.808\n",
      "    learn_time_ms: 634.193\n",
      "    load_throughput: 51147.741\n",
      "    load_time_ms: 19.551\n",
      "    sample_throughput: 39.814\n",
      "    sample_time_ms: 25117.037\n",
      "    update_time_ms: 2.031\n",
      "  timestamp: 1633798531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         7416.61</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             353.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-55-56\n",
      "  done: false\n",
      "  episode_len_mean: 350.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 836\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.264414562119378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0104510197071071\n",
      "          policy_loss: -0.08587806928488943\n",
      "          total_loss: -0.09213119455509716\n",
      "          vf_explained_var: 0.08557872474193573\n",
      "          vf_loss: 0.015135482460674312\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.616216216216216\n",
      "    ram_util_percent: 71.15405405405403\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680999604164171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.07614542831487\n",
      "    mean_inference_ms: 1.7199567121113624\n",
      "    mean_raw_obs_processing_ms: 1.4575405801297487\n",
      "  time_since_restore: 7442.414448738098\n",
      "  time_this_iter_s: 25.801769971847534\n",
      "  time_total_s: 7442.414448738098\n",
      "  timers:\n",
      "    learn_throughput: 1573.356\n",
      "    learn_time_ms: 635.584\n",
      "    load_throughput: 51491.905\n",
      "    load_time_ms: 19.421\n",
      "    sample_throughput: 39.437\n",
      "    sample_time_ms: 25357.102\n",
      "    update_time_ms: 2.024\n",
      "  timestamp: 1633798556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         7442.41</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            350.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 348.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 838\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.367915725708008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010369736369837904\n",
      "          policy_loss: -0.1086830832891994\n",
      "          total_loss: -0.12187303362621202\n",
      "          vf_explained_var: 0.16375568509101868\n",
      "          vf_loss: 0.009243437471903032\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.08181818181818\n",
      "    ram_util_percent: 71.33333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036810023770380965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.084903337853426\n",
      "    mean_inference_ms: 1.7199712225529522\n",
      "    mean_raw_obs_processing_ms: 1.457880616727179\n",
      "  time_since_restore: 7465.475732088089\n",
      "  time_this_iter_s: 23.061283349990845\n",
      "  time_total_s: 7465.475732088089\n",
      "  timers:\n",
      "    learn_throughput: 1572.48\n",
      "    learn_time_ms: 635.938\n",
      "    load_throughput: 51003.879\n",
      "    load_time_ms: 19.606\n",
      "    sample_throughput: 39.359\n",
      "    sample_time_ms: 25406.949\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633798580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         7465.48</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            348.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 346.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 841\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.371286784278022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007230513274701058\n",
      "          policy_loss: 0.007149384646779961\n",
      "          total_loss: -0.014184323615498012\n",
      "          vf_explained_var: -0.8613851070404053\n",
      "          vf_loss: 0.0015105172985285106\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.04426229508197\n",
      "    ram_util_percent: 71.29836065573771\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681004899449924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.098263141836572\n",
      "    mean_inference_ms: 1.7199927471234162\n",
      "    mean_raw_obs_processing_ms: 1.459870234321247\n",
      "  time_since_restore: 7507.88347697258\n",
      "  time_this_iter_s: 42.40774488449097\n",
      "  time_total_s: 7507.88347697258\n",
      "  timers:\n",
      "    learn_throughput: 1572.718\n",
      "    learn_time_ms: 635.842\n",
      "    load_throughput: 51311.492\n",
      "    load_time_ms: 19.489\n",
      "    sample_throughput: 39.357\n",
      "    sample_time_ms: 25408.254\n",
      "    update_time_ms: 2.034\n",
      "  timestamp: 1633798622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         7507.88</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            346.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 341.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 845\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106144579251607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017432922751909965\n",
      "          policy_loss: 0.05543830378188027\n",
      "          total_loss: 0.49222902539703584\n",
      "          vf_explained_var: -0.49819010496139526\n",
      "          vf_loss: 0.45575785381822953\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.02105263157895\n",
      "    ram_util_percent: 71.2894736842105\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681006976402143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.11659200819648\n",
      "    mean_inference_ms: 1.7200206956380515\n",
      "    mean_raw_obs_processing_ms: 1.4625621127944834\n",
      "  time_since_restore: 7534.809124469757\n",
      "  time_this_iter_s: 26.925647497177124\n",
      "  time_total_s: 7534.809124469757\n",
      "  timers:\n",
      "    learn_throughput: 1573.192\n",
      "    learn_time_ms: 635.65\n",
      "    load_throughput: 51602.388\n",
      "    load_time_ms: 19.379\n",
      "    sample_throughput: 39.204\n",
      "    sample_time_ms: 25507.543\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633798649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         7534.81</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            341.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 339.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.39\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 848\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0075257142384846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018856201763833368\n",
      "          policy_loss: 0.04835364123185475\n",
      "          total_loss: 0.12406854298379687\n",
      "          vf_explained_var: -0.37520632147789\n",
      "          vf_loss: 0.09352485628074242\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81951219512195\n",
      "    ram_util_percent: 70.98292682926828\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681007990648961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.130725397595032\n",
      "    mean_inference_ms: 1.7200408044638598\n",
      "    mean_raw_obs_processing_ms: 1.4646462274035241\n",
      "  time_since_restore: 7563.527236938477\n",
      "  time_this_iter_s: 28.718112468719482\n",
      "  time_total_s: 7563.527236938477\n",
      "  timers:\n",
      "    learn_throughput: 1573.454\n",
      "    learn_time_ms: 635.544\n",
      "    load_throughput: 51774.947\n",
      "    load_time_ms: 19.314\n",
      "    sample_throughput: 38.791\n",
      "    sample_time_ms: 25778.898\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633798678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         7563.53</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">   -0.39</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            339.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 336.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.45\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 851\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.156254866388109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011582136657604833\n",
      "          policy_loss: 0.009370446039570702\n",
      "          total_loss: 0.2833574898954895\n",
      "          vf_explained_var: -0.11706239730119705\n",
      "          vf_loss: 0.2941581719710181\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.961111111111116\n",
      "    ram_util_percent: 70.84444444444446\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681014138659084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.145093470465977\n",
      "    mean_inference_ms: 1.7200603116622686\n",
      "    mean_raw_obs_processing_ms: 1.4651813740574338\n",
      "  time_since_restore: 7588.833060741425\n",
      "  time_this_iter_s: 25.305823802947998\n",
      "  time_total_s: 7588.833060741425\n",
      "  timers:\n",
      "    learn_throughput: 1572.12\n",
      "    learn_time_ms: 636.084\n",
      "    load_throughput: 52017.794\n",
      "    load_time_ms: 19.224\n",
      "    sample_throughput: 38.354\n",
      "    sample_time_ms: 26072.602\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633798703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         7588.83</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">   -0.45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            336.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 333.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.45\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 854\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3166798750559487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012512173953008777\n",
      "          policy_loss: -0.07892813757061959\n",
      "          total_loss: -0.08458800812562307\n",
      "          vf_explained_var: -0.056873735040426254\n",
      "          vf_loss: 0.016003772797476914\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58888888888889\n",
      "    ram_util_percent: 70.81666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036810192726525914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.15979386952097\n",
      "    mean_inference_ms: 1.7200802524568024\n",
      "    mean_raw_obs_processing_ms: 1.4657674518307697\n",
      "  time_since_restore: 7613.414664745331\n",
      "  time_this_iter_s: 24.58160400390625\n",
      "  time_total_s: 7613.414664745331\n",
      "  timers:\n",
      "    learn_throughput: 1575.452\n",
      "    learn_time_ms: 634.738\n",
      "    load_throughput: 51400.784\n",
      "    load_time_ms: 19.455\n",
      "    sample_throughput: 38.33\n",
      "    sample_time_ms: 26089.446\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1633798728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         7613.41</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">   -0.45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            333.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 332.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.45\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 857\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4174422396553887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005719955797088044\n",
      "          policy_loss: -0.21105662484963736\n",
      "          total_loss: -0.22637926273875766\n",
      "          vf_explained_var: -0.3681158423423767\n",
      "          vf_loss: 0.008164612979938587\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53428571428571\n",
      "    ram_util_percent: 70.89142857142856\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681018884149252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.174629838649853\n",
      "    mean_inference_ms: 1.7200999604269154\n",
      "    mean_raw_obs_processing_ms: 1.4664023187764785\n",
      "  time_since_restore: 7637.908677339554\n",
      "  time_this_iter_s: 24.494012594223022\n",
      "  time_total_s: 7637.908677339554\n",
      "  timers:\n",
      "    learn_throughput: 1571.09\n",
      "    learn_time_ms: 636.501\n",
      "    load_throughput: 51770.346\n",
      "    load_time_ms: 19.316\n",
      "    sample_throughput: 38.239\n",
      "    sample_time_ms: 26151.399\n",
      "    update_time_ms: 2.168\n",
      "  timestamp: 1633798752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         7637.91</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">   -0.45</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            332.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_16-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 330.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 860\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.33583136399587\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015504148770614387\n",
      "          policy_loss: 0.003234295795361201\n",
      "          total_loss: -0.014536575890249676\n",
      "          vf_explained_var: -0.6733130812644958\n",
      "          vf_loss: 0.0037248429894033404\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.665714285714294\n",
      "    ram_util_percent: 70.99714285714285\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03681013606109426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.189635689413613\n",
      "    mean_inference_ms: 1.7201195260247524\n",
      "    mean_raw_obs_processing_ms: 1.467046311958288\n",
      "  time_since_restore: 7663.101271152496\n",
      "  time_this_iter_s: 25.192593812942505\n",
      "  time_total_s: 7663.101271152496\n",
      "  timers:\n",
      "    learn_throughput: 1575.459\n",
      "    learn_time_ms: 634.736\n",
      "    load_throughput: 51708.883\n",
      "    load_time_ms: 19.339\n",
      "    sample_throughput: 37.904\n",
      "    sample_time_ms: 26382.37\n",
      "    update_time_ms: 2.155\n",
      "  timestamp: 1633798777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">          7663.1</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            330.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 329.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 864\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.400808933046129\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017224519737464028\n",
      "          policy_loss: -0.06270839700268374\n",
      "          total_loss: -0.08172897129423089\n",
      "          vf_explained_var: -0.6589635014533997\n",
      "          vf_loss: 0.0029182407966194053\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67567567567567\n",
      "    ram_util_percent: 71.03513513513514\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809945202528185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.209591006375476\n",
      "    mean_inference_ms: 1.7201447382321065\n",
      "    mean_raw_obs_processing_ms: 1.4679740720039376\n",
      "  time_since_restore: 7688.421024799347\n",
      "  time_this_iter_s: 25.319753646850586\n",
      "  time_total_s: 7688.421024799347\n",
      "  timers:\n",
      "    learn_throughput: 1574.35\n",
      "    learn_time_ms: 635.183\n",
      "    load_throughput: 52093.966\n",
      "    load_time_ms: 19.196\n",
      "    sample_throughput: 37.706\n",
      "    sample_time_ms: 26520.831\n",
      "    update_time_ms: 2.158\n",
      "  timestamp: 1633798803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         7688.42</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            329.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 329.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 866\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.470906374189589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009787612186919346\n",
      "          policy_loss: -0.0233644704023997\n",
      "          total_loss: -0.04536548571454154\n",
      "          vf_explained_var: -0.6284942030906677\n",
      "          vf_loss: 0.0015322098010478334\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.146874999999994\n",
      "    ram_util_percent: 71.21875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809824947467075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.219500031761182\n",
      "    mean_inference_ms: 1.7201571444361423\n",
      "    mean_raw_obs_processing_ms: 1.468442535044891\n",
      "  time_since_restore: 7710.788469076157\n",
      "  time_this_iter_s: 22.367444276809692\n",
      "  time_total_s: 7710.788469076157\n",
      "  timers:\n",
      "    learn_throughput: 1577.108\n",
      "    learn_time_ms: 634.072\n",
      "    load_throughput: 51841.308\n",
      "    load_time_ms: 19.29\n",
      "    sample_throughput: 38.199\n",
      "    sample_time_ms: 26178.398\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1633798825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         7710.79</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            329.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 329.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 869\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3447822252909343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010131820315761628\n",
      "          policy_loss: 0.05242419160074658\n",
      "          total_loss: 0.031653200172715716\n",
      "          vf_explained_var: -0.6284095644950867\n",
      "          vf_loss: 0.0014596394188831456\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.01176470588235\n",
      "    ram_util_percent: 71.2529411764706\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809623776311924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.23417866277351\n",
      "    mean_inference_ms: 1.7201757487645692\n",
      "    mean_raw_obs_processing_ms: 1.4691534807165139\n",
      "  time_since_restore: 7734.947957038879\n",
      "  time_this_iter_s: 24.15948796272278\n",
      "  time_total_s: 7734.947957038879\n",
      "  timers:\n",
      "    learn_throughput: 1576.083\n",
      "    learn_time_ms: 634.484\n",
      "    load_throughput: 52196.331\n",
      "    load_time_ms: 19.158\n",
      "    sample_throughput: 38.04\n",
      "    sample_time_ms: 26287.956\n",
      "    update_time_ms: 2.169\n",
      "  timestamp: 1633798849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">         7734.95</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            329.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 327.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 873\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.231315522723728\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010456445062714339\n",
      "          policy_loss: 0.05564342161847485\n",
      "          total_loss: 0.03621201432413525\n",
      "          vf_explained_var: 0.5021857619285583\n",
      "          vf_loss: 0.0016255564793633919\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.03114754098361\n",
      "    ram_util_percent: 71.25901639344264\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036809323062568254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.253614293778554\n",
      "    mean_inference_ms: 1.720199963203268\n",
      "    mean_raw_obs_processing_ms: 1.4720469475471398\n",
      "  time_since_restore: 7777.356332063675\n",
      "  time_this_iter_s: 42.40837502479553\n",
      "  time_total_s: 7777.356332063675\n",
      "  timers:\n",
      "    learn_throughput: 1576.231\n",
      "    learn_time_ms: 634.425\n",
      "    load_throughput: 52457.324\n",
      "    load_time_ms: 19.063\n",
      "    sample_throughput: 38.04\n",
      "    sample_time_ms: 26288.149\n",
      "    update_time_ms: 2.159\n",
      "  timestamp: 1633798892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         7777.36</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            327.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 328.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 876\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3257033639483984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010921695162028507\n",
      "          policy_loss: -0.013536716004212697\n",
      "          total_loss: -0.034432058326072164\n",
      "          vf_explained_var: -0.2795926332473755\n",
      "          vf_loss: 0.0010496055480972346\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.730555555555554\n",
      "    ram_util_percent: 71.15277777777777\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680906470574264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.26790561448279\n",
      "    mean_inference_ms: 1.7202225388221555\n",
      "    mean_raw_obs_processing_ms: 1.474228610105325\n",
      "  time_since_restore: 7802.678193807602\n",
      "  time_this_iter_s: 25.321861743927002\n",
      "  time_total_s: 7802.678193807602\n",
      "  timers:\n",
      "    learn_throughput: 1576.167\n",
      "    learn_time_ms: 634.45\n",
      "    load_throughput: 52667.78\n",
      "    load_time_ms: 18.987\n",
      "    sample_throughput: 38.273\n",
      "    sample_time_ms: 26127.851\n",
      "    update_time_ms: 2.15\n",
      "  timestamp: 1633798917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         7802.68</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            328.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 328.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 879\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.411293927828471\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009830558032819603\n",
      "          policy_loss: -0.10048558964497513\n",
      "          total_loss: -0.1230775727579991\n",
      "          vf_explained_var: -0.3958461880683899\n",
      "          vf_loss: 0.0003399567081942223\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10882352941176\n",
      "    ram_util_percent: 71.04117647058823\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680880236412166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.281847997627416\n",
      "    mean_inference_ms: 1.7202448841851739\n",
      "    mean_raw_obs_processing_ms: 1.4759152503986237\n",
      "  time_since_restore: 7826.517549991608\n",
      "  time_this_iter_s: 23.839356184005737\n",
      "  time_total_s: 7826.517549991608\n",
      "  timers:\n",
      "    learn_throughput: 1573.669\n",
      "    learn_time_ms: 635.458\n",
      "    load_throughput: 52230.65\n",
      "    load_time_ms: 19.146\n",
      "    sample_throughput: 39.003\n",
      "    sample_time_ms: 25638.795\n",
      "    update_time_ms: 2.158\n",
      "  timestamp: 1633798941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         7826.52</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            328.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 328.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.52\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 882\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1632990333769055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011023844264875156\n",
      "          policy_loss: -0.043244702203406225\n",
      "          total_loss: -0.06191612026757664\n",
      "          vf_explained_var: 0.2949404716491699\n",
      "          vf_loss: 0.0016372148102770248\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84358974358973\n",
      "    ram_util_percent: 70.97948717948718\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680851772884963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.295747167356808\n",
      "    mean_inference_ms: 1.7202667985411622\n",
      "    mean_raw_obs_processing_ms: 1.4766126439646339\n",
      "  time_since_restore: 7854.098918914795\n",
      "  time_this_iter_s: 27.581368923187256\n",
      "  time_total_s: 7854.098918914795\n",
      "  timers:\n",
      "    learn_throughput: 1569.648\n",
      "    learn_time_ms: 637.085\n",
      "    load_throughput: 51897.307\n",
      "    load_time_ms: 19.269\n",
      "    sample_throughput: 38.663\n",
      "    sample_time_ms: 25864.597\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1633798968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">          7854.1</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">   -0.52</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            328.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 328.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.46\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 886\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0097436984380086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014781128824406133\n",
      "          policy_loss: -0.043261899633540046\n",
      "          total_loss: -0.06052739686436123\n",
      "          vf_explained_var: 0.4619762897491455\n",
      "          vf_loss: 0.0010562014449129087\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65853658536585\n",
      "    ram_util_percent: 71.03902439024391\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680812480827744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.314313658034788\n",
      "    mean_inference_ms: 1.720295238346448\n",
      "    mean_raw_obs_processing_ms: 1.477542450443739\n",
      "  time_since_restore: 7882.667289495468\n",
      "  time_this_iter_s: 28.568370580673218\n",
      "  time_total_s: 7882.667289495468\n",
      "  timers:\n",
      "    learn_throughput: 1567.368\n",
      "    learn_time_ms: 638.012\n",
      "    load_throughput: 52390.489\n",
      "    load_time_ms: 19.087\n",
      "    sample_throughput: 38.077\n",
      "    sample_time_ms: 26262.615\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633798997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         7882.67</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">   -0.46</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            328.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 328.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 889\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1215440299775867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019701953964460808\n",
      "          policy_loss: -0.11158579281634755\n",
      "          total_loss: -0.12979162865214877\n",
      "          vf_explained_var: 0.6100659966468811\n",
      "          vf_loss: 0.0006426991034661317\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73333333333333\n",
      "    ram_util_percent: 71.14358974358971\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680782689902907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.327894628885716\n",
      "    mean_inference_ms: 1.720315185174358\n",
      "    mean_raw_obs_processing_ms: 1.478255571229738\n",
      "  time_since_restore: 7909.969487905502\n",
      "  time_this_iter_s: 27.30219841003418\n",
      "  time_total_s: 7909.969487905502\n",
      "  timers:\n",
      "    learn_throughput: 1567.014\n",
      "    learn_time_ms: 638.156\n",
      "    load_throughput: 52172.308\n",
      "    load_time_ms: 19.167\n",
      "    sample_throughput: 37.674\n",
      "    sample_time_ms: 26543.216\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1633799024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         7909.97</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            328.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-04-13\n",
      "  done: false\n",
      "  episode_len_mean: 328.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 892\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.021900251176622\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01149704787602715\n",
      "          policy_loss: -0.1530060422089365\n",
      "          total_loss: -0.17071684416797425\n",
      "          vf_explained_var: -0.25890544056892395\n",
      "          vf_loss: 0.0011269952759094951\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9\n",
      "    ram_util_percent: 71.26500000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807517846397374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.341575704397684\n",
      "    mean_inference_ms: 1.720334689233819\n",
      "    mean_raw_obs_processing_ms: 1.4789368043511544\n",
      "  time_since_restore: 7938.306100845337\n",
      "  time_this_iter_s: 28.336612939834595\n",
      "  time_total_s: 7938.306100845337\n",
      "  timers:\n",
      "    learn_throughput: 1562.18\n",
      "    learn_time_ms: 640.131\n",
      "    load_throughput: 52858.74\n",
      "    load_time_ms: 18.918\n",
      "    sample_throughput: 37.236\n",
      "    sample_time_ms: 26855.886\n",
      "    update_time_ms: 2.108\n",
      "  timestamp: 1633799053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         7938.31</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            328.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 327.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 896\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0370572606722512\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009515668790115188\n",
      "          policy_loss: -0.0956707792977492\n",
      "          total_loss: -0.11435766203535927\n",
      "          vf_explained_var: 0.41102054715156555\n",
      "          vf_loss: 0.0005405202799011021\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.689189189189186\n",
      "    ram_util_percent: 71.29189189189192\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807091755282434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.35982060937435\n",
      "    mean_inference_ms: 1.7203606905281037\n",
      "    mean_raw_obs_processing_ms: 1.479897216653049\n",
      "  time_since_restore: 7964.175176620483\n",
      "  time_this_iter_s: 25.869075775146484\n",
      "  time_total_s: 7964.175176620483\n",
      "  timers:\n",
      "    learn_throughput: 1563.282\n",
      "    learn_time_ms: 639.68\n",
      "    load_throughput: 52695.969\n",
      "    load_time_ms: 18.977\n",
      "    sample_throughput: 37.159\n",
      "    sample_time_ms: 26911.146\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1633799079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         7964.18</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">             327.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-05-05\n",
      "  done: false\n",
      "  episode_len_mean: 324.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 899\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.025974537266625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010626835028291303\n",
      "          policy_loss: 0.03172116480353806\n",
      "          total_loss: 0.01327938193248378\n",
      "          vf_explained_var: -0.8166541457176208\n",
      "          vf_loss: 0.0005413029996109091\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77105263157895\n",
      "    ram_util_percent: 71.38684210526317\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680678553098145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.37377592302033\n",
      "    mean_inference_ms: 1.7203803060599125\n",
      "    mean_raw_obs_processing_ms: 1.4806358292255886\n",
      "  time_since_restore: 7990.7895176410675\n",
      "  time_this_iter_s: 26.614341020584106\n",
      "  time_total_s: 7990.7895176410675\n",
      "  timers:\n",
      "    learn_throughput: 1560.743\n",
      "    learn_time_ms: 640.72\n",
      "    load_throughput: 52791.477\n",
      "    load_time_ms: 18.942\n",
      "    sample_throughput: 36.583\n",
      "    sample_time_ms: 27334.848\n",
      "    update_time_ms: 2.162\n",
      "  timestamp: 1633799105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         7990.79</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            324.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 323.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 902\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.063798577255673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01341756448492717\n",
      "          policy_loss: -0.026330521785550648\n",
      "          total_loss: -0.04477609027591017\n",
      "          vf_explained_var: -0.6461657285690308\n",
      "          vf_loss: 0.0005804863195711126\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.845312500000006\n",
      "    ram_util_percent: 71.40312499999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368064853350045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.38805585495954\n",
      "    mean_inference_ms: 1.7204007828427144\n",
      "    mean_raw_obs_processing_ms: 1.4827536856207357\n",
      "  time_since_restore: 8035.438139915466\n",
      "  time_this_iter_s: 44.648622274398804\n",
      "  time_total_s: 8035.438139915466\n",
      "  timers:\n",
      "    learn_throughput: 1563.975\n",
      "    learn_time_ms: 639.396\n",
      "    load_throughput: 52491.659\n",
      "    load_time_ms: 19.051\n",
      "    sample_throughput: 34.031\n",
      "    sample_time_ms: 29384.968\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1633799150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         8035.44</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            323.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 320.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 906\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9054103546672396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008679015301724767\n",
      "          policy_loss: -0.050015993830230504\n",
      "          total_loss: -0.06762494535909759\n",
      "          vf_explained_var: 0.3361447751522064\n",
      "          vf_loss: 0.0004024967829334653\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.79\n",
      "    ram_util_percent: 71.33250000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806096596728816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.407425865345598\n",
      "    mean_inference_ms: 1.720428317159276\n",
      "    mean_raw_obs_processing_ms: 1.485598805730035\n",
      "  time_since_restore: 8062.983011722565\n",
      "  time_this_iter_s: 27.54487180709839\n",
      "  time_total_s: 8062.983011722565\n",
      "  timers:\n",
      "    learn_throughput: 1565.806\n",
      "    learn_time_ms: 638.649\n",
      "    load_throughput: 52939.066\n",
      "    load_time_ms: 18.89\n",
      "    sample_throughput: 35.843\n",
      "    sample_time_ms: 27899.534\n",
      "    update_time_ms: 2.183\n",
      "  timestamp: 1633799177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         8062.98</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            320.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 318.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 909\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9680428981781006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01701467185320548\n",
      "          policy_loss: 0.0396829593512747\n",
      "          total_loss: 0.022561144083738327\n",
      "          vf_explained_var: -0.08580709248781204\n",
      "          vf_loss: 0.0005145483575082229\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.833333333333336\n",
      "    ram_util_percent: 70.97692307692309\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680580111106045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.42220496110527\n",
      "    mean_inference_ms: 1.720449206071899\n",
      "    mean_raw_obs_processing_ms: 1.487764651977111\n",
      "  time_since_restore: 8090.416594028473\n",
      "  time_this_iter_s: 27.433582305908203\n",
      "  time_total_s: 8090.416594028473\n",
      "  timers:\n",
      "    learn_throughput: 1564.04\n",
      "    learn_time_ms: 639.37\n",
      "    load_throughput: 52573.509\n",
      "    load_time_ms: 19.021\n",
      "    sample_throughput: 35.575\n",
      "    sample_time_ms: 28109.851\n",
      "    update_time_ms: 2.173\n",
      "  timestamp: 1633799205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         8090.42</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            318.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 317.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.28\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 913\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9117276867230732\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006812812576606022\n",
      "          policy_loss: 0.2098203119304445\n",
      "          total_loss: 0.19200426323546305\n",
      "          vf_explained_var: 0.1788189709186554\n",
      "          vf_loss: 0.0004827668421461971\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73157894736842\n",
      "    ram_util_percent: 70.89210526315792\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680545552759441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.441774044909042\n",
      "    mean_inference_ms: 1.720477576542716\n",
      "    mean_raw_obs_processing_ms: 1.4887240625637543\n",
      "  time_since_restore: 8117.306605100632\n",
      "  time_this_iter_s: 26.890011072158813\n",
      "  time_total_s: 8117.306605100632\n",
      "  timers:\n",
      "    learn_throughput: 1568.266\n",
      "    learn_time_ms: 637.647\n",
      "    load_throughput: 53188.06\n",
      "    load_time_ms: 18.801\n",
      "    sample_throughput: 35.19\n",
      "    sample_time_ms: 28416.876\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1633799232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         8117.31</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">   -0.28</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            317.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 316.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 916\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1361022684309217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010974380372148455\n",
      "          policy_loss: 0.007145387265417311\n",
      "          total_loss: -0.012673980370163918\n",
      "          vf_explained_var: -0.618773877620697\n",
      "          vf_loss: 0.0002232428957035558\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77567567567568\n",
      "    ram_util_percent: 70.92702702702704\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036805169712326447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.45632050980802\n",
      "    mean_inference_ms: 1.7204987605081217\n",
      "    mean_raw_obs_processing_ms: 1.4894605827926453\n",
      "  time_since_restore: 8143.24976682663\n",
      "  time_this_iter_s: 25.943161725997925\n",
      "  time_total_s: 8143.24976682663\n",
      "  timers:\n",
      "    learn_throughput: 1570.14\n",
      "    learn_time_ms: 636.886\n",
      "    load_throughput: 53317.198\n",
      "    load_time_ms: 18.756\n",
      "    sample_throughput: 35.393\n",
      "    sample_time_ms: 28253.85\n",
      "    update_time_ms: 2.159\n",
      "  timestamp: 1633799258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         8143.25</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            316.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-08-05\n",
      "  done: false\n",
      "  episode_len_mean: 316.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 919\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.166238522529602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013245123389553252\n",
      "          policy_loss: -0.0864840779453516\n",
      "          total_loss: -0.10635975342657832\n",
      "          vf_explained_var: -0.24278460443019867\n",
      "          vf_loss: 0.00019550024169600672\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.620512820512815\n",
      "    ram_util_percent: 71.03846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036804905717626915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.470954703962917\n",
      "    mean_inference_ms: 1.7205198856505888\n",
      "    mean_raw_obs_processing_ms: 1.4902054495084658\n",
      "  time_since_restore: 8170.100607872009\n",
      "  time_this_iter_s: 26.85084104537964\n",
      "  time_total_s: 8170.100607872009\n",
      "  timers:\n",
      "    learn_throughput: 1570.151\n",
      "    learn_time_ms: 636.881\n",
      "    load_throughput: 53050.754\n",
      "    load_time_ms: 18.85\n",
      "    sample_throughput: 35.61\n",
      "    sample_time_ms: 28082.035\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1633799285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">          8170.1</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            316.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 314.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 922\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.128589733441671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007363524578794964\n",
      "          policy_loss: -0.017702442821529177\n",
      "          total_loss: -0.037753161953555214\n",
      "          vf_explained_var: -0.11111903935670853\n",
      "          vf_loss: 0.00035055714292361195\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.260000000000005\n",
      "    ram_util_percent: 71.12285714285713\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036804622199118404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.48568212707745\n",
      "    mean_inference_ms: 1.7205423909246051\n",
      "    mean_raw_obs_processing_ms: 1.4909569578340922\n",
      "  time_since_restore: 8195.11274933815\n",
      "  time_this_iter_s: 25.012141466140747\n",
      "  time_total_s: 8195.11274933815\n",
      "  timers:\n",
      "    learn_throughput: 1572.5\n",
      "    learn_time_ms: 635.93\n",
      "    load_throughput: 53556.431\n",
      "    load_time_ms: 18.672\n",
      "    sample_throughput: 35.901\n",
      "    sample_time_ms: 27854.177\n",
      "    update_time_ms: 2.121\n",
      "  timestamp: 1633799310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         8195.11</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            314.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 315.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 925\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1248839484320747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007550307494121\n",
      "          policy_loss: -0.07740944145868221\n",
      "          total_loss: -0.09764894958999422\n",
      "          vf_explained_var: -0.41890114545822144\n",
      "          vf_loss: 0.00010227296111730135\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.9\n",
      "    ram_util_percent: 71.14166666666665\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036804327342799245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.50041360180585\n",
      "    mean_inference_ms: 1.720564991371519\n",
      "    mean_raw_obs_processing_ms: 1.4917160065549362\n",
      "  time_since_restore: 8220.28560590744\n",
      "  time_this_iter_s: 25.17285656929016\n",
      "  time_total_s: 8220.28560590744\n",
      "  timers:\n",
      "    learn_throughput: 1576.239\n",
      "    learn_time_ms: 634.421\n",
      "    load_throughput: 53522.601\n",
      "    load_time_ms: 18.684\n",
      "    sample_throughput: 36.312\n",
      "    sample_time_ms: 27539.298\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1633799335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         8220.29</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            315.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-09-21\n",
      "  done: false\n",
      "  episode_len_mean: 312.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 929\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1156390137142607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010268620792903506\n",
      "          policy_loss: -0.015085064247250557\n",
      "          total_loss: -0.034810280013415545\n",
      "          vf_explained_var: -0.9757000207901001\n",
      "          vf_loss: 0.00019754506057425815\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7\n",
      "    ram_util_percent: 71.2\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680389894228149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.520134813796627\n",
      "    mean_inference_ms: 1.720595458508538\n",
      "    mean_raw_obs_processing_ms: 1.4927658791515297\n",
      "  time_since_restore: 8245.99718594551\n",
      "  time_this_iter_s: 25.71158003807068\n",
      "  time_total_s: 8245.99718594551\n",
      "  timers:\n",
      "    learn_throughput: 1576.305\n",
      "    learn_time_ms: 634.395\n",
      "    load_throughput: 53614.005\n",
      "    load_time_ms: 18.652\n",
      "    sample_throughput: 36.333\n",
      "    sample_time_ms: 27523.402\n",
      "    update_time_ms: 2.299\n",
      "  timestamp: 1633799361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">            8246</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            312.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 311.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 932\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1258209572898017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0099690631609939\n",
      "          policy_loss: -0.04517589037617047\n",
      "          total_loss: -0.0651603733499845\n",
      "          vf_explained_var: 0.08702895045280457\n",
      "          vf_loss: 7.608603314616226e-05\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.771874999999994\n",
      "    ram_util_percent: 71.2765625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680359063842911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.53515923080036\n",
      "    mean_inference_ms: 1.7206183198695824\n",
      "    mean_raw_obs_processing_ms: 1.4949042534592514\n",
      "  time_since_restore: 8290.748848676682\n",
      "  time_this_iter_s: 44.751662731170654\n",
      "  time_total_s: 8290.748848676682\n",
      "  timers:\n",
      "    learn_throughput: 1577.005\n",
      "    learn_time_ms: 634.113\n",
      "    load_throughput: 53923.63\n",
      "    load_time_ms: 18.545\n",
      "    sample_throughput: 34.086\n",
      "    sample_time_ms: 29337.535\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633799405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         8290.75</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            311.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 310.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 935\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2204616652594673\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014079195343365372\n",
      "          policy_loss: -0.08204346340563562\n",
      "          total_loss: -0.10234046855734455\n",
      "          vf_explained_var: -0.6844297647476196\n",
      "          vf_loss: 0.00021619777997127838\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.38611111111112\n",
      "    ram_util_percent: 71.23333333333332\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680328479188474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.549983807939853\n",
      "    mean_inference_ms: 1.7206406175302658\n",
      "    mean_raw_obs_processing_ms: 1.497080800601488\n",
      "  time_since_restore: 8316.442912340164\n",
      "  time_this_iter_s: 25.694063663482666\n",
      "  time_total_s: 8316.442912340164\n",
      "  timers:\n",
      "    learn_throughput: 1579.304\n",
      "    learn_time_ms: 633.19\n",
      "    load_throughput: 54429.704\n",
      "    load_time_ms: 18.372\n",
      "    sample_throughput: 36.439\n",
      "    sample_time_ms: 27443.083\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1633799431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         8316.44</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            310.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 309.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 939\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2241334438323976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007065707296546976\n",
      "          policy_loss: 0.01842221054765913\n",
      "          total_loss: -0.0028479055398040348\n",
      "          vf_explained_var: -0.690258264541626\n",
      "          vf_loss: 0.000122376170192082\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.544736842105266\n",
      "    ram_util_percent: 70.82105263157892\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036802887607385044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.569892120906726\n",
      "    mean_inference_ms: 1.7206713248838725\n",
      "    mean_raw_obs_processing_ms: 1.499516066045687\n",
      "  time_since_restore: 8342.373409032822\n",
      "  time_this_iter_s: 25.93049669265747\n",
      "  time_total_s: 8342.373409032822\n",
      "  timers:\n",
      "    learn_throughput: 1579.098\n",
      "    learn_time_ms: 633.273\n",
      "    load_throughput: 54151.145\n",
      "    load_time_ms: 18.467\n",
      "    sample_throughput: 36.655\n",
      "    sample_time_ms: 27281.489\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1633799457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         8342.37</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            309.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-11-21\n",
      "  done: false\n",
      "  episode_len_mean: 309.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.23\n",
      "  episode_reward_min: -11.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 942\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.257630043559604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00792232456974236\n",
      "          policy_loss: -0.11669892143044207\n",
      "          total_loss: -0.1381396096613672\n",
      "          vf_explained_var: -0.8424361348152161\n",
      "          vf_loss: 0.0001838576880497082\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.932352941176475\n",
      "    ram_util_percent: 70.69999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680258643940336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.58450502198239\n",
      "    mean_inference_ms: 1.7206944119141352\n",
      "    mean_raw_obs_processing_ms: 1.5003113597339415\n",
      "  time_since_restore: 8366.722682714462\n",
      "  time_this_iter_s: 24.349273681640625\n",
      "  time_total_s: 8366.722682714462\n",
      "  timers:\n",
      "    learn_throughput: 1580.455\n",
      "    learn_time_ms: 632.729\n",
      "    load_throughput: 54059.017\n",
      "    load_time_ms: 18.498\n",
      "    sample_throughput: 37.073\n",
      "    sample_time_ms: 26973.569\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1633799481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         8366.72</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">   -0.23</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -11</td><td style=\"text-align: right;\">            309.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 310.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 945\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.228438032997979\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008044995213893813\n",
      "          policy_loss: 0.012300935718748305\n",
      "          total_loss: -0.008858174540930325\n",
      "          vf_explained_var: -0.9930218458175659\n",
      "          vf_loss: 0.00015878070456286272\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63333333333334\n",
      "    ram_util_percent: 70.70833333333331\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036802251979041196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.599069061152523\n",
      "    mean_inference_ms: 1.7207173632796977\n",
      "    mean_raw_obs_processing_ms: 1.501074013996191\n",
      "  time_since_restore: 8391.555669546127\n",
      "  time_this_iter_s: 24.83298683166504\n",
      "  time_total_s: 8391.555669546127\n",
      "  timers:\n",
      "    learn_throughput: 1580.305\n",
      "    learn_time_ms: 632.789\n",
      "    load_throughput: 54072.956\n",
      "    load_time_ms: 18.494\n",
      "    sample_throughput: 37.358\n",
      "    sample_time_ms: 26767.818\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1633799506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         8391.56</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             310.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 311.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 948\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1910951084560817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0073981004442610085\n",
      "          policy_loss: -0.07567330582274331\n",
      "          total_loss: -0.09656660142872069\n",
      "          vf_explained_var: 0.2862665057182312\n",
      "          vf_loss: 0.00012887839523803752\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.94285714285714\n",
      "    ram_util_percent: 70.80571428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801911827292276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.613233235263834\n",
      "    mean_inference_ms: 1.720740088361025\n",
      "    mean_raw_obs_processing_ms: 1.5018429810790883\n",
      "  time_since_restore: 8416.379610776901\n",
      "  time_this_iter_s: 24.823941230773926\n",
      "  time_total_s: 8416.379610776901\n",
      "  timers:\n",
      "    learn_throughput: 1580.521\n",
      "    learn_time_ms: 632.703\n",
      "    load_throughput: 54453.377\n",
      "    load_time_ms: 18.364\n",
      "    sample_throughput: 37.515\n",
      "    sample_time_ms: 26656.146\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1633799531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         8416.38</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            311.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-12-37\n",
      "  done: false\n",
      "  episode_len_mean: 311.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 951\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.921828677919176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010015775566976654\n",
      "          policy_loss: -0.14672990093628566\n",
      "          total_loss: -0.16458703577518463\n",
      "          vf_explained_var: -0.3426990211009979\n",
      "          vf_loss: 0.00015790278149425933\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32631578947369\n",
      "    ram_util_percent: 70.9184210526316\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368015214610704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.627399951960744\n",
      "    mean_inference_ms: 1.720762398628597\n",
      "    mean_raw_obs_processing_ms: 1.5026177992285534\n",
      "  time_since_restore: 8442.62772512436\n",
      "  time_this_iter_s: 26.248114347457886\n",
      "  time_total_s: 8442.62772512436\n",
      "  timers:\n",
      "    learn_throughput: 1582.659\n",
      "    learn_time_ms: 631.848\n",
      "    load_throughput: 54532.316\n",
      "    load_time_ms: 18.338\n",
      "    sample_throughput: 37.599\n",
      "    sample_time_ms: 26596.738\n",
      "    update_time_ms: 2.329\n",
      "  timestamp: 1633799557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         8442.63</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            311.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 310.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 954\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.002597671084934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008409671955654948\n",
      "          policy_loss: -0.05860916500290235\n",
      "          total_loss: -0.0774202358805471\n",
      "          vf_explained_var: 0.10836570709943771\n",
      "          vf_loss: 0.00020460862255681098\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55675675675676\n",
      "    ram_util_percent: 71.03513513513512\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680114140670918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.641591609364653\n",
      "    mean_inference_ms: 1.720784617406474\n",
      "    mean_raw_obs_processing_ms: 1.5033991921024046\n",
      "  time_since_restore: 8468.404081106186\n",
      "  time_this_iter_s: 25.776355981826782\n",
      "  time_total_s: 8468.404081106186\n",
      "  timers:\n",
      "    learn_throughput: 1581.241\n",
      "    learn_time_ms: 632.415\n",
      "    load_throughput: 56368.084\n",
      "    load_time_ms: 17.741\n",
      "    sample_throughput: 37.491\n",
      "    sample_time_ms: 26672.957\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1633799583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">          8468.4</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             310.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 309.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 957\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0251202079984876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009496897480229076\n",
      "          policy_loss: -0.05359183210465643\n",
      "          total_loss: -0.07258268495400746\n",
      "          vf_explained_var: -0.856799840927124\n",
      "          vf_loss: 0.00011943364556322598\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54571428571428\n",
      "    ram_util_percent: 71.09714285714284\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680079596636868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.655766155595977\n",
      "    mean_inference_ms: 1.720807236881358\n",
      "    mean_raw_obs_processing_ms: 1.5041875244810108\n",
      "  time_since_restore: 8493.53744316101\n",
      "  time_this_iter_s: 25.13336205482483\n",
      "  time_total_s: 8493.53744316101\n",
      "  timers:\n",
      "    learn_throughput: 1578.484\n",
      "    learn_time_ms: 633.519\n",
      "    load_throughput: 58360.243\n",
      "    load_time_ms: 17.135\n",
      "    sample_throughput: 37.497\n",
      "    sample_time_ms: 26668.506\n",
      "    update_time_ms: 2.504\n",
      "  timestamp: 1633799608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         8493.54</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            309.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 308.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 961\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0430133395724828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013501323865921932\n",
      "          policy_loss: -0.0585606956647502\n",
      "          total_loss: -0.07721186917689112\n",
      "          vf_explained_var: -0.7376992702484131\n",
      "          vf_loss: 0.00015697168508065968\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.675384615384615\n",
      "    ram_util_percent: 71.21230769230769\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680031693036222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.67480445926317\n",
      "    mean_inference_ms: 1.7208381778000046\n",
      "    mean_raw_obs_processing_ms: 1.5069713885472698\n",
      "  time_since_restore: 8538.916536331177\n",
      "  time_this_iter_s: 45.379093170166016\n",
      "  time_total_s: 8538.916536331177\n",
      "  timers:\n",
      "    learn_throughput: 1577.541\n",
      "    learn_time_ms: 633.898\n",
      "    load_throughput: 58452.636\n",
      "    load_time_ms: 17.108\n",
      "    sample_throughput: 34.922\n",
      "    sample_time_ms: 28634.941\n",
      "    update_time_ms: 2.24\n",
      "  timestamp: 1633799654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         8538.92</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            308.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 308.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 964\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.097333867020077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017570537983291366\n",
      "          policy_loss: -0.10734670749968953\n",
      "          total_loss: -0.12608582948644956\n",
      "          vf_explained_var: -0.4656691551208496\n",
      "          vf_loss: 0.00012337003164349073\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81944444444445\n",
      "    ram_util_percent: 71.32777777777781\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799965886917745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.689084315561917\n",
      "    mean_inference_ms: 1.7208615224996264\n",
      "    mean_raw_obs_processing_ms: 1.5090334756545056\n",
      "  time_since_restore: 8563.7196996212\n",
      "  time_this_iter_s: 24.803163290023804\n",
      "  time_total_s: 8563.7196996212\n",
      "  timers:\n",
      "    learn_throughput: 1578.063\n",
      "    learn_time_ms: 633.688\n",
      "    load_throughput: 58144.878\n",
      "    load_time_ms: 17.198\n",
      "    sample_throughput: 37.537\n",
      "    sample_time_ms: 26640.193\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1633799679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         8563.72</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             308.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 307.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 967\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1024550716082255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009933960353453999\n",
      "          policy_loss: -0.08697397079732683\n",
      "          total_loss: -0.10665470759073893\n",
      "          vf_explained_var: 0.09174838662147522\n",
      "          vf_loss: 0.0001503890830564261\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.459375\n",
      "    ram_util_percent: 71.140625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679962575837866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.70322878305186\n",
      "    mean_inference_ms: 1.7208843527905848\n",
      "    mean_raw_obs_processing_ms: 1.5111369614282046\n",
      "  time_since_restore: 8586.485146284103\n",
      "  time_this_iter_s: 22.765446662902832\n",
      "  time_total_s: 8586.485146284103\n",
      "  timers:\n",
      "    learn_throughput: 1577.348\n",
      "    learn_time_ms: 633.975\n",
      "    load_throughput: 57524.382\n",
      "    load_time_ms: 17.384\n",
      "    sample_throughput: 37.955\n",
      "    sample_time_ms: 26346.95\n",
      "    update_time_ms: 2.174\n",
      "  timestamp: 1633799701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         8586.49</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            307.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 309.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 970\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0202060990863377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012813959116762617\n",
      "          policy_loss: -0.10571975509325664\n",
      "          total_loss: -0.12387492871946759\n",
      "          vf_explained_var: -0.3596545457839966\n",
      "          vf_loss: 0.0005074790711255951\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.41764705882353\n",
      "    ram_util_percent: 70.92058823529413\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799284742195154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.717228758374205\n",
      "    mean_inference_ms: 1.720907144317519\n",
      "    mean_raw_obs_processing_ms: 1.512771694725981\n",
      "  time_since_restore: 8609.958119869232\n",
      "  time_this_iter_s: 23.472973585128784\n",
      "  time_total_s: 8609.958119869232\n",
      "  timers:\n",
      "    learn_throughput: 1578.943\n",
      "    learn_time_ms: 633.335\n",
      "    load_throughput: 57769.918\n",
      "    load_time_ms: 17.31\n",
      "    sample_throughput: 38.311\n",
      "    sample_time_ms: 26101.902\n",
      "    update_time_ms: 2.183\n",
      "  timestamp: 1633799725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         8609.96</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            309.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 310.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 973\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.035703115993076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007227934708715426\n",
      "          policy_loss: 0.014818825489944882\n",
      "          total_loss: -0.0044044381628433864\n",
      "          vf_explained_var: -0.3266737163066864\n",
      "          vf_loss: 0.00026543715050340524\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.344117647058816\n",
      "    ram_util_percent: 70.9\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679889042918263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.731208326468536\n",
      "    mean_inference_ms: 1.7209294413024674\n",
      "    mean_raw_obs_processing_ms: 1.5134258360037478\n",
      "  time_since_restore: 8633.933023691177\n",
      "  time_this_iter_s: 23.97490382194519\n",
      "  time_total_s: 8633.933023691177\n",
      "  timers:\n",
      "    learn_throughput: 1578.152\n",
      "    learn_time_ms: 633.652\n",
      "    load_throughput: 57988.682\n",
      "    load_time_ms: 17.245\n",
      "    sample_throughput: 38.367\n",
      "    sample_time_ms: 26064.186\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633799749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         8633.93</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            310.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 310.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 975\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8835302617814806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012246329533769688\n",
      "          policy_loss: 0.008162193414237764\n",
      "          total_loss: -0.008859807501236598\n",
      "          vf_explained_var: -0.4837474822998047\n",
      "          vf_loss: 0.00034208562477336575\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.85142857142857\n",
      "    ram_util_percent: 70.94285714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036798635679443296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.74044363527089\n",
      "    mean_inference_ms: 1.720940727211175\n",
      "    mean_raw_obs_processing_ms: 1.5138652866225664\n",
      "  time_since_restore: 8658.406576871872\n",
      "  time_this_iter_s: 24.47355318069458\n",
      "  time_total_s: 8658.406576871872\n",
      "  timers:\n",
      "    learn_throughput: 1576.22\n",
      "    learn_time_ms: 634.429\n",
      "    load_throughput: 57783.925\n",
      "    load_time_ms: 17.306\n",
      "    sample_throughput: 38.421\n",
      "    sample_time_ms: 26027.425\n",
      "    update_time_ms: 2.206\n",
      "  timestamp: 1633799773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         8658.41</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            310.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 311.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 978\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.872957968711853\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012904271387802287\n",
      "          policy_loss: -0.06543059065524075\n",
      "          total_loss: -0.08222460194180409\n",
      "          vf_explained_var: 0.5892431139945984\n",
      "          vf_loss: 0.00038530724171626695\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.445161290322574\n",
      "    ram_util_percent: 71.01290322580644\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679819401374997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.754197489793714\n",
      "    mean_inference_ms: 1.720956429761099\n",
      "    mean_raw_obs_processing_ms: 1.5144956161991678\n",
      "  time_since_restore: 8680.355304718018\n",
      "  time_this_iter_s: 21.94872784614563\n",
      "  time_total_s: 8680.355304718018\n",
      "  timers:\n",
      "    learn_throughput: 1577.464\n",
      "    learn_time_ms: 633.929\n",
      "    load_throughput: 57731.909\n",
      "    load_time_ms: 17.321\n",
      "    sample_throughput: 38.85\n",
      "    sample_time_ms: 25740.352\n",
      "    update_time_ms: 2.214\n",
      "  timestamp: 1633799795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         8680.36</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            311.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-16-55\n",
      "  done: false\n",
      "  episode_len_mean: 313.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 980\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.775383636686537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069795416810568035\n",
      "          policy_loss: -0.035234041263659796\n",
      "          total_loss: -0.05171392286817233\n",
      "          vf_explained_var: 0.27871787548065186\n",
      "          vf_loss: 0.00043546042773717395\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.310714285714276\n",
      "    ram_util_percent: 71.06785714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679789094769209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.7630470712582\n",
      "    mean_inference_ms: 1.720966177102756\n",
      "    mean_raw_obs_processing_ms: 1.5149067630872408\n",
      "  time_since_restore: 8699.917228937149\n",
      "  time_this_iter_s: 19.56192421913147\n",
      "  time_total_s: 8699.917228937149\n",
      "  timers:\n",
      "    learn_throughput: 1577.528\n",
      "    learn_time_ms: 633.903\n",
      "    load_throughput: 57903.984\n",
      "    load_time_ms: 17.27\n",
      "    sample_throughput: 39.885\n",
      "    sample_time_ms: 25071.803\n",
      "    update_time_ms: 2.234\n",
      "  timestamp: 1633799815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         8699.92</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            313.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-17-21\n",
      "  done: false\n",
      "  episode_len_mean: 315.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 984\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9352126055293613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008372077054433148\n",
      "          policy_loss: 0.12495279966129197\n",
      "          total_loss: 0.10683369342651632\n",
      "          vf_explained_var: -0.06763919442892075\n",
      "          vf_loss: 0.00022723493562403342\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.22631578947369\n",
      "    ram_util_percent: 71.08947368421049\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367973184162985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.780445710253158\n",
      "    mean_inference_ms: 1.7209851102429852\n",
      "    mean_raw_obs_processing_ms: 1.5157383234763555\n",
      "  time_since_restore: 8726.11267209053\n",
      "  time_this_iter_s: 26.195443153381348\n",
      "  time_total_s: 8726.11267209053\n",
      "  timers:\n",
      "    learn_throughput: 1579.247\n",
      "    learn_time_ms: 633.213\n",
      "    load_throughput: 54793.338\n",
      "    load_time_ms: 18.25\n",
      "    sample_throughput: 39.819\n",
      "    sample_time_ms: 25113.636\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633799841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         8726.11</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            315.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 317.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 986\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9720727099312676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011462793074324571\n",
      "          policy_loss: -0.09986008521583345\n",
      "          total_loss: -0.11780493093861474\n",
      "          vf_explained_var: -0.11783729493618011\n",
      "          vf_loss: 0.00039879657593297046\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.76944444444443\n",
      "    ram_util_percent: 71.66666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679711467131788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.78917731601121\n",
      "    mean_inference_ms: 1.7209961027326965\n",
      "    mean_raw_obs_processing_ms: 1.516120422274359\n",
      "  time_since_restore: 8751.933295488358\n",
      "  time_this_iter_s: 25.82062339782715\n",
      "  time_total_s: 8751.933295488358\n",
      "  timers:\n",
      "    learn_throughput: 1573.781\n",
      "    learn_time_ms: 635.412\n",
      "    load_throughput: 52935.926\n",
      "    load_time_ms: 18.891\n",
      "    sample_throughput: 39.715\n",
      "    sample_time_ms: 25179.508\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633799867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         8751.93</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             317.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 317.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 990\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9729576680395338\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010351621865153095\n",
      "          policy_loss: -0.030480165200101005\n",
      "          total_loss: -0.04875798912511931\n",
      "          vf_explained_var: -0.30145546793937683\n",
      "          vf_loss: 0.00020815568839477945\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.67692307692308\n",
      "    ram_util_percent: 72.41076923076925\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679708950766375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.8063725097861\n",
      "    mean_inference_ms: 1.7210251989168537\n",
      "    mean_raw_obs_processing_ms: 1.5186282114696161\n",
      "  time_since_restore: 8796.916716814041\n",
      "  time_this_iter_s: 44.983421325683594\n",
      "  time_total_s: 8796.916716814041\n",
      "  timers:\n",
      "    learn_throughput: 1564.564\n",
      "    learn_time_ms: 639.156\n",
      "    load_throughput: 54092.97\n",
      "    load_time_ms: 18.487\n",
      "    sample_throughput: 39.782\n",
      "    sample_time_ms: 25136.831\n",
      "    update_time_ms: 2.099\n",
      "  timestamp: 1633799912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         8796.92</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 317.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 993\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9278349121411642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008873446982060232\n",
      "          policy_loss: 0.02065134271979332\n",
      "          total_loss: 0.0026256128317779966\n",
      "          vf_explained_var: -0.7886888980865479\n",
      "          vf_loss: 0.00018660413588804657\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.73829787234043\n",
      "    ram_util_percent: 72.72978723404255\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679725292249951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.819522396596323\n",
      "    mean_inference_ms: 1.7210517891588335\n",
      "    mean_raw_obs_processing_ms: 1.520521404395029\n",
      "  time_since_restore: 8829.760638237\n",
      "  time_this_iter_s: 32.843921422958374\n",
      "  time_total_s: 8829.760638237\n",
      "  timers:\n",
      "    learn_throughput: 1542.348\n",
      "    learn_time_ms: 648.362\n",
      "    load_throughput: 55532.659\n",
      "    load_time_ms: 18.007\n",
      "    sample_throughput: 38.562\n",
      "    sample_time_ms: 25932.168\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1633799945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         8829.76</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-19-33\n",
      "  done: false\n",
      "  episode_len_mean: 316.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 997\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.004711351129744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008134717342667842\n",
      "          policy_loss: -0.12052340753790405\n",
      "          total_loss: -0.1395253432707654\n",
      "          vf_explained_var: -0.4215483069419861\n",
      "          vf_loss: 6.79121562118073e-05\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.975\n",
      "    ram_util_percent: 72.6275\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036797596068479474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.83723562129628\n",
      "    mean_inference_ms: 1.7210908936157217\n",
      "    mean_raw_obs_processing_ms: 1.5230401795766266\n",
      "  time_since_restore: 8858.095088720322\n",
      "  time_this_iter_s: 28.334450483322144\n",
      "  time_total_s: 8858.095088720322\n",
      "  timers:\n",
      "    learn_throughput: 1541.305\n",
      "    learn_time_ms: 648.801\n",
      "    load_throughput: 56279.515\n",
      "    load_time_ms: 17.768\n",
      "    sample_throughput: 37.752\n",
      "    sample_time_ms: 26488.854\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1633799973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">          8858.1</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            316.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-20-02\n",
      "  done: false\n",
      "  episode_len_mean: 317.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.983719571431478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009378538903117257\n",
      "          policy_loss: 0.06399224764770932\n",
      "          total_loss: 0.04556356113817957\n",
      "          vf_explained_var: 0.20882640779018402\n",
      "          vf_loss: 0.000281814331295512\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.7390243902439\n",
      "    ram_util_percent: 72.12926829268294\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679785088934358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.850534995272184\n",
      "    mean_inference_ms: 1.7211201971326497\n",
      "    mean_raw_obs_processing_ms: 1.524497740724857\n",
      "  time_since_restore: 8887.023464679718\n",
      "  time_this_iter_s: 28.928375959396362\n",
      "  time_total_s: 8887.023464679718\n",
      "  timers:\n",
      "    learn_throughput: 1537.495\n",
      "    learn_time_ms: 650.409\n",
      "    load_throughput: 57061.168\n",
      "    load_time_ms: 17.525\n",
      "    sample_throughput: 36.992\n",
      "    sample_time_ms: 27033.027\n",
      "    update_time_ms: 2.102\n",
      "  timestamp: 1633800002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         8887.02</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-20-31\n",
      "  done: false\n",
      "  episode_len_mean: 316.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1004\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8372895426220364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006024360131510578\n",
      "          policy_loss: -0.06538885003990597\n",
      "          total_loss: -0.08285989165306092\n",
      "          vf_explained_var: -0.3738316297531128\n",
      "          vf_loss: 0.00017811043977013064\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.297560975609755\n",
      "    ram_util_percent: 72.00975609756097\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679820116490993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.86808744481154\n",
      "    mean_inference_ms: 1.7211592403073082\n",
      "    mean_raw_obs_processing_ms: 1.5252886750982624\n",
      "  time_since_restore: 8915.562981128693\n",
      "  time_this_iter_s: 28.53951644897461\n",
      "  time_total_s: 8915.562981128693\n",
      "  timers:\n",
      "    learn_throughput: 1539.235\n",
      "    learn_time_ms: 649.674\n",
      "    load_throughput: 57438.122\n",
      "    load_time_ms: 17.41\n",
      "    sample_throughput: 36.376\n",
      "    sample_time_ms: 27490.366\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633800031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         8915.56</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            316.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-20-56\n",
      "  done: false\n",
      "  episode_len_mean: 317.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1007\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9988259487681919\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019951989755858553\n",
      "          policy_loss: -0.11935066613886092\n",
      "          total_loss: -0.13672556802630426\n",
      "          vf_explained_var: 0.4331044554710388\n",
      "          vf_loss: 0.00021641575119954522\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.537142857142854\n",
      "    ram_util_percent: 72.14285714285712\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367985001430938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.881071967376652\n",
      "    mean_inference_ms: 1.7211885706877645\n",
      "    mean_raw_obs_processing_ms: 1.5258702486254558\n",
      "  time_since_restore: 8940.387276649475\n",
      "  time_this_iter_s: 24.82429552078247\n",
      "  time_total_s: 8940.387276649475\n",
      "  timers:\n",
      "    learn_throughput: 1535.011\n",
      "    learn_time_ms: 651.461\n",
      "    load_throughput: 58125.78\n",
      "    load_time_ms: 17.204\n",
      "    sample_throughput: 36.332\n",
      "    sample_time_ms: 27523.838\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633800056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         8940.39</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-21-20\n",
      "  done: false\n",
      "  episode_len_mean: 318.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8374237484402127\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015346449547311201\n",
      "          policy_loss: -0.021125292426182163\n",
      "          total_loss: -0.03723814537127813\n",
      "          vf_explained_var: 0.14402922987937927\n",
      "          vf_loss: 0.00041773476299972065\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.963888888888896\n",
      "    ram_util_percent: 72.14166666666665\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679882244123242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.893781839523207\n",
      "    mean_inference_ms: 1.7212174844544554\n",
      "    mean_raw_obs_processing_ms: 1.5264579622070533\n",
      "  time_since_restore: 8964.934256792068\n",
      "  time_this_iter_s: 24.546980142593384\n",
      "  time_total_s: 8964.934256792068\n",
      "  timers:\n",
      "    learn_throughput: 1534.651\n",
      "    learn_time_ms: 651.614\n",
      "    load_throughput: 58716.698\n",
      "    load_time_ms: 17.031\n",
      "    sample_throughput: 35.992\n",
      "    sample_time_ms: 27783.679\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1633800080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         8964.93</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             318.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 319.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1013\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8576217161284552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010813931135963727\n",
      "          policy_loss: 0.053761702651778855\n",
      "          total_loss: 0.03684975720114178\n",
      "          vf_explained_var: 0.2911844551563263\n",
      "          vf_loss: 0.0003651358112822183\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.28181818181818\n",
      "    ram_util_percent: 72.17878787878789\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799120942259095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.90636167316308\n",
      "    mean_inference_ms: 1.7212457088484894\n",
      "    mean_raw_obs_processing_ms: 1.527015209553519\n",
      "  time_since_restore: 8988.642893791199\n",
      "  time_this_iter_s: 23.70863699913025\n",
      "  time_total_s: 8988.642893791199\n",
      "  timers:\n",
      "    learn_throughput: 1531.302\n",
      "    learn_time_ms: 653.039\n",
      "    load_throughput: 59491.817\n",
      "    load_time_ms: 16.809\n",
      "    sample_throughput: 35.465\n",
      "    sample_time_ms: 28197.162\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633800104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         8988.64</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             319.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 319.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1016\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6790370795461866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0093915675631264\n",
      "          policy_loss: -0.004376292477051417\n",
      "          total_loss: -0.01984717758993308\n",
      "          vf_explained_var: -0.10282328724861145\n",
      "          vf_loss: 0.0001912268921538877\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.824324324324316\n",
      "    ram_util_percent: 72.28378378378379\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679947497820558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.918852051901226\n",
      "    mean_inference_ms: 1.7212740197483327\n",
      "    mean_raw_obs_processing_ms: 1.527579680751386\n",
      "  time_since_restore: 9014.265620470047\n",
      "  time_this_iter_s: 25.622726678848267\n",
      "  time_total_s: 9014.265620470047\n",
      "  timers:\n",
      "    learn_throughput: 1529.335\n",
      "    learn_time_ms: 653.879\n",
      "    load_throughput: 62811.925\n",
      "    load_time_ms: 15.921\n",
      "    sample_throughput: 35.537\n",
      "    sample_time_ms: 28139.927\n",
      "    update_time_ms: 2.093\n",
      "  timestamp: 1633800129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         9014.27</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            319.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-22-33\n",
      "  done: false\n",
      "  episode_len_mean: 320.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1019\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9449763827853732\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011152641130288762\n",
      "          policy_loss: 0.05150345385902458\n",
      "          total_loss: 0.03357336214847035\n",
      "          vf_explained_var: -0.09217643737792969\n",
      "          vf_loss: 0.00017984621566332257\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.72727272727272\n",
      "    ram_util_percent: 72.34545454545456\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799795681394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.931000215218884\n",
      "    mean_inference_ms: 1.721301583968346\n",
      "    mean_raw_obs_processing_ms: 1.5281500610103549\n",
      "  time_since_restore: 9037.395931243896\n",
      "  time_this_iter_s: 23.130310773849487\n",
      "  time_total_s: 9037.395931243896\n",
      "  timers:\n",
      "    learn_throughput: 1535.84\n",
      "    learn_time_ms: 651.11\n",
      "    load_throughput: 62902.451\n",
      "    load_time_ms: 15.898\n",
      "    sample_throughput: 35.876\n",
      "    sample_time_ms: 27873.708\n",
      "    update_time_ms: 2.095\n",
      "  timestamp: 1633800153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">          9037.4</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            320.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-23-15\n",
      "  done: false\n",
      "  episode_len_mean: 321.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1022\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8808266666200426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012272188888362498\n",
      "          policy_loss: 0.027015956656800375\n",
      "          total_loss: 0.009955768328573969\n",
      "          vf_explained_var: -0.3216725289821625\n",
      "          vf_loss: 0.0002737509461844133\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.99166666666667\n",
      "    ram_util_percent: 72.32000000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036800112884910356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.943078097754423\n",
      "    mean_inference_ms: 1.7213279314474994\n",
      "    mean_raw_obs_processing_ms: 1.5299733286970414\n",
      "  time_since_restore: 9079.264350891113\n",
      "  time_this_iter_s: 41.8684196472168\n",
      "  time_total_s: 9079.264350891113\n",
      "  timers:\n",
      "    learn_throughput: 1546.769\n",
      "    learn_time_ms: 646.509\n",
      "    load_throughput: 61514.761\n",
      "    load_time_ms: 16.256\n",
      "    sample_throughput: 36.276\n",
      "    sample_time_ms: 27566.459\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1633800195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         9079.26</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 321.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1025\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9196832007831997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010918479365462887\n",
      "          policy_loss: -0.04894003276195791\n",
      "          total_loss: -0.0665223045895497\n",
      "          vf_explained_var: -0.2778627276420593\n",
      "          vf_loss: 0.0003028637652783396\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.1027027027027\n",
      "    ram_util_percent: 72.36216216216216\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680038073178435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.955140327904438\n",
      "    mean_inference_ms: 1.721353980183582\n",
      "    mean_raw_obs_processing_ms: 1.5318015526886248\n",
      "  time_since_restore: 9105.07378077507\n",
      "  time_this_iter_s: 25.80942988395691\n",
      "  time_total_s: 9105.07378077507\n",
      "  timers:\n",
      "    learn_throughput: 1563.521\n",
      "    learn_time_ms: 639.582\n",
      "    load_throughput: 60708.094\n",
      "    load_time_ms: 16.472\n",
      "    sample_throughput: 37.217\n",
      "    sample_time_ms: 26869.738\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633800220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         9105.07</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 322.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1028\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8994915154245164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009084116193190954\n",
      "          policy_loss: 0.01412793609003226\n",
      "          total_loss: -0.003606014657351706\n",
      "          vf_explained_var: 0.06755346059799194\n",
      "          vf_loss: 0.0001696390947068317\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68055555555556\n",
      "    ram_util_percent: 71.89999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680066657512255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.967125401078157\n",
      "    mean_inference_ms: 1.7213795301202106\n",
      "    mean_raw_obs_processing_ms: 1.533631274046969\n",
      "  time_since_restore: 9130.527370929718\n",
      "  time_this_iter_s: 25.453590154647827\n",
      "  time_total_s: 9130.527370929718\n",
      "  timers:\n",
      "    learn_throughput: 1563.889\n",
      "    learn_time_ms: 639.432\n",
      "    load_throughput: 60380.105\n",
      "    load_time_ms: 16.562\n",
      "    sample_throughput: 37.62\n",
      "    sample_time_ms: 26581.745\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1633800246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         9130.53</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            322.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 321.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1031\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6558554967244465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007457224829136856\n",
      "          policy_loss: -0.0445638808939192\n",
      "          total_loss: -0.05997591101460987\n",
      "          vf_explained_var: -0.5324363708496094\n",
      "          vf_loss: 0.000250648963265121\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.838095238095235\n",
      "    ram_util_percent: 71.88809523809523\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036800962632808513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.979346581207373\n",
      "    mean_inference_ms: 1.721405375136576\n",
      "    mean_raw_obs_processing_ms: 1.5345485797923168\n",
      "  time_since_restore: 9160.039860725403\n",
      "  time_this_iter_s: 29.512489795684814\n",
      "  time_total_s: 9160.039860725403\n",
      "  timers:\n",
      "    learn_throughput: 1565.59\n",
      "    learn_time_ms: 638.737\n",
      "    load_throughput: 59917.516\n",
      "    load_time_ms: 16.69\n",
      "    sample_throughput: 37.536\n",
      "    sample_time_ms: 26640.75\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633800275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         9160.04</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-25-04\n",
      "  done: false\n",
      "  episode_len_mean: 321.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1035\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7536212669478521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012963082082759354\n",
      "          policy_loss: 0.09425199247068829\n",
      "          total_loss: 0.0784575922621621\n",
      "          vf_explained_var: -0.3991397023200989\n",
      "          vf_loss: 0.00018448572655971576\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.75\n",
      "    ram_util_percent: 71.925\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680134560690933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.995674040891167\n",
      "    mean_inference_ms: 1.7214400535502552\n",
      "    mean_raw_obs_processing_ms: 1.5352059883196183\n",
      "  time_since_restore: 9188.25830578804\n",
      "  time_this_iter_s: 28.21844506263733\n",
      "  time_total_s: 9188.25830578804\n",
      "  timers:\n",
      "    learn_throughput: 1563.83\n",
      "    learn_time_ms: 639.456\n",
      "    load_throughput: 60524.129\n",
      "    load_time_ms: 16.522\n",
      "    sample_throughput: 37.583\n",
      "    sample_time_ms: 26608.072\n",
      "    update_time_ms: 2.055\n",
      "  timestamp: 1633800304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         9188.26</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-25-34\n",
      "  done: false\n",
      "  episode_len_mean: 319.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1038\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3232086287604439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012138882654915535\n",
      "          policy_loss: -0.13348948479526573\n",
      "          total_loss: -0.14504865172008674\n",
      "          vf_explained_var: -0.009103468619287014\n",
      "          vf_loss: 0.0002146116455454224\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.711363636363636\n",
      "    ram_util_percent: 71.97954545454546\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801608022215854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.00806425510455\n",
      "    mean_inference_ms: 1.7214658153417333\n",
      "    mean_raw_obs_processing_ms: 1.5357307700481189\n",
      "  time_since_restore: 9218.462144613266\n",
      "  time_this_iter_s: 30.20383882522583\n",
      "  time_total_s: 9218.462144613266\n",
      "  timers:\n",
      "    learn_throughput: 1568.497\n",
      "    learn_time_ms: 637.553\n",
      "    load_throughput: 59902.883\n",
      "    load_time_ms: 16.694\n",
      "    sample_throughput: 36.835\n",
      "    sample_time_ms: 27147.762\n",
      "    update_time_ms: 2.056\n",
      "  timestamp: 1633800334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         9218.46</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            319.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 319.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1042\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3672072019841937\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011956870846856779\n",
      "          policy_loss: -0.06523793136907949\n",
      "          total_loss: -0.07729737704826725\n",
      "          vf_explained_var: 0.08776421844959259\n",
      "          vf_loss: 0.00017617986777622717\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.853846153846156\n",
      "    ram_util_percent: 72.08974358974356\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036801950276432335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.024902929943735\n",
      "    mean_inference_ms: 1.7215005930433958\n",
      "    mean_raw_obs_processing_ms: 1.5364075374324218\n",
      "  time_since_restore: 9245.838465452194\n",
      "  time_this_iter_s: 27.376320838928223\n",
      "  time_total_s: 9245.838465452194\n",
      "  timers:\n",
      "    learn_throughput: 1568.82\n",
      "    learn_time_ms: 637.422\n",
      "    load_throughput: 59938.323\n",
      "    load_time_ms: 16.684\n",
      "    sample_throughput: 36.455\n",
      "    sample_time_ms: 27430.864\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1633800361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         9245.84</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            319.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 320.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1045\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9395441757308112\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011473893984335445\n",
      "          policy_loss: 0.018163981568068265\n",
      "          total_loss: 0.00030032813342081177\n",
      "          vf_explained_var: -0.4937950670719147\n",
      "          vf_loss: 0.000153366953681042\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5342857142857\n",
      "    ram_util_percent: 72.42285714285714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368023180324665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.037381876836058\n",
      "    mean_inference_ms: 1.7215283990784576\n",
      "    mean_raw_obs_processing_ms: 1.5369488383861283\n",
      "  time_since_restore: 9270.498328447342\n",
      "  time_this_iter_s: 24.659862995147705\n",
      "  time_total_s: 9270.498328447342\n",
      "  timers:\n",
      "    learn_throughput: 1564.942\n",
      "    learn_time_ms: 639.001\n",
      "    load_throughput: 60304.144\n",
      "    load_time_ms: 16.583\n",
      "    sample_throughput: 36.331\n",
      "    sample_time_ms: 27524.482\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633800386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">          9270.5</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            320.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-26-52\n",
      "  done: false\n",
      "  episode_len_mean: 320.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1048\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7221060660150316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012162809673355814\n",
      "          policy_loss: 0.030989650471342934\n",
      "          total_loss: 0.015383781161573198\n",
      "          vf_explained_var: -0.1660178005695343\n",
      "          vf_loss: 0.00015400672976587277\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.137837837837836\n",
      "    ram_util_percent: 72.5054054054054\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680272242356116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.04986583884902\n",
      "    mean_inference_ms: 1.7215567869127535\n",
      "    mean_raw_obs_processing_ms: 1.5374972301724465\n",
      "  time_since_restore: 9296.190965652466\n",
      "  time_this_iter_s: 25.6926372051239\n",
      "  time_total_s: 9296.190965652466\n",
      "  timers:\n",
      "    learn_throughput: 1562.489\n",
      "    learn_time_ms: 640.005\n",
      "    load_throughput: 59521.028\n",
      "    load_time_ms: 16.801\n",
      "    sample_throughput: 36.324\n",
      "    sample_time_ms: 27530.27\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633800412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         9296.19</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            320.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-27-35\n",
      "  done: false\n",
      "  episode_len_mean: 320.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1051\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7313152617878385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0112577806377335\n",
      "          policy_loss: -0.052520890865061017\n",
      "          total_loss: -0.06836691324909529\n",
      "          vf_explained_var: -0.026687515899538994\n",
      "          vf_loss: 0.00011467112106199946\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.25806451612903\n",
      "    ram_util_percent: 72.61935483870968\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680314996767399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.06233247985918\n",
      "    mean_inference_ms: 1.7215857536338623\n",
      "    mean_raw_obs_processing_ms: 1.5392426709955924\n",
      "  time_since_restore: 9339.747523069382\n",
      "  time_this_iter_s: 43.556557416915894\n",
      "  time_total_s: 9339.747523069382\n",
      "  timers:\n",
      "    learn_throughput: 1558.944\n",
      "    learn_time_ms: 641.46\n",
      "    load_throughput: 60046.442\n",
      "    load_time_ms: 16.654\n",
      "    sample_throughput: 33.816\n",
      "    sample_time_ms: 29571.589\n",
      "    update_time_ms: 2.038\n",
      "  timestamp: 1633800455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         9339.75</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            320.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 321.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1054\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.72621015575197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011273039045416924\n",
      "          policy_loss: -0.08868344616558817\n",
      "          total_loss: -0.10445359986689355\n",
      "          vf_explained_var: 0.27548646926879883\n",
      "          vf_loss: 0.00013765417323965165\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.74848484848485\n",
      "    ram_util_percent: 72.67878787878787\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680357957106082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.074568653239968\n",
      "    mean_inference_ms: 1.7216141531829414\n",
      "    mean_raw_obs_processing_ms: 1.5409928933275387\n",
      "  time_since_restore: 9363.164224386215\n",
      "  time_this_iter_s: 23.416701316833496\n",
      "  time_total_s: 9363.164224386215\n",
      "  timers:\n",
      "    learn_throughput: 1557.996\n",
      "    learn_time_ms: 641.85\n",
      "    load_throughput: 60221.37\n",
      "    load_time_ms: 16.605\n",
      "    sample_throughput: 36.067\n",
      "    sample_time_ms: 27726.068\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633800479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         9363.16</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 322.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1057\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7390697028901843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013351154066221671\n",
      "          policy_loss: -0.07130924368070232\n",
      "          total_loss: -0.08691547811031342\n",
      "          vf_explained_var: -0.5341047644615173\n",
      "          vf_loss: 0.0001805135349665458\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.43888888888889\n",
      "    ram_util_percent: 72.5388888888889\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680397368847133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.086728200423423\n",
      "    mean_inference_ms: 1.7216426157261984\n",
      "    mean_raw_obs_processing_ms: 1.5427490731174829\n",
      "  time_since_restore: 9388.042334318161\n",
      "  time_this_iter_s: 24.8781099319458\n",
      "  time_total_s: 9388.042334318161\n",
      "  timers:\n",
      "    learn_throughput: 1564.653\n",
      "    learn_time_ms: 639.119\n",
      "    load_throughput: 60160.905\n",
      "    load_time_ms: 16.622\n",
      "    sample_throughput: 36.185\n",
      "    sample_time_ms: 27635.642\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633800503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         9388.04</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            322.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-28-45\n",
      "  done: false\n",
      "  episode_len_mean: 324.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1059\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0755680243174237\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0113049023634137\n",
      "          policy_loss: -0.07794397957623005\n",
      "          total_loss: -0.09723817201124298\n",
      "          vf_explained_var: -0.323440819978714\n",
      "          vf_loss: 0.00010336801719353792\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.36451612903225\n",
      "    ram_util_percent: 72.32903225806453\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036804264705298025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.094437506993305\n",
      "    mean_inference_ms: 1.721661403423444\n",
      "    mean_raw_obs_processing_ms: 1.5430764377709039\n",
      "  time_since_restore: 9409.633734226227\n",
      "  time_this_iter_s: 21.591399908065796\n",
      "  time_total_s: 9409.633734226227\n",
      "  timers:\n",
      "    learn_throughput: 1561.778\n",
      "    learn_time_ms: 640.296\n",
      "    load_throughput: 60349.09\n",
      "    load_time_ms: 16.57\n",
      "    sample_throughput: 36.7\n",
      "    sample_time_ms: 27248.279\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633800525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         9409.63</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             324.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-29-09\n",
      "  done: false\n",
      "  episode_len_mean: 326.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1062\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.877926692697737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009127831374036147\n",
      "          policy_loss: -0.0024872659809059565\n",
      "          total_loss: -0.02004565865629249\n",
      "          vf_explained_var: -0.47139930725097656\n",
      "          vf_loss: 0.00012429842972778716\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.03529411764706\n",
      "    ram_util_percent: 72.20294117647057\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680468804991494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.106096208933575\n",
      "    mean_inference_ms: 1.7216898702931167\n",
      "    mean_raw_obs_processing_ms: 1.5435042588624586\n",
      "  time_since_restore: 9433.446797847748\n",
      "  time_this_iter_s: 23.813063621520996\n",
      "  time_total_s: 9433.446797847748\n",
      "  timers:\n",
      "    learn_throughput: 1557.184\n",
      "    learn_time_ms: 642.185\n",
      "    load_throughput: 60803.581\n",
      "    load_time_ms: 16.446\n",
      "    sample_throughput: 37.486\n",
      "    sample_time_ms: 26676.535\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633800549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         9433.45</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            326.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-29-34\n",
      "  done: false\n",
      "  episode_len_mean: 327.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1065\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8522854897711012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016368735875658933\n",
      "          policy_loss: -0.02100357694758309\n",
      "          total_loss: -0.03740169778466225\n",
      "          vf_explained_var: -0.4233207702636719\n",
      "          vf_loss: 0.0001582645229063928\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.882857142857134\n",
      "    ram_util_percent: 72.45428571428573\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680515315811343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.11773990488068\n",
      "    mean_inference_ms: 1.7217199151510223\n",
      "    mean_raw_obs_processing_ms: 1.5439410280974797\n",
      "  time_since_restore: 9458.107919454575\n",
      "  time_this_iter_s: 24.661121606826782\n",
      "  time_total_s: 9458.107919454575\n",
      "  timers:\n",
      "    learn_throughput: 1556.936\n",
      "    learn_time_ms: 642.287\n",
      "    load_throughput: 60125.805\n",
      "    load_time_ms: 16.632\n",
      "    sample_throughput: 37.993\n",
      "    sample_time_ms: 26320.524\n",
      "    update_time_ms: 2.049\n",
      "  timestamp: 1633800574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         9458.11</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            327.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 327.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1068\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.961084074444241\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010874312673906792\n",
      "          policy_loss: 0.007674745677245988\n",
      "          total_loss: -0.010434757421414058\n",
      "          vf_explained_var: -0.90251624584198\n",
      "          vf_loss: 0.00019494925032227507\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.92058823529412\n",
      "    ram_util_percent: 72.34705882352942\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036805582627574926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.12941533255016\n",
      "    mean_inference_ms: 1.7217497088876308\n",
      "    mean_raw_obs_processing_ms: 1.544385832359137\n",
      "  time_since_restore: 9482.170119047165\n",
      "  time_this_iter_s: 24.062199592590332\n",
      "  time_total_s: 9482.170119047165\n",
      "  timers:\n",
      "    learn_throughput: 1555.984\n",
      "    learn_time_ms: 642.68\n",
      "    load_throughput: 60549.468\n",
      "    load_time_ms: 16.515\n",
      "    sample_throughput: 38.901\n",
      "    sample_time_ms: 25706.062\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633800598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         9482.17</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             327.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-30-20\n",
      "  done: false\n",
      "  episode_len_mean: 328.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1070\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9120444258054097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012520624950944952\n",
      "          policy_loss: -0.029634001602729162\n",
      "          total_loss: -0.047132004300753275\n",
      "          vf_explained_var: -0.8067418336868286\n",
      "          vf_loss: 0.00011827107801865269\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.045161290322575\n",
      "    ram_util_percent: 72.48064516129033\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680585775439803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.137144055175494\n",
      "    mean_inference_ms: 1.721769210483581\n",
      "    mean_raw_obs_processing_ms: 1.5446615847817942\n",
      "  time_since_restore: 9503.997874498367\n",
      "  time_this_iter_s: 21.827755451202393\n",
      "  time_total_s: 9503.997874498367\n",
      "  timers:\n",
      "    learn_throughput: 1557.376\n",
      "    learn_time_ms: 642.106\n",
      "    load_throughput: 59827.52\n",
      "    load_time_ms: 16.715\n",
      "    sample_throughput: 39.759\n",
      "    sample_time_ms: 25151.565\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633800620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">            9504</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             328.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 329.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1073\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7586801621649\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00947329527549404\n",
      "          policy_loss: -0.09727907793389426\n",
      "          total_loss: -0.11362743568089273\n",
      "          vf_explained_var: -0.5616359114646912\n",
      "          vf_loss: 0.00010036434266819722\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54375\n",
      "    ram_util_percent: 72.52187500000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806279663209145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.148548504524527\n",
      "    mean_inference_ms: 1.7217987150594154\n",
      "    mean_raw_obs_processing_ms: 1.5450820796547333\n",
      "  time_since_restore: 9525.965812206268\n",
      "  time_this_iter_s: 21.967937707901\n",
      "  time_total_s: 9525.965812206268\n",
      "  timers:\n",
      "    learn_throughput: 1561.427\n",
      "    learn_time_ms: 640.44\n",
      "    load_throughput: 59122.085\n",
      "    load_time_ms: 16.914\n",
      "    sample_throughput: 40.187\n",
      "    sample_time_ms: 24883.859\n",
      "    update_time_ms: 2.034\n",
      "  timestamp: 1633800642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         9525.97</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            329.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-31-06\n",
      "  done: false\n",
      "  episode_len_mean: 329.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1075\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8879193319214715\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012765341811073503\n",
      "          policy_loss: -0.011179713987641865\n",
      "          total_loss: -0.028426031147440276\n",
      "          vf_explained_var: -0.7416925430297852\n",
      "          vf_loss: 9.930669022853382e-05\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.15882352941176\n",
      "    ram_util_percent: 72.53823529411765\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680653207307993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.15609468041285\n",
      "    mean_inference_ms: 1.721818614600308\n",
      "    mean_raw_obs_processing_ms: 1.5453672811363388\n",
      "  time_since_restore: 9549.94926738739\n",
      "  time_this_iter_s: 23.983455181121826\n",
      "  time_total_s: 9549.94926738739\n",
      "  timers:\n",
      "    learn_throughput: 1564.242\n",
      "    learn_time_ms: 639.287\n",
      "    load_throughput: 57867.554\n",
      "    load_time_ms: 17.281\n",
      "    sample_throughput: 40.463\n",
      "    sample_time_ms: 24713.712\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633800666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         9549.95</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            329.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-31-31\n",
      "  done: false\n",
      "  episode_len_mean: 329.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1078\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6431107838948569\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0060371226490985915\n",
      "          policy_loss: 0.04568743082798189\n",
      "          total_loss: 0.03004958958675464\n",
      "          vf_explained_var: 0.01893872208893299\n",
      "          vf_loss: 6.799399527355693e-05\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7\n",
      "    ram_util_percent: 72.5611111111111\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680696079663155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.16760545010458\n",
      "    mean_inference_ms: 1.7218497815838885\n",
      "    mean_raw_obs_processing_ms: 1.545801412249755\n",
      "  time_since_restore: 9575.140358686447\n",
      "  time_this_iter_s: 25.191091299057007\n",
      "  time_total_s: 9575.140358686447\n",
      "  timers:\n",
      "    learn_throughput: 1566.42\n",
      "    learn_time_ms: 638.398\n",
      "    load_throughput: 58048.553\n",
      "    load_time_ms: 17.227\n",
      "    sample_throughput: 43.71\n",
      "    sample_time_ms: 22878.116\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633800691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         9575.14</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             329.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 326.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1081\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7844889190461901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01308432463669003\n",
      "          policy_loss: -0.019457774857680004\n",
      "          total_loss: -0.035607517427868315\n",
      "          vf_explained_var: -0.6856301426887512\n",
      "          vf_loss: 0.00012325477008643147\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.59180327868852\n",
      "    ram_util_percent: 72.67868852459014\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807385426398845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.179391712895594\n",
      "    mean_inference_ms: 1.7218817615773694\n",
      "    mean_raw_obs_processing_ms: 1.5474342843979352\n",
      "  time_since_restore: 9617.602264642715\n",
      "  time_this_iter_s: 42.46190595626831\n",
      "  time_total_s: 9617.602264642715\n",
      "  timers:\n",
      "    learn_throughput: 1566.584\n",
      "    learn_time_ms: 638.332\n",
      "    load_throughput: 57943.26\n",
      "    load_time_ms: 17.258\n",
      "    sample_throughput: 40.351\n",
      "    sample_time_ms: 24782.678\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633800733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">          9617.6</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            326.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 328.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1084\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.080077902475993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015304047717427308\n",
      "          policy_loss: 0.04324005179935032\n",
      "          total_loss: 0.024372961868842444\n",
      "          vf_explained_var: -0.6761136054992676\n",
      "          vf_loss: 9.513110227190837e-05\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.97352941176471\n",
      "    ram_util_percent: 72.65294117647059\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807803308193214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.191116950860675\n",
      "    mean_inference_ms: 1.7219136126347205\n",
      "    mean_raw_obs_processing_ms: 1.5490366081178024\n",
      "  time_since_restore: 9641.743839979172\n",
      "  time_this_iter_s: 24.1415753364563\n",
      "  time_total_s: 9641.743839979172\n",
      "  timers:\n",
      "    learn_throughput: 1564.694\n",
      "    learn_time_ms: 639.103\n",
      "    load_throughput: 57530.378\n",
      "    load_time_ms: 17.382\n",
      "    sample_throughput: 40.472\n",
      "    sample_time_ms: 24708.144\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1633800757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         9641.74</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            328.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 328.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1087\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.087642214033339\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012427892326926907\n",
      "          policy_loss: -0.06283024731609556\n",
      "          total_loss: -0.08205965815318955\n",
      "          vf_explained_var: -0.8817446231842041\n",
      "          vf_loss: 0.00015397930409461778\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.88125\n",
      "    ram_util_percent: 72.4125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680798599072333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.20238450074385\n",
      "    mean_inference_ms: 1.7219409137367565\n",
      "    mean_raw_obs_processing_ms: 1.5502544914771113\n",
      "  time_since_restore: 9664.402109861374\n",
      "  time_this_iter_s: 22.65826988220215\n",
      "  time_total_s: 9664.402109861374\n",
      "  timers:\n",
      "    learn_throughput: 1563.307\n",
      "    learn_time_ms: 639.67\n",
      "    load_throughput: 57275.84\n",
      "    load_time_ms: 17.459\n",
      "    sample_throughput: 40.3\n",
      "    sample_time_ms: 24814.167\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1633800780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">          9664.4</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            328.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-33-22\n",
      "  done: false\n",
      "  episode_len_mean: 330.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1089\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0724970632129245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014555495974418273\n",
      "          policy_loss: -0.002351941085524029\n",
      "          total_loss: -0.021232514538698725\n",
      "          vf_explained_var: 0.14586395025253296\n",
      "          vf_loss: 9.576364399334933e-05\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63870967741935\n",
      "    ram_util_percent: 72.23548387096777\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680797737033861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.209678478469513\n",
      "    mean_inference_ms: 1.7219566689577297\n",
      "    mean_raw_obs_processing_ms: 1.5504796412497683\n",
      "  time_since_restore: 9685.901081562042\n",
      "  time_this_iter_s: 21.498971700668335\n",
      "  time_total_s: 9685.901081562042\n",
      "  timers:\n",
      "    learn_throughput: 1568.41\n",
      "    learn_time_ms: 637.588\n",
      "    load_throughput: 56739.36\n",
      "    load_time_ms: 17.624\n",
      "    sample_throughput: 40.676\n",
      "    sample_time_ms: 24584.693\n",
      "    update_time_ms: 2.074\n",
      "  timestamp: 1633800802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">          9685.9</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            330.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 333.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1092\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9403729293081495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00786189680517116\n",
      "          policy_loss: -0.06701412689354684\n",
      "          total_loss: -0.08540114234719011\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 7.221872652331108e-05\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76285714285714\n",
      "    ram_util_percent: 72.1942857142857\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807866131535256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.220263903385877\n",
      "    mean_inference_ms: 1.7219771283504186\n",
      "    mean_raw_obs_processing_ms: 1.5507906696706562\n",
      "  time_since_restore: 9710.47665143013\n",
      "  time_this_iter_s: 24.57556986808777\n",
      "  time_total_s: 9710.47665143013\n",
      "  timers:\n",
      "    learn_throughput: 1563.979\n",
      "    learn_time_ms: 639.395\n",
      "    load_throughput: 56930.122\n",
      "    load_time_ms: 17.565\n",
      "    sample_throughput: 40.693\n",
      "    sample_time_ms: 24574.419\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633800826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         9710.48</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             333.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 335.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1095\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9431594755914476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014918011973589135\n",
      "          policy_loss: -0.1486215522719754\n",
      "          total_loss: -0.16612167035539946\n",
      "          vf_explained_var: 0.1836821436882019\n",
      "          vf_loss: 0.00013929296890435378\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64722222222222\n",
      "    ram_util_percent: 72.20833333333331\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680762315390478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.230470182072754\n",
      "    mean_inference_ms: 1.721993963210912\n",
      "    mean_raw_obs_processing_ms: 1.5511081364167847\n",
      "  time_since_restore: 9735.84457397461\n",
      "  time_this_iter_s: 25.36792254447937\n",
      "  time_total_s: 9735.84457397461\n",
      "  timers:\n",
      "    learn_throughput: 1563.94\n",
      "    learn_time_ms: 639.411\n",
      "    load_throughput: 56599.779\n",
      "    load_time_ms: 17.668\n",
      "    sample_throughput: 40.478\n",
      "    sample_time_ms: 24704.876\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633800852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         9735.84</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 337.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1098\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.818774355782403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010075504500856555\n",
      "          policy_loss: -0.02986517691363891\n",
      "          total_loss: -0.046718779330452286\n",
      "          vf_explained_var: -0.7297544479370117\n",
      "          vf_loss: 0.00012371768397214408\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.17777777777778\n",
      "    ram_util_percent: 72.35555555555557\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036807331882904804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.240556130430495\n",
      "    mean_inference_ms: 1.7220097335669307\n",
      "    mean_raw_obs_processing_ms: 1.5513984117839683\n",
      "  time_since_restore: 9760.843636512756\n",
      "  time_this_iter_s: 24.999062538146973\n",
      "  time_total_s: 9760.843636512756\n",
      "  timers:\n",
      "    learn_throughput: 1559.994\n",
      "    learn_time_ms: 641.028\n",
      "    load_throughput: 57307.22\n",
      "    load_time_ms: 17.45\n",
      "    sample_throughput: 39.967\n",
      "    sample_time_ms: 25020.596\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1633800877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         9760.84</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             337.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 339.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9600948346985712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010739857905406009\n",
      "          policy_loss: 0.05109250458578269\n",
      "          total_loss: 0.03286981561945544\n",
      "          vf_explained_var: -0.22597351670265198\n",
      "          vf_loss: 8.802093923602823e-05\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.69117647058823\n",
      "    ram_util_percent: 72.46176470588236\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680713892777875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.247129156548258\n",
      "    mean_inference_ms: 1.7220201195213205\n",
      "    mean_raw_obs_processing_ms: 1.5515736477124733\n",
      "  time_since_restore: 9784.513225078583\n",
      "  time_this_iter_s: 23.669588565826416\n",
      "  time_total_s: 9784.513225078583\n",
      "  timers:\n",
      "    learn_throughput: 1559.877\n",
      "    learn_time_ms: 641.076\n",
      "    load_throughput: 56603.598\n",
      "    load_time_ms: 17.667\n",
      "    sample_throughput: 39.697\n",
      "    sample_time_ms: 25190.509\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633800900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         9784.51</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            339.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 340.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1103\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7655558890766567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01208471954037312\n",
      "          policy_loss: -0.03882612693640921\n",
      "          total_loss: -0.05491331244508425\n",
      "          vf_explained_var: -0.3221602141857147\n",
      "          vf_loss: 0.00011657199413295732\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89166666666667\n",
      "    ram_util_percent: 72.54722222222222\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806843247029035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.256699400124802\n",
      "    mean_inference_ms: 1.722035113985812\n",
      "    mean_raw_obs_processing_ms: 1.5518428003730864\n",
      "  time_since_restore: 9809.810609817505\n",
      "  time_this_iter_s: 25.29738473892212\n",
      "  time_total_s: 9809.810609817505\n",
      "  timers:\n",
      "    learn_throughput: 1559.767\n",
      "    learn_time_ms: 641.122\n",
      "    load_throughput: 56446.978\n",
      "    load_time_ms: 17.716\n",
      "    sample_throughput: 39.492\n",
      "    sample_time_ms: 25321.81\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633800926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         9809.81</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            340.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-35-47\n",
      "  done: false\n",
      "  episode_len_mean: 344.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1106\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9226986289024353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013810877106388158\n",
      "          policy_loss: -0.07037132655580838\n",
      "          total_loss: -0.0878308327972061\n",
      "          vf_explained_var: -0.29172050952911377\n",
      "          vf_loss: 0.0001083011923684454\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76129032258064\n",
      "    ram_util_percent: 72.61290322580643\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806536943433985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.266094284369505\n",
      "    mean_inference_ms: 1.7220498488578986\n",
      "    mean_raw_obs_processing_ms: 1.5520868246804285\n",
      "  time_since_restore: 9831.250621318817\n",
      "  time_this_iter_s: 21.440011501312256\n",
      "  time_total_s: 9831.250621318817\n",
      "  timers:\n",
      "    learn_throughput: 1560.975\n",
      "    learn_time_ms: 640.625\n",
      "    load_throughput: 55706.427\n",
      "    load_time_ms: 17.951\n",
      "    sample_throughput: 40.085\n",
      "    sample_time_ms: 24946.973\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633800947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         9831.25</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-36-08\n",
      "  done: false\n",
      "  episode_len_mean: 345.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1108\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0219956755638124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01520885275581475\n",
      "          policy_loss: -0.05730314221647051\n",
      "          total_loss: -0.07563262399699953\n",
      "          vf_explained_var: -0.582245409488678\n",
      "          vf_loss: 6.334861787359437e-05\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.99655172413792\n",
      "    ram_util_percent: 72.71724137931035\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036806323088429724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.27218937683044\n",
      "    mean_inference_ms: 1.7220595539860952\n",
      "    mean_raw_obs_processing_ms: 1.5522418266197928\n",
      "  time_since_restore: 9852.114671945572\n",
      "  time_this_iter_s: 20.86405062675476\n",
      "  time_total_s: 9852.114671945572\n",
      "  timers:\n",
      "    learn_throughput: 1559.518\n",
      "    learn_time_ms: 641.224\n",
      "    load_throughput: 55496.068\n",
      "    load_time_ms: 18.019\n",
      "    sample_throughput: 43.886\n",
      "    sample_time_ms: 22786.527\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633800968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         9852.11</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            345.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-36-51\n",
      "  done: false\n",
      "  episode_len_mean: 346.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1111\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.875592013200124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008696451794857474\n",
      "          policy_loss: -0.03133699562814501\n",
      "          total_loss: -0.04894294432467884\n",
      "          vf_explained_var: -0.50581955909729\n",
      "          vf_loss: 0.00010521746314932696\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.291935483870965\n",
      "    ram_util_percent: 72.77258064516131\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036805974997467204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.281458066198798\n",
      "    mean_inference_ms: 1.7220742303505425\n",
      "    mean_raw_obs_processing_ms: 1.5536198888014991\n",
      "  time_since_restore: 9895.13264799118\n",
      "  time_this_iter_s: 43.01797604560852\n",
      "  time_total_s: 9895.13264799118\n",
      "  timers:\n",
      "    learn_throughput: 1559.219\n",
      "    learn_time_ms: 641.347\n",
      "    load_throughput: 55603.407\n",
      "    load_time_ms: 17.985\n",
      "    sample_throughput: 40.528\n",
      "    sample_time_ms: 24674.08\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633801011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         9895.13</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            346.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 348.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1114\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013549804687496\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.032684670554267\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032746091490130622\n",
      "          policy_loss: 0.00445742474661933\n",
      "          total_loss: -0.0055780247060789\n",
      "          vf_explained_var: -0.5365487933158875\n",
      "          vf_loss: 0.009897999776472311\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24375\n",
      "    ram_util_percent: 72.684375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036805634299738405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.29054572999336\n",
      "    mean_inference_ms: 1.7220888817055968\n",
      "    mean_raw_obs_processing_ms: 1.555004522039156\n",
      "  time_since_restore: 9917.473938941956\n",
      "  time_this_iter_s: 22.341290950775146\n",
      "  time_total_s: 9917.473938941956\n",
      "  timers:\n",
      "    learn_throughput: 1559.029\n",
      "    learn_time_ms: 641.425\n",
      "    load_throughput: 55523.911\n",
      "    load_time_ms: 18.01\n",
      "    sample_throughput: 40.582\n",
      "    sample_time_ms: 24641.489\n",
      "    update_time_ms: 2.848\n",
      "  timestamp: 1633801033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         9917.47</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            348.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 350.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1116\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9393280294206408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015732615186630123\n",
      "          policy_loss: -0.02568628357516395\n",
      "          total_loss: -0.042787277201811476\n",
      "          vf_explained_var: 0.4561188220977783\n",
      "          vf_loss: 0.0013472622321892736\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77666666666667\n",
      "    ram_util_percent: 72.51666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680539854101056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.296454203903533\n",
      "    mean_inference_ms: 1.7220984755027593\n",
      "    mean_raw_obs_processing_ms: 1.5559084104094483\n",
      "  time_since_restore: 9938.723647356033\n",
      "  time_this_iter_s: 21.24970841407776\n",
      "  time_total_s: 9938.723647356033\n",
      "  timers:\n",
      "    learn_throughput: 1557.964\n",
      "    learn_time_ms: 641.863\n",
      "    load_throughput: 55344.34\n",
      "    load_time_ms: 18.069\n",
      "    sample_throughput: 40.624\n",
      "    sample_time_ms: 24616.063\n",
      "    update_time_ms: 2.833\n",
      "  timestamp: 1633801055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         9938.72</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 347.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1119\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3143485956721837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009399912286475498\n",
      "          policy_loss: 0.013465722650289535\n",
      "          total_loss: 0.0012899638877974617\n",
      "          vf_explained_var: 0.39283984899520874\n",
      "          vf_loss: 0.0004030961730879628\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.114999999999995\n",
      "    ram_util_percent: 72.13250000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680506345498112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.305589294521198\n",
      "    mean_inference_ms: 1.722113603618932\n",
      "    mean_raw_obs_processing_ms: 1.5572690381040482\n",
      "  time_since_restore: 9966.408354520798\n",
      "  time_this_iter_s: 27.684707164764404\n",
      "  time_total_s: 9966.408354520798\n",
      "  timers:\n",
      "    learn_throughput: 1560.754\n",
      "    learn_time_ms: 640.716\n",
      "    load_throughput: 54990.606\n",
      "    load_time_ms: 18.185\n",
      "    sample_throughput: 40.116\n",
      "    sample_time_ms: 24927.999\n",
      "    update_time_ms: 2.829\n",
      "  timestamp: 1633801082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         9966.41</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 349.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1122\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7882184187571208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01865450836415662\n",
      "          policy_loss: -0.08172121337718434\n",
      "          total_loss: -0.09826577065719498\n",
      "          vf_explained_var: 0.6932497620582581\n",
      "          vf_loss: 0.0002170914392788998\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.09090909090909\n",
      "    ram_util_percent: 72.07575757575756\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680474416187595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.31456317260378\n",
      "    mean_inference_ms: 1.7221287385176915\n",
      "    mean_raw_obs_processing_ms: 1.5573910890891003\n",
      "  time_since_restore: 9989.561805486679\n",
      "  time_this_iter_s: 23.153450965881348\n",
      "  time_total_s: 9989.561805486679\n",
      "  timers:\n",
      "    learn_throughput: 1558.321\n",
      "    learn_time_ms: 641.716\n",
      "    load_throughput: 54688.102\n",
      "    load_time_ms: 18.286\n",
      "    sample_throughput: 40.477\n",
      "    sample_time_ms: 24705.464\n",
      "    update_time_ms: 2.83\n",
      "  timestamp: 1633801105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         9989.56</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            349.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-38-50\n",
      "  done: false\n",
      "  episode_len_mean: 350.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1125\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8689912133746678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018045749918440636\n",
      "          policy_loss: -0.09106687721278932\n",
      "          total_loss: -0.10846876775225003\n",
      "          vf_explained_var: 0.7380120158195496\n",
      "          vf_loss: 0.0002040491416765791\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93823529411765\n",
      "    ram_util_percent: 72.17941176470586\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680443523450769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.323388112756827\n",
      "    mean_inference_ms: 1.722143927002557\n",
      "    mean_raw_obs_processing_ms: 1.55752161800806\n",
      "  time_since_restore: 10013.877101659775\n",
      "  time_this_iter_s: 24.315296173095703\n",
      "  time_total_s: 10013.877101659775\n",
      "  timers:\n",
      "    learn_throughput: 1561.307\n",
      "    learn_time_ms: 640.489\n",
      "    load_throughput: 54503.687\n",
      "    load_time_ms: 18.347\n",
      "    sample_throughput: 40.587\n",
      "    sample_time_ms: 24638.285\n",
      "    update_time_ms: 2.805\n",
      "  timestamp: 1633801130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         10013.9</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-39-14\n",
      "  done: false\n",
      "  episode_len_mean: 350.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1128\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.01289914449056\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015166645483739326\n",
      "          policy_loss: 0.06820165134138531\n",
      "          total_loss: 0.04912999528977606\n",
      "          vf_explained_var: 0.5222734808921814\n",
      "          vf_loss: 0.00014630276321743926\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.745714285714286\n",
      "    ram_util_percent: 72.26571428571431\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680410465342381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.332067311595157\n",
      "    mean_inference_ms: 1.7221593833692639\n",
      "    mean_raw_obs_processing_ms: 1.5576613891080293\n",
      "  time_since_restore: 10037.885046482086\n",
      "  time_this_iter_s: 24.0079448223114\n",
      "  time_total_s: 10037.885046482086\n",
      "  timers:\n",
      "    learn_throughput: 1562.726\n",
      "    learn_time_ms: 639.907\n",
      "    load_throughput: 54731.062\n",
      "    load_time_ms: 18.271\n",
      "    sample_throughput: 40.531\n",
      "    sample_time_ms: 24672.776\n",
      "    update_time_ms: 2.813\n",
      "  timestamp: 1633801154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         10037.9</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 351.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1131\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8133036507500542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012082946125170264\n",
      "          policy_loss: 0.03400396505991618\n",
      "          total_loss: 0.016826879647043016\n",
      "          vf_explained_var: 0.6780738830566406\n",
      "          vf_loss: 0.00023015715851215646\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56\n",
      "    ram_util_percent: 72.3975\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680375785742928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.34058219528398\n",
      "    mean_inference_ms: 1.7221749606137648\n",
      "    mean_raw_obs_processing_ms: 1.557807220531353\n",
      "  time_since_restore: 10065.95923423767\n",
      "  time_this_iter_s: 28.074187755584717\n",
      "  time_total_s: 10065.95923423767\n",
      "  timers:\n",
      "    learn_throughput: 1562.453\n",
      "    learn_time_ms: 640.019\n",
      "    load_throughput: 54634.248\n",
      "    load_time_ms: 18.304\n",
      "    sample_throughput: 40.08\n",
      "    sample_time_ms: 24950.336\n",
      "    update_time_ms: 2.8\n",
      "  timestamp: 1633801182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">           10066</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            351.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 351.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1134\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9414383557107713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010941362482822924\n",
      "          policy_loss: -0.08613848123285506\n",
      "          total_loss: -0.10471459676822027\n",
      "          vf_explained_var: 0.5124402642250061\n",
      "          vf_loss: 0.00018104314787908353\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.06216216216217\n",
      "    ram_util_percent: 72.48918918918918\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680341307805459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.348909824899984\n",
      "    mean_inference_ms: 1.7221902128175122\n",
      "    mean_raw_obs_processing_ms: 1.5579582250730442\n",
      "  time_since_restore: 10092.265437602997\n",
      "  time_this_iter_s: 26.306203365325928\n",
      "  time_total_s: 10092.265437602997\n",
      "  timers:\n",
      "    learn_throughput: 1558.337\n",
      "    learn_time_ms: 641.71\n",
      "    load_throughput: 54703.152\n",
      "    load_time_ms: 18.28\n",
      "    sample_throughput: 39.316\n",
      "    sample_time_ms: 25435.258\n",
      "    update_time_ms: 2.816\n",
      "  timestamp: 1633801208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         10092.3</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            351.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-40-34\n",
      "  done: false\n",
      "  episode_len_mean: 352.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1137\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9656315591600206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016597694625438128\n",
      "          policy_loss: 0.01666417842109998\n",
      "          total_loss: -0.0018297998441590203\n",
      "          vf_explained_var: 0.0369882732629776\n",
      "          vf_loss: 0.00016535058320086036\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62972972972973\n",
      "    ram_util_percent: 72.64324324324322\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036803051760291736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.357095607519206\n",
      "    mean_inference_ms: 1.722205251004579\n",
      "    mean_raw_obs_processing_ms: 1.5580834742415803\n",
      "  time_since_restore: 10118.02492237091\n",
      "  time_this_iter_s: 25.75948476791382\n",
      "  time_total_s: 10118.02492237091\n",
      "  timers:\n",
      "    learn_throughput: 1558.984\n",
      "    learn_time_ms: 641.443\n",
      "    load_throughput: 54723.636\n",
      "    load_time_ms: 18.274\n",
      "    sample_throughput: 38.573\n",
      "    sample_time_ms: 25925.072\n",
      "    update_time_ms: 2.817\n",
      "  timestamp: 1633801234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">           10118</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             352.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 354.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9927511705292595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012247895975186118\n",
      "          policy_loss: -0.02590305449234115\n",
      "          total_loss: -0.04493282727069325\n",
      "          vf_explained_var: -0.3488783538341522\n",
      "          vf_loss: 0.0001620370484791541\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.32459016393443\n",
      "    ram_util_percent: 72.69836065573772\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036802666634173434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.365016964821383\n",
      "    mean_inference_ms: 1.7222197172818396\n",
      "    mean_raw_obs_processing_ms: 1.5593633171355337\n",
      "  time_since_restore: 10160.526371240616\n",
      "  time_this_iter_s: 42.5014488697052\n",
      "  time_total_s: 10160.526371240616\n",
      "  timers:\n",
      "    learn_throughput: 1557.922\n",
      "    learn_time_ms: 641.881\n",
      "    load_throughput: 54387.639\n",
      "    load_time_ms: 18.387\n",
      "    sample_throughput: 38.651\n",
      "    sample_time_ms: 25872.839\n",
      "    update_time_ms: 2.832\n",
      "  timestamp: 1633801277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         10160.5</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 354.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1143\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1498214509752063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008495291964314412\n",
      "          policy_loss: -0.030117812669939466\n",
      "          total_loss: -0.05094417466057671\n",
      "          vf_explained_var: -0.5902255177497864\n",
      "          vf_loss: 0.00016155764375677488\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.96666666666667\n",
      "    ram_util_percent: 72.48888888888888\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680223872598326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.37296756944887\n",
      "    mean_inference_ms: 1.7222334606061867\n",
      "    mean_raw_obs_processing_ms: 1.5606143031344464\n",
      "  time_since_restore: 10185.799360513687\n",
      "  time_this_iter_s: 25.27298927307129\n",
      "  time_total_s: 10185.799360513687\n",
      "  timers:\n",
      "    learn_throughput: 1559.151\n",
      "    learn_time_ms: 641.375\n",
      "    load_throughput: 54232.646\n",
      "    load_time_ms: 18.439\n",
      "    sample_throughput: 38.216\n",
      "    sample_time_ms: 26167.226\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633801302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         10185.8</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 354.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1146\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9879118389553494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016931829319147853\n",
      "          policy_loss: -0.06296837048398124\n",
      "          total_loss: -0.0816758735312356\n",
      "          vf_explained_var: -0.43337181210517883\n",
      "          vf_loss: 0.00015455705926999347\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56388888888889\n",
      "    ram_util_percent: 72.38055555555557\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0368017536827068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.38090461385673\n",
      "    mean_inference_ms: 1.7222455554834166\n",
      "    mean_raw_obs_processing_ms: 1.5618708655475513\n",
      "  time_since_restore: 10211.212397575378\n",
      "  time_this_iter_s: 25.413037061691284\n",
      "  time_total_s: 10211.212397575378\n",
      "  timers:\n",
      "    learn_throughput: 1553.562\n",
      "    learn_time_ms: 643.682\n",
      "    load_throughput: 54082.02\n",
      "    load_time_ms: 18.49\n",
      "    sample_throughput: 37.621\n",
      "    sample_time_ms: 26581.207\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1633801327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         10211.2</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 354.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.08501005437639\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014990890952044684\n",
      "          policy_loss: -0.048611842923694186\n",
      "          total_loss: -0.06848731396926774\n",
      "          vf_explained_var: -0.28432992100715637\n",
      "          vf_loss: 7.416354431673729e-05\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.943243243243245\n",
      "    ram_util_percent: 72.25945945945944\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03680104108060628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.39132073646714\n",
      "    mean_inference_ms: 1.722260490312013\n",
      "    mean_raw_obs_processing_ms: 1.562784685478355\n",
      "  time_since_restore: 10237.25039434433\n",
      "  time_this_iter_s: 26.037996768951416\n",
      "  time_total_s: 10237.25039434433\n",
      "  timers:\n",
      "    learn_throughput: 1554.157\n",
      "    learn_time_ms: 643.436\n",
      "    load_throughput: 53990.961\n",
      "    load_time_ms: 18.522\n",
      "    sample_throughput: 37.855\n",
      "    sample_time_ms: 26416.728\n",
      "    update_time_ms: 2.105\n",
      "  timestamp: 1633801353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         10237.3</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 354.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1153\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2376663102044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009421181580436647\n",
      "          policy_loss: -0.03413349000944032\n",
      "          total_loss: -0.055888733598921034\n",
      "          vf_explained_var: -0.45184338092803955\n",
      "          vf_loss: 5.5509803102419636e-05\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.37837837837838\n",
      "    ram_util_percent: 72.2243243243243\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036800477853353025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.3990809262489\n",
      "    mean_inference_ms: 1.722271465414229\n",
      "    mean_raw_obs_processing_ms: 1.5628977771350674\n",
      "  time_since_restore: 10262.715075969696\n",
      "  time_this_iter_s: 25.46468162536621\n",
      "  time_total_s: 10262.715075969696\n",
      "  timers:\n",
      "    learn_throughput: 1554.662\n",
      "    learn_time_ms: 643.226\n",
      "    load_throughput: 54198.097\n",
      "    load_time_ms: 18.451\n",
      "    sample_throughput: 37.526\n",
      "    sample_time_ms: 26648.136\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1633801379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         10262.7</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-43-25\n",
      "  done: false\n",
      "  episode_len_mean: 353.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1156\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8918008857303195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009578072245659482\n",
      "          policy_loss: -0.0045929960906505585\n",
      "          total_loss: -0.02285449869102902\n",
      "          vf_explained_var: 0.02206122688949108\n",
      "          vf_loss: 8.117131033537185e-05\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.724324324324314\n",
      "    ram_util_percent: 72.29189189189192\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799902165487064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.40692589814323\n",
      "    mean_inference_ms: 1.7222821220374183\n",
      "    mean_raw_obs_processing_ms: 1.563017098174661\n",
      "  time_since_restore: 10288.935409784317\n",
      "  time_this_iter_s: 26.22033381462097\n",
      "  time_total_s: 10288.935409784317\n",
      "  timers:\n",
      "    learn_throughput: 1552.089\n",
      "    learn_time_ms: 644.293\n",
      "    load_throughput: 53925.017\n",
      "    load_time_ms: 18.544\n",
      "    sample_throughput: 37.261\n",
      "    sample_time_ms: 26837.46\n",
      "    update_time_ms: 2.124\n",
      "  timestamp: 1633801405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         10288.9</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            353.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 350.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1159\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8447875963317024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010495845567741056\n",
      "          policy_loss: 0.0308304063975811\n",
      "          total_loss: 0.013135049160983828\n",
      "          vf_explained_var: -0.5456375479698181\n",
      "          vf_loss: 0.00012205569615212477\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46923076923076\n",
      "    ram_util_percent: 72.36666666666669\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036799314159122604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.4150139482405\n",
      "    mean_inference_ms: 1.7222921479916817\n",
      "    mean_raw_obs_processing_ms: 1.5631448104275327\n",
      "  time_since_restore: 10315.743603229523\n",
      "  time_this_iter_s: 26.80819344520569\n",
      "  time_total_s: 10315.743603229523\n",
      "  timers:\n",
      "    learn_throughput: 1551.123\n",
      "    learn_time_ms: 644.694\n",
      "    load_throughput: 54104.623\n",
      "    load_time_ms: 18.483\n",
      "    sample_throughput: 36.877\n",
      "    sample_time_ms: 27117.164\n",
      "    update_time_ms: 2.105\n",
      "  timestamp: 1633801432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         10315.7</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 350.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1162\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1934904283947416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013004205748893188\n",
      "          policy_loss: 0.04197799128790696\n",
      "          total_loss: 0.020953824702236386\n",
      "          vf_explained_var: -0.8058944940567017\n",
      "          vf_loss: 0.00012960402042760203\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89722222222222\n",
      "    ram_util_percent: 72.45555555555556\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679874059934694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.423174591313145\n",
      "    mean_inference_ms: 1.7223016867293588\n",
      "    mean_raw_obs_processing_ms: 1.563314506221213\n",
      "  time_since_restore: 10341.428121328354\n",
      "  time_this_iter_s: 25.684518098831177\n",
      "  time_total_s: 10341.428121328354\n",
      "  timers:\n",
      "    learn_throughput: 1550.23\n",
      "    learn_time_ms: 645.066\n",
      "    load_throughput: 54569.209\n",
      "    load_time_ms: 18.325\n",
      "    sample_throughput: 37.205\n",
      "    sample_time_ms: 26877.966\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1633801458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         10341.4</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 348.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1165\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8458211713367039\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019563388906395772\n",
      "          policy_loss: -0.11999105695221159\n",
      "          total_loss: -0.1371795129444864\n",
      "          vf_explained_var: -0.108194500207901\n",
      "          vf_loss: 9.463086324912082e-05\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.565789473684205\n",
      "    ram_util_percent: 72.57631578947367\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036798099939593966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.431425367966373\n",
      "    mean_inference_ms: 1.7223097246778953\n",
      "    mean_raw_obs_processing_ms: 1.563489571454033\n",
      "  time_since_restore: 10367.98242354393\n",
      "  time_this_iter_s: 26.554302215576172\n",
      "  time_total_s: 10367.98242354393\n",
      "  timers:\n",
      "    learn_throughput: 1556.334\n",
      "    learn_time_ms: 642.535\n",
      "    load_throughput: 54685.892\n",
      "    load_time_ms: 18.286\n",
      "    sample_throughput: 37.167\n",
      "    sample_time_ms: 26905.357\n",
      "    update_time_ms: 2.104\n",
      "  timestamp: 1633801484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">           10368</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             348.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 347.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1168\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7782330049408808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010617524178707823\n",
      "          policy_loss: -0.014359873284896215\n",
      "          total_loss: -0.031404746075471245\n",
      "          vf_explained_var: 0.4099505543708801\n",
      "          vf_loss: 9.968698575701435e-05\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.56578947368421\n",
      "    ram_util_percent: 72.69210526315787\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679746623659063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.439791012846896\n",
      "    mean_inference_ms: 1.7223182875578034\n",
      "    mean_raw_obs_processing_ms: 1.5636700050895993\n",
      "  time_since_restore: 10394.341319322586\n",
      "  time_this_iter_s: 26.358895778656006\n",
      "  time_total_s: 10394.341319322586\n",
      "  timers:\n",
      "    learn_throughput: 1554.045\n",
      "    learn_time_ms: 643.482\n",
      "    load_throughput: 54606.863\n",
      "    load_time_ms: 18.313\n",
      "    sample_throughput: 37.086\n",
      "    sample_time_ms: 26964.309\n",
      "    update_time_ms: 2.104\n",
      "  timestamp: 1633801511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         10394.3</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 344.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1172\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9844532105657788\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010594868683749755\n",
      "          policy_loss: 0.008359353927274545\n",
      "          total_loss: -0.010686246740321318\n",
      "          vf_explained_var: -0.20333336293697357\n",
      "          vf_loss: 0.00016252557244216505\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.455555555555556\n",
      "    ram_util_percent: 72.78888888888889\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367966746276223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.451365710062444\n",
      "    mean_inference_ms: 1.7223304471524474\n",
      "    mean_raw_obs_processing_ms: 1.565447035123897\n",
      "  time_since_restore: 10438.804367780685\n",
      "  time_this_iter_s: 44.463048458099365\n",
      "  time_total_s: 10438.804367780685\n",
      "  timers:\n",
      "    learn_throughput: 1553.546\n",
      "    learn_time_ms: 643.689\n",
      "    load_throughput: 54805.366\n",
      "    load_time_ms: 18.246\n",
      "    sample_throughput: 36.818\n",
      "    sample_time_ms: 27160.342\n",
      "    update_time_ms: 2.099\n",
      "  timestamp: 1633801555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         10438.8</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 342.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1175\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0328180697229175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017075540987654767\n",
      "          policy_loss: -0.031022835440105864\n",
      "          total_loss: -0.05014766907940308\n",
      "          vf_explained_var: -0.5302733778953552\n",
      "          vf_loss: 0.00017766023108075994\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.137837837837836\n",
      "    ram_util_percent: 72.76486486486485\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679611046073895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.460156132778817\n",
      "    mean_inference_ms: 1.7223396238547573\n",
      "    mean_raw_obs_processing_ms: 1.5668017898243527\n",
      "  time_since_restore: 10464.486516237259\n",
      "  time_this_iter_s: 25.682148456573486\n",
      "  time_total_s: 10464.486516237259\n",
      "  timers:\n",
      "    learn_throughput: 1556.145\n",
      "    learn_time_ms: 642.614\n",
      "    load_throughput: 54831.23\n",
      "    load_time_ms: 18.238\n",
      "    sample_throughput: 36.761\n",
      "    sample_time_ms: 27202.386\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633801581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         10464.5</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            342.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-46-48\n",
      "  done: false\n",
      "  episode_len_mean: 341.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1178\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06006774902343748\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7932559649149578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023364133544677795\n",
      "          policy_loss: -0.029875435100661384\n",
      "          total_loss: -0.04627819649047322\n",
      "          vf_explained_var: -0.027525313198566437\n",
      "          vf_loss: 0.00012636692643152653\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.582051282051275\n",
      "    ram_util_percent: 72.37948717948714\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679556485497746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.468994970749282\n",
      "    mean_inference_ms: 1.7223487655290983\n",
      "    mean_raw_obs_processing_ms: 1.5681932793164264\n",
      "  time_since_restore: 10492.157774686813\n",
      "  time_this_iter_s: 27.671258449554443\n",
      "  time_total_s: 10492.157774686813\n",
      "  timers:\n",
      "    learn_throughput: 1563.473\n",
      "    learn_time_ms: 639.602\n",
      "    load_throughput: 54840.837\n",
      "    load_time_ms: 18.235\n",
      "    sample_throughput: 36.455\n",
      "    sample_time_ms: 27431.216\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633801608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">         10492.2</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            341.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-47-16\n",
      "  done: false\n",
      "  episode_len_mean: 340.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1181\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6184582802984449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010978510957228356\n",
      "          policy_loss: -0.0060015395283699036\n",
      "          total_loss: -0.020922634667820402\n",
      "          vf_explained_var: 0.09927268326282501\n",
      "          vf_loss: 0.00027430526238782075\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.66153846153846\n",
      "    ram_util_percent: 72.19999999999997\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036794998670353976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.477882979947722\n",
      "    mean_inference_ms: 1.7223576634958093\n",
      "    mean_raw_obs_processing_ms: 1.5684326239869741\n",
      "  time_since_restore: 10519.483700275421\n",
      "  time_this_iter_s: 27.325925588607788\n",
      "  time_total_s: 10519.483700275421\n",
      "  timers:\n",
      "    learn_throughput: 1565.263\n",
      "    learn_time_ms: 638.87\n",
      "    load_throughput: 55013.903\n",
      "    load_time_ms: 18.177\n",
      "    sample_throughput: 36.283\n",
      "    sample_time_ms: 27560.819\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633801636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         10519.5</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            340.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-47-41\n",
      "  done: false\n",
      "  episode_len_mean: 338.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1184\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9801068915261162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011590975589373562\n",
      "          policy_loss: -0.015246873204078939\n",
      "          total_loss: -0.033875494822859765\n",
      "          vf_explained_var: -0.7559942603111267\n",
      "          vf_loss: 0.0001280802834015857\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60833333333332\n",
      "    ram_util_percent: 72.19444444444441\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679444459424172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.486794008244807\n",
      "    mean_inference_ms: 1.7223666341997057\n",
      "    mean_raw_obs_processing_ms: 1.5686783250610987\n",
      "  time_since_restore: 10544.517150640488\n",
      "  time_this_iter_s: 25.03345036506653\n",
      "  time_total_s: 10544.517150640488\n",
      "  timers:\n",
      "    learn_throughput: 1568.536\n",
      "    learn_time_ms: 637.537\n",
      "    load_throughput: 55175.215\n",
      "    load_time_ms: 18.124\n",
      "    sample_throughput: 36.338\n",
      "    sample_time_ms: 27519.074\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633801661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">         10544.5</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            338.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 339.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1187\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.173541439904107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011862391045977697\n",
      "          policy_loss: -0.053731841759549245\n",
      "          total_loss: -0.07427558857533667\n",
      "          vf_explained_var: -0.5368295907974243\n",
      "          vf_loss: 0.00012284681417642988\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54117647058823\n",
      "    ram_util_percent: 72.30882352941178\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679389053743703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.49572827708835\n",
      "    mean_inference_ms: 1.7223755771380098\n",
      "    mean_raw_obs_processing_ms: 1.5689310501461031\n",
      "  time_since_restore: 10567.953350543976\n",
      "  time_this_iter_s: 23.43619990348816\n",
      "  time_total_s: 10567.953350543976\n",
      "  timers:\n",
      "    learn_throughput: 1573.244\n",
      "    learn_time_ms: 635.629\n",
      "    load_throughput: 54522.959\n",
      "    load_time_ms: 18.341\n",
      "    sample_throughput: 36.708\n",
      "    sample_time_ms: 27242.372\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633801684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">           10568</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            339.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 336.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1190\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8421559439765083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015480926704361448\n",
      "          policy_loss: 0.023279894888401032\n",
      "          total_loss: 0.006392775062057707\n",
      "          vf_explained_var: -0.2881089448928833\n",
      "          vf_loss: 0.0001395838328688923\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.06285714285715\n",
      "    ram_util_percent: 72.43142857142858\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679332746897483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.50475035903557\n",
      "    mean_inference_ms: 1.7223844049886057\n",
      "    mean_raw_obs_processing_ms: 1.56922481675891\n",
      "  time_since_restore: 10592.488180160522\n",
      "  time_this_iter_s: 24.53482961654663\n",
      "  time_total_s: 10592.488180160522\n",
      "  timers:\n",
      "    learn_throughput: 1571.94\n",
      "    learn_time_ms: 636.157\n",
      "    load_throughput: 54511.904\n",
      "    load_time_ms: 18.345\n",
      "    sample_throughput: 37.017\n",
      "    sample_time_ms: 27014.469\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633801709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         10592.5</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            336.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-48-54\n",
      "  done: false\n",
      "  episode_len_mean: 335.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1193\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7211375733216603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01067394128789968\n",
      "          policy_loss: -0.041640547662973405\n",
      "          total_loss: -0.05772922221157286\n",
      "          vf_explained_var: -0.822404146194458\n",
      "          vf_loss: 0.0001609639436032416\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62777777777778\n",
      "    ram_util_percent: 72.51944444444445\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036792752761806435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.51377992318678\n",
      "    mean_inference_ms: 1.7223927722776187\n",
      "    mean_raw_obs_processing_ms: 1.5695251256990306\n",
      "  time_since_restore: 10618.020530700684\n",
      "  time_this_iter_s: 25.532350540161133\n",
      "  time_total_s: 10618.020530700684\n",
      "  timers:\n",
      "    learn_throughput: 1575.783\n",
      "    learn_time_ms: 634.605\n",
      "    load_throughput: 54300.188\n",
      "    load_time_ms: 18.416\n",
      "    sample_throughput: 37.036\n",
      "    sample_time_ms: 27000.739\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633801734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">           10618</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-49-19\n",
      "  done: false\n",
      "  episode_len_mean: 335.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1196\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7804573668373955\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011620852094503517\n",
      "          policy_loss: -0.025073491119676165\n",
      "          total_loss: -0.04171981120275126\n",
      "          vf_explained_var: -0.4912753701210022\n",
      "          vf_loss: 0.00011119791161036119\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51714285714286\n",
      "    ram_util_percent: 72.60285714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036792171978349074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.52271211038662\n",
      "    mean_inference_ms: 1.722400867759393\n",
      "    mean_raw_obs_processing_ms: 1.5698324411355424\n",
      "  time_since_restore: 10642.376077890396\n",
      "  time_this_iter_s: 24.355547189712524\n",
      "  time_total_s: 10642.376077890396\n",
      "  timers:\n",
      "    learn_throughput: 1573.409\n",
      "    learn_time_ms: 635.563\n",
      "    load_throughput: 54252.849\n",
      "    load_time_ms: 18.432\n",
      "    sample_throughput: 37.341\n",
      "    sample_time_ms: 26779.898\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633801759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         10642.4</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             335.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-49-45\n",
      "  done: false\n",
      "  episode_len_mean: 334.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1199\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6221444076961942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010270651271057278\n",
      "          policy_loss: -0.008554815997680027\n",
      "          total_loss: -0.023694802530937725\n",
      "          vf_explained_var: -0.5590023994445801\n",
      "          vf_loss: 0.00015605527462159646\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53243243243243\n",
      "    ram_util_percent: 72.70540540540539\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367915781364555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.531696087356405\n",
      "    mean_inference_ms: 1.722408590272891\n",
      "    mean_raw_obs_processing_ms: 1.5701473809428725\n",
      "  time_since_restore: 10668.319677352905\n",
      "  time_this_iter_s: 25.943599462509155\n",
      "  time_total_s: 10668.319677352905\n",
      "  timers:\n",
      "    learn_throughput: 1578.574\n",
      "    learn_time_ms: 633.483\n",
      "    load_throughput: 54307.359\n",
      "    load_time_ms: 18.414\n",
      "    sample_throughput: 37.397\n",
      "    sample_time_ms: 26740.461\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1633801785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">         10668.3</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            334.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-50-24\n",
      "  done: false\n",
      "  episode_len_mean: 336.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1202\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0750296420521206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011363615488361529\n",
      "          policy_loss: -0.05404717922210693\n",
      "          total_loss: -0.07358449983100096\n",
      "          vf_explained_var: -0.8172051310539246\n",
      "          vf_loss: 0.00018909418269888394\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.74464285714286\n",
      "    ram_util_percent: 72.79821428571428\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036790979883134615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.540456281043365\n",
      "    mean_inference_ms: 1.7224159325879136\n",
      "    mean_raw_obs_processing_ms: 1.5715589347023107\n",
      "  time_since_restore: 10707.7326836586\n",
      "  time_this_iter_s: 39.41300630569458\n",
      "  time_total_s: 10707.7326836586\n",
      "  timers:\n",
      "    learn_throughput: 1577.205\n",
      "    learn_time_ms: 634.033\n",
      "    load_throughput: 54010.219\n",
      "    load_time_ms: 18.515\n",
      "    sample_throughput: 38.117\n",
      "    sample_time_ms: 26234.814\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1633801824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">         10707.7</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            336.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 335.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1204\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.101630218823751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01185327769658699\n",
      "          policy_loss: -0.04515853180653519\n",
      "          total_loss: -0.06501005612727669\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.677809462623877e-05\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87941176470588\n",
      "    ram_util_percent: 72.87647058823529\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03679058439549298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.54632689110762\n",
      "    mean_inference_ms: 1.7224207201173614\n",
      "    mean_raw_obs_processing_ms: 1.5724910426286507\n",
      "  time_since_restore: 10731.50579237938\n",
      "  time_this_iter_s: 23.77310872077942\n",
      "  time_total_s: 10731.50579237938\n",
      "  timers:\n",
      "    learn_throughput: 1575.251\n",
      "    learn_time_ms: 634.819\n",
      "    load_throughput: 53884.974\n",
      "    load_time_ms: 18.558\n",
      "    sample_throughput: 38.398\n",
      "    sample_time_ms: 26043.062\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1633801848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">         10731.5</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 333.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1207\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6733544985453288\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011302742305142032\n",
      "          policy_loss: -0.0374960840990146\n",
      "          total_loss: -0.0531154849463039\n",
      "          vf_explained_var: -0.7163342833518982\n",
      "          vf_loss: 9.574846472888667e-05\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76285714285714\n",
      "    ram_util_percent: 72.56285714285715\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367899552008351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.555296646062065\n",
      "    mean_inference_ms: 1.7224274873179193\n",
      "    mean_raw_obs_processing_ms: 1.5738786754637701\n",
      "  time_since_restore: 10755.59636092186\n",
      "  time_this_iter_s: 24.09056854248047\n",
      "  time_total_s: 10755.59636092186\n",
      "  timers:\n",
      "    learn_throughput: 1572.161\n",
      "    learn_time_ms: 636.067\n",
      "    load_throughput: 53886.22\n",
      "    load_time_ms: 18.558\n",
      "    sample_throughput: 38.935\n",
      "    sample_time_ms: 25683.735\n",
      "    update_time_ms: 2.098\n",
      "  timestamp: 1633801872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         10755.6</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 333.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9780812078052097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011290562030325477\n",
      "          policy_loss: 0.015522994763321347\n",
      "          total_loss: -0.0031101625826623703\n",
      "          vf_explained_var: -0.5786213874816895\n",
      "          vf_loss: 0.0001303576865514818\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5939393939394\n",
      "    ram_util_percent: 72.30909090909091\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036789330246503826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.56418282927615\n",
      "    mean_inference_ms: 1.722434081258005\n",
      "    mean_raw_obs_processing_ms: 1.5745348439137459\n",
      "  time_since_restore: 10779.120589256287\n",
      "  time_this_iter_s: 23.52422833442688\n",
      "  time_total_s: 10779.120589256287\n",
      "  timers:\n",
      "    learn_throughput: 1573.386\n",
      "    learn_time_ms: 635.572\n",
      "    load_throughput: 53823.156\n",
      "    load_time_ms: 18.579\n",
      "    sample_throughput: 39.519\n",
      "    sample_time_ms: 25304.041\n",
      "    update_time_ms: 2.083\n",
      "  timestamp: 1633801896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">         10779.1</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 334.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1212\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0207506484455533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009749747737758222\n",
      "          policy_loss: -0.024391951080825595\n",
      "          total_loss: -0.04365227934386995\n",
      "          vf_explained_var: -0.7483714818954468\n",
      "          vf_loss: 6.871097830298822e-05\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55454545454546\n",
      "    ram_util_percent: 72.28181818181818\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678891189672186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.570062742325987\n",
      "    mean_inference_ms: 1.7224380335786451\n",
      "    mean_raw_obs_processing_ms: 1.5747079266153952\n",
      "  time_since_restore: 10801.872466564178\n",
      "  time_this_iter_s: 22.751877307891846\n",
      "  time_total_s: 10801.872466564178\n",
      "  timers:\n",
      "    learn_throughput: 1570.161\n",
      "    learn_time_ms: 636.877\n",
      "    load_throughput: 53219.239\n",
      "    load_time_ms: 18.79\n",
      "    sample_throughput: 39.881\n",
      "    sample_time_ms: 25074.388\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1633801918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         10801.9</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            334.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 334.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1215\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0301479246881273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00995397386257761\n",
      "          policy_loss: -0.021480186076627838\n",
      "          total_loss: -0.04075782754355007\n",
      "          vf_explained_var: -0.7163305282592773\n",
      "          vf_loss: 0.00012696582909686388\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.023333333333326\n",
      "    ram_util_percent: 72.34666666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678827576339317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.578828943898316\n",
      "    mean_inference_ms: 1.7224435785855365\n",
      "    mean_raw_obs_processing_ms: 1.5749587195360528\n",
      "  time_since_restore: 10823.125696897507\n",
      "  time_this_iter_s: 21.253230333328247\n",
      "  time_total_s: 10823.125696897507\n",
      "  timers:\n",
      "    learn_throughput: 1568.056\n",
      "    learn_time_ms: 637.732\n",
      "    load_throughput: 53872.17\n",
      "    load_time_ms: 18.562\n",
      "    sample_throughput: 40.233\n",
      "    sample_time_ms: 24855.435\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1633801940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">         10823.1</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            334.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-52-41\n",
      "  done: false\n",
      "  episode_len_mean: 335.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1217\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0336864524417453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010018133671489692\n",
      "          policy_loss: 0.009323620547850927\n",
      "          total_loss: -0.009985389229324128\n",
      "          vf_explained_var: -0.9063633680343628\n",
      "          vf_loss: 0.000125204777416204\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.44333333333334\n",
      "    ram_util_percent: 72.44333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036787844497087135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.584502668584193\n",
      "    mean_inference_ms: 1.7224467565521444\n",
      "    mean_raw_obs_processing_ms: 1.575139531874028\n",
      "  time_since_restore: 10844.066744565964\n",
      "  time_this_iter_s: 20.94104766845703\n",
      "  time_total_s: 10844.066744565964\n",
      "  timers:\n",
      "    learn_throughput: 1571.288\n",
      "    learn_time_ms: 636.421\n",
      "    load_throughput: 53655.156\n",
      "    load_time_ms: 18.638\n",
      "    sample_throughput: 40.821\n",
      "    sample_time_ms: 24497.302\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1633801961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         10844.1</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            335.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 337.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09010162353515627\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9617074489593507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020534243469304294\n",
      "          policy_loss: 0.030236334933174982\n",
      "          total_loss: 0.023449485003948212\n",
      "          vf_explained_var: -0.6553041934967041\n",
      "          vf_loss: 0.010980058288259898\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68387096774194\n",
      "    ram_util_percent: 72.47741935483872\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036787179959808146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.592786817619977\n",
      "    mean_inference_ms: 1.7224506275235132\n",
      "    mean_raw_obs_processing_ms: 1.5754009147767536\n",
      "  time_since_restore: 10866.09555888176\n",
      "  time_this_iter_s: 22.0288143157959\n",
      "  time_total_s: 10866.09555888176\n",
      "  timers:\n",
      "    learn_throughput: 1568.896\n",
      "    learn_time_ms: 637.391\n",
      "    load_throughput: 53702.489\n",
      "    load_time_ms: 18.621\n",
      "    sample_throughput: 41.415\n",
      "    sample_time_ms: 24145.977\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633801983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         10866.1</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            337.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 338.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1222\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9678347322675918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013856902464414213\n",
      "          policy_loss: -0.0967021317117744\n",
      "          total_loss: -0.11429879317680995\n",
      "          vf_explained_var: 0.06782595068216324\n",
      "          vf_loss: 0.00020889077091447284\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.455882352941174\n",
      "    ram_util_percent: 72.50882352941176\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036786769844058646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.59833486418678\n",
      "    mean_inference_ms: 1.7224527775475378\n",
      "    mean_raw_obs_processing_ms: 1.5755560784979936\n",
      "  time_since_restore: 10889.472422361374\n",
      "  time_this_iter_s: 23.376863479614258\n",
      "  time_total_s: 10889.472422361374\n",
      "  timers:\n",
      "    learn_throughput: 1566.774\n",
      "    learn_time_ms: 638.254\n",
      "    load_throughput: 54026.777\n",
      "    load_time_ms: 18.509\n",
      "    sample_throughput: 41.585\n",
      "    sample_time_ms: 24047.345\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633802006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         10889.5</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            338.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-53-48\n",
      "  done: false\n",
      "  episode_len_mean: 338.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1225\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8420290205213758\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013110084206476379\n",
      "          policy_loss: -0.0888643273876773\n",
      "          total_loss: -0.10526330090231366\n",
      "          vf_explained_var: 0.20519621670246124\n",
      "          vf_loss: 0.00024945747847798176\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.4125\n",
      "    ram_util_percent: 72.584375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678614765036242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.60650348928806\n",
      "    mean_inference_ms: 1.7224553833062126\n",
      "    mean_raw_obs_processing_ms: 1.5757931821212425\n",
      "  time_since_restore: 10911.828973054886\n",
      "  time_this_iter_s: 22.356550693511963\n",
      "  time_total_s: 10911.828973054886\n",
      "  timers:\n",
      "    learn_throughput: 1563.966\n",
      "    learn_time_ms: 639.4\n",
      "    load_throughput: 53996.522\n",
      "    load_time_ms: 18.52\n",
      "    sample_throughput: 42.216\n",
      "    sample_time_ms: 23687.499\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1633802028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         10911.8</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            338.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 339.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1228\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9959371169408162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016602143631714267\n",
      "          policy_loss: 0.022808645748429827\n",
      "          total_loss: 0.005243645111719767\n",
      "          vf_explained_var: -0.036942485719919205\n",
      "          vf_loss: 0.00015055117029179303\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.48125\n",
      "    ram_util_percent: 72.665625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678553326564857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.614565745372655\n",
      "    mean_inference_ms: 1.7224575211422328\n",
      "    mean_raw_obs_processing_ms: 1.5760348026529911\n",
      "  time_since_restore: 10934.601404428482\n",
      "  time_this_iter_s: 22.77243137359619\n",
      "  time_total_s: 10934.601404428482\n",
      "  timers:\n",
      "    learn_throughput: 1566.75\n",
      "    learn_time_ms: 638.264\n",
      "    load_throughput: 54205.732\n",
      "    load_time_ms: 18.448\n",
      "    sample_throughput: 45.404\n",
      "    sample_time_ms: 22024.632\n",
      "    update_time_ms: 2.085\n",
      "  timestamp: 1633802051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         10934.6</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            339.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-54-53\n",
      "  done: false\n",
      "  episode_len_mean: 341.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8969899151060317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018038563375843433\n",
      "          policy_loss: -0.029216835854782\n",
      "          total_loss: -0.04560089686678515\n",
      "          vf_explained_var: -0.2436368763446808\n",
      "          vf_loss: 0.00014788010707383767\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.574576271186444\n",
      "    ram_util_percent: 72.7186440677966\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678513374636919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.619764078948634\n",
      "    mean_inference_ms: 1.722458304149014\n",
      "    mean_raw_obs_processing_ms: 1.5768965839038527\n",
      "  time_since_restore: 10975.985953569412\n",
      "  time_this_iter_s: 41.384549140930176\n",
      "  time_total_s: 10975.985953569412\n",
      "  timers:\n",
      "    learn_throughput: 1566.974\n",
      "    learn_time_ms: 638.173\n",
      "    load_throughput: 56751.797\n",
      "    load_time_ms: 17.621\n",
      "    sample_throughput: 42.04\n",
      "    sample_time_ms: 23786.708\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633802093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">           10976</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             341.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 343.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.01\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1233\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.040954959392548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01195283328476831\n",
      "          policy_loss: -0.024853681441810396\n",
      "          total_loss: -0.04349958904915386\n",
      "          vf_explained_var: -0.32130032777786255\n",
      "          vf_loss: 0.0001481863935219331\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.346875\n",
      "    ram_util_percent: 72.778125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367845512807044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.62736107743884\n",
      "    mean_inference_ms: 1.7224590743124093\n",
      "    mean_raw_obs_processing_ms: 1.578164887133145\n",
      "  time_since_restore: 10997.871542930603\n",
      "  time_this_iter_s: 21.885589361190796\n",
      "  time_total_s: 10997.871542930603\n",
      "  timers:\n",
      "    learn_throughput: 1569.402\n",
      "    learn_time_ms: 637.185\n",
      "    load_throughput: 56843.015\n",
      "    load_time_ms: 17.592\n",
      "    sample_throughput: 42.432\n",
      "    sample_time_ms: 23567.246\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633802115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         10997.9</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\">   -0.01</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            343.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 345.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1236\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1351524353027344\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.965387749671936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026788656052428195\n",
      "          policy_loss: 0.02961403396394518\n",
      "          total_loss: 0.044927839934825894\n",
      "          vf_explained_var: -0.6109439134597778\n",
      "          vf_loss: 0.03134712892286997\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.733333333333334\n",
      "    ram_util_percent: 72.73636363636363\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036783956950549965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.634741128881856\n",
      "    mean_inference_ms: 1.7224590243697433\n",
      "    mean_raw_obs_processing_ms: 1.5794387614081635\n",
      "  time_since_restore: 11020.945688724518\n",
      "  time_this_iter_s: 23.074145793914795\n",
      "  time_total_s: 11020.945688724518\n",
      "  timers:\n",
      "    learn_throughput: 1566.193\n",
      "    learn_time_ms: 638.491\n",
      "    load_throughput: 56885.108\n",
      "    load_time_ms: 17.579\n",
      "    sample_throughput: 42.515\n",
      "    sample_time_ms: 23520.897\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633802138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         11020.9</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            345.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 346.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1238\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8954002526071336\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010571371393014199\n",
      "          policy_loss: -0.15028227219978968\n",
      "          total_loss: -0.16670248690578673\n",
      "          vf_explained_var: 0.7758537530899048\n",
      "          vf_loss: 0.00039066400948084063\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58387096774193\n",
      "    ram_util_percent: 72.45806451612904\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678357477338359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.639525824813006\n",
      "    mean_inference_ms: 1.7224588586081797\n",
      "    mean_raw_obs_processing_ms: 1.5798973681560875\n",
      "  time_since_restore: 11042.894914388657\n",
      "  time_this_iter_s: 21.949225664138794\n",
      "  time_total_s: 11042.894914388657\n",
      "  timers:\n",
      "    learn_throughput: 1567.421\n",
      "    learn_time_ms: 637.991\n",
      "    load_throughput: 57264.501\n",
      "    load_time_ms: 17.463\n",
      "    sample_throughput: 42.66\n",
      "    sample_time_ms: 23441.221\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1633802160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         11042.9</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            346.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 348.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1241\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9540720595253838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010218227698326822\n",
      "          policy_loss: 0.004817003177271949\n",
      "          total_loss: -0.012415820194615259\n",
      "          vf_explained_var: 0.555424690246582\n",
      "          vf_loss: 0.00023636982578965318\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.91515151515151\n",
      "    ram_util_percent: 72.34545454545456\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678302076503243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.646565932745037\n",
      "    mean_inference_ms: 1.722458359969978\n",
      "    mean_raw_obs_processing_ms: 1.5800029507619267\n",
      "  time_since_restore: 11065.767679929733\n",
      "  time_this_iter_s: 22.87276554107666\n",
      "  time_total_s: 11065.767679929733\n",
      "  timers:\n",
      "    learn_throughput: 1567.9\n",
      "    learn_time_ms: 637.796\n",
      "    load_throughput: 57247.618\n",
      "    load_time_ms: 17.468\n",
      "    sample_throughput: 42.367\n",
      "    sample_time_ms: 23603.382\n",
      "    update_time_ms: 2.098\n",
      "  timestamp: 1633802182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         11065.8</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            348.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 350.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1243\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9279448098606533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010213805823752567\n",
      "          policy_loss: -0.08954066381686264\n",
      "          total_loss: -0.10656019527879027\n",
      "          vf_explained_var: 0.5897126197814941\n",
      "          vf_loss: 0.00018928338621561932\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86451612903226\n",
      "    ram_util_percent: 72.2967741935484\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678265336003581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.65116421253192\n",
      "    mean_inference_ms: 1.7224578225226241\n",
      "    mean_raw_obs_processing_ms: 1.5800568738853884\n",
      "  time_since_restore: 11087.782934188843\n",
      "  time_this_iter_s: 22.015254259109497\n",
      "  time_total_s: 11087.782934188843\n",
      "  timers:\n",
      "    learn_throughput: 1565.85\n",
      "    learn_time_ms: 638.631\n",
      "    load_throughput: 57199.449\n",
      "    load_time_ms: 17.483\n",
      "    sample_throughput: 42.176\n",
      "    sample_time_ms: 23709.971\n",
      "    update_time_ms: 2.099\n",
      "  timestamp: 1633802205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         11087.8</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            350.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 352.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1246\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8367931021584405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009900415087845064\n",
      "          policy_loss: 0.03237685523927212\n",
      "          total_loss: 0.01619465094473627\n",
      "          vf_explained_var: -0.2768970727920532\n",
      "          vf_loss: 0.00017863014468780925\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.539999999999985\n",
      "    ram_util_percent: 72.31666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678209171447537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.65774166266576\n",
      "    mean_inference_ms: 1.722456569358503\n",
      "    mean_raw_obs_processing_ms: 1.5801454711405145\n",
      "  time_since_restore: 11108.531208276749\n",
      "  time_this_iter_s: 20.748274087905884\n",
      "  time_total_s: 11108.531208276749\n",
      "  timers:\n",
      "    learn_throughput: 1567.497\n",
      "    learn_time_ms: 637.96\n",
      "    load_throughput: 56966.154\n",
      "    load_time_ms: 17.554\n",
      "    sample_throughput: 42.404\n",
      "    sample_time_ms: 23582.518\n",
      "    update_time_ms: 2.095\n",
      "  timestamp: 1633802225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         11108.5</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            352.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 353.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1248\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8715436591042414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006566391589740227\n",
      "          policy_loss: -0.046776481428080136\n",
      "          total_loss: -0.0639927691883511\n",
      "          vf_explained_var: -0.5975600481033325\n",
      "          vf_loss: 0.00016795134466115593\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62647058823529\n",
      "    ram_util_percent: 72.38823529411764\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678173315596598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.662017108766076\n",
      "    mean_inference_ms: 1.722455653878628\n",
      "    mean_raw_obs_processing_ms: 1.5802067480744382\n",
      "  time_since_restore: 11132.434755563736\n",
      "  time_this_iter_s: 23.903547286987305\n",
      "  time_total_s: 11132.434755563736\n",
      "  timers:\n",
      "    learn_throughput: 1568.381\n",
      "    learn_time_ms: 637.6\n",
      "    load_throughput: 56383.617\n",
      "    load_time_ms: 17.736\n",
      "    sample_throughput: 42.309\n",
      "    sample_time_ms: 23635.377\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1633802249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         11132.4</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            353.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 355.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1251\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.867292214764489\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007835092724498062\n",
      "          policy_loss: -0.06455815757314363\n",
      "          total_loss: -0.08153098275264105\n",
      "          vf_explained_var: -0.3875837028026581\n",
      "          vf_loss: 0.00011169871031597722\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52903225806452\n",
      "    ram_util_percent: 72.49677419354839\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036781166361054225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.668374137648495\n",
      "    mean_inference_ms: 1.7224535833569492\n",
      "    mean_raw_obs_processing_ms: 1.580244774529033\n",
      "  time_since_restore: 11154.435712814331\n",
      "  time_this_iter_s: 22.000957250595093\n",
      "  time_total_s: 11154.435712814331\n",
      "  timers:\n",
      "    learn_throughput: 1570.11\n",
      "    learn_time_ms: 636.898\n",
      "    load_throughput: 56000.737\n",
      "    load_time_ms: 17.857\n",
      "    sample_throughput: 42.372\n",
      "    sample_time_ms: 23600.407\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633802271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         11154.4</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            355.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-58-13\n",
      "  done: false\n",
      "  episode_len_mean: 356.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1253\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8708825363053216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00978402781162081\n",
      "          policy_loss: -0.006050688442256716\n",
      "          total_loss: -0.022596681490540504\n",
      "          vf_explained_var: -0.278639554977417\n",
      "          vf_loss: 0.00017932881989205876\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.97741935483871\n",
      "    ram_util_percent: 72.61612903225806\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03678078302783744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.672485256954587\n",
      "    mean_inference_ms: 1.722451989513438\n",
      "    mean_raw_obs_processing_ms: 1.580253534570472\n",
      "  time_since_restore: 11175.760046720505\n",
      "  time_this_iter_s: 21.324333906173706\n",
      "  time_total_s: 11175.760046720505\n",
      "  timers:\n",
      "    learn_throughput: 1571.766\n",
      "    learn_time_ms: 636.227\n",
      "    load_throughput: 56191.977\n",
      "    load_time_ms: 17.796\n",
      "    sample_throughput: 42.632\n",
      "    sample_time_ms: 23456.361\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633802293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         11175.8</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            356.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-58-35\n",
      "  done: false\n",
      "  episode_len_mean: 358.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1256\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865295410147\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8642119103007846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025833023338915694\n",
      "          policy_loss: -0.03328143357195788\n",
      "          total_loss: -0.046461743995961216\n",
      "          vf_explained_var: -0.4709561765193939\n",
      "          vf_loss: 0.00022471395358378586\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65806451612904\n",
      "    ram_util_percent: 72.68387096774192\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036780223630555046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.67836984542883\n",
      "    mean_inference_ms: 1.722449224088941\n",
      "    mean_raw_obs_processing_ms: 1.5802738909118341\n",
      "  time_since_restore: 11197.951564073563\n",
      "  time_this_iter_s: 22.19151735305786\n",
      "  time_total_s: 11197.951564073563\n",
      "  timers:\n",
      "    learn_throughput: 1572.706\n",
      "    learn_time_ms: 635.847\n",
      "    load_throughput: 53693.345\n",
      "    load_time_ms: 18.624\n",
      "    sample_throughput: 46.433\n",
      "    sample_time_ms: 21536.618\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633802315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">           11198</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            358.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 360.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1258\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8456408593389724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008486976161423203\n",
      "          policy_loss: -0.029558578216367298\n",
      "          total_loss: -0.04527211553520626\n",
      "          vf_explained_var: -0.2384376972913742\n",
      "          vf_loss: 0.00016204048960288573\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53\n",
      "    ram_util_percent: 72.72333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677985354098468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.68202270117555\n",
      "    mean_inference_ms: 1.7224467768514635\n",
      "    mean_raw_obs_processing_ms: 1.580290695586416\n",
      "  time_since_restore: 11218.696693181992\n",
      "  time_this_iter_s: 20.745129108428955\n",
      "  time_total_s: 11218.696693181992\n",
      "  timers:\n",
      "    learn_throughput: 1572.034\n",
      "    learn_time_ms: 636.118\n",
      "    load_throughput: 53688.259\n",
      "    load_time_ms: 18.626\n",
      "    sample_throughput: 46.68\n",
      "    sample_time_ms: 21422.289\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633802336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         11218.7</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            360.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 362.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1261\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.927603464656406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010863747587061419\n",
      "          policy_loss: 0.012272036655081643\n",
      "          total_loss: -0.003519730476869477\n",
      "          vf_explained_var: -0.8039476871490479\n",
      "          vf_loss: 0.00018067956294140055\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.83620689655173\n",
      "    ram_util_percent: 72.83275862068965\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677929171745755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.68747320628032\n",
      "    mean_inference_ms: 1.7224432630969675\n",
      "    mean_raw_obs_processing_ms: 1.5812861475404094\n",
      "  time_since_restore: 11259.185356140137\n",
      "  time_this_iter_s: 40.48866295814514\n",
      "  time_total_s: 11259.185356140137\n",
      "  timers:\n",
      "    learn_throughput: 1574.79\n",
      "    learn_time_ms: 635.005\n",
      "    load_throughput: 53526.016\n",
      "    load_time_ms: 18.683\n",
      "    sample_throughput: 43.169\n",
      "    sample_time_ms: 23164.733\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1633802376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         11259.2</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             362.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_17-59-59\n",
      "  done: false\n",
      "  episode_len_mean: 364.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1264\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.977193378077613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00932986831760516\n",
      "          policy_loss: 0.009973694880803427\n",
      "          total_loss: -0.006787752442889743\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00017333785346838543\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68181818181818\n",
      "    ram_util_percent: 72.84242424242424\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036778719593166295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.692713827160688\n",
      "    mean_inference_ms: 1.7224395752112454\n",
      "    mean_raw_obs_processing_ms: 1.5822865504938155\n",
      "  time_since_restore: 11282.584169864655\n",
      "  time_this_iter_s: 23.398813724517822\n",
      "  time_total_s: 11282.584169864655\n",
      "  timers:\n",
      "    learn_throughput: 1573.627\n",
      "    learn_time_ms: 635.475\n",
      "    load_throughput: 53625.658\n",
      "    load_time_ms: 18.648\n",
      "    sample_throughput: 42.901\n",
      "    sample_time_ms: 23309.225\n",
      "    update_time_ms: 2.204\n",
      "  timestamp: 1633802399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         11282.6</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            364.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-00-22\n",
      "  done: false\n",
      "  episode_len_mean: 365.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1266\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.938465326362186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007094061877686543\n",
      "          policy_loss: -0.09121195148262713\n",
      "          total_loss: -0.10832868038366238\n",
      "          vf_explained_var: -0.8180835247039795\n",
      "          vf_loss: 0.00011066859014034789\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.815625000000004\n",
      "    ram_util_percent: 72.65625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677834392356477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.6960454468725\n",
      "    mean_inference_ms: 1.7224370278015795\n",
      "    mean_raw_obs_processing_ms: 1.5829461119328787\n",
      "  time_since_restore: 11304.629213094711\n",
      "  time_this_iter_s: 22.045043230056763\n",
      "  time_total_s: 11304.629213094711\n",
      "  timers:\n",
      "    learn_throughput: 1573.885\n",
      "    learn_time_ms: 635.37\n",
      "    load_throughput: 53232.815\n",
      "    load_time_ms: 18.785\n",
      "    sample_throughput: 43.054\n",
      "    sample_time_ms: 23226.416\n",
      "    update_time_ms: 2.209\n",
      "  timestamp: 1633802422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         11304.6</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            365.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 367.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1269\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.928679683473375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010917037483054375\n",
      "          policy_loss: -0.03268705668548743\n",
      "          total_loss: -0.04851673804223537\n",
      "          vf_explained_var: -0.9952496886253357\n",
      "          vf_loss: 0.00013732295014455708\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54545454545455\n",
      "    ram_util_percent: 72.42121212121214\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677779969555553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.700850296889552\n",
      "    mean_inference_ms: 1.722432718593962\n",
      "    mean_raw_obs_processing_ms: 1.5835619216090076\n",
      "  time_since_restore: 11327.912419319153\n",
      "  time_this_iter_s: 23.28320622444153\n",
      "  time_total_s: 11327.912419319153\n",
      "  timers:\n",
      "    learn_throughput: 1576.006\n",
      "    learn_time_ms: 634.516\n",
      "    load_throughput: 53085.938\n",
      "    load_time_ms: 18.837\n",
      "    sample_throughput: 42.819\n",
      "    sample_time_ms: 23353.991\n",
      "    update_time_ms: 2.22\n",
      "  timestamp: 1633802445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         11327.9</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            367.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 370.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1272\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7607002351019116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011217946968452205\n",
      "          policy_loss: -0.01349127023584313\n",
      "          total_loss: -0.027580098062753678\n",
      "          vf_explained_var: -0.23568294942378998\n",
      "          vf_loss: 0.00010687537712025611\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61935483870967\n",
      "    ram_util_percent: 72.38064516129035\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677724849481683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.705388259344673\n",
      "    mean_inference_ms: 1.7224278628499528\n",
      "    mean_raw_obs_processing_ms: 1.5834238620539667\n",
      "  time_since_restore: 11349.49058175087\n",
      "  time_this_iter_s: 21.57816243171692\n",
      "  time_total_s: 11349.49058175087\n",
      "  timers:\n",
      "    learn_throughput: 1575.283\n",
      "    learn_time_ms: 634.806\n",
      "    load_throughput: 53382.138\n",
      "    load_time_ms: 18.733\n",
      "    sample_throughput: 42.668\n",
      "    sample_time_ms: 23436.801\n",
      "    update_time_ms: 2.215\n",
      "  timestamp: 1633802466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">         11349.5</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            370.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 372.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1274\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9992495205667284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00732112619636249\n",
      "          policy_loss: -0.07186120324250725\n",
      "          total_loss: -0.08948164766447411\n",
      "          vf_explained_var: -0.729261040687561\n",
      "          vf_loss: 0.00014574744475087046\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.49032258064516\n",
      "    ram_util_percent: 72.45806451612904\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677689660815507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.70822882963221\n",
      "    mean_inference_ms: 1.7224244792934007\n",
      "    mean_raw_obs_processing_ms: 1.583335772132109\n",
      "  time_since_restore: 11371.05466413498\n",
      "  time_this_iter_s: 21.564082384109497\n",
      "  time_total_s: 11371.05466413498\n",
      "  timers:\n",
      "    learn_throughput: 1575.232\n",
      "    learn_time_ms: 634.827\n",
      "    load_throughput: 53705.239\n",
      "    load_time_ms: 18.62\n",
      "    sample_throughput: 43.098\n",
      "    sample_time_ms: 23202.94\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633802488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         11371.1</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            372.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 374.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1277\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.982496032449934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007563971971904277\n",
      "          policy_loss: -0.08594619805614154\n",
      "          total_loss: -0.10338130998942587\n",
      "          vf_explained_var: -0.12748372554779053\n",
      "          vf_loss: 8.969750465944849e-05\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.693749999999994\n",
      "    ram_util_percent: 72.496875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367763633453193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.712304392177174\n",
      "    mean_inference_ms: 1.7224192195014123\n",
      "    mean_raw_obs_processing_ms: 1.5831807170187329\n",
      "  time_since_restore: 11393.98359489441\n",
      "  time_this_iter_s: 22.92893075942993\n",
      "  time_total_s: 11393.98359489441\n",
      "  timers:\n",
      "    learn_throughput: 1575.37\n",
      "    learn_time_ms: 634.772\n",
      "    load_throughput: 53970.327\n",
      "    load_time_ms: 18.529\n",
      "    sample_throughput: 42.926\n",
      "    sample_time_ms: 23295.883\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1633802511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">           11394</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            374.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-02-12\n",
      "  done: false\n",
      "  episode_len_mean: 377.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1279\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.109001061651442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009129082858445593\n",
      "          policy_loss: -0.12093195352289411\n",
      "          total_loss: -0.13907989511887234\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00016597815912165162\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71290322580646\n",
      "    ram_util_percent: 72.60322580645159\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677603833969354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.71479861426422\n",
      "    mean_inference_ms: 1.7224155420541747\n",
      "    mean_raw_obs_processing_ms: 1.5830706753841446\n",
      "  time_since_restore: 11415.358298778534\n",
      "  time_this_iter_s: 21.374703884124756\n",
      "  time_total_s: 11415.358298778534\n",
      "  timers:\n",
      "    learn_throughput: 1572.993\n",
      "    learn_time_ms: 635.731\n",
      "    load_throughput: 53777.954\n",
      "    load_time_ms: 18.595\n",
      "    sample_throughput: 42.919\n",
      "    sample_time_ms: 23299.877\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1633802532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">         11415.4</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            377.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 379.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1282\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8937624335289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00968196926003801\n",
      "          policy_loss: -0.06667740932769245\n",
      "          total_loss: -0.08246769317322307\n",
      "          vf_explained_var: -0.5476077795028687\n",
      "          vf_loss: 0.00020311993850757263\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51875\n",
      "    ram_util_percent: 72.671875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036775547902101406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.71834434910314\n",
      "    mean_inference_ms: 1.7224101782668095\n",
      "    mean_raw_obs_processing_ms: 1.5828973600273115\n",
      "  time_since_restore: 11438.10340833664\n",
      "  time_this_iter_s: 22.74510955810547\n",
      "  time_total_s: 11438.10340833664\n",
      "  timers:\n",
      "    learn_throughput: 1575.122\n",
      "    learn_time_ms: 634.871\n",
      "    load_throughput: 53913.372\n",
      "    load_time_ms: 18.548\n",
      "    sample_throughput: 42.815\n",
      "    sample_time_ms: 23356.134\n",
      "    update_time_ms: 2.22\n",
      "  timestamp: 1633802555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         11438.1</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            379.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 381.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1284\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0146983636750115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008406772078993402\n",
      "          policy_loss: -0.05729822917944855\n",
      "          total_loss: -0.0746305676177144\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002582029895468925\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46129032258065\n",
      "    ram_util_percent: 72.75483870967744\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677521254026499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.720608803054507\n",
      "    mean_inference_ms: 1.7224063798920561\n",
      "    mean_raw_obs_processing_ms: 1.5827651860645477\n",
      "  time_since_restore: 11459.67663359642\n",
      "  time_this_iter_s: 21.573225259780884\n",
      "  time_total_s: 11459.67663359642\n",
      "  timers:\n",
      "    learn_throughput: 1574.935\n",
      "    learn_time_ms: 634.947\n",
      "    load_throughput: 53598.863\n",
      "    load_time_ms: 18.657\n",
      "    sample_throughput: 42.664\n",
      "    sample_time_ms: 23438.781\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633802577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         11459.7</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            381.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-03-19\n",
      "  done: false\n",
      "  episode_len_mean: 381.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1287\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8391591919793022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008562824155669209\n",
      "          policy_loss: 0.07685695971465772\n",
      "          total_loss: 0.06127461550964249\n",
      "          vf_explained_var: -0.7533277869224548\n",
      "          vf_loss: 0.00020535145393094152\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.568749999999994\n",
      "    ram_util_percent: 72.803125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677473674523636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.72393770021958\n",
      "    mean_inference_ms: 1.7224009188619902\n",
      "    mean_raw_obs_processing_ms: 1.582573300990102\n",
      "  time_since_restore: 11482.268161058426\n",
      "  time_this_iter_s: 22.591527462005615\n",
      "  time_total_s: 11482.268161058426\n",
      "  timers:\n",
      "    learn_throughput: 1572.009\n",
      "    learn_time_ms: 636.129\n",
      "    load_throughput: 53856.744\n",
      "    load_time_ms: 18.568\n",
      "    sample_throughput: 46.193\n",
      "    sample_time_ms: 21648.079\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1633802599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         11482.3</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             381.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 381.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1289\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9941617276933459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0066850405678210755\n",
      "          policy_loss: -0.09087777193635702\n",
      "          total_loss: -0.10867480906761355\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011170629812921915\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.444117647058825\n",
      "    ram_util_percent: 72.8529411764706\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677443215838597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.726108213379085\n",
      "    mean_inference_ms: 1.7223973618705783\n",
      "    mean_raw_obs_processing_ms: 1.5824484535577035\n",
      "  time_since_restore: 11505.905973911285\n",
      "  time_this_iter_s: 23.637812852859497\n",
      "  time_total_s: 11505.905973911285\n",
      "  timers:\n",
      "    learn_throughput: 1572.197\n",
      "    learn_time_ms: 636.053\n",
      "    load_throughput: 53773.473\n",
      "    load_time_ms: 18.597\n",
      "    sample_throughput: 46.142\n",
      "    sample_time_ms: 21672.067\n",
      "    update_time_ms: 2.064\n",
      "  timestamp: 1633802623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">         11505.9</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            381.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 383.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1292\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9898854573567708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009672464064140879\n",
      "          policy_loss: -0.022649069420165487\n",
      "          total_loss: -0.0394686219178968\n",
      "          vf_explained_var: -0.652441680431366\n",
      "          vf_loss: 0.00013797233639504863\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.50363636363637\n",
      "    ram_util_percent: 72.89090909090908\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677397622077294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.729195219656546\n",
      "    mean_inference_ms: 1.7223918058854009\n",
      "    mean_raw_obs_processing_ms: 1.583229760570169\n",
      "  time_since_restore: 11544.236941337585\n",
      "  time_this_iter_s: 38.33096742630005\n",
      "  time_total_s: 11544.236941337585\n",
      "  timers:\n",
      "    learn_throughput: 1570.92\n",
      "    learn_time_ms: 636.57\n",
      "    load_throughput: 54095.831\n",
      "    load_time_ms: 18.486\n",
      "    sample_throughput: 42.918\n",
      "    sample_time_ms: 23300.253\n",
      "    update_time_ms: 2.055\n",
      "  timestamp: 1633802661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         11544.2</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            383.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 385.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1294\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9726806203524272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009563107681710864\n",
      "          policy_loss: -0.07806181328164206\n",
      "          total_loss: -0.09473998703890377\n",
      "          vf_explained_var: -0.8023274540901184\n",
      "          vf_loss: 0.00014055750200188616\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.75625\n",
      "    ram_util_percent: 73.05625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367736659154238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.73116909671136\n",
      "    mean_inference_ms: 1.7223876946774812\n",
      "    mean_raw_obs_processing_ms: 1.5837432423396711\n",
      "  time_since_restore: 11566.659482002258\n",
      "  time_this_iter_s: 22.42254066467285\n",
      "  time_total_s: 11566.659482002258\n",
      "  timers:\n",
      "    learn_throughput: 1569.448\n",
      "    learn_time_ms: 637.167\n",
      "    load_throughput: 54378.825\n",
      "    load_time_ms: 18.39\n",
      "    sample_throughput: 43.078\n",
      "    sample_time_ms: 23213.705\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633802684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         11566.7</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            385.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-05-06\n",
      "  done: false\n",
      "  episode_len_mean: 386.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1297\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0525240593486362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017455980678318116\n",
      "          policy_loss: -0.005918734106752608\n",
      "          total_loss: -0.020951338443491195\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00018439600575624758\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58387096774193\n",
      "    ram_util_percent: 72.78709677419354\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677320373924769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.733956572854762\n",
      "    mean_inference_ms: 1.722381081532337\n",
      "    mean_raw_obs_processing_ms: 1.584503784984383\n",
      "  time_since_restore: 11588.444791555405\n",
      "  time_this_iter_s: 21.785309553146362\n",
      "  time_total_s: 11588.444791555405\n",
      "  timers:\n",
      "    learn_throughput: 1569.207\n",
      "    learn_time_ms: 637.265\n",
      "    load_throughput: 54168.209\n",
      "    load_time_ms: 18.461\n",
      "    sample_throughput: 43.04\n",
      "    sample_time_ms: 23234.256\n",
      "    update_time_ms: 2.042\n",
      "  timestamp: 1633802706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         11588.4</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            386.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-05-28\n",
      "  done: false\n",
      "  episode_len_mean: 388.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7673344254493712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011154669727423638\n",
      "          policy_loss: -0.02769115360246764\n",
      "          total_loss: -0.04167950054009755\n",
      "          vf_explained_var: -0.9492846727371216\n",
      "          vf_loss: 0.00029293843318656503\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.490624999999994\n",
      "    ram_util_percent: 72.578125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677274357040499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.73655989058898\n",
      "    mean_inference_ms: 1.7223744102628147\n",
      "    mean_raw_obs_processing_ms: 1.584916596195416\n",
      "  time_since_restore: 11610.590630054474\n",
      "  time_this_iter_s: 22.145838499069214\n",
      "  time_total_s: 11610.590630054474\n",
      "  timers:\n",
      "    learn_throughput: 1571.486\n",
      "    learn_time_ms: 636.34\n",
      "    load_throughput: 54450.549\n",
      "    load_time_ms: 18.365\n",
      "    sample_throughput: 42.931\n",
      "    sample_time_ms: 23293.444\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1633802728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         11610.6</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            388.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 387.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1302\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0817722850375704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010331889628439781\n",
      "          policy_loss: -0.08550549579991235\n",
      "          total_loss: -0.10302148622771104\n",
      "          vf_explained_var: -0.8728621006011963\n",
      "          vf_loss: 0.00015987798162516104\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86451612903225\n",
      "    ram_util_percent: 72.5\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677242874644933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.738286733499958\n",
      "    mean_inference_ms: 1.7223695771036418\n",
      "    mean_raw_obs_processing_ms: 1.584704294354072\n",
      "  time_since_restore: 11632.793974161148\n",
      "  time_this_iter_s: 22.203344106674194\n",
      "  time_total_s: 11632.793974161148\n",
      "  timers:\n",
      "    learn_throughput: 1570.401\n",
      "    learn_time_ms: 636.78\n",
      "    load_throughput: 54380.305\n",
      "    load_time_ms: 18.389\n",
      "    sample_throughput: 43.066\n",
      "    sample_time_ms: 23220.43\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1633802750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         11632.8</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             387.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 388.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1305\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9119209461741977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01444550676669982\n",
      "          policy_loss: -0.04302084238992797\n",
      "          total_loss: -0.057502912481625874\n",
      "          vf_explained_var: -0.7661482691764832\n",
      "          vf_loss: 0.000244359698182153\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.815625\n",
      "    ram_util_percent: 72.5375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367719522812457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.74068398189999\n",
      "    mean_inference_ms: 1.7223625348368998\n",
      "    mean_raw_obs_processing_ms: 1.5844249780096653\n",
      "  time_since_restore: 11654.582851409912\n",
      "  time_this_iter_s: 21.788877248764038\n",
      "  time_total_s: 11654.582851409912\n",
      "  timers:\n",
      "    learn_throughput: 1572.442\n",
      "    learn_time_ms: 635.953\n",
      "    load_throughput: 56780.223\n",
      "    load_time_ms: 17.612\n",
      "    sample_throughput: 42.986\n",
      "    sample_time_ms: 23263.459\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633802772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         11654.6</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            388.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 387.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1308\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8779076841142444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009862364494201033\n",
      "          policy_loss: -0.005019678278929658\n",
      "          total_loss: -0.020560545639859304\n",
      "          vf_explained_var: -0.5727751851081848\n",
      "          vf_loss: 0.00023913468725772366\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.50277777777778\n",
      "    ram_util_percent: 72.61666666666665\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677148680620313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.743144030322274\n",
      "    mean_inference_ms: 1.7223555795948078\n",
      "    mean_raw_obs_processing_ms: 1.584152315005975\n",
      "  time_since_restore: 11679.84017753601\n",
      "  time_this_iter_s: 25.257326126098633\n",
      "  time_total_s: 11679.84017753601\n",
      "  timers:\n",
      "    learn_throughput: 1568.157\n",
      "    learn_time_ms: 637.691\n",
      "    load_throughput: 56731.993\n",
      "    load_time_ms: 17.627\n",
      "    sample_throughput: 42.53\n",
      "    sample_time_ms: 23512.93\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1633802797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         11679.8</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            387.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-07-01\n",
      "  done: false\n",
      "  episode_len_mean: 387.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1311\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.133110476864709\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008489428690613367\n",
      "          policy_loss: -0.046818869271212156\n",
      "          total_loss: -0.06543399509456423\n",
      "          vf_explained_var: -0.7560714483261108\n",
      "          vf_loss: 0.0001344011307891277\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64848484848485\n",
      "    ram_util_percent: 72.71515151515153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677103427119841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.74561491661071\n",
      "    mean_inference_ms: 1.7223485130903537\n",
      "    mean_raw_obs_processing_ms: 1.5838860020334813\n",
      "  time_since_restore: 11703.515192985535\n",
      "  time_this_iter_s: 23.675015449523926\n",
      "  time_total_s: 11703.515192985535\n",
      "  timers:\n",
      "    learn_throughput: 1570.022\n",
      "    learn_time_ms: 636.934\n",
      "    load_throughput: 56659.34\n",
      "    load_time_ms: 17.649\n",
      "    sample_throughput: 42.152\n",
      "    sample_time_ms: 23723.792\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1633802821\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">         11703.5</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            387.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 386.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1313\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1557595200008817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01047952804361467\n",
      "          policy_loss: -0.00500803180038929\n",
      "          total_loss: -0.02322229018641843\n",
      "          vf_explained_var: -0.535767138004303\n",
      "          vf_loss: 0.00015658490964496095\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51818181818183\n",
      "    ram_util_percent: 72.8181818181818\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677074548415492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.747279698044917\n",
      "    mean_inference_ms: 1.7223439623020727\n",
      "    mean_raw_obs_processing_ms: 1.5837219560416713\n",
      "  time_since_restore: 11726.34022140503\n",
      "  time_this_iter_s: 22.82502841949463\n",
      "  time_total_s: 11726.34022140503\n",
      "  timers:\n",
      "    learn_throughput: 1569.03\n",
      "    learn_time_ms: 637.337\n",
      "    load_throughput: 56404.166\n",
      "    load_time_ms: 17.729\n",
      "    sample_throughput: 42.111\n",
      "    sample_time_ms: 23746.661\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633802844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         11726.3</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             386.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-07-46\n",
      "  done: false\n",
      "  episode_len_mean: 386.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1316\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.066214752197266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01236920443821688\n",
      "          policy_loss: 0.07009722102019522\n",
      "          total_loss: 0.05334332262476285\n",
      "          vf_explained_var: -0.37906304001808167\n",
      "          vf_loss: 0.00014685879941680468\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.684375\n",
      "    ram_util_percent: 72.89375\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03677027531518445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.749834771093077\n",
      "    mean_inference_ms: 1.722336908098989\n",
      "    mean_raw_obs_processing_ms: 1.5834694601556025\n",
      "  time_since_restore: 11748.591904878616\n",
      "  time_this_iter_s: 22.251683473587036\n",
      "  time_total_s: 11748.591904878616\n",
      "  timers:\n",
      "    learn_throughput: 1568.443\n",
      "    learn_time_ms: 637.575\n",
      "    load_throughput: 56440.901\n",
      "    load_time_ms: 17.718\n",
      "    sample_throughput: 42.359\n",
      "    sample_time_ms: 23607.825\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1633802866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         11748.6</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            386.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-08-08\n",
      "  done: false\n",
      "  episode_len_mean: 385.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.03\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1318\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0033093319998847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01370913809335899\n",
      "          policy_loss: -0.050652140213383566\n",
      "          total_loss: -0.06636414229869843\n",
      "          vf_explained_var: -0.42157742381095886\n",
      "          vf_loss: 0.00015224099665323996\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.912499999999994\n",
      "    ram_util_percent: 72.94062500000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676997868715111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.751571094466268\n",
      "    mean_inference_ms: 1.7223323690210515\n",
      "    mean_raw_obs_processing_ms: 1.5833164284802583\n",
      "  time_since_restore: 11770.987259864807\n",
      "  time_this_iter_s: 22.395354986190796\n",
      "  time_total_s: 11770.987259864807\n",
      "  timers:\n",
      "    learn_throughput: 1567.322\n",
      "    learn_time_ms: 638.031\n",
      "    load_throughput: 56367.099\n",
      "    load_time_ms: 17.741\n",
      "    sample_throughput: 45.426\n",
      "    sample_time_ms: 22013.764\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633802888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">           11771</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">   -0.03</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            385.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-08-49\n",
      "  done: false\n",
      "  episode_len_mean: 384.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1321\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9546663377020095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016555113461921976\n",
      "          policy_loss: -0.07079720276718339\n",
      "          total_loss: -0.08508734525077873\n",
      "          vf_explained_var: -0.488555908203125\n",
      "          vf_loss: 0.00022222747955109097\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.984210526315785\n",
      "    ram_util_percent: 73.1157894736842\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676952622065095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.754200354885743\n",
      "    mean_inference_ms: 1.7223258212802621\n",
      "    mean_raw_obs_processing_ms: 1.584086103997062\n",
      "  time_since_restore: 11811.29119849205\n",
      "  time_this_iter_s: 40.30393862724304\n",
      "  time_total_s: 11811.29119849205\n",
      "  timers:\n",
      "    learn_throughput: 1569.14\n",
      "    learn_time_ms: 637.292\n",
      "    load_throughput: 55941.059\n",
      "    load_time_ms: 17.876\n",
      "    sample_throughput: 42.012\n",
      "    sample_time_ms: 23802.495\n",
      "    update_time_ms: 2.058\n",
      "  timestamp: 1633802929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         11811.3</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             384.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 384.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1324\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0490801970163983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012597955045648727\n",
      "          policy_loss: -0.01205539374301831\n",
      "          total_loss: -0.028556744340393278\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00015850190862288905\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.546875\n",
      "    ram_util_percent: 72.9875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676903229920349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.756730347084364\n",
      "    mean_inference_ms: 1.7223191858053708\n",
      "    mean_raw_obs_processing_ms: 1.5848939580663448\n",
      "  time_since_restore: 11833.27709722519\n",
      "  time_this_iter_s: 21.985898733139038\n",
      "  time_total_s: 11833.27709722519\n",
      "  timers:\n",
      "    learn_throughput: 1566.297\n",
      "    learn_time_ms: 638.449\n",
      "    load_throughput: 55972.413\n",
      "    load_time_ms: 17.866\n",
      "    sample_throughput: 41.979\n",
      "    sample_time_ms: 23821.389\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1633802951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         11833.3</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            384.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-09-35\n",
      "  done: false\n",
      "  episode_len_mean: 383.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1327\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1051625185542635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008783646914337358\n",
      "          policy_loss: -0.06016969204776817\n",
      "          total_loss: -0.07835999743805991\n",
      "          vf_explained_var: -0.32022368907928467\n",
      "          vf_loss: 0.0001902722378468348\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63823529411765\n",
      "    ram_util_percent: 72.54705882352941\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036768541111495974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.75932712212399\n",
      "    mean_inference_ms: 1.7223131346391032\n",
      "    mean_raw_obs_processing_ms: 1.5857086653787358\n",
      "  time_since_restore: 11857.295664548874\n",
      "  time_this_iter_s: 24.018567323684692\n",
      "  time_total_s: 11857.295664548874\n",
      "  timers:\n",
      "    learn_throughput: 1564.493\n",
      "    learn_time_ms: 639.185\n",
      "    load_throughput: 55203.827\n",
      "    load_time_ms: 18.115\n",
      "    sample_throughput: 41.653\n",
      "    sample_time_ms: 24007.678\n",
      "    update_time_ms: 2.074\n",
      "  timestamp: 1633802975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         11857.3</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            383.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-09-56\n",
      "  done: false\n",
      "  episode_len_mean: 384.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1329\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.042126860883501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00993470443283149\n",
      "          policy_loss: 0.03483097648455037\n",
      "          total_loss: 0.017578681682546934\n",
      "          vf_explained_var: -0.9319359660148621\n",
      "          vf_loss: 0.00014790307646358593\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.739999999999995\n",
      "    ram_util_percent: 72.35333333333332\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676821645129274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.760952382655958\n",
      "    mean_inference_ms: 1.7223090227662892\n",
      "    mean_raw_obs_processing_ms: 1.585895923265429\n",
      "  time_since_restore: 11878.215558767319\n",
      "  time_this_iter_s: 20.919894218444824\n",
      "  time_total_s: 11878.215558767319\n",
      "  timers:\n",
      "    learn_throughput: 1564.459\n",
      "    learn_time_ms: 639.199\n",
      "    load_throughput: 54843.275\n",
      "    load_time_ms: 18.234\n",
      "    sample_throughput: 41.877\n",
      "    sample_time_ms: 23879.189\n",
      "    update_time_ms: 2.085\n",
      "  timestamp: 1633802996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         11878.2</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            384.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-10-18\n",
      "  done: false\n",
      "  episode_len_mean: 384.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.02\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1332\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9992787387635973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009845182094176232\n",
      "          policy_loss: -0.055825716795192826\n",
      "          total_loss: -0.07268505932556259\n",
      "          vf_explained_var: -0.6206755638122559\n",
      "          vf_loss: 0.00013959266596227988\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89375\n",
      "    ram_util_percent: 72.31875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676771302176758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.763331265329153\n",
      "    mean_inference_ms: 1.722302922526495\n",
      "    mean_raw_obs_processing_ms: 1.5856744444248085\n",
      "  time_since_restore: 11900.74976015091\n",
      "  time_this_iter_s: 22.5342013835907\n",
      "  time_total_s: 11900.74976015091\n",
      "  timers:\n",
      "    learn_throughput: 1566.051\n",
      "    learn_time_ms: 638.549\n",
      "    load_throughput: 52645.965\n",
      "    load_time_ms: 18.995\n",
      "    sample_throughput: 41.747\n",
      "    sample_time_ms: 23953.535\n",
      "    update_time_ms: 2.163\n",
      "  timestamp: 1633803018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">         11900.7</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">   -0.02</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            384.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-10-40\n",
      "  done: false\n",
      "  episode_len_mean: 384.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1334\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0852510770161947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007712204655930854\n",
      "          policy_loss: -0.016212834790349007\n",
      "          total_loss: -0.034526223896278276\n",
      "          vf_explained_var: -0.6202932000160217\n",
      "          vf_loss: 0.00019388948736983973\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62580645161291\n",
      "    ram_util_percent: 72.35806451612906\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676738499945583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.764895548938945\n",
      "    mean_inference_ms: 1.7222988570704953\n",
      "    mean_raw_obs_processing_ms: 1.5855198489177844\n",
      "  time_since_restore: 11922.692080259323\n",
      "  time_this_iter_s: 21.942320108413696\n",
      "  time_total_s: 11922.692080259323\n",
      "  timers:\n",
      "    learn_throughput: 1567.011\n",
      "    learn_time_ms: 638.158\n",
      "    load_throughput: 52679.422\n",
      "    load_time_ms: 18.983\n",
      "    sample_throughput: 42.333\n",
      "    sample_time_ms: 23622.438\n",
      "    update_time_ms: 2.163\n",
      "  timestamp: 1633803040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         11922.7</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-11-03\n",
      "  done: false\n",
      "  episode_len_mean: 384.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1337\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9549530731307136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011813401863602893\n",
      "          policy_loss: -0.08018293132384618\n",
      "          total_loss: -0.09598400125073062\n",
      "          vf_explained_var: -0.6141341924667358\n",
      "          vf_loss: 0.00015609226013313875\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.36363636363637\n",
      "    ram_util_percent: 72.41818181818182\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676689965669682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.76725452915896\n",
      "    mean_inference_ms: 1.7222927245968414\n",
      "    mean_raw_obs_processing_ms: 1.5852810649856326\n",
      "  time_since_restore: 11945.510928153992\n",
      "  time_this_iter_s: 22.81884789466858\n",
      "  time_total_s: 11945.510928153992\n",
      "  timers:\n",
      "    learn_throughput: 1562.73\n",
      "    learn_time_ms: 639.906\n",
      "    load_throughput: 52948.222\n",
      "    load_time_ms: 18.886\n",
      "    sample_throughput: 42.49\n",
      "    sample_time_ms: 23535.206\n",
      "    update_time_ms: 2.131\n",
      "  timestamp: 1633803063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         11945.5</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             384.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-11-25\n",
      "  done: false\n",
      "  episode_len_mean: 384.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7075801756646898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01136713655576123\n",
      "          policy_loss: -0.03651740292294158\n",
      "          total_loss: -0.05000641865448819\n",
      "          vf_explained_var: -0.9625991582870483\n",
      "          vf_loss: 0.000130120131749815\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.406451612903226\n",
      "    ram_util_percent: 72.55161290322579\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676640883070584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.76953672896335\n",
      "    mean_inference_ms: 1.7222862609874496\n",
      "    mean_raw_obs_processing_ms: 1.5850779879038617\n",
      "  time_since_restore: 11967.112859725952\n",
      "  time_this_iter_s: 21.60193157196045\n",
      "  time_total_s: 11967.112859725952\n",
      "  timers:\n",
      "    learn_throughput: 1565.247\n",
      "    learn_time_ms: 638.877\n",
      "    load_throughput: 53081.1\n",
      "    load_time_ms: 18.839\n",
      "    sample_throughput: 42.71\n",
      "    sample_time_ms: 23413.924\n",
      "    update_time_ms: 2.182\n",
      "  timestamp: 1633803085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         11967.1</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             384.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 384.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1342\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0524352908134462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01004569376606494\n",
      "          policy_loss: -0.027176740517218908\n",
      "          total_loss: -0.04453759027851952\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00010867844507124068\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.618750000000006\n",
      "    ram_util_percent: 72.63749999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367660768113534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.77107300458499\n",
      "    mean_inference_ms: 1.722281951266219\n",
      "    mean_raw_obs_processing_ms: 1.5849364080157358\n",
      "  time_since_restore: 11989.816921710968\n",
      "  time_this_iter_s: 22.70406198501587\n",
      "  time_total_s: 11989.816921710968\n",
      "  timers:\n",
      "    learn_throughput: 1564.36\n",
      "    learn_time_ms: 639.239\n",
      "    load_throughput: 53377.519\n",
      "    load_time_ms: 18.734\n",
      "    sample_throughput: 42.628\n",
      "    sample_time_ms: 23458.912\n",
      "    update_time_ms: 2.167\n",
      "  timestamp: 1633803107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         11989.8</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 384.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1345\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9903757360246446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011805462168532286\n",
      "          policy_loss: -0.07643341142684221\n",
      "          total_loss: -0.09265258188048998\n",
      "          vf_explained_var: -0.78813236951828\n",
      "          vf_loss: 9.463052269388249e-05\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.81666666666667\n",
      "    ram_util_percent: 72.73666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036765579393700905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.773329391391066\n",
      "    mean_inference_ms: 1.7222753817556054\n",
      "    mean_raw_obs_processing_ms: 1.5847450693354026\n",
      "  time_since_restore: 12010.339673757553\n",
      "  time_this_iter_s: 20.522752046585083\n",
      "  time_total_s: 12010.339673757553\n",
      "  timers:\n",
      "    learn_throughput: 1563.969\n",
      "    learn_time_ms: 639.399\n",
      "    load_throughput: 53291.117\n",
      "    load_time_ms: 18.765\n",
      "    sample_throughput: 42.971\n",
      "    sample_time_ms: 23271.47\n",
      "    update_time_ms: 2.166\n",
      "  timestamp: 1633803128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         12010.3</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 384.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1347\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9152963015768263\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008201522243700094\n",
      "          policy_loss: 0.001217634841385815\n",
      "          total_loss: -0.015201089469095071\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00024021536228246987\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.75161290322581\n",
      "    ram_util_percent: 72.75161290322582\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676523145616077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.77481073084124\n",
      "    mean_inference_ms: 1.722271018837944\n",
      "    mean_raw_obs_processing_ms: 1.584611343748247\n",
      "  time_since_restore: 12032.294774055481\n",
      "  time_this_iter_s: 21.955100297927856\n",
      "  time_total_s: 12032.294774055481\n",
      "  timers:\n",
      "    learn_throughput: 1561.714\n",
      "    learn_time_ms: 640.322\n",
      "    load_throughput: 53776.644\n",
      "    load_time_ms: 18.595\n",
      "    sample_throughput: 46.651\n",
      "    sample_time_ms: 21435.823\n",
      "    update_time_ms: 2.172\n",
      "  timestamp: 1633803150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         12032.3</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 384.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1350\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0568999422921075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009673489947813883\n",
      "          policy_loss: 0.043287428468465804\n",
      "          total_loss: 0.025817278979553118\n",
      "          vf_explained_var: -0.9894541501998901\n",
      "          vf_loss: 0.0001572094587673847\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.485454545454544\n",
      "    ram_util_percent: 72.78545454545456\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676473924629943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.776951653938415\n",
      "    mean_inference_ms: 1.7222650896231275\n",
      "    mean_raw_obs_processing_ms: 1.5853957849683145\n",
      "  time_since_restore: 12071.115435361862\n",
      "  time_this_iter_s: 38.820661306381226\n",
      "  time_total_s: 12071.115435361862\n",
      "  timers:\n",
      "    learn_throughput: 1564.239\n",
      "    learn_time_ms: 639.289\n",
      "    load_throughput: 53098.908\n",
      "    load_time_ms: 18.833\n",
      "    sample_throughput: 43.252\n",
      "    sample_time_ms: 23120.109\n",
      "    update_time_ms: 2.159\n",
      "  timestamp: 1633803189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         12071.1</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 385.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1352\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9624014099438984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009881252450924964\n",
      "          policy_loss: -0.006708382732338375\n",
      "          total_loss: -0.023171363605393305\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00015621542851375933\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.12068965517241\n",
      "    ram_util_percent: 73.00000000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036764426160703086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.778327616842354\n",
      "    mean_inference_ms: 1.7222613232261035\n",
      "    mean_raw_obs_processing_ms: 1.5859116933050958\n",
      "  time_since_restore: 12091.400045871735\n",
      "  time_this_iter_s: 20.284610509872437\n",
      "  time_total_s: 12091.400045871735\n",
      "  timers:\n",
      "    learn_throughput: 1561.755\n",
      "    learn_time_ms: 640.305\n",
      "    load_throughput: 53576.064\n",
      "    load_time_ms: 18.665\n",
      "    sample_throughput: 43.964\n",
      "    sample_time_ms: 22745.872\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1633803209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         12091.4</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-13-49\n",
      "  done: false\n",
      "  episode_len_mean: 385.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1355\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8245330188009474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008958459945126802\n",
      "          policy_loss: -0.002476801143752204\n",
      "          total_loss: -0.01782913777149386\n",
      "          vf_explained_var: -0.9996849894523621\n",
      "          vf_loss: 0.00016878711329708392\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.506666666666675\n",
      "    ram_util_percent: 72.77666666666667\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036763963063460414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.780301864071177\n",
      "    mean_inference_ms: 1.7222552786514729\n",
      "    mean_raw_obs_processing_ms: 1.5867049243332583\n",
      "  time_since_restore: 12111.824983119965\n",
      "  time_this_iter_s: 20.42493724822998\n",
      "  time_total_s: 12111.824983119965\n",
      "  timers:\n",
      "    learn_throughput: 1562.474\n",
      "    learn_time_ms: 640.011\n",
      "    load_throughput: 54331.137\n",
      "    load_time_ms: 18.406\n",
      "    sample_throughput: 44.059\n",
      "    sample_time_ms: 22696.933\n",
      "    update_time_ms: 2.146\n",
      "  timestamp: 1633803229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         12111.8</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-14-10\n",
      "  done: false\n",
      "  episode_len_mean: 386.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1357\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.044090900156233\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009815308963053503\n",
      "          policy_loss: -0.00043607960558599896\n",
      "          total_loss: -0.01773984879255295\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00015237369346626412\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.00689655172414\n",
      "    ram_util_percent: 72.47241379310346\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676364224370785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.781576064903824\n",
      "    mean_inference_ms: 1.7222515272267567\n",
      "    mean_raw_obs_processing_ms: 1.5872274741135481\n",
      "  time_since_restore: 12132.121024608612\n",
      "  time_this_iter_s: 20.29604148864746\n",
      "  time_total_s: 12132.121024608612\n",
      "  timers:\n",
      "    learn_throughput: 1559.86\n",
      "    learn_time_ms: 641.083\n",
      "    load_throughput: 54387.357\n",
      "    load_time_ms: 18.387\n",
      "    sample_throughput: 44.501\n",
      "    sample_time_ms: 22471.487\n",
      "    update_time_ms: 2.697\n",
      "  timestamp: 1633803250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         12132.1</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-14-31\n",
      "  done: false\n",
      "  episode_len_mean: 386.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1359\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8250451776716443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01167043157821897\n",
      "          policy_loss: -0.04719805692632993\n",
      "          total_loss: -0.06175270173698664\n",
      "          vf_explained_var: -0.999273419380188\n",
      "          vf_loss: 0.0001469088274461683\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73793103448276\n",
      "    ram_util_percent: 72.36206896551725\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367633109900021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.782788377884756\n",
      "    mean_inference_ms: 1.7222479016163685\n",
      "    mean_raw_obs_processing_ms: 1.5874218947815573\n",
      "  time_since_restore: 12153.05555510521\n",
      "  time_this_iter_s: 20.93453049659729\n",
      "  time_total_s: 12153.05555510521\n",
      "  timers:\n",
      "    learn_throughput: 1561.818\n",
      "    learn_time_ms: 640.28\n",
      "    load_throughput: 54407.463\n",
      "    load_time_ms: 18.38\n",
      "    sample_throughput: 44.7\n",
      "    sample_time_ms: 22371.534\n",
      "    update_time_ms: 2.69\n",
      "  timestamp: 1633803271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">         12153.1</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-14-55\n",
      "  done: false\n",
      "  episode_len_mean: 386.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1362\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.821203476852841\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009268304645189534\n",
      "          policy_loss: 0.007795010714067353\n",
      "          total_loss: -0.007466470532947116\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00013212497756790576\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65714285714286\n",
      "    ram_util_percent: 72.38285714285716\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036762807907202565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78462545543316\n",
      "    mean_inference_ms: 1.722242196869035\n",
      "    mean_raw_obs_processing_ms: 1.5872086374509713\n",
      "  time_since_restore: 12177.04175209999\n",
      "  time_this_iter_s: 23.986196994781494\n",
      "  time_total_s: 12177.04175209999\n",
      "  timers:\n",
      "    learn_throughput: 1564.798\n",
      "    learn_time_ms: 639.06\n",
      "    load_throughput: 54360.148\n",
      "    load_time_ms: 18.396\n",
      "    sample_throughput: 44.465\n",
      "    sample_time_ms: 22489.473\n",
      "    update_time_ms: 2.698\n",
      "  timestamp: 1633803295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">           12177</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 387.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1364\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.856311900085873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012108029217302037\n",
      "          policy_loss: -0.05760965227252907\n",
      "          total_loss: -0.07221726253628731\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002735432475876425\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61379310344827\n",
      "    ram_util_percent: 72.39310344827587\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676247321170465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78575600282838\n",
      "    mean_inference_ms: 1.722238193779412\n",
      "    mean_raw_obs_processing_ms: 1.5870513611920238\n",
      "  time_since_restore: 12197.401926517487\n",
      "  time_this_iter_s: 20.360174417495728\n",
      "  time_total_s: 12197.401926517487\n",
      "  timers:\n",
      "    learn_throughput: 1563.002\n",
      "    learn_time_ms: 639.794\n",
      "    load_throughput: 54382.985\n",
      "    load_time_ms: 18.388\n",
      "    sample_throughput: 44.713\n",
      "    sample_time_ms: 22364.613\n",
      "    update_time_ms: 2.661\n",
      "  timestamp: 1633803315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         12197.4</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 387.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1367\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9481301758024427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010343906218704724\n",
      "          policy_loss: -0.05461048943301042\n",
      "          total_loss: -0.07076058321528965\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00018569946065933134\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.646875\n",
      "    ram_util_percent: 72.50625\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761972427222095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.787446810385347\n",
      "    mean_inference_ms: 1.7222325893325825\n",
      "    mean_raw_obs_processing_ms: 1.5868521608745665\n",
      "  time_since_restore: 12220.203585624695\n",
      "  time_this_iter_s: 22.801659107208252\n",
      "  time_total_s: 12220.203585624695\n",
      "  timers:\n",
      "    learn_throughput: 1562.935\n",
      "    learn_time_ms: 639.822\n",
      "    load_throughput: 54003.126\n",
      "    load_time_ms: 18.517\n",
      "    sample_throughput: 44.694\n",
      "    sample_time_ms: 22374.185\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1633803338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">         12220.2</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             387.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 388.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1369\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8447206179300943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009208061197148254\n",
      "          policy_loss: 0.009497135256727537\n",
      "          total_loss: -0.006038643129997783\n",
      "          vf_explained_var: -0.6874533891677856\n",
      "          vf_loss: 0.00011132022607651177\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.435483870967744\n",
      "    ram_util_percent: 72.59999999999997\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676161896746302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78851493960479\n",
      "    mean_inference_ms: 1.7222287542694101\n",
      "    mean_raw_obs_processing_ms: 1.5867035634876043\n",
      "  time_since_restore: 12241.379393339157\n",
      "  time_this_iter_s: 21.17580771446228\n",
      "  time_total_s: 12241.379393339157\n",
      "  timers:\n",
      "    learn_throughput: 1568.138\n",
      "    learn_time_ms: 637.699\n",
      "    load_throughput: 53708.471\n",
      "    load_time_ms: 18.619\n",
      "    sample_throughput: 44.56\n",
      "    sample_time_ms: 22441.531\n",
      "    update_time_ms: 2.661\n",
      "  timestamp: 1633803359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         12241.4</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            388.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 388.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1372\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8911737269825406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009581566782987968\n",
      "          policy_loss: -0.03575632588730918\n",
      "          total_loss: -0.051534044908152686\n",
      "          vf_explained_var: -0.5446764826774597\n",
      "          vf_loss: 0.00022033251298125833\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10909090909091\n",
      "    ram_util_percent: 72.74848484848485\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036761078310963084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.79022140311883\n",
      "    mean_inference_ms: 1.722223279805333\n",
      "    mean_raw_obs_processing_ms: 1.5864873406108544\n",
      "  time_since_restore: 12264.904969453812\n",
      "  time_this_iter_s: 23.52557611465454\n",
      "  time_total_s: 12264.904969453812\n",
      "  timers:\n",
      "    learn_throughput: 1566.947\n",
      "    learn_time_ms: 638.184\n",
      "    load_throughput: 54095.272\n",
      "    load_time_ms: 18.486\n",
      "    sample_throughput: 44.251\n",
      "    sample_time_ms: 22598.245\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1633803383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         12264.9</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             388.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-16-44\n",
      "  done: false\n",
      "  episode_len_mean: 387.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1375\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.797194254398346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013246448130947306\n",
      "          policy_loss: 0.04679078339702553\n",
      "          total_loss: 0.0330796979367733\n",
      "          vf_explained_var: -0.8373427391052246\n",
      "          vf_loss: 0.00023270325610711654\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.648387096774194\n",
      "    ram_util_percent: 72.80645161290323\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676054029143767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.79188309990738\n",
      "    mean_inference_ms: 1.72221780360898\n",
      "    mean_raw_obs_processing_ms: 1.5863072700156815\n",
      "  time_since_restore: 12286.323422670364\n",
      "  time_this_iter_s: 21.418453216552734\n",
      "  time_total_s: 12286.323422670364\n",
      "  timers:\n",
      "    learn_throughput: 1566.397\n",
      "    learn_time_ms: 638.408\n",
      "    load_throughput: 55025.884\n",
      "    load_time_ms: 18.173\n",
      "    sample_throughput: 47.944\n",
      "    sample_time_ms: 20857.875\n",
      "    update_time_ms: 2.696\n",
      "  timestamp: 1633803404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">         12286.3</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-17-07\n",
      "  done: false\n",
      "  episode_len_mean: 386.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1377\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9290532721413507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01453959498962484\n",
      "          policy_loss: 0.003919476229283545\n",
      "          total_loss: -0.010778426089220576\n",
      "          vf_explained_var: -0.7414442896842957\n",
      "          vf_loss: 0.00017124088675094147\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53636363636364\n",
      "    ram_util_percent: 72.87272727272726\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03676017011999746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.793020117183474\n",
      "    mean_inference_ms: 1.7222141010662966\n",
      "    mean_raw_obs_processing_ms: 1.5861714276234182\n",
      "  time_since_restore: 12309.630051374435\n",
      "  time_this_iter_s: 23.306628704071045\n",
      "  time_total_s: 12309.630051374435\n",
      "  timers:\n",
      "    learn_throughput: 1567.871\n",
      "    learn_time_ms: 637.808\n",
      "    load_throughput: 54368.111\n",
      "    load_time_ms: 18.393\n",
      "    sample_throughput: 47.258\n",
      "    sample_time_ms: 21160.459\n",
      "    update_time_ms: 2.691\n",
      "  timestamp: 1633803427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         12309.6</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 384.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8970601386494106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009689823058732077\n",
      "          policy_loss: -0.06652134066033695\n",
      "          total_loss: -0.0823913660314348\n",
      "          vf_explained_var: -0.6865550875663757\n",
      "          vf_loss: 0.00015396833995408895\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.35087719298246\n",
      "    ram_util_percent: 72.94561403508771\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675959281231395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.794803390191458\n",
      "    mean_inference_ms: 1.7222087201840772\n",
      "    mean_raw_obs_processing_ms: 1.5869344395097338\n",
      "  time_since_restore: 12349.728825569153\n",
      "  time_this_iter_s: 40.09877419471741\n",
      "  time_total_s: 12349.728825569153\n",
      "  timers:\n",
      "    learn_throughput: 1568.713\n",
      "    learn_time_ms: 637.465\n",
      "    load_throughput: 53998.259\n",
      "    load_time_ms: 18.519\n",
      "    sample_throughput: 43.238\n",
      "    sample_time_ms: 23128.047\n",
      "    update_time_ms: 2.699\n",
      "  timestamp: 1633803468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         12349.7</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            384.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 385.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1383\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8223902450667486\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008386696132681613\n",
      "          policy_loss: -0.015562566556036472\n",
      "          total_loss: -0.031082256655726167\n",
      "          vf_explained_var: -0.46738848090171814\n",
      "          vf_loss: 0.00015387705821518063\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.734375\n",
      "    ram_util_percent: 73.00312500000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675901456665011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.796558760758288\n",
      "    mean_inference_ms: 1.7222032704557726\n",
      "    mean_raw_obs_processing_ms: 1.5877030460993435\n",
      "  time_since_restore: 12371.745028495789\n",
      "  time_this_iter_s: 22.016202926635742\n",
      "  time_total_s: 12371.745028495789\n",
      "  timers:\n",
      "    learn_throughput: 1569.062\n",
      "    learn_time_ms: 637.323\n",
      "    load_throughput: 53541.869\n",
      "    load_time_ms: 18.677\n",
      "    sample_throughput: 42.918\n",
      "    sample_time_ms: 23300.31\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1633803490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         12371.7</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             385.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 385.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1385\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8563162273830838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007098352380508801\n",
      "          policy_loss: -0.01398759393228425\n",
      "          total_loss: -0.0302026589297586\n",
      "          vf_explained_var: -0.7306466102600098\n",
      "          vf_loss: 0.00018953681994591737\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.670967741935485\n",
      "    ram_util_percent: 72.98387096774196\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036758609536680524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.797726576753302\n",
      "    mean_inference_ms: 1.7221996707189504\n",
      "    mean_raw_obs_processing_ms: 1.5882277079945493\n",
      "  time_since_restore: 12393.908519744873\n",
      "  time_this_iter_s: 22.163491249084473\n",
      "  time_total_s: 12393.908519744873\n",
      "  timers:\n",
      "    learn_throughput: 1566.546\n",
      "    learn_time_ms: 638.347\n",
      "    load_throughput: 53459.704\n",
      "    load_time_ms: 18.706\n",
      "    sample_throughput: 42.695\n",
      "    sample_time_ms: 23422.125\n",
      "    update_time_ms: 2.456\n",
      "  timestamp: 1633803512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         12393.9</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 385.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1388\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8792046507199605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010679598295964135\n",
      "          policy_loss: -0.05302681128184001\n",
      "          total_loss: -0.0684132950173484\n",
      "          vf_explained_var: -0.9041951298713684\n",
      "          vf_loss: 0.00015797278134010008\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43030303030303\n",
      "    ram_util_percent: 72.62424242424242\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675798114092908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.799458094987173\n",
      "    mean_inference_ms: 1.722193716444503\n",
      "    mean_raw_obs_processing_ms: 1.5890076590319\n",
      "  time_since_restore: 12416.465215444565\n",
      "  time_this_iter_s: 22.556695699691772\n",
      "  time_total_s: 12416.465215444565\n",
      "  timers:\n",
      "    learn_throughput: 1568.134\n",
      "    learn_time_ms: 637.7\n",
      "    load_throughput: 53453.64\n",
      "    load_time_ms: 18.708\n",
      "    sample_throughput: 42.956\n",
      "    sample_time_ms: 23279.817\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1633803534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         12416.5</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 385.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1391\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8391053981251186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012225656720883845\n",
      "          policy_loss: -0.06329420951919423\n",
      "          total_loss: -0.07780010938230487\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001674163670233813\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.487500000000004\n",
      "    ram_util_percent: 72.48124999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675734557321879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80116003499454\n",
      "    mean_inference_ms: 1.72218743170703\n",
      "    mean_raw_obs_processing_ms: 1.589160477831887\n",
      "  time_since_restore: 12439.178276777267\n",
      "  time_this_iter_s: 22.713061332702637\n",
      "  time_total_s: 12439.178276777267\n",
      "  timers:\n",
      "    learn_throughput: 1567.759\n",
      "    learn_time_ms: 637.853\n",
      "    load_throughput: 53587.016\n",
      "    load_time_ms: 18.661\n",
      "    sample_throughput: 42.526\n",
      "    sample_time_ms: 23515.002\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1633803557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">         12439.2</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 385.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1393\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9002250512441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011906249935778766\n",
      "          policy_loss: -0.018320196639332507\n",
      "          total_loss: -0.03342199998183383\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002798405008636312\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6551724137931\n",
      "    ram_util_percent: 72.43793103448279\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675691968296406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.802227228353317\n",
      "    mean_inference_ms: 1.722183236485897\n",
      "    mean_raw_obs_processing_ms: 1.5890361596385418\n",
      "  time_since_restore: 12459.415594100952\n",
      "  time_this_iter_s: 20.237317323684692\n",
      "  time_total_s: 12459.415594100952\n",
      "  timers:\n",
      "    learn_throughput: 1569.482\n",
      "    learn_time_ms: 637.153\n",
      "    load_throughput: 53591.398\n",
      "    load_time_ms: 18.66\n",
      "    sample_throughput: 42.994\n",
      "    sample_time_ms: 23259.317\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1633803577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         12459.4</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 386.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1395\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9084676305452983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010649295039860441\n",
      "          policy_loss: -0.06472128998074267\n",
      "          total_loss: -0.08027870315644477\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00028888605108174184\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53448275862069\n",
      "    ram_util_percent: 72.53103448275861\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367564817403278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80323522252982\n",
      "    mean_inference_ms: 1.7221789939828738\n",
      "    mean_raw_obs_processing_ms: 1.5889154863341708\n",
      "  time_since_restore: 12479.980197906494\n",
      "  time_this_iter_s: 20.564603805541992\n",
      "  time_total_s: 12479.980197906494\n",
      "  timers:\n",
      "    learn_throughput: 1568.645\n",
      "    learn_time_ms: 637.493\n",
      "    load_throughput: 54368.251\n",
      "    load_time_ms: 18.393\n",
      "    sample_throughput: 43.107\n",
      "    sample_time_ms: 23198.133\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1633803598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">           12480</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 387.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1397\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.873881623480055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009917927662038235\n",
      "          policy_loss: -0.11015058697925674\n",
      "          total_loss: -0.1256785096393691\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00019492241570131025\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.16666666666668\n",
      "    ram_util_percent: 72.67037037037038\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036756039082927724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80414147254269\n",
      "    mean_inference_ms: 1.7221746769631887\n",
      "    mean_raw_obs_processing_ms: 1.588770524619191\n",
      "  time_since_restore: 12498.832269906998\n",
      "  time_this_iter_s: 18.85207200050354\n",
      "  time_total_s: 12498.832269906998\n",
      "  timers:\n",
      "    learn_throughput: 1570.577\n",
      "    learn_time_ms: 636.709\n",
      "    load_throughput: 53690.459\n",
      "    load_time_ms: 18.625\n",
      "    sample_throughput: 43.992\n",
      "    sample_time_ms: 22731.325\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1633803617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         12498.8</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 388.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.614620245827569\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011734531574611514\n",
      "          policy_loss: -0.03614903071688281\n",
      "          total_loss: -0.048550260812044145\n",
      "          vf_explained_var: -0.7241076827049255\n",
      "          vf_loss: 0.00017658216109137155\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48064516129031\n",
      "    ram_util_percent: 72.70967741935482\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675538574773048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.805475386888425\n",
      "    mean_inference_ms: 1.722168086412351\n",
      "    mean_raw_obs_processing_ms: 1.5885603273720763\n",
      "  time_since_restore: 12520.64270067215\n",
      "  time_this_iter_s: 21.810430765151978\n",
      "  time_total_s: 12520.64270067215\n",
      "  timers:\n",
      "    learn_throughput: 1570.58\n",
      "    learn_time_ms: 636.708\n",
      "    load_throughput: 53481.653\n",
      "    load_time_ms: 18.698\n",
      "    sample_throughput: 43.916\n",
      "    sample_time_ms: 22770.672\n",
      "    update_time_ms: 2.427\n",
      "  timestamp: 1633803639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         12520.6</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             388.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-20-59\n",
      "  done: false\n",
      "  episode_len_mean: 388.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1402\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7624740256203546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008146594846750371\n",
      "          policy_loss: -0.055013928117437494\n",
      "          total_loss: -0.0699237147346139\n",
      "          vf_explained_var: -0.9556871056556702\n",
      "          vf_loss: 0.0002376313069059203\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.556666666666665\n",
      "    ram_util_percent: 72.82\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036754962109757566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.806301022056992\n",
      "    mean_inference_ms: 1.7221642139765203\n",
      "    mean_raw_obs_processing_ms: 1.5884248980461229\n",
      "  time_since_restore: 12541.221891880035\n",
      "  time_this_iter_s: 20.579191207885742\n",
      "  time_total_s: 12541.221891880035\n",
      "  timers:\n",
      "    learn_throughput: 1573.874\n",
      "    learn_time_ms: 635.375\n",
      "    load_throughput: 54279.388\n",
      "    load_time_ms: 18.423\n",
      "    sample_throughput: 44.445\n",
      "    sample_time_ms: 22499.55\n",
      "    update_time_ms: 2.436\n",
      "  timestamp: 1633803659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         12541.2</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             388.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-21-20\n",
      "  done: false\n",
      "  episode_len_mean: 389.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1405\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.660128927230835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010911954874447795\n",
      "          policy_loss: -0.027652154945664937\n",
      "          total_loss: -0.040748710309465724\n",
      "          vf_explained_var: -0.9986883997917175\n",
      "          vf_loss: 0.00018648289850615484\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.683333333333344\n",
      "    ram_util_percent: 72.88666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675431200299603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.807502764004216\n",
      "    mean_inference_ms: 1.722157995242796\n",
      "    mean_raw_obs_processing_ms: 1.5882282816320565\n",
      "  time_since_restore: 12562.420264959335\n",
      "  time_this_iter_s: 21.198373079299927\n",
      "  time_total_s: 12562.420264959335\n",
      "  timers:\n",
      "    learn_throughput: 1572.234\n",
      "    learn_time_ms: 636.038\n",
      "    load_throughput: 54049.961\n",
      "    load_time_ms: 18.501\n",
      "    sample_throughput: 48.523\n",
      "    sample_time_ms: 20608.752\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1633803680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         12562.4</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            389.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 391.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1407\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4460546718703375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0071607794985180506\n",
      "          policy_loss: -0.0661852935122119\n",
      "          total_loss: -0.07826558608147834\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00020270910220763957\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.67586206896552\n",
      "    ram_util_percent: 72.95517241379311\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675387622170613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.808109628256197\n",
      "    mean_inference_ms: 1.7221536718507615\n",
      "    mean_raw_obs_processing_ms: 1.5881003929255126\n",
      "  time_since_restore: 12582.618329524994\n",
      "  time_this_iter_s: 20.19806456565857\n",
      "  time_total_s: 12582.618329524994\n",
      "  timers:\n",
      "    learn_throughput: 1572.594\n",
      "    learn_time_ms: 635.892\n",
      "    load_throughput: 54736.99\n",
      "    load_time_ms: 18.269\n",
      "    sample_throughput: 48.953\n",
      "    sample_time_ms: 20427.68\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633803701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">         12582.6</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-22-19\n",
      "  done: false\n",
      "  episode_len_mean: 393.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1410\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5702924251556396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008661427086169718\n",
      "          policy_loss: -0.035889260884788304\n",
      "          total_loss: -0.04880929332640436\n",
      "          vf_explained_var: -0.9942392110824585\n",
      "          vf_loss: 0.00014901252798154018\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64181818181819\n",
      "    ram_util_percent: 73.01636363636364\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675321153294724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.80893270359734\n",
      "    mean_inference_ms: 1.7221471617923563\n",
      "    mean_raw_obs_processing_ms: 1.588810028096817\n",
      "  time_since_restore: 12621.057061195374\n",
      "  time_this_iter_s: 38.43873167037964\n",
      "  time_total_s: 12621.057061195374\n",
      "  timers:\n",
      "    learn_throughput: 1573.926\n",
      "    learn_time_ms: 635.354\n",
      "    load_throughput: 54796.345\n",
      "    load_time_ms: 18.249\n",
      "    sample_throughput: 45.34\n",
      "    sample_time_ms: 22055.776\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633803739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         12621.1</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 393.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1412\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6270660003026327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008899996099940743\n",
      "          policy_loss: -0.03412623343368371\n",
      "          total_loss: -0.04751347659362687\n",
      "          vf_explained_var: -0.9766271114349365\n",
      "          vf_loss: 0.00017698937242837726\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.20967741935485\n",
      "    ram_util_percent: 72.88709677419357\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675275622173225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.809429348148814\n",
      "    mean_inference_ms: 1.7221427336756705\n",
      "    mean_raw_obs_processing_ms: 1.5892773947404493\n",
      "  time_since_restore: 12642.631687164307\n",
      "  time_this_iter_s: 21.574625968933105\n",
      "  time_total_s: 12642.631687164307\n",
      "  timers:\n",
      "    learn_throughput: 1572.694\n",
      "    learn_time_ms: 635.852\n",
      "    load_throughput: 54515.589\n",
      "    load_time_ms: 18.343\n",
      "    sample_throughput: 45.544\n",
      "    sample_time_ms: 21956.978\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633803761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         12642.6</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-23-03\n",
      "  done: false\n",
      "  episode_len_mean: 393.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1415\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.541213877995809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009033391445844903\n",
      "          policy_loss: -0.07741014435887336\n",
      "          total_loss: -0.0899755410850048\n",
      "          vf_explained_var: -0.9568971395492554\n",
      "          vf_loss: 9.975096355548076e-05\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82258064516128\n",
      "    ram_util_percent: 72.80000000000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675207525889681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.810125543270946\n",
      "    mean_inference_ms: 1.7221362961700724\n",
      "    mean_raw_obs_processing_ms: 1.58999758020994\n",
      "  time_since_restore: 12664.546477556229\n",
      "  time_this_iter_s: 21.914790391921997\n",
      "  time_total_s: 12664.546477556229\n",
      "  timers:\n",
      "    learn_throughput: 1574.813\n",
      "    learn_time_ms: 634.996\n",
      "    load_throughput: 54330.011\n",
      "    load_time_ms: 18.406\n",
      "    sample_throughput: 45.708\n",
      "    sample_time_ms: 21877.946\n",
      "    update_time_ms: 2.076\n",
      "  timestamp: 1633803783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         12664.5</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 393.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1417\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5228526605500114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008964660366903882\n",
      "          policy_loss: -0.011754664199219809\n",
      "          total_loss: -0.024155711755156517\n",
      "          vf_explained_var: -0.9799001216888428\n",
      "          vf_loss: 0.00010138857109672648\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7896551724138\n",
      "    ram_util_percent: 72.71724137931035\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675162706001434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81052340357382\n",
      "    mean_inference_ms: 1.7221319224677025\n",
      "    mean_raw_obs_processing_ms: 1.5904709637447139\n",
      "  time_since_restore: 12684.945402383804\n",
      "  time_this_iter_s: 20.398924827575684\n",
      "  time_total_s: 12684.945402383804\n",
      "  timers:\n",
      "    learn_throughput: 1575.435\n",
      "    learn_time_ms: 634.745\n",
      "    load_throughput: 54908.538\n",
      "    load_time_ms: 18.212\n",
      "    sample_throughput: 45.673\n",
      "    sample_time_ms: 21894.557\n",
      "    update_time_ms: 2.077\n",
      "  timestamp: 1633803803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         12684.9</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-23-45\n",
      "  done: false\n",
      "  episode_len_mean: 394.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5726435396406386\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01056052406738098\n",
      "          policy_loss: -0.022031203698780803\n",
      "          total_loss: -0.03437952581379149\n",
      "          vf_explained_var: -0.1149015724658966\n",
      "          vf_loss: 0.00016673295588892262\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63225806451614\n",
      "    ram_util_percent: 72.6064516129032\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036750941252905706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81105447272778\n",
      "    mean_inference_ms: 1.72212529392151\n",
      "    mean_raw_obs_processing_ms: 1.5905299936228834\n",
      "  time_since_restore: 12706.83275938034\n",
      "  time_this_iter_s: 21.887356996536255\n",
      "  time_total_s: 12706.83275938034\n",
      "  timers:\n",
      "    learn_throughput: 1573.508\n",
      "    learn_time_ms: 635.523\n",
      "    load_throughput: 54204.191\n",
      "    load_time_ms: 18.449\n",
      "    sample_throughput: 45.401\n",
      "    sample_time_ms: 22025.798\n",
      "    update_time_ms: 2.074\n",
      "  timestamp: 1633803825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         12706.8</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            394.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 395.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1423\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6129961411158245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011174999274599577\n",
      "          policy_loss: 0.023322202399786975\n",
      "          total_loss: 0.010798444195340077\n",
      "          vf_explained_var: -0.5465312600135803\n",
      "          vf_loss: 0.00020796220099631075\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53870967741935\n",
      "    ram_util_percent: 72.62258064516128\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03675026730264222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.811551128002055\n",
      "    mean_inference_ms: 1.7221189354378823\n",
      "    mean_raw_obs_processing_ms: 1.5902590588732568\n",
      "  time_since_restore: 12728.585339069366\n",
      "  time_this_iter_s: 21.75257968902588\n",
      "  time_total_s: 12728.585339069366\n",
      "  timers:\n",
      "    learn_throughput: 1572.977\n",
      "    learn_time_ms: 635.737\n",
      "    load_throughput: 54936.011\n",
      "    load_time_ms: 18.203\n",
      "    sample_throughput: 44.811\n",
      "    sample_time_ms: 22315.863\n",
      "    update_time_ms: 2.078\n",
      "  timestamp: 1633803847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">         12728.6</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 395.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1425\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8452752974298265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011893359757895737\n",
      "          policy_loss: -0.048855351077185735\n",
      "          total_loss: -0.06355612894727124\n",
      "          vf_explained_var: -0.4300387501716614\n",
      "          vf_loss: 0.0001352874536678428\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.883333333333326\n",
      "    ram_util_percent: 72.72333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674982783686648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.811798398683226\n",
      "    mean_inference_ms: 1.7221145802593998\n",
      "    mean_raw_obs_processing_ms: 1.5900718575941448\n",
      "  time_since_restore: 12749.230063676834\n",
      "  time_this_iter_s: 20.64472460746765\n",
      "  time_total_s: 12749.230063676834\n",
      "  timers:\n",
      "    learn_throughput: 1573.134\n",
      "    learn_time_ms: 635.674\n",
      "    load_throughput: 55107.29\n",
      "    load_time_ms: 18.146\n",
      "    sample_throughput: 45.046\n",
      "    sample_time_ms: 22199.42\n",
      "    update_time_ms: 2.073\n",
      "  timestamp: 1633803867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">         12749.2</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 395.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1428\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5499713606304593\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01141965283674106\n",
      "          policy_loss: -0.025520221640666325\n",
      "          total_loss: -0.03740296198262109\n",
      "          vf_explained_var: -0.5487781167030334\n",
      "          vf_loss: 0.00014433779805484746\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.83939393939394\n",
      "    ram_util_percent: 72.81818181818181\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674915548738642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81218449369701\n",
      "    mean_inference_ms: 1.7221080381100027\n",
      "    mean_raw_obs_processing_ms: 1.5897844654340445\n",
      "  time_since_restore: 12772.175249814987\n",
      "  time_this_iter_s: 22.945186138153076\n",
      "  time_total_s: 12772.175249814987\n",
      "  timers:\n",
      "    learn_throughput: 1571.811\n",
      "    learn_time_ms: 636.209\n",
      "    load_throughput: 54757.928\n",
      "    load_time_ms: 18.262\n",
      "    sample_throughput: 44.572\n",
      "    sample_time_ms: 22435.363\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633803890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         12772.2</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-25-15\n",
      "  done: false\n",
      "  episode_len_mean: 395.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1431\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.686185442076789\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009008846274287703\n",
      "          policy_loss: -0.0919784025185638\n",
      "          total_loss: -0.10600811392068862\n",
      "          vf_explained_var: -0.7740691304206848\n",
      "          vf_loss: 9.261712617040354e-05\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60588235294117\n",
      "    ram_util_percent: 72.92941176470589\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674849095504762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.812689895090585\n",
      "    mean_inference_ms: 1.722101601116945\n",
      "    mean_raw_obs_processing_ms: 1.5895329162647307\n",
      "  time_since_restore: 12796.31993484497\n",
      "  time_this_iter_s: 24.14468502998352\n",
      "  time_total_s: 12796.31993484497\n",
      "  timers:\n",
      "    learn_throughput: 1572.738\n",
      "    learn_time_ms: 635.834\n",
      "    load_throughput: 55104.104\n",
      "    load_time_ms: 18.147\n",
      "    sample_throughput: 43.994\n",
      "    sample_time_ms: 22730.51\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633803915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         12796.3</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             395.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 392.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1434\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.679256100124783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015786123141022033\n",
      "          policy_loss: -0.04092238582670689\n",
      "          total_loss: -0.052763895814617476\n",
      "          vf_explained_var: -0.9008462429046631\n",
      "          vf_loss: 0.00015060024610041484\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57837837837838\n",
      "    ram_util_percent: 73.06486486486486\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674781708420703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.813368382782315\n",
      "    mean_inference_ms: 1.7220954913451345\n",
      "    mean_raw_obs_processing_ms: 1.5892896567470514\n",
      "  time_since_restore: 12821.818731546402\n",
      "  time_this_iter_s: 25.498796701431274\n",
      "  time_total_s: 12821.818731546402\n",
      "  timers:\n",
      "    learn_throughput: 1571.253\n",
      "    learn_time_ms: 636.435\n",
      "    load_throughput: 54674.344\n",
      "    load_time_ms: 18.29\n",
      "    sample_throughput: 42.993\n",
      "    sample_time_ms: 23259.87\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633803940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         12821.8</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 393.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1436\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8084465861320496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00847308674006043\n",
      "          policy_loss: -0.061718675090620914\n",
      "          total_loss: -0.07707334319129586\n",
      "          vf_explained_var: -0.9474290013313293\n",
      "          vf_loss: 0.0001531916790150313\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48\n",
      "    ram_util_percent: 73.10666666666664\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367473682750201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.813746316267693\n",
      "    mean_inference_ms: 1.7220915893514195\n",
      "    mean_raw_obs_processing_ms: 1.589148660867554\n",
      "  time_since_restore: 12842.825323820114\n",
      "  time_this_iter_s: 21.006592273712158\n",
      "  time_total_s: 12842.825323820114\n",
      "  timers:\n",
      "    learn_throughput: 1570.531\n",
      "    learn_time_ms: 636.727\n",
      "    load_throughput: 54581.494\n",
      "    load_time_ms: 18.321\n",
      "    sample_throughput: 46.476\n",
      "    sample_time_ms: 21516.324\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1633803961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         12842.8</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 394.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1439\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8004954165882534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010653559806683453\n",
      "          policy_loss: -0.06656418417890866\n",
      "          total_loss: -0.08121585970123608\n",
      "          vf_explained_var: -0.7537735104560852\n",
      "          vf_loss: 0.00011360527868317958\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.06774193548386\n",
      "    ram_util_percent: 73.19999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036746712526368186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.814349570969245\n",
      "    mean_inference_ms: 1.722085751038977\n",
      "    mean_raw_obs_processing_ms: 1.5889161370900067\n",
      "  time_since_restore: 12865.072877883911\n",
      "  time_this_iter_s: 22.247554063796997\n",
      "  time_total_s: 12865.072877883911\n",
      "  timers:\n",
      "    learn_throughput: 1571.527\n",
      "    learn_time_ms: 636.324\n",
      "    load_throughput: 54976.118\n",
      "    load_time_ms: 18.19\n",
      "    sample_throughput: 46.33\n",
      "    sample_time_ms: 21584.173\n",
      "    update_time_ms: 2.057\n",
      "  timestamp: 1633803983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         12865.1</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             394.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 393.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1441\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7386241952578227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008724361851771611\n",
      "          policy_loss: 0.06911139190196991\n",
      "          total_loss: 0.05451837993330426\n",
      "          vf_explained_var: -0.550580620765686\n",
      "          vf_loss: 0.00014021248656111615\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.88666666666667\n",
      "    ram_util_percent: 73.22166666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674626809193128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.814846587876723\n",
      "    mean_inference_ms: 1.722082004541213\n",
      "    mean_raw_obs_processing_ms: 1.5893469856424038\n",
      "  time_since_restore: 12906.598597764969\n",
      "  time_this_iter_s: 41.52571988105774\n",
      "  time_total_s: 12906.598597764969\n",
      "  timers:\n",
      "    learn_throughput: 1571.873\n",
      "    learn_time_ms: 636.184\n",
      "    load_throughput: 55391.336\n",
      "    load_time_ms: 18.053\n",
      "    sample_throughput: 42.471\n",
      "    sample_time_ms: 23545.535\n",
      "    update_time_ms: 2.062\n",
      "  timestamp: 1633804025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">         12906.6</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-27-28\n",
      "  done: false\n",
      "  episode_len_mean: 392.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1444\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7156676358646816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014223739832522043\n",
      "          policy_loss: -0.03173539609544807\n",
      "          total_loss: -0.044376802258193494\n",
      "          vf_explained_var: -0.21322983503341675\n",
      "          vf_loss: 0.0001899337624005663\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93030303030303\n",
      "    ram_util_percent: 73.13030303030303\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674560618946266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.815691505702375\n",
      "    mean_inference_ms: 1.7220767632819842\n",
      "    mean_raw_obs_processing_ms: 1.590012697133983\n",
      "  time_since_restore: 12929.98709988594\n",
      "  time_this_iter_s: 23.38850212097168\n",
      "  time_total_s: 12929.98709988594\n",
      "  timers:\n",
      "    learn_throughput: 1571.15\n",
      "    learn_time_ms: 636.477\n",
      "    load_throughput: 54510.346\n",
      "    load_time_ms: 18.345\n",
      "    sample_throughput: 41.939\n",
      "    sample_time_ms: 23843.896\n",
      "    update_time_ms: 2.068\n",
      "  timestamp: 1633804048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">           12930</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 390.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1447\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5913733734024895\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008716060874255721\n",
      "          policy_loss: -0.08545620731181569\n",
      "          total_loss: -0.09853331231408649\n",
      "          vf_explained_var: -0.4443170726299286\n",
      "          vf_loss: 0.0001861335416227424\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.455882352941174\n",
      "    ram_util_percent: 72.9264705882353\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674496167578531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81663954975963\n",
      "    mean_inference_ms: 1.7220720382252153\n",
      "    mean_raw_obs_processing_ms: 1.5906837959901876\n",
      "  time_since_restore: 12953.46500468254\n",
      "  time_this_iter_s: 23.477904796600342\n",
      "  time_total_s: 12953.46500468254\n",
      "  timers:\n",
      "    learn_throughput: 1570.421\n",
      "    learn_time_ms: 636.772\n",
      "    load_throughput: 54686.177\n",
      "    load_time_ms: 18.286\n",
      "    sample_throughput: 41.662\n",
      "    sample_time_ms: 24002.717\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633804072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         12953.5</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-28-13\n",
      "  done: false\n",
      "  episode_len_mean: 390.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1449\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.848844658003913\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008040195971698782\n",
      "          policy_loss: -0.027049206073085467\n",
      "          total_loss: -0.04292248828957478\n",
      "          vf_explained_var: -0.9992870688438416\n",
      "          vf_loss: 0.00017019850112976403\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.92666666666666\n",
      "    ram_util_percent: 72.72999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674453937892112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.817251065298265\n",
      "    mean_inference_ms: 1.722068633405643\n",
      "    mean_raw_obs_processing_ms: 1.5905084323482965\n",
      "  time_since_restore: 12974.968899726868\n",
      "  time_this_iter_s: 21.503895044326782\n",
      "  time_total_s: 12974.968899726868\n",
      "  timers:\n",
      "    learn_throughput: 1567.78\n",
      "    learn_time_ms: 637.845\n",
      "    load_throughput: 54321.989\n",
      "    load_time_ms: 18.409\n",
      "    sample_throughput: 41.707\n",
      "    sample_time_ms: 23976.661\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1633804093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">           12975</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 389.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1452\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5874826934602526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012221281551503399\n",
      "          policy_loss: -0.07852260321378708\n",
      "          total_loss: -0.09047836814489629\n",
      "          vf_explained_var: -0.2047935128211975\n",
      "          vf_loss: 0.0002026526094090918\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.60606060606061\n",
      "    ram_util_percent: 72.56666666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674388174276719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.818280941147535\n",
      "    mean_inference_ms: 1.722063679130583\n",
      "    mean_raw_obs_processing_ms: 1.5902270366928655\n",
      "  time_since_restore: 12997.797825813293\n",
      "  time_this_iter_s: 22.82892608642578\n",
      "  time_total_s: 12997.797825813293\n",
      "  timers:\n",
      "    learn_throughput: 1565.593\n",
      "    learn_time_ms: 638.736\n",
      "    load_throughput: 54213.158\n",
      "    load_time_ms: 18.446\n",
      "    sample_throughput: 41.332\n",
      "    sample_time_ms: 24194.159\n",
      "    update_time_ms: 2.09\n",
      "  timestamp: 1633804116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         12997.8</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            389.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 391.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1454\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8621554030312433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011236296467245112\n",
      "          policy_loss: -0.013164308791359266\n",
      "          total_loss: -0.02824096091919475\n",
      "          vf_explained_var: -0.8963847756385803\n",
      "          vf_loss: 0.00012802189553945534\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27307692307693\n",
      "    ram_util_percent: 72.58846153846153\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674345337980658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81890026069849\n",
      "    mean_inference_ms: 1.7220606060599537\n",
      "    mean_raw_obs_processing_ms: 1.5900604760453647\n",
      "  time_since_restore: 13015.836508989334\n",
      "  time_this_iter_s: 18.03868317604065\n",
      "  time_total_s: 13015.836508989334\n",
      "  timers:\n",
      "    learn_throughput: 1562.695\n",
      "    learn_time_ms: 639.92\n",
      "    load_throughput: 56572.296\n",
      "    load_time_ms: 17.676\n",
      "    sample_throughput: 42.189\n",
      "    sample_time_ms: 23703.08\n",
      "    update_time_ms: 2.091\n",
      "  timestamp: 1633804134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         13015.8</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 390.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1457\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7269093010160659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014606784753302944\n",
      "          policy_loss: -0.0670205140279399\n",
      "          total_loss: -0.07957795908053716\n",
      "          vf_explained_var: -0.722243070602417\n",
      "          vf_loss: 0.0002698270722046598\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77419354838709\n",
      "    ram_util_percent: 72.65806451612902\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674284091722353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.81989514577194\n",
      "    mean_inference_ms: 1.7220562091587124\n",
      "    mean_raw_obs_processing_ms: 1.589791789391519\n",
      "  time_since_restore: 13037.765654325485\n",
      "  time_this_iter_s: 21.929145336151123\n",
      "  time_total_s: 13037.765654325485\n",
      "  timers:\n",
      "    learn_throughput: 1565.049\n",
      "    learn_time_ms: 638.957\n",
      "    load_throughput: 56282.158\n",
      "    load_time_ms: 17.768\n",
      "    sample_throughput: 42.585\n",
      "    sample_time_ms: 23482.38\n",
      "    update_time_ms: 2.109\n",
      "  timestamp: 1633804156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         13037.8</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 387.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4937816076808506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010233572831678946\n",
      "          policy_loss: 0.031246241099304625\n",
      "          total_loss: 0.019579549299346077\n",
      "          vf_explained_var: 0.674088716506958\n",
      "          vf_loss: 0.00015916781679455502\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64594594594594\n",
      "    ram_util_percent: 72.7135135135135\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674226897522892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.821123738903353\n",
      "    mean_inference_ms: 1.7220520378080806\n",
      "    mean_raw_obs_processing_ms: 1.5895839573349158\n",
      "  time_since_restore: 13063.687789201736\n",
      "  time_this_iter_s: 25.92213487625122\n",
      "  time_total_s: 13063.687789201736\n",
      "  timers:\n",
      "    learn_throughput: 1567.283\n",
      "    learn_time_ms: 638.047\n",
      "    load_throughput: 56418.353\n",
      "    load_time_ms: 17.725\n",
      "    sample_throughput: 42.507\n",
      "    sample_time_ms: 23525.647\n",
      "    update_time_ms: 2.102\n",
      "  timestamp: 1633804182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">         13063.7</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 387.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1462\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5906337844000922\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009421177330227218\n",
      "          policy_loss: -0.07665900509390566\n",
      "          total_loss: -0.08952858256590035\n",
      "          vf_explained_var: -0.5993196964263916\n",
      "          vf_loss: 0.00017184508954718087\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55\n",
      "    ram_util_percent: 72.8125\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674189499205818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82192436385376\n",
      "    mean_inference_ms: 1.7220492291584442\n",
      "    mean_raw_obs_processing_ms: 1.5894305119097272\n",
      "  time_since_restore: 13086.42505478859\n",
      "  time_this_iter_s: 22.737265586853027\n",
      "  time_total_s: 13086.42505478859\n",
      "  timers:\n",
      "    learn_throughput: 1568.785\n",
      "    learn_time_ms: 637.436\n",
      "    load_throughput: 56516.497\n",
      "    load_time_ms: 17.694\n",
      "    sample_throughput: 42.195\n",
      "    sample_time_ms: 23699.318\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1633804205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         13086.4</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 573000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 389.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1465\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4794954233699376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007318381626914297\n",
      "          policy_loss: -0.014705407867829005\n",
      "          total_loss: -0.02705611561735471\n",
      "          vf_explained_var: -0.9658825993537903\n",
      "          vf_loss: 0.00021878087750843003\n",
      "    num_agent_steps_sampled: 573000\n",
      "    num_agent_steps_trained: 573000\n",
      "    num_steps_sampled: 573000\n",
      "    num_steps_trained: 573000\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.16296296296296\n",
      "    ram_util_percent: 72.98518518518519\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674133553228556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.823007767911765\n",
      "    mean_inference_ms: 1.7220449164613039\n",
      "    mean_raw_obs_processing_ms: 1.5892336667791642\n",
      "  time_since_restore: 13105.009765386581\n",
      "  time_this_iter_s: 18.584710597991943\n",
      "  time_total_s: 13105.009765386581\n",
      "  timers:\n",
      "    learn_throughput: 1565.971\n",
      "    learn_time_ms: 638.581\n",
      "    load_throughput: 56508.654\n",
      "    load_time_ms: 17.696\n",
      "    sample_throughput: 42.86\n",
      "    sample_time_ms: 23331.855\n",
      "    update_time_ms: 2.123\n",
      "  timestamp: 1633804223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 573000\n",
      "  training_iteration: 573\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">           13105</td><td style=\"text-align: right;\">573000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            389.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 574000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 390.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1467\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.882386130756802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011380221269737194\n",
      "          policy_loss: -0.008161441733439764\n",
      "          total_loss: -0.023318451560205882\n",
      "          vf_explained_var: -0.9448954463005066\n",
      "          vf_loss: 0.00020620676110007075\n",
      "    num_agent_steps_sampled: 574000\n",
      "    num_agent_steps_trained: 574000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57142857142857\n",
      "    ram_util_percent: 73.03214285714286\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036740965073142944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82362964855728\n",
      "    mean_inference_ms: 1.722041461238432\n",
      "    mean_raw_obs_processing_ms: 1.5890879356879577\n",
      "  time_since_restore: 13124.522341489792\n",
      "  time_this_iter_s: 19.51257610321045\n",
      "  time_total_s: 13124.522341489792\n",
      "  timers:\n",
      "    learn_throughput: 1567.343\n",
      "    learn_time_ms: 638.023\n",
      "    load_throughput: 55979.137\n",
      "    load_time_ms: 17.864\n",
      "    sample_throughput: 47.324\n",
      "    sample_time_ms: 21130.922\n",
      "    update_time_ms: 2.128\n",
      "  timestamp: 1633804243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   574</td><td style=\"text-align: right;\">         13124.5</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 575000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-31-02\n",
      "  done: false\n",
      "  episode_len_mean: 391.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1469\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7808996107843187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011522898790848985\n",
      "          policy_loss: -0.04176984777053197\n",
      "          total_loss: -0.05587275500098864\n",
      "          vf_explained_var: 0.46324074268341064\n",
      "          vf_loss: 0.00020205409681592655\n",
      "    num_agent_steps_sampled: 575000\n",
      "    num_agent_steps_trained: 575000\n",
      "    num_steps_sampled: 575000\n",
      "    num_steps_trained: 575000\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.574074074074076\n",
      "    ram_util_percent: 73.08148148148146\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036740614920712004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.824169057320027\n",
      "    mean_inference_ms: 1.7220380904491583\n",
      "    mean_raw_obs_processing_ms: 1.588946020280593\n",
      "  time_since_restore: 13143.358259916306\n",
      "  time_this_iter_s: 18.835918426513672\n",
      "  time_total_s: 13143.358259916306\n",
      "  timers:\n",
      "    learn_throughput: 1567.99\n",
      "    learn_time_ms: 637.759\n",
      "    load_throughput: 56399.463\n",
      "    load_time_ms: 17.731\n",
      "    sample_throughput: 48.365\n",
      "    sample_time_ms: 20676.059\n",
      "    update_time_ms: 2.134\n",
      "  timestamp: 1633804262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575000\n",
      "  training_iteration: 575\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">         13143.4</td><td style=\"text-align: right;\">575000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 391.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1471\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8095108879937065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009161609906822892\n",
      "          policy_loss: -0.002741288745568858\n",
      "          total_loss: -0.01792154349386692\n",
      "          vf_explained_var: -0.9078553915023804\n",
      "          vf_loss: 0.00012887279606527754\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.84444444444444\n",
      "    ram_util_percent: 73.10740740740738\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03674027836593997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.824609526291443\n",
      "    mean_inference_ms: 1.7220346077991018\n",
      "    mean_raw_obs_processing_ms: 1.5894084613232655\n",
      "  time_since_restore: 13181.501754760742\n",
      "  time_this_iter_s: 38.143494844436646\n",
      "  time_total_s: 13181.501754760742\n",
      "  timers:\n",
      "    learn_throughput: 1567.831\n",
      "    learn_time_ms: 637.824\n",
      "    load_throughput: 56632.106\n",
      "    load_time_ms: 17.658\n",
      "    sample_throughput: 45.162\n",
      "    sample_time_ms: 22142.631\n",
      "    update_time_ms: 2.131\n",
      "  timestamp: 1633804300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 576\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         13181.5</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 577000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 393.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1473\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8931437810262044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011994279194253683\n",
      "          policy_loss: -0.07375799732075797\n",
      "          total_loss: -0.08894659669862853\n",
      "          vf_explained_var: -0.6443395614624023\n",
      "          vf_loss: 9.545995477916828e-05\n",
      "    num_agent_steps_sampled: 577000\n",
      "    num_agent_steps_trained: 577000\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.624\n",
      "    ram_util_percent: 72.992\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673993169422061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8249166902752\n",
      "    mean_inference_ms: 1.7220305704488665\n",
      "    mean_raw_obs_processing_ms: 1.5898468847468485\n",
      "  time_since_restore: 13199.058052301407\n",
      "  time_this_iter_s: 17.556297540664673\n",
      "  time_total_s: 13199.058052301407\n",
      "  timers:\n",
      "    learn_throughput: 1572.759\n",
      "    learn_time_ms: 635.825\n",
      "    load_throughput: 56067.511\n",
      "    load_time_ms: 17.836\n",
      "    sample_throughput: 45.978\n",
      "    sample_time_ms: 21749.701\n",
      "    update_time_ms: 2.096\n",
      "  timestamp: 1633804318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   577</td><td style=\"text-align: right;\">         13199.1</td><td style=\"text-align: right;\">577000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 578000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 395.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1475\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9163869818051655\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013451127887007806\n",
      "          policy_loss: -0.035058987223439746\n",
      "          total_loss: -0.05005277039276229\n",
      "          vf_explained_var: -0.4341140687465668\n",
      "          vf_loss: 7.969285903123415e-05\n",
      "    num_agent_steps_sampled: 578000\n",
      "    num_agent_steps_trained: 578000\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.91481481481482\n",
      "    ram_util_percent: 72.69259259259259\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673958338766676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825130462950618\n",
      "    mean_inference_ms: 1.72202596867691\n",
      "    mean_raw_obs_processing_ms: 1.590261208408391\n",
      "  time_since_restore: 13217.75655412674\n",
      "  time_this_iter_s: 18.69850182533264\n",
      "  time_total_s: 13217.75655412674\n",
      "  timers:\n",
      "    learn_throughput: 1574.844\n",
      "    learn_time_ms: 634.984\n",
      "    load_throughput: 56221.503\n",
      "    load_time_ms: 17.787\n",
      "    sample_throughput: 46.866\n",
      "    sample_time_ms: 21337.552\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1633804336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 578\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         13217.8</td><td style=\"text-align: right;\">578000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 579000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 396.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1478\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7241643799675836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00879371691473963\n",
      "          policy_loss: -0.03905981143729554\n",
      "          total_loss: -0.0535268105359541\n",
      "          vf_explained_var: 0.07260831445455551\n",
      "          vf_loss: 0.00010053430321729846\n",
      "    num_agent_steps_sampled: 579000\n",
      "    num_agent_steps_trained: 579000\n",
      "    num_steps_sampled: 579000\n",
      "    num_steps_trained: 579000\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53666666666666\n",
      "    ram_util_percent: 72.43000000000002\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673908976375626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825296869983923\n",
      "    mean_inference_ms: 1.7220187970709617\n",
      "    mean_raw_obs_processing_ms: 1.5906042647368557\n",
      "  time_since_restore: 13238.930903673172\n",
      "  time_this_iter_s: 21.174349546432495\n",
      "  time_total_s: 13238.930903673172\n",
      "  timers:\n",
      "    learn_throughput: 1579.049\n",
      "    learn_time_ms: 633.293\n",
      "    load_throughput: 53718.033\n",
      "    load_time_ms: 18.616\n",
      "    sample_throughput: 46.185\n",
      "    sample_time_ms: 21651.979\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1633804357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579000\n",
      "  training_iteration: 579\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         13238.9</td><td style=\"text-align: right;\">579000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 397.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1481\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.587540324529012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005460133487273626\n",
      "          policy_loss: 0.0063602199157079065\n",
      "          total_loss: -0.007702702532211939\n",
      "          vf_explained_var: -0.7639926671981812\n",
      "          vf_loss: 0.0001520896337751765\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51818181818182\n",
      "    ram_util_percent: 72.34242424242424\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673859683746289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825472839673054\n",
      "    mean_inference_ms: 1.722011566596068\n",
      "    mean_raw_obs_processing_ms: 1.5903314058112137\n",
      "  time_since_restore: 13262.131836175919\n",
      "  time_this_iter_s: 23.200932502746582\n",
      "  time_total_s: 13262.131836175919\n",
      "  timers:\n",
      "    learn_throughput: 1575.543\n",
      "    learn_time_ms: 634.702\n",
      "    load_throughput: 53757.483\n",
      "    load_time_ms: 18.602\n",
      "    sample_throughput: 45.918\n",
      "    sample_time_ms: 21777.774\n",
      "    update_time_ms: 2.071\n",
      "  timestamp: 1633804381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 580\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   580</td><td style=\"text-align: right;\">         13262.1</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             397.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 581000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 396.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1484\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6936156272888183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00882422674594196\n",
      "          policy_loss: 0.04137305716673533\n",
      "          total_loss: 0.027317110531859926\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00019682206022682497\n",
      "    num_agent_steps_sampled: 581000\n",
      "    num_agent_steps_trained: 581000\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57575757575757\n",
      "    ram_util_percent: 72.33939393939394\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673810323433837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82568258674452\n",
      "    mean_inference_ms: 1.722003922637173\n",
      "    mean_raw_obs_processing_ms: 1.590064343620361\n",
      "  time_since_restore: 13284.897414684296\n",
      "  time_this_iter_s: 22.765578508377075\n",
      "  time_total_s: 13284.897414684296\n",
      "  timers:\n",
      "    learn_throughput: 1571.667\n",
      "    learn_time_ms: 636.267\n",
      "    load_throughput: 53756.45\n",
      "    load_time_ms: 18.602\n",
      "    sample_throughput: 46.597\n",
      "    sample_time_ms: 21460.561\n",
      "    update_time_ms: 2.07\n",
      "  timestamp: 1633804404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">         13284.9</td><td style=\"text-align: right;\">581000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 582000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 398.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1486\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9100376036432054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013064204883467115\n",
      "          policy_loss: -0.08973910106966893\n",
      "          total_loss: -0.10478516103078922\n",
      "          vf_explained_var: -0.796758770942688\n",
      "          vf_loss: 8.158384682448943e-05\n",
      "    num_agent_steps_sampled: 582000\n",
      "    num_agent_steps_trained: 582000\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.71153846153846\n",
      "    ram_util_percent: 72.41923076923078\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367377976724584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825684294471017\n",
      "    mean_inference_ms: 1.7219986169321846\n",
      "    mean_raw_obs_processing_ms: 1.5898976047826112\n",
      "  time_since_restore: 13303.391089439392\n",
      "  time_this_iter_s: 18.493674755096436\n",
      "  time_total_s: 13303.391089439392\n",
      "  timers:\n",
      "    learn_throughput: 1571.571\n",
      "    learn_time_ms: 636.306\n",
      "    load_throughput: 54111.882\n",
      "    load_time_ms: 18.48\n",
      "    sample_throughput: 47.537\n",
      "    sample_time_ms: 21036.347\n",
      "    update_time_ms: 2.055\n",
      "  timestamp: 1633804422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 582\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         13303.4</td><td style=\"text-align: right;\">582000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 583000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 398.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1488\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7140396568510268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008514477541386532\n",
      "          policy_loss: -0.0016908417559332318\n",
      "          total_loss: -0.016123394171396892\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011865057919446069\n",
      "    num_agent_steps_sampled: 583000\n",
      "    num_agent_steps_trained: 583000\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.487096774193546\n",
      "    ram_util_percent: 72.51290322580645\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673749797680856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825670708589683\n",
      "    mean_inference_ms: 1.7219932584761362\n",
      "    mean_raw_obs_processing_ms: 1.5897069454849366\n",
      "  time_since_restore: 13324.931627511978\n",
      "  time_this_iter_s: 21.54053807258606\n",
      "  time_total_s: 13324.931627511978\n",
      "  timers:\n",
      "    learn_throughput: 1575.572\n",
      "    learn_time_ms: 634.69\n",
      "    load_throughput: 53758.724\n",
      "    load_time_ms: 18.602\n",
      "    sample_throughput: 46.875\n",
      "    sample_time_ms: 21333.438\n",
      "    update_time_ms: 2.039\n",
      "  timestamp: 1633804444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 583\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         13324.9</td><td style=\"text-align: right;\">583000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 399.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1491\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6948585947354635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008806030437596648\n",
      "          policy_loss: -0.023002307779259153\n",
      "          total_loss: -0.03712347617579831\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001495640086229994\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.873333333333335\n",
      "    ram_util_percent: 72.62999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036737044932566985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825543356560534\n",
      "    mean_inference_ms: 1.7219855269801403\n",
      "    mean_raw_obs_processing_ms: 1.589425551341892\n",
      "  time_since_restore: 13345.639589548111\n",
      "  time_this_iter_s: 20.707962036132812\n",
      "  time_total_s: 13345.639589548111\n",
      "  timers:\n",
      "    learn_throughput: 1571.039\n",
      "    learn_time_ms: 636.521\n",
      "    load_throughput: 53810.451\n",
      "    load_time_ms: 18.584\n",
      "    sample_throughput: 46.617\n",
      "    sample_time_ms: 21451.18\n",
      "    update_time_ms: 2.03\n",
      "  timestamp: 1633804464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   584</td><td style=\"text-align: right;\">         13345.6</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-34-44\n",
      "  done: false\n",
      "  episode_len_mean: 399.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1493\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8628423637813991\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00938976438768738\n",
      "          policy_loss: 0.004587456811633375\n",
      "          total_loss: -0.011116907186806201\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 6.869842084900786e-05\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63928571428571\n",
      "    ram_util_percent: 72.76071428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036736742493217206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82544473056945\n",
      "    mean_inference_ms: 1.721980314380439\n",
      "    mean_raw_obs_processing_ms: 1.5892410625078492\n",
      "  time_since_restore: 13365.360320806503\n",
      "  time_this_iter_s: 19.720731258392334\n",
      "  time_total_s: 13365.360320806503\n",
      "  timers:\n",
      "    learn_throughput: 1570.083\n",
      "    learn_time_ms: 636.909\n",
      "    load_throughput: 53599.48\n",
      "    load_time_ms: 18.657\n",
      "    sample_throughput: 46.427\n",
      "    sample_time_ms: 21539.179\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1633804484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 585\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">         13365.4</td><td style=\"text-align: right;\">585000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 586000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-35-05\n",
      "  done: false\n",
      "  episode_len_mean: 399.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1495\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7641771541701423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007937680605634215\n",
      "          policy_loss: -0.03853387838850419\n",
      "          total_loss: -0.053674615919589996\n",
      "          vf_explained_var: -0.789711058139801\n",
      "          vf_loss: 8.724096850427385e-05\n",
      "    num_agent_steps_sampled: 586000\n",
      "    num_agent_steps_trained: 586000\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.72000000000001\n",
      "    ram_util_percent: 72.83666666666666\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673645263119854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8253716400314\n",
      "    mean_inference_ms: 1.7219753113727005\n",
      "    mean_raw_obs_processing_ms: 1.5890597417406902\n",
      "  time_since_restore: 13386.598688364029\n",
      "  time_this_iter_s: 21.238367557525635\n",
      "  time_total_s: 13386.598688364029\n",
      "  timers:\n",
      "    learn_throughput: 1571.501\n",
      "    learn_time_ms: 636.334\n",
      "    load_throughput: 53429.263\n",
      "    load_time_ms: 18.716\n",
      "    sample_throughput: 50.38\n",
      "    sample_time_ms: 19849.184\n",
      "    update_time_ms: 2.016\n",
      "  timestamp: 1633804505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 586\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         13386.6</td><td style=\"text-align: right;\">586000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 587000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 399.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1498\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8878992451561822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01340493899345664\n",
      "          policy_loss: -0.03745813605686029\n",
      "          total_loss: -0.05215510183738337\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00010567571759262743\n",
      "    num_agent_steps_sampled: 587000\n",
      "    num_agent_steps_trained: 587000\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.4357142857143\n",
      "    ram_util_percent: 72.88571428571429\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036736033924563094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825283364008374\n",
      "    mean_inference_ms: 1.7219676279986424\n",
      "    mean_raw_obs_processing_ms: 1.5888188621203283\n",
      "  time_since_restore: 13405.807068824768\n",
      "  time_this_iter_s: 19.208380460739136\n",
      "  time_total_s: 13405.807068824768\n",
      "  timers:\n",
      "    learn_throughput: 1567.017\n",
      "    learn_time_ms: 638.155\n",
      "    load_throughput: 53927.028\n",
      "    load_time_ms: 18.544\n",
      "    sample_throughput: 49.968\n",
      "    sample_time_ms: 20012.755\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1633804525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   587</td><td style=\"text-align: right;\">         13405.8</td><td style=\"text-align: right;\">587000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 399.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.842109861638811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011307444479254929\n",
      "          policy_loss: -0.12935532095531624\n",
      "          total_loss: -0.1441984944873386\n",
      "          vf_explained_var: -0.7248565554618835\n",
      "          vf_loss: 0.00013941020693487694\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.992982456140346\n",
      "    ram_util_percent: 73.00526315789475\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673575797794009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825283464113802\n",
      "    mean_inference_ms: 1.721962717628937\n",
      "    mean_raw_obs_processing_ms: 1.5892101477195422\n",
      "  time_since_restore: 13445.87374329567\n",
      "  time_this_iter_s: 40.06667447090149\n",
      "  time_total_s: 13445.87374329567\n",
      "  timers:\n",
      "    learn_throughput: 1565.075\n",
      "    learn_time_ms: 638.947\n",
      "    load_throughput: 53695.064\n",
      "    load_time_ms: 18.624\n",
      "    sample_throughput: 45.149\n",
      "    sample_time_ms: 22148.688\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633804565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 588\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         13445.9</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 589000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 396.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1503\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8036289241578844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017613859268287454\n",
      "          policy_loss: -0.03025223505165842\n",
      "          total_loss: -0.04273254540231493\n",
      "          vf_explained_var: -0.8394441604614258\n",
      "          vf_loss: 0.00019972586067322278\n",
      "    num_agent_steps_sampled: 589000\n",
      "    num_agent_steps_trained: 589000\n",
      "    num_steps_sampled: 589000\n",
      "    num_steps_trained: 589000\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.38709677419356\n",
      "    ram_util_percent: 73.13870967741933\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673536030303042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825371352283852\n",
      "    mean_inference_ms: 1.721954980653679\n",
      "    mean_raw_obs_processing_ms: 1.5898265545142902\n",
      "  time_since_restore: 13468.015325307846\n",
      "  time_this_iter_s: 22.141582012176514\n",
      "  time_total_s: 13468.015325307846\n",
      "  timers:\n",
      "    learn_throughput: 1565.428\n",
      "    learn_time_ms: 638.803\n",
      "    load_throughput: 53333.944\n",
      "    load_time_ms: 18.75\n",
      "    sample_throughput: 44.953\n",
      "    sample_time_ms: 22245.417\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633804587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589000\n",
      "  training_iteration: 589\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">           13468</td><td style=\"text-align: right;\">589000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 590000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 395.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1506\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.761054684056176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011013433034638274\n",
      "          policy_loss: -0.017288407766156727\n",
      "          total_loss: -0.03139306786987517\n",
      "          vf_explained_var: -0.9938374161720276\n",
      "          vf_loss: 0.00015678011857542313\n",
      "    num_agent_steps_sampled: 590000\n",
      "    num_agent_steps_trained: 590000\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.50967741935483\n",
      "    ram_util_percent: 72.95806451612906\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673499553362394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825496305580106\n",
      "    mean_inference_ms: 1.721947343169775\n",
      "    mean_raw_obs_processing_ms: 1.5904469865344568\n",
      "  time_since_restore: 13489.57481598854\n",
      "  time_this_iter_s: 21.55949068069458\n",
      "  time_total_s: 13489.57481598854\n",
      "  timers:\n",
      "    learn_throughput: 1564.25\n",
      "    learn_time_ms: 639.284\n",
      "    load_throughput: 54882.529\n",
      "    load_time_ms: 18.221\n",
      "    sample_throughput: 45.287\n",
      "    sample_time_ms: 22081.314\n",
      "    update_time_ms: 2.056\n",
      "  timestamp: 1633804608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 590\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">         13489.6</td><td style=\"text-align: right;\">590000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 591000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-37-10\n",
      "  done: false\n",
      "  episode_len_mean: 395.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1508\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.711025471157498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01038065333397778\n",
      "          policy_loss: -0.0345836915800141\n",
      "          total_loss: -0.048361710728042656\n",
      "          vf_explained_var: -0.8237192630767822\n",
      "          vf_loss: 0.00017555064397230228\n",
      "    num_agent_steps_sampled: 591000\n",
      "    num_agent_steps_trained: 591000\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.57419354838711\n",
      "    ram_util_percent: 72.73548387096773\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673476223571296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82561583906215\n",
      "    mean_inference_ms: 1.7219423996659815\n",
      "    mean_raw_obs_processing_ms: 1.5905632340061278\n",
      "  time_since_restore: 13511.196177244186\n",
      "  time_this_iter_s: 21.621361255645752\n",
      "  time_total_s: 13511.196177244186\n",
      "  timers:\n",
      "    learn_throughput: 1565.862\n",
      "    learn_time_ms: 638.626\n",
      "    load_throughput: 54888.49\n",
      "    load_time_ms: 18.219\n",
      "    sample_throughput: 45.522\n",
      "    sample_time_ms: 21967.509\n",
      "    update_time_ms: 2.069\n",
      "  timestamp: 1633804630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   591</td><td style=\"text-align: right;\">         13511.2</td><td style=\"text-align: right;\">591000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 396.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1511\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6357042498058743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010202860087797996\n",
      "          policy_loss: -0.06599693968892098\n",
      "          total_loss: -0.07879479920698537\n",
      "          vf_explained_var: -0.8573732376098633\n",
      "          vf_loss: 0.00045656782232173204\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.720689655172414\n",
      "    ram_util_percent: 72.65862068965517\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036734421135610214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825736735493905\n",
      "    mean_inference_ms: 1.7219349271995927\n",
      "    mean_raw_obs_processing_ms: 1.5902685781894066\n",
      "  time_since_restore: 13531.570749521255\n",
      "  time_this_iter_s: 20.374572277069092\n",
      "  time_total_s: 13531.570749521255\n",
      "  timers:\n",
      "    learn_throughput: 1561.401\n",
      "    learn_time_ms: 640.45\n",
      "    load_throughput: 54655.891\n",
      "    load_time_ms: 18.296\n",
      "    sample_throughput: 45.139\n",
      "    sample_time_ms: 22153.666\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633804650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 592\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">         13531.6</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 593000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 398.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1513\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.728585085603926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009183723281710382\n",
      "          policy_loss: -0.016740760227872267\n",
      "          total_loss: -0.0310285196122196\n",
      "          vf_explained_var: -0.8382988572120667\n",
      "          vf_loss: 0.00020538649340677591\n",
      "    num_agent_steps_sampled: 593000\n",
      "    num_agent_steps_trained: 593000\n",
      "    num_steps_sampled: 593000\n",
      "    num_steps_trained: 593000\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.99285714285715\n",
      "    ram_util_percent: 72.53928571428571\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673421734711971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825736652346283\n",
      "    mean_inference_ms: 1.7219299769599161\n",
      "    mean_raw_obs_processing_ms: 1.5900835304778969\n",
      "  time_since_restore: 13550.949413776398\n",
      "  time_this_iter_s: 19.378664255142212\n",
      "  time_total_s: 13550.949413776398\n",
      "  timers:\n",
      "    learn_throughput: 1558.8\n",
      "    learn_time_ms: 641.519\n",
      "    load_throughput: 54310.383\n",
      "    load_time_ms: 18.413\n",
      "    sample_throughput: 45.587\n",
      "    sample_time_ms: 21936.264\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633804670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593000\n",
      "  training_iteration: 593\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">         13550.9</td><td style=\"text-align: right;\">593000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 594000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1515\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.826596846845415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013072910888684557\n",
      "          policy_loss: -0.08422180103758971\n",
      "          total_loss: -0.09839228234357304\n",
      "          vf_explained_var: -0.8783835768699646\n",
      "          vf_loss: 0.00012010638408052424\n",
      "    num_agent_steps_sampled: 594000\n",
      "    num_agent_steps_trained: 594000\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10769230769232\n",
      "    ram_util_percent: 72.63076923076923\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673401655516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825630570862376\n",
      "    mean_inference_ms: 1.7219247988055424\n",
      "    mean_raw_obs_processing_ms: 1.5898751046689636\n",
      "  time_since_restore: 13569.44802236557\n",
      "  time_this_iter_s: 18.498608589172363\n",
      "  time_total_s: 13569.44802236557\n",
      "  timers:\n",
      "    learn_throughput: 1559.82\n",
      "    learn_time_ms: 641.099\n",
      "    load_throughput: 54301.664\n",
      "    load_time_ms: 18.416\n",
      "    sample_throughput: 46.05\n",
      "    sample_time_ms: 21715.728\n",
      "    update_time_ms: 2.082\n",
      "  timestamp: 1633804688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   594</td><td style=\"text-align: right;\">         13569.4</td><td style=\"text-align: right;\">594000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 595000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-38-29\n",
      "  done: false\n",
      "  episode_len_mean: 401.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1517\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.808407065603468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008757031519306\n",
      "          policy_loss: -0.019793038215074273\n",
      "          total_loss: -0.03508594857735766\n",
      "          vf_explained_var: -0.803881824016571\n",
      "          vf_loss: 0.00012820746002641196\n",
      "    num_agent_steps_sampled: 595000\n",
      "    num_agent_steps_trained: 595000\n",
      "    num_steps_sampled: 595000\n",
      "    num_steps_trained: 595000\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.433333333333344\n",
      "    ram_util_percent: 72.70666666666668\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036733804365095074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825528098130373\n",
      "    mean_inference_ms: 1.7219195282509698\n",
      "    mean_raw_obs_processing_ms: 1.5896697964893736\n",
      "  time_since_restore: 13589.858794212341\n",
      "  time_this_iter_s: 20.41077184677124\n",
      "  time_total_s: 13589.858794212341\n",
      "  timers:\n",
      "    learn_throughput: 1559.651\n",
      "    learn_time_ms: 641.169\n",
      "    load_throughput: 54431.187\n",
      "    load_time_ms: 18.372\n",
      "    sample_throughput: 45.904\n",
      "    sample_time_ms: 21784.694\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633804709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595000\n",
      "  training_iteration: 595\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         13589.9</td><td style=\"text-align: right;\">595000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            401.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7755463931295608\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009235729171259206\n",
      "          policy_loss: -0.04433804266154766\n",
      "          total_loss: -0.059149386112888655\n",
      "          vf_explained_var: -0.6808264255523682\n",
      "          vf_loss: 0.00013559875418044006\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55757575757576\n",
      "    ram_util_percent: 72.73636363636365\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036733506154697865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825464146627763\n",
      "    mean_inference_ms: 1.7219116714641323\n",
      "    mean_raw_obs_processing_ms: 1.5893654331214\n",
      "  time_since_restore: 13613.522993326187\n",
      "  time_this_iter_s: 23.664199113845825\n",
      "  time_total_s: 13613.522993326187\n",
      "  timers:\n",
      "    learn_throughput: 1557.098\n",
      "    learn_time_ms: 642.22\n",
      "    load_throughput: 54646.064\n",
      "    load_time_ms: 18.3\n",
      "    sample_throughput: 45.4\n",
      "    sample_time_ms: 22026.274\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633804732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 596\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         13613.5</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 597000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-39-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1523\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8364399909973144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010336719750484096\n",
      "          policy_loss: -0.0538011423829529\n",
      "          total_loss: -0.06891399253573682\n",
      "          vf_explained_var: -0.9412980675697327\n",
      "          vf_loss: 0.00010822490896518704\n",
      "    num_agent_steps_sampled: 597000\n",
      "    num_agent_steps_trained: 597000\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5774193548387\n",
      "    ram_util_percent: 72.86451612903228\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673319830509814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82540123597696\n",
      "    mean_inference_ms: 1.7219037500794474\n",
      "    mean_raw_obs_processing_ms: 1.5890654829689197\n",
      "  time_since_restore: 13635.291168928146\n",
      "  time_this_iter_s: 21.76817560195923\n",
      "  time_total_s: 13635.291168928146\n",
      "  timers:\n",
      "    learn_throughput: 1559.343\n",
      "    learn_time_ms: 641.296\n",
      "    load_throughput: 54998.754\n",
      "    load_time_ms: 18.182\n",
      "    sample_throughput: 44.877\n",
      "    sample_time_ms: 22283.287\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633804754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   597</td><td style=\"text-align: right;\">         13635.3</td><td style=\"text-align: right;\">597000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 598000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-39-39\n",
      "  done: false\n",
      "  episode_len_mean: 398.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1525\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7780751321050856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007172698604782523\n",
      "          policy_loss: -0.02187675181776285\n",
      "          total_loss: -0.037373629579734474\n",
      "          vf_explained_var: -0.9926095604896545\n",
      "          vf_loss: 0.00010270385444325964\n",
      "    num_agent_steps_sampled: 598000\n",
      "    num_agent_steps_trained: 598000\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.461111111111116\n",
      "    ram_util_percent: 72.90555555555557\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673300306663767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825507272513086\n",
      "    mean_inference_ms: 1.7218985578667871\n",
      "    mean_raw_obs_processing_ms: 1.5888685852414273\n",
      "  time_since_restore: 13660.275074005127\n",
      "  time_this_iter_s: 24.98390507698059\n",
      "  time_total_s: 13660.275074005127\n",
      "  timers:\n",
      "    learn_throughput: 1563.657\n",
      "    learn_time_ms: 639.526\n",
      "    load_throughput: 55222.579\n",
      "    load_time_ms: 18.109\n",
      "    sample_throughput: 48.13\n",
      "    sample_time_ms: 20776.873\n",
      "    update_time_ms: 2.066\n",
      "  timestamp: 1633804779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 598\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         13660.3</td><td style=\"text-align: right;\">598000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 599000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-40-03\n",
      "  done: false\n",
      "  episode_len_mean: 398.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1528\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.869541969564226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008675191374117652\n",
      "          policy_loss: -0.10100384213858181\n",
      "          total_loss: -0.11697159277068245\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 8.960365602332685e-05\n",
      "    num_agent_steps_sampled: 599000\n",
      "    num_agent_steps_trained: 599000\n",
      "    num_steps_sampled: 599000\n",
      "    num_steps_trained: 599000\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.41470588235294\n",
      "    ram_util_percent: 72.97941176470587\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673274259946644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82569508185147\n",
      "    mean_inference_ms: 1.7218906929432325\n",
      "    mean_raw_obs_processing_ms: 1.5885775188376605\n",
      "  time_since_restore: 13683.880460977554\n",
      "  time_this_iter_s: 23.605386972427368\n",
      "  time_total_s: 13683.880460977554\n",
      "  timers:\n",
      "    learn_throughput: 1561.449\n",
      "    learn_time_ms: 640.431\n",
      "    load_throughput: 56422.603\n",
      "    load_time_ms: 17.723\n",
      "    sample_throughput: 47.795\n",
      "    sample_time_ms: 20922.755\n",
      "    update_time_ms: 2.072\n",
      "  timestamp: 1633804803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599000\n",
      "  training_iteration: 599\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         13683.9</td><td style=\"text-align: right;\">599000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            398.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 397.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1531\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7371543208758036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009501999325453427\n",
      "          policy_loss: -0.01775085073378351\n",
      "          total_loss: -0.032050532102584836\n",
      "          vf_explained_var: -0.8425804972648621\n",
      "          vf_loss: 0.0001823706018816059\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.43620689655172\n",
      "    ram_util_percent: 73.09655172413794\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673249231579772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825830982024186\n",
      "    mean_inference_ms: 1.721883765040988\n",
      "    mean_raw_obs_processing_ms: 1.589161560800614\n",
      "  time_since_restore: 13724.596697807312\n",
      "  time_this_iter_s: 40.71623682975769\n",
      "  time_total_s: 13724.596697807312\n",
      "  timers:\n",
      "    learn_throughput: 1563.993\n",
      "    learn_time_ms: 639.389\n",
      "    load_throughput: 54651.048\n",
      "    load_time_ms: 18.298\n",
      "    sample_throughput: 43.785\n",
      "    sample_time_ms: 22838.893\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1633804844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 600\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         13724.6</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            397.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 601000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 399.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1533\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9105999681684707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011154511323521948\n",
      "          policy_loss: -0.07610616638428636\n",
      "          total_loss: -0.09171840581629011\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00010175252613812013\n",
      "    num_agent_steps_sampled: 601000\n",
      "    num_agent_steps_trained: 601000\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.49285714285714\n",
      "    ram_util_percent: 73.02857142857144\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673232918315081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82570970268673\n",
      "    mean_inference_ms: 1.7218790935384203\n",
      "    mean_raw_obs_processing_ms: 1.5895518132381392\n",
      "  time_since_restore: 13743.968445062637\n",
      "  time_this_iter_s: 19.371747255325317\n",
      "  time_total_s: 13743.968445062637\n",
      "  timers:\n",
      "    learn_throughput: 1563.407\n",
      "    learn_time_ms: 639.629\n",
      "    load_throughput: 54492.782\n",
      "    load_time_ms: 18.351\n",
      "    sample_throughput: 44.221\n",
      "    sample_time_ms: 22613.69\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1633804863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">           13744</td><td style=\"text-align: right;\">601000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             399.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 602000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1536\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6746617939737107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01168503504606592\n",
      "          policy_loss: 0.03748427174157566\n",
      "          total_loss: 0.024377393474181493\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 8.640496319761345e-05\n",
      "    num_agent_steps_sampled: 602000\n",
      "    num_agent_steps_trained: 602000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59375\n",
      "    ram_util_percent: 72.66875\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036732103907913406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825613719574196\n",
      "    mean_inference_ms: 1.7218724227268973\n",
      "    mean_raw_obs_processing_ms: 1.5901164647649122\n",
      "  time_since_restore: 13766.842948198318\n",
      "  time_this_iter_s: 22.874503135681152\n",
      "  time_total_s: 13766.842948198318\n",
      "  timers:\n",
      "    learn_throughput: 1564.85\n",
      "    learn_time_ms: 639.039\n",
      "    load_throughput: 53673.282\n",
      "    load_time_ms: 18.631\n",
      "    sample_throughput: 43.737\n",
      "    sample_time_ms: 22864.003\n",
      "    update_time_ms: 2.023\n",
      "  timestamp: 1633804886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 602\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   602</td><td style=\"text-align: right;\">         13766.8</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 603000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 399.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1539\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.711650452348921\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010775633811311122\n",
      "          policy_loss: -0.044062719059487186\n",
      "          total_loss: -0.057791418727073406\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011101317423809734\n",
      "    num_agent_steps_sampled: 603000\n",
      "    num_agent_steps_trained: 603000\n",
      "    num_steps_sampled: 603000\n",
      "    num_steps_trained: 603000\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54411764705882\n",
      "    ram_util_percent: 72.36176470588236\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673188130915327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825583968271975\n",
      "    mean_inference_ms: 1.7218664347120223\n",
      "    mean_raw_obs_processing_ms: 1.5907095872369836\n",
      "  time_since_restore: 13790.322880268097\n",
      "  time_this_iter_s: 23.479932069778442\n",
      "  time_total_s: 13790.322880268097\n",
      "  timers:\n",
      "    learn_throughput: 1565.73\n",
      "    learn_time_ms: 638.68\n",
      "    load_throughput: 54302.649\n",
      "    load_time_ms: 18.415\n",
      "    sample_throughput: 42.965\n",
      "    sample_time_ms: 23274.723\n",
      "    update_time_ms: 2.026\n",
      "  timestamp: 1633804909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 603000\n",
      "  training_iteration: 603\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         13790.3</td><td style=\"text-align: right;\">603000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 399.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1542\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7949948032697043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009235830788895328\n",
      "          policy_loss: -0.04817543021506733\n",
      "          total_loss: -0.0632435590442684\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 7.32689197320724e-05\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.832352941176474\n",
      "    ram_util_percent: 72.32941176470588\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367316730013345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82545963230462\n",
      "    mean_inference_ms: 1.721860695037401\n",
      "    mean_raw_obs_processing_ms: 1.5904452327012104\n",
      "  time_since_restore: 13814.207248449326\n",
      "  time_this_iter_s: 23.884368181228638\n",
      "  time_total_s: 13814.207248449326\n",
      "  timers:\n",
      "    learn_throughput: 1568.281\n",
      "    learn_time_ms: 637.641\n",
      "    load_throughput: 54378.684\n",
      "    load_time_ms: 18.39\n",
      "    sample_throughput: 41.991\n",
      "    sample_time_ms: 23814.367\n",
      "    update_time_ms: 2.02\n",
      "  timestamp: 1633804933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         13814.2</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 605000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 399.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1544\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7652578698264227\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010568890202098145\n",
      "          policy_loss: -0.012426255705455939\n",
      "          total_loss: -0.02675252455390162\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00011238600822657139\n",
      "    num_agent_steps_sampled: 605000\n",
      "    num_agent_steps_trained: 605000\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52058823529412\n",
      "    ram_util_percent: 72.41470588235295\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673154264917941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82540978709111\n",
      "    mean_inference_ms: 1.7218569590943082\n",
      "    mean_raw_obs_processing_ms: 1.5902543958429223\n",
      "  time_since_restore: 13837.99162364006\n",
      "  time_this_iter_s: 23.784375190734863\n",
      "  time_total_s: 13837.99162364006\n",
      "  timers:\n",
      "    learn_throughput: 1570.778\n",
      "    learn_time_ms: 636.627\n",
      "    load_throughput: 54514.526\n",
      "    load_time_ms: 18.344\n",
      "    sample_throughput: 41.403\n",
      "    sample_time_ms: 24152.801\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1633804957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 605\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">           13838</td><td style=\"text-align: right;\">605000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 606000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 397.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1548\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7298839304182265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009972340913947894\n",
      "          policy_loss: -0.0016164982277486058\n",
      "          total_loss: -0.01582144753386577\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 6.136892641532339e-05\n",
      "    num_agent_steps_sampled: 606000\n",
      "    num_agent_steps_trained: 606000\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.528947368421065\n",
      "    ram_util_percent: 72.51052631578946\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673128395050008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82551189422691\n",
      "    mean_inference_ms: 1.7218495276581973\n",
      "    mean_raw_obs_processing_ms: 1.5899078799280681\n",
      "  time_since_restore: 13864.725957870483\n",
      "  time_this_iter_s: 26.734334230422974\n",
      "  time_total_s: 13864.725957870483\n",
      "  timers:\n",
      "    learn_throughput: 1569.612\n",
      "    learn_time_ms: 637.1\n",
      "    load_throughput: 54719.995\n",
      "    load_time_ms: 18.275\n",
      "    sample_throughput: 40.884\n",
      "    sample_time_ms: 24459.436\n",
      "    update_time_ms: 2.035\n",
      "  timestamp: 1633804984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 606\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">         13864.7</td><td style=\"text-align: right;\">606000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            397.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 607000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 396.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8803984562555949\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011395442673741327\n",
      "          policy_loss: -0.028911850684218938\n",
      "          total_loss: -0.04415278145008617\n",
      "          vf_explained_var: -0.34938645362854004\n",
      "          vf_loss: 9.777917879950514e-05\n",
      "    num_agent_steps_sampled: 607000\n",
      "    num_agent_steps_trained: 607000\n",
      "    num_steps_sampled: 607000\n",
      "    num_steps_trained: 607000\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73611111111111\n",
      "    ram_util_percent: 72.68333333333335\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367311513480478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.825655163083926\n",
      "    mean_inference_ms: 1.7218458744650706\n",
      "    mean_raw_obs_processing_ms: 1.589749728584594\n",
      "  time_since_restore: 13889.715374708176\n",
      "  time_this_iter_s: 24.98941683769226\n",
      "  time_total_s: 13889.715374708176\n",
      "  timers:\n",
      "    learn_throughput: 1568.15\n",
      "    learn_time_ms: 637.694\n",
      "    load_throughput: 54760.002\n",
      "    load_time_ms: 18.262\n",
      "    sample_throughput: 40.354\n",
      "    sample_time_ms: 24780.979\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1633805009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607000\n",
      "  training_iteration: 607\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   607</td><td style=\"text-align: right;\">         13889.7</td><td style=\"text-align: right;\">607000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-43-53\n",
      "  done: false\n",
      "  episode_len_mean: 395.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1553\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7785817212528652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006491973155749382\n",
      "          policy_loss: 0.018034598603844643\n",
      "          total_loss: 0.0023108429378933377\n",
      "          vf_explained_var: -0.40164974331855774\n",
      "          vf_loss: 8.789719051694394e-05\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.794117647058826\n",
      "    ram_util_percent: 72.78823529411767\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036730939963302955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.826026051965776\n",
      "    mean_inference_ms: 1.7218404132818759\n",
      "    mean_raw_obs_processing_ms: 1.5895052348187144\n",
      "  time_since_restore: 13913.858293533325\n",
      "  time_this_iter_s: 24.142918825149536\n",
      "  time_total_s: 13913.858293533325\n",
      "  timers:\n",
      "    learn_throughput: 1564.509\n",
      "    learn_time_ms: 639.178\n",
      "    load_throughput: 54694.948\n",
      "    load_time_ms: 18.283\n",
      "    sample_throughput: 40.493\n",
      "    sample_time_ms: 24695.374\n",
      "    update_time_ms: 2.047\n",
      "  timestamp: 1633805033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">         13913.9</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             395.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 609000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-44-18\n",
      "  done: false\n",
      "  episode_len_mean: 392.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1556\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5963063756624858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006955938450779245\n",
      "          policy_loss: -0.08295053260193931\n",
      "          total_loss: -0.09659700372980701\n",
      "          vf_explained_var: -0.7278886437416077\n",
      "          vf_loss: 0.00020134104060060863\n",
      "    num_agent_steps_sampled: 609000\n",
      "    num_agent_steps_trained: 609000\n",
      "    num_steps_sampled: 609000\n",
      "    num_steps_trained: 609000\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.10277777777778\n",
      "    ram_util_percent: 72.8777777777778\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673074887786573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.826667071866733\n",
      "    mean_inference_ms: 1.7218352020571546\n",
      "    mean_raw_obs_processing_ms: 1.589291327627486\n",
      "  time_since_restore: 13938.729650735855\n",
      "  time_this_iter_s: 24.871357202529907\n",
      "  time_total_s: 13938.729650735855\n",
      "  timers:\n",
      "    learn_throughput: 1564.515\n",
      "    learn_time_ms: 639.176\n",
      "    load_throughput: 53780.092\n",
      "    load_time_ms: 18.594\n",
      "    sample_throughput: 40.287\n",
      "    sample_time_ms: 24821.661\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1633805058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609000\n",
      "  training_iteration: 609\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   609</td><td style=\"text-align: right;\">         13938.7</td><td style=\"text-align: right;\">609000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 610000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 393.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1559\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8403609633445739\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010674837577546784\n",
      "          policy_loss: -0.05888792640633053\n",
      "          total_loss: -0.07398102593918642\n",
      "          vf_explained_var: -0.9956801533699036\n",
      "          vf_loss: 6.43656656595542e-05\n",
      "    num_agent_steps_sampled: 610000\n",
      "    num_agent_steps_trained: 610000\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63125\n",
      "    ram_util_percent: 72.89687500000001\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673053861408783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82719836649557\n",
      "    mean_inference_ms: 1.721829780068792\n",
      "    mean_raw_obs_processing_ms: 1.589081526699146\n",
      "  time_since_restore: 13961.302809476852\n",
      "  time_this_iter_s: 22.573158740997314\n",
      "  time_total_s: 13961.302809476852\n",
      "  timers:\n",
      "    learn_throughput: 1564.93\n",
      "    learn_time_ms: 639.006\n",
      "    load_throughput: 54367.335\n",
      "    load_time_ms: 18.393\n",
      "    sample_throughput: 43.464\n",
      "    sample_time_ms: 23007.757\n",
      "    update_time_ms: 2.043\n",
      "  timestamp: 1633805080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 610\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   610</td><td style=\"text-align: right;\">         13961.3</td><td style=\"text-align: right;\">610000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             393.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 611000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-45-25\n",
      "  done: false\n",
      "  episode_len_mean: 391.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1562\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6417025089263917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01077348714556826\n",
      "          policy_loss: -0.01013812203374174\n",
      "          total_loss: -0.0232033914162053\n",
      "          vf_explained_var: -0.936432421207428\n",
      "          vf_loss: 7.561314131534244e-05\n",
      "    num_agent_steps_sampled: 611000\n",
      "    num_agent_steps_trained: 611000\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.57936507936508\n",
      "    ram_util_percent: 73.00317460317463\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673031021226815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.82791194929467\n",
      "    mean_inference_ms: 1.721824628927987\n",
      "    mean_raw_obs_processing_ms: 1.5896913037933218\n",
      "  time_since_restore: 14005.586405992508\n",
      "  time_this_iter_s: 44.28359651565552\n",
      "  time_total_s: 14005.586405992508\n",
      "  timers:\n",
      "    learn_throughput: 1568.068\n",
      "    learn_time_ms: 637.728\n",
      "    load_throughput: 54633.536\n",
      "    load_time_ms: 18.304\n",
      "    sample_throughput: 39.215\n",
      "    sample_time_ms: 25500.193\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1633805125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   611</td><td style=\"text-align: right;\">         14005.6</td><td style=\"text-align: right;\">611000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            391.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 386.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1565\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.608531822098626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009689327958421146\n",
      "          policy_loss: -0.018980151828792362\n",
      "          total_loss: -0.031958529187573324\n",
      "          vf_explained_var: -0.6594614386558533\n",
      "          vf_loss: 0.00016048125366473364\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.602777777777774\n",
      "    ram_util_percent: 73.09444444444443\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03673009004123096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.828929533218126\n",
      "    mean_inference_ms: 1.7218195943593653\n",
      "    mean_raw_obs_processing_ms: 1.5903281367318451\n",
      "  time_since_restore: 14030.614530086517\n",
      "  time_this_iter_s: 25.0281240940094\n",
      "  time_total_s: 14030.614530086517\n",
      "  timers:\n",
      "    learn_throughput: 1572.133\n",
      "    learn_time_ms: 636.079\n",
      "    load_throughput: 55605.913\n",
      "    load_time_ms: 17.984\n",
      "    sample_throughput: 38.884\n",
      "    sample_time_ms: 25717.509\n",
      "    update_time_ms: 2.174\n",
      "  timestamp: 1633805150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 612\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   612</td><td style=\"text-align: right;\">         14030.6</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            386.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 613000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 381.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1569\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4891714003351\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010556822781488269\n",
      "          policy_loss: -0.017310114546368518\n",
      "          total_loss: -0.028887184150516988\n",
      "          vf_explained_var: -0.44563013315200806\n",
      "          vf_loss: 0.00010438694456145943\n",
      "    num_agent_steps_sampled: 613000\n",
      "    num_agent_steps_trained: 613000\n",
      "    num_steps_sampled: 613000\n",
      "    num_steps_trained: 613000\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.935897435897424\n",
      "    ram_util_percent: 72.82564102564099\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672979114945962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.830913663524967\n",
      "    mean_inference_ms: 1.7218140072067394\n",
      "    mean_raw_obs_processing_ms: 1.5912341520707947\n",
      "  time_since_restore: 14058.315084934235\n",
      "  time_this_iter_s: 27.700554847717285\n",
      "  time_total_s: 14058.315084934235\n",
      "  timers:\n",
      "    learn_throughput: 1572.353\n",
      "    learn_time_ms: 635.989\n",
      "    load_throughput: 55035.198\n",
      "    load_time_ms: 18.17\n",
      "    sample_throughput: 38.256\n",
      "    sample_time_ms: 26139.474\n",
      "    update_time_ms: 2.185\n",
      "  timestamp: 1633805178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613000\n",
      "  training_iteration: 613\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   613</td><td style=\"text-align: right;\">         14058.3</td><td style=\"text-align: right;\">613000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            381.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 614000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 378.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1572\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5574216193623014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010746995360870986\n",
      "          policy_loss: -0.0874287953807248\n",
      "          total_loss: -0.09955073408782482\n",
      "          vf_explained_var: -0.4864913821220398\n",
      "          vf_loss: 0.00018419071323781585\n",
      "    num_agent_steps_sampled: 614000\n",
      "    num_agent_steps_trained: 614000\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.410000000000004\n",
      "    ram_util_percent: 72.705\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036729546606094406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83287129485054\n",
      "    mean_inference_ms: 1.7218104156298975\n",
      "    mean_raw_obs_processing_ms: 1.591077864126816\n",
      "  time_since_restore: 14086.028433561325\n",
      "  time_this_iter_s: 27.713348627090454\n",
      "  time_total_s: 14086.028433561325\n",
      "  timers:\n",
      "    learn_throughput: 1569.801\n",
      "    learn_time_ms: 637.024\n",
      "    load_throughput: 55076.246\n",
      "    load_time_ms: 18.157\n",
      "    sample_throughput: 37.705\n",
      "    sample_time_ms: 26521.35\n",
      "    update_time_ms: 2.191\n",
      "  timestamp: 1633805205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 614\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   614</td><td style=\"text-align: right;\">           14086</td><td style=\"text-align: right;\">614000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            378.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-47-11\n",
      "  done: false\n",
      "  episode_len_mean: 372.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1575\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4883401248190138\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008294760147039821\n",
      "          policy_loss: -0.06059948081771533\n",
      "          total_loss: -0.07276062741875648\n",
      "          vf_explained_var: -0.9773719906806946\n",
      "          vf_loss: 0.00019987703740803733\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.69189189189189\n",
      "    ram_util_percent: 72.73243243243246\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672931483496758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83528387534113\n",
      "    mean_inference_ms: 1.7218081612544445\n",
      "    mean_raw_obs_processing_ms: 1.5909525073793842\n",
      "  time_since_restore: 14112.214465856552\n",
      "  time_this_iter_s: 26.18603229522705\n",
      "  time_total_s: 14112.214465856552\n",
      "  timers:\n",
      "    learn_throughput: 1568.408\n",
      "    learn_time_ms: 637.589\n",
      "    load_throughput: 55163.46\n",
      "    load_time_ms: 18.128\n",
      "    sample_throughput: 37.368\n",
      "    sample_time_ms: 26760.977\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1633805231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   615</td><td style=\"text-align: right;\">         14112.2</td><td style=\"text-align: right;\">615000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            372.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 369.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1578\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4305045154359606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00973863838795322\n",
      "          policy_loss: -0.08652499674095047\n",
      "          total_loss: -0.09768950169285139\n",
      "          vf_explained_var: -0.849452018737793\n",
      "          vf_loss: 0.00017908700909982953\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58378378378379\n",
      "    ram_util_percent: 72.81891891891894\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672910143196805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.837966849494812\n",
      "    mean_inference_ms: 1.721806610136378\n",
      "    mean_raw_obs_processing_ms: 1.5908579219377166\n",
      "  time_since_restore: 14138.086912870407\n",
      "  time_this_iter_s: 25.87244701385498\n",
      "  time_total_s: 14138.086912870407\n",
      "  timers:\n",
      "    learn_throughput: 1569.451\n",
      "    learn_time_ms: 637.166\n",
      "    load_throughput: 54941.552\n",
      "    load_time_ms: 18.201\n",
      "    sample_throughput: 37.488\n",
      "    sample_time_ms: 26675.087\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633805257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 616\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   616</td><td style=\"text-align: right;\">         14138.1</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            369.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 617000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-48-05\n",
      "  done: false\n",
      "  episode_len_mean: 367.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1582\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4538627558284336\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008188454632612925\n",
      "          policy_loss: -0.030296713031000563\n",
      "          total_loss: -0.04223529133531782\n",
      "          vf_explained_var: -0.49836036562919617\n",
      "          vf_loss: 0.00010999535726215173\n",
      "    num_agent_steps_sampled: 617000\n",
      "    num_agent_steps_trained: 617000\n",
      "    num_steps_sampled: 617000\n",
      "    num_steps_trained: 617000\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43\n",
      "    ram_util_percent: 72.9\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672881863154017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.841796451458535\n",
      "    mean_inference_ms: 1.7218050967448724\n",
      "    mean_raw_obs_processing_ms: 1.590765230566233\n",
      "  time_since_restore: 14165.779497146606\n",
      "  time_this_iter_s: 27.69258427619934\n",
      "  time_total_s: 14165.779497146606\n",
      "  timers:\n",
      "    learn_throughput: 1570.539\n",
      "    learn_time_ms: 636.724\n",
      "    load_throughput: 54375.088\n",
      "    load_time_ms: 18.391\n",
      "    sample_throughput: 37.112\n",
      "    sample_time_ms: 26945.644\n",
      "    update_time_ms: 2.19\n",
      "  timestamp: 1633805285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 617000\n",
      "  training_iteration: 617\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   617</td><td style=\"text-align: right;\">         14165.8</td><td style=\"text-align: right;\">617000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            367.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 618000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-48-34\n",
      "  done: false\n",
      "  episode_len_mean: 362.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1585\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3432129184405008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0043256703855828645\n",
      "          policy_loss: -0.005320023828082615\n",
      "          total_loss: -0.017309594402710595\n",
      "          vf_explained_var: -0.2544165551662445\n",
      "          vf_loss: 0.00012715234075181392\n",
      "    num_agent_steps_sampled: 618000\n",
      "    num_agent_steps_trained: 618000\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87073170731708\n",
      "    ram_util_percent: 73.06341463414634\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672861032447262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.84500295039625\n",
      "    mean_inference_ms: 1.7218049403743885\n",
      "    mean_raw_obs_processing_ms: 1.590706946446659\n",
      "  time_since_restore: 14194.240795373917\n",
      "  time_this_iter_s: 28.46129822731018\n",
      "  time_total_s: 14194.240795373917\n",
      "  timers:\n",
      "    learn_throughput: 1572.5\n",
      "    learn_time_ms: 635.93\n",
      "    load_throughput: 54368.815\n",
      "    load_time_ms: 18.393\n",
      "    sample_throughput: 36.525\n",
      "    sample_time_ms: 27378.262\n",
      "    update_time_ms: 2.192\n",
      "  timestamp: 1633805314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   618</td><td style=\"text-align: right;\">         14194.2</td><td style=\"text-align: right;\">618000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            362.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 619000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 357.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1589\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648971557622\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3012219468752544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004281887538137299\n",
      "          policy_loss: -0.022388462639517253\n",
      "          total_loss: -0.034631645493209365\n",
      "          vf_explained_var: -0.7444595098495483\n",
      "          vf_loss: 0.00011799041013647285\n",
      "    num_agent_steps_sampled: 619000\n",
      "    num_agent_steps_trained: 619000\n",
      "    num_steps_sampled: 619000\n",
      "    num_steps_trained: 619000\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65681818181818\n",
      "    ram_util_percent: 73.09090909090908\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036728344229814844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.850029449672515\n",
      "    mean_inference_ms: 1.7218062945254045\n",
      "    mean_raw_obs_processing_ms: 1.590704544121149\n",
      "  time_since_restore: 14225.318380594254\n",
      "  time_this_iter_s: 31.077585220336914\n",
      "  time_total_s: 14225.318380594254\n",
      "  timers:\n",
      "    learn_throughput: 1571.873\n",
      "    learn_time_ms: 636.184\n",
      "    load_throughput: 53847.064\n",
      "    load_time_ms: 18.571\n",
      "    sample_throughput: 35.716\n",
      "    sample_time_ms: 27998.442\n",
      "    update_time_ms: 2.189\n",
      "  timestamp: 1633805345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619000\n",
      "  training_iteration: 619\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   619</td><td style=\"text-align: right;\">         14225.3</td><td style=\"text-align: right;\">619000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            357.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-49-51\n",
      "  done: false\n",
      "  episode_len_mean: 352.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1592\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4891355011198255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0163832868957141\n",
      "          policy_loss: 0.035379119714101154\n",
      "          total_loss: 0.02188757848408487\n",
      "          vf_explained_var: -0.00011508663737913594\n",
      "          vf_loss: 0.0001543030831170553\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.7030303030303\n",
      "    ram_util_percent: 73.16818181818182\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672815268163862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.854230294046054\n",
      "    mean_inference_ms: 1.7218080627546448\n",
      "    mean_raw_obs_processing_ms: 1.591546258998536\n",
      "  time_since_restore: 14271.471867799759\n",
      "  time_this_iter_s: 46.15348720550537\n",
      "  time_total_s: 14271.471867799759\n",
      "  timers:\n",
      "    learn_throughput: 1572.445\n",
      "    learn_time_ms: 635.952\n",
      "    load_throughput: 54184.234\n",
      "    load_time_ms: 18.456\n",
      "    sample_throughput: 32.942\n",
      "    sample_time_ms: 30356.82\n",
      "    update_time_ms: 2.195\n",
      "  timestamp: 1633805391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 620\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   620</td><td style=\"text-align: right;\">         14271.5</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            352.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 621000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 347.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1596\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4069956554306877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012940669078007025\n",
      "          policy_loss: -0.07571395403809017\n",
      "          total_loss: -0.08867906199561225\n",
      "          vf_explained_var: -0.6677120923995972\n",
      "          vf_loss: 0.00012105500540605539\n",
      "    num_agent_steps_sampled: 621000\n",
      "    num_agent_steps_trained: 621000\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.963414634146346\n",
      "    ram_util_percent: 73.08536585365854\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672794484410762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.86042704286922\n",
      "    mean_inference_ms: 1.7218120000675128\n",
      "    mean_raw_obs_processing_ms: 1.5927407891177596\n",
      "  time_since_restore: 14300.428940296173\n",
      "  time_this_iter_s: 28.957072496414185\n",
      "  time_total_s: 14300.428940296173\n",
      "  timers:\n",
      "    learn_throughput: 1570.58\n",
      "    learn_time_ms: 636.707\n",
      "    load_throughput: 53945.477\n",
      "    load_time_ms: 18.537\n",
      "    sample_throughput: 34.694\n",
      "    sample_time_ms: 28823.439\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633805420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   621</td><td style=\"text-align: right;\">         14300.4</td><td style=\"text-align: right;\">621000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             347.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 622000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-50-45\n",
      "  done: false\n",
      "  episode_len_mean: 344.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1599\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3815520193841722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01187512803822997\n",
      "          policy_loss: 0.01071446215113004\n",
      "          total_loss: -0.0020445007416937085\n",
      "          vf_explained_var: -0.9862143397331238\n",
      "          vf_loss: 0.0001537711620686524\n",
      "    num_agent_steps_sampled: 622000\n",
      "    num_agent_steps_trained: 622000\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.45945945945947\n",
      "    ram_util_percent: 72.67027027027025\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672778737797253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.865324901042953\n",
      "    mean_inference_ms: 1.721815323656306\n",
      "    mean_raw_obs_processing_ms: 1.5933804341825653\n",
      "  time_since_restore: 14326.095838546753\n",
      "  time_this_iter_s: 25.666898250579834\n",
      "  time_total_s: 14326.095838546753\n",
      "  timers:\n",
      "    learn_throughput: 1567.437\n",
      "    learn_time_ms: 637.984\n",
      "    load_throughput: 54018.497\n",
      "    load_time_ms: 18.512\n",
      "    sample_throughput: 34.619\n",
      "    sample_time_ms: 28886.068\n",
      "    update_time_ms: 2.075\n",
      "  timestamp: 1633805445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 622\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   622</td><td style=\"text-align: right;\">         14326.1</td><td style=\"text-align: right;\">622000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 623000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 343.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1602\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4737687971856859\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011102270417989443\n",
      "          policy_loss: -0.07297731182641454\n",
      "          total_loss: -0.08679705758889517\n",
      "          vf_explained_var: -0.9193966388702393\n",
      "          vf_loss: 7.391049245294804e-05\n",
      "    num_agent_steps_sampled: 623000\n",
      "    num_agent_steps_trained: 623000\n",
      "    num_steps_sampled: 623000\n",
      "    num_steps_trained: 623000\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.76666666666667\n",
      "    ram_util_percent: 72.59999999999998\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036727621013866625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8703154877951\n",
      "    mean_inference_ms: 1.7218186535511095\n",
      "    mean_raw_obs_processing_ms: 1.5934812845713315\n",
      "  time_since_restore: 14351.518830776215\n",
      "  time_this_iter_s: 25.42299222946167\n",
      "  time_total_s: 14351.518830776215\n",
      "  timers:\n",
      "    learn_throughput: 1566.756\n",
      "    learn_time_ms: 638.261\n",
      "    load_throughput: 54680.83\n",
      "    load_time_ms: 18.288\n",
      "    sample_throughput: 34.894\n",
      "    sample_time_ms: 28658.289\n",
      "    update_time_ms: 2.061\n",
      "  timestamp: 1633805471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623000\n",
      "  training_iteration: 623\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   623</td><td style=\"text-align: right;\">         14351.5</td><td style=\"text-align: right;\">623000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            343.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 341.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1605\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.429443249437544\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01229740220481664\n",
      "          policy_loss: -0.07009209659364489\n",
      "          total_loss: -0.08328825686540868\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00016338448416111835\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55555555555556\n",
      "    ram_util_percent: 72.57499999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036727434059091624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.875474673743582\n",
      "    mean_inference_ms: 1.7218224802721815\n",
      "    mean_raw_obs_processing_ms: 1.5935850951872117\n",
      "  time_since_restore: 14376.95479798317\n",
      "  time_this_iter_s: 25.435967206954956\n",
      "  time_total_s: 14376.95479798317\n",
      "  timers:\n",
      "    learn_throughput: 1568.298\n",
      "    learn_time_ms: 637.634\n",
      "    load_throughput: 54594.14\n",
      "    load_time_ms: 18.317\n",
      "    sample_throughput: 35.173\n",
      "    sample_time_ms: 28431.163\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633805496\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 624\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   624</td><td style=\"text-align: right;\">           14377</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            341.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 625000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 337.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1609\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4738904648356967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013858202333307092\n",
      "          policy_loss: 0.07096607403622733\n",
      "          total_loss: 0.05797781352367666\n",
      "          vf_explained_var: 0.1282629668712616\n",
      "          vf_loss: 0.0006970988538847046\n",
      "    num_agent_steps_sampled: 625000\n",
      "    num_agent_steps_trained: 625000\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.593023255813954\n",
      "    ram_util_percent: 72.67906976744186\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672717781284697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.882887915301634\n",
      "    mean_inference_ms: 1.7218284304279592\n",
      "    mean_raw_obs_processing_ms: 1.5937645803974931\n",
      "  time_since_restore: 14406.881956100464\n",
      "  time_this_iter_s: 29.92715811729431\n",
      "  time_total_s: 14406.881956100464\n",
      "  timers:\n",
      "    learn_throughput: 1569.944\n",
      "    learn_time_ms: 636.965\n",
      "    load_throughput: 54361.487\n",
      "    load_time_ms: 18.395\n",
      "    sample_throughput: 34.715\n",
      "    sample_time_ms: 28805.889\n",
      "    update_time_ms: 2.051\n",
      "  timestamp: 1633805526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   625</td><td style=\"text-align: right;\">         14406.9</td><td style=\"text-align: right;\">625000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            337.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 626000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-52-34\n",
      "  done: false\n",
      "  episode_len_mean: 333.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1612\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5033999231126574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011837618920158378\n",
      "          policy_loss: -0.05333304697026809\n",
      "          total_loss: -0.06738757830527094\n",
      "          vf_explained_var: -0.9244667291641235\n",
      "          vf_loss: 7.953371653760163e-05\n",
      "    num_agent_steps_sampled: 626000\n",
      "    num_agent_steps_trained: 626000\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.1175\n",
      "    ram_util_percent: 72.8075\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036726992275978955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.88880371488267\n",
      "    mean_inference_ms: 1.7218335740816793\n",
      "    mean_raw_obs_processing_ms: 1.5939275948090623\n",
      "  time_since_restore: 14434.429005384445\n",
      "  time_this_iter_s: 27.547049283981323\n",
      "  time_total_s: 14434.429005384445\n",
      "  timers:\n",
      "    learn_throughput: 1570.47\n",
      "    learn_time_ms: 636.752\n",
      "    load_throughput: 54118.515\n",
      "    load_time_ms: 18.478\n",
      "    sample_throughput: 34.514\n",
      "    sample_time_ms: 28973.522\n",
      "    update_time_ms: 2.022\n",
      "  timestamp: 1633805554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 626\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   626</td><td style=\"text-align: right;\">         14434.4</td><td style=\"text-align: right;\">626000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            333.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 627000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-53-02\n",
      "  done: false\n",
      "  episode_len_mean: 326.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1616\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3843620194329156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016090137408347104\n",
      "          policy_loss: -0.06535233217808935\n",
      "          total_loss: -0.07786351963877677\n",
      "          vf_explained_var: -0.40746238827705383\n",
      "          vf_loss: 0.00010920604238183135\n",
      "    num_agent_steps_sampled: 627000\n",
      "    num_agent_steps_trained: 627000\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7\n",
      "    ram_util_percent: 72.8425\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672674274308279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.897380075677162\n",
      "    mean_inference_ms: 1.7218417170642568\n",
      "    mean_raw_obs_processing_ms: 1.5942176987679122\n",
      "  time_since_restore: 14462.896298408508\n",
      "  time_this_iter_s: 28.46729302406311\n",
      "  time_total_s: 14462.896298408508\n",
      "  timers:\n",
      "    learn_throughput: 1570.206\n",
      "    learn_time_ms: 636.859\n",
      "    load_throughput: 54244.78\n",
      "    load_time_ms: 18.435\n",
      "    sample_throughput: 34.422\n",
      "    sample_time_ms: 29050.935\n",
      "    update_time_ms: 2.024\n",
      "  timestamp: 1633805582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 627\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   627</td><td style=\"text-align: right;\">         14462.9</td><td style=\"text-align: right;\">627000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            326.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 323.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1619\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4325194054179722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013108925730382422\n",
      "          policy_loss: -0.09160297840005821\n",
      "          total_loss: -0.10482891307522853\n",
      "          vf_explained_var: -0.2799719572067261\n",
      "          vf_loss: 0.0001026753081128441\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61025641025641\n",
      "    ram_util_percent: 72.94358974358975\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672657175167592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.90408974641365\n",
      "    mean_inference_ms: 1.721848795572022\n",
      "    mean_raw_obs_processing_ms: 1.594488052804694\n",
      "  time_since_restore: 14490.39383149147\n",
      "  time_this_iter_s: 27.497533082962036\n",
      "  time_total_s: 14490.39383149147\n",
      "  timers:\n",
      "    learn_throughput: 1568.502\n",
      "    learn_time_ms: 637.551\n",
      "    load_throughput: 54349.723\n",
      "    load_time_ms: 18.399\n",
      "    sample_throughput: 34.538\n",
      "    sample_time_ms: 28953.909\n",
      "    update_time_ms: 2.024\n",
      "  timestamp: 1633805610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   628</td><td style=\"text-align: right;\">         14490.4</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            323.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 629000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-54-15\n",
      "  done: false\n",
      "  episode_len_mean: 318.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1623\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07602324485778811\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2157660762468974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020417550916144556\n",
      "          policy_loss: -0.07469871408409542\n",
      "          total_loss: -0.08512012557023102\n",
      "          vf_explained_var: 0.4207146167755127\n",
      "          vf_loss: 0.00018403816094279238\n",
      "    num_agent_steps_sampled: 629000\n",
      "    num_agent_steps_trained: 629000\n",
      "    num_steps_sampled: 629000\n",
      "    num_steps_trained: 629000\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.49384615384614\n",
      "    ram_util_percent: 73.01538461538462\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036726369803981515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.91339897219073\n",
      "    mean_inference_ms: 1.721858710058616\n",
      "    mean_raw_obs_processing_ms: 1.5959456972273176\n",
      "  time_since_restore: 14535.881744623184\n",
      "  time_this_iter_s: 45.48791313171387\n",
      "  time_total_s: 14535.881744623184\n",
      "  timers:\n",
      "    learn_throughput: 1568.542\n",
      "    learn_time_ms: 637.535\n",
      "    load_throughput: 55117.863\n",
      "    load_time_ms: 18.143\n",
      "    sample_throughput: 32.9\n",
      "    sample_time_ms: 30395.143\n",
      "    update_time_ms: 2.087\n",
      "  timestamp: 1633805655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629000\n",
      "  training_iteration: 629\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   629</td><td style=\"text-align: right;\">         14535.9</td><td style=\"text-align: right;\">629000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            318.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 317.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1626\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.479263218243917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01623654363409036\n",
      "          policy_loss: -0.019783060273362532\n",
      "          total_loss: -0.03259086461944712\n",
      "          vf_explained_var: -0.6050949692726135\n",
      "          vf_loss: 0.00013329602499854648\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.87105263157895\n",
      "    ram_util_percent: 72.92368421052635\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672621537940053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.920372307471947\n",
      "    mean_inference_ms: 1.7218665023663948\n",
      "    mean_raw_obs_processing_ms: 1.5970831006297612\n",
      "  time_since_restore: 14561.991599798203\n",
      "  time_this_iter_s: 26.10985517501831\n",
      "  time_total_s: 14561.991599798203\n",
      "  timers:\n",
      "    learn_throughput: 1569.153\n",
      "    learn_time_ms: 637.287\n",
      "    load_throughput: 54561.826\n",
      "    load_time_ms: 18.328\n",
      "    sample_throughput: 35.223\n",
      "    sample_time_ms: 28390.827\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1633805682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 630\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   630</td><td style=\"text-align: right;\">           14562</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 631000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 314.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1630\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3565082046720716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012424923652271817\n",
      "          policy_loss: -0.02145263109770086\n",
      "          total_loss: -0.03349621084829171\n",
      "          vf_explained_var: -0.582496166229248\n",
      "          vf_loss: 0.00010462551233811408\n",
      "    num_agent_steps_sampled: 631000\n",
      "    num_agent_steps_trained: 631000\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7025641025641\n",
      "    ram_util_percent: 72.71282051282049\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672601159829428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.929864259870087\n",
      "    mean_inference_ms: 1.7218767896133458\n",
      "    mean_raw_obs_processing_ms: 1.598038164371389\n",
      "  time_since_restore: 14589.239824533463\n",
      "  time_this_iter_s: 27.24822473526001\n",
      "  time_total_s: 14589.239824533463\n",
      "  timers:\n",
      "    learn_throughput: 1571.117\n",
      "    learn_time_ms: 636.49\n",
      "    load_throughput: 54687.96\n",
      "    load_time_ms: 18.286\n",
      "    sample_throughput: 35.435\n",
      "    sample_time_ms: 28220.779\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633805709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   631</td><td style=\"text-align: right;\">         14589.2</td><td style=\"text-align: right;\">631000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            314.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 312.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1633\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7370899187193976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0123476072461459\n",
      "          policy_loss: 0.0014956391313009792\n",
      "          total_loss: -0.014309815855489837\n",
      "          vf_explained_var: -0.362280011177063\n",
      "          vf_loss: 0.0001573865205702734\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55555555555555\n",
      "    ram_util_percent: 72.71111111111111\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036725846017409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.937180545271453\n",
      "    mean_inference_ms: 1.7218840663882713\n",
      "    mean_raw_obs_processing_ms: 1.598334163624436\n",
      "  time_since_restore: 14614.602377414703\n",
      "  time_this_iter_s: 25.362552881240845\n",
      "  time_total_s: 14614.602377414703\n",
      "  timers:\n",
      "    learn_throughput: 1572.841\n",
      "    learn_time_ms: 635.792\n",
      "    load_throughput: 55496.949\n",
      "    load_time_ms: 18.019\n",
      "    sample_throughput: 35.472\n",
      "    sample_time_ms: 28191.317\n",
      "    update_time_ms: 2.079\n",
      "  timestamp: 1633805734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 632\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   632</td><td style=\"text-align: right;\">         14614.6</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            312.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 633000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 309.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1636\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6154872006840175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01783916491614546\n",
      "          policy_loss: 0.01900504893726773\n",
      "          total_loss: 0.005012137360042996\n",
      "          vf_explained_var: 0.006365272682160139\n",
      "          vf_loss: 0.00012767103398800828\n",
      "    num_agent_steps_sampled: 633000\n",
      "    num_agent_steps_trained: 633000\n",
      "    num_steps_sampled: 633000\n",
      "    num_steps_trained: 633000\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62307692307691\n",
      "    ram_util_percent: 72.80000000000003\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672569291525277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.944735936325763\n",
      "    mean_inference_ms: 1.7218912602504204\n",
      "    mean_raw_obs_processing_ms: 1.598656991327963\n",
      "  time_since_restore: 14642.093600988388\n",
      "  time_this_iter_s: 27.491223573684692\n",
      "  time_total_s: 14642.093600988388\n",
      "  timers:\n",
      "    learn_throughput: 1573.373\n",
      "    learn_time_ms: 635.577\n",
      "    load_throughput: 55535.012\n",
      "    load_time_ms: 18.007\n",
      "    sample_throughput: 35.213\n",
      "    sample_time_ms: 28398.341\n",
      "    update_time_ms: 2.084\n",
      "  timestamp: 1633805762\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 633000\n",
      "  training_iteration: 633\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   633</td><td style=\"text-align: right;\">         14642.1</td><td style=\"text-align: right;\">633000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            309.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 634000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 308.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1639\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.787611711025238\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01782135921923352\n",
      "          policy_loss: -0.04935743328597811\n",
      "          total_loss: -0.06507380329486397\n",
      "          vf_explained_var: -0.4998512268066406\n",
      "          vf_loss: 0.00012749046759886874\n",
      "    num_agent_steps_sampled: 634000\n",
      "    num_agent_steps_trained: 634000\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.09428571428572\n",
      "    ram_util_percent: 72.84571428571431\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672555947835915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95231802394284\n",
      "    mean_inference_ms: 1.7218982762690402\n",
      "    mean_raw_obs_processing_ms: 1.5989827363523665\n",
      "  time_since_restore: 14666.49997973442\n",
      "  time_this_iter_s: 24.406378746032715\n",
      "  time_total_s: 14666.49997973442\n",
      "  timers:\n",
      "    learn_throughput: 1571.653\n",
      "    learn_time_ms: 636.273\n",
      "    load_throughput: 55598.616\n",
      "    load_time_ms: 17.986\n",
      "    sample_throughput: 35.342\n",
      "    sample_time_ms: 28294.723\n",
      "    update_time_ms: 2.08\n",
      "  timestamp: 1633805786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   634</td><td style=\"text-align: right;\">         14666.5</td><td style=\"text-align: right;\">634000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            308.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 635000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 307.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1642\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6243395646413168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010425181277833871\n",
      "          policy_loss: -0.03018711705485152\n",
      "          total_loss: -0.045114668148259325\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00012701114303328924\n",
      "    num_agent_steps_sampled: 635000\n",
      "    num_agent_steps_trained: 635000\n",
      "    num_steps_sampled: 635000\n",
      "    num_steps_trained: 635000\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.616666666666674\n",
      "    ram_util_percent: 72.90277777777779\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367254255129389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.95995694939654\n",
      "    mean_inference_ms: 1.7219050621027676\n",
      "    mean_raw_obs_processing_ms: 1.5993113990987167\n",
      "  time_since_restore: 14691.964838027954\n",
      "  time_this_iter_s: 25.464858293533325\n",
      "  time_total_s: 14691.964838027954\n",
      "  timers:\n",
      "    learn_throughput: 1570.427\n",
      "    learn_time_ms: 636.77\n",
      "    load_throughput: 55590.805\n",
      "    load_time_ms: 17.989\n",
      "    sample_throughput: 35.909\n",
      "    sample_time_ms: 27847.976\n",
      "    update_time_ms: 2.086\n",
      "  timestamp: 1633805812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635000\n",
      "  training_iteration: 635\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   635</td><td style=\"text-align: right;\">           14692</td><td style=\"text-align: right;\">635000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            307.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 306.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1646\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4813420295715332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01386745455960489\n",
      "          policy_loss: -0.020373688058720696\n",
      "          total_loss: -0.033485279252959625\n",
      "          vf_explained_var: -0.5931133031845093\n",
      "          vf_loss: 0.00012045595544299836\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43947368421053\n",
      "    ram_util_percent: 72.95526315789475\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672522757163202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.970087981792663\n",
      "    mean_inference_ms: 1.7219136534981287\n",
      "    mean_raw_obs_processing_ms: 1.5998018331586894\n",
      "  time_since_restore: 14718.051696062088\n",
      "  time_this_iter_s: 26.08685803413391\n",
      "  time_total_s: 14718.051696062088\n",
      "  timers:\n",
      "    learn_throughput: 1573.004\n",
      "    learn_time_ms: 635.726\n",
      "    load_throughput: 55775.616\n",
      "    load_time_ms: 17.929\n",
      "    sample_throughput: 36.097\n",
      "    sample_time_ms: 27703.058\n",
      "    update_time_ms: 2.091\n",
      "  timestamp: 1633805838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 636\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   636</td><td style=\"text-align: right;\">         14718.1</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            306.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 637000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 306.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1649\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.449593683083852\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013605608342966806\n",
      "          policy_loss: 0.0355719936804639\n",
      "          total_loss: 0.022714842648969755\n",
      "          vf_explained_var: 0.17978373169898987\n",
      "          vf_loss: 8.727124877623282e-05\n",
      "    num_agent_steps_sampled: 637000\n",
      "    num_agent_steps_trained: 637000\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.770270270270274\n",
      "    ram_util_percent: 73.0108108108108\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036725086660218266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.977711267453916\n",
      "    mean_inference_ms: 1.7219200354756277\n",
      "    mean_raw_obs_processing_ms: 1.6001600728253496\n",
      "  time_since_restore: 14744.282089233398\n",
      "  time_this_iter_s: 26.230393171310425\n",
      "  time_total_s: 14744.282089233398\n",
      "  timers:\n",
      "    learn_throughput: 1573.504\n",
      "    learn_time_ms: 635.524\n",
      "    load_throughput: 55542.587\n",
      "    load_time_ms: 18.004\n",
      "    sample_throughput: 36.391\n",
      "    sample_time_ms: 27479.486\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1633805864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   637</td><td style=\"text-align: right;\">         14744.3</td><td style=\"text-align: right;\">637000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               306</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=202261)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 638000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 306.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1652\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11403486728668213\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7466045008765327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02128897396047471\n",
      "          policy_loss: 0.020504464436736373\n",
      "          total_loss: 0.0057088771628008945\n",
      "          vf_explained_var: -0.5471435785293579\n",
      "          vf_loss: 0.00024277255175143687\n",
      "    num_agent_steps_sampled: 638000\n",
      "    num_agent_steps_trained: 638000\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.208474576271186\n",
      "    ram_util_percent: 73.1050847457627\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672496716894227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.985254907539847\n",
      "    mean_inference_ms: 1.721926277092967\n",
      "    mean_raw_obs_processing_ms: 1.6013285502390224\n",
      "  time_since_restore: 14785.320749521255\n",
      "  time_this_iter_s: 41.038660287857056\n",
      "  time_total_s: 14785.320749521255\n",
      "  timers:\n",
      "    learn_throughput: 1573.966\n",
      "    learn_time_ms: 635.338\n",
      "    load_throughput: 55411.094\n",
      "    load_time_ms: 18.047\n",
      "    sample_throughput: 34.682\n",
      "    sample_time_ms: 28833.732\n",
      "    update_time_ms: 2.094\n",
      "  timestamp: 1633805905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 638\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   638</td><td style=\"text-align: right;\">         14785.3</td><td style=\"text-align: right;\">638000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            306.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 639000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 305.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1654\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7985235585106745\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013978570738141164\n",
      "          policy_loss: -0.06066268293393983\n",
      "          total_loss: -0.07611013129353524\n",
      "          vf_explained_var: 0.3897990882396698\n",
      "          vf_loss: 0.00014671902778597238\n",
      "    num_agent_steps_sampled: 639000\n",
      "    num_agent_steps_trained: 639000\n",
      "    num_steps_sampled: 639000\n",
      "    num_steps_trained: 639000\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.01666666666667\n",
      "    ram_util_percent: 73.13333333333334\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672487552190156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.990177964096613\n",
      "    mean_inference_ms: 1.721930134305037\n",
      "    mean_raw_obs_processing_ms: 1.6020993406267598\n",
      "  time_since_restore: 14806.322213411331\n",
      "  time_this_iter_s: 21.001463890075684\n",
      "  time_total_s: 14806.322213411331\n",
      "  timers:\n",
      "    learn_throughput: 1572.853\n",
      "    learn_time_ms: 635.787\n",
      "    load_throughput: 55517.297\n",
      "    load_time_ms: 18.012\n",
      "    sample_throughput: 37.901\n",
      "    sample_time_ms: 26384.759\n",
      "    update_time_ms: 2.033\n",
      "  timestamp: 1633805926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639000\n",
      "  training_iteration: 639\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   639</td><td style=\"text-align: right;\">         14806.3</td><td style=\"text-align: right;\">639000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            305.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-59-05\n",
      "  done: false\n",
      "  episode_len_mean: 308.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1657\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.856855775250329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01604116674915282\n",
      "          policy_loss: 0.0016537024743027158\n",
      "          total_loss: -0.014056532747215696\n",
      "          vf_explained_var: 0.06404173374176025\n",
      "          vf_loss: 0.00011444391525906717\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.007407407407406\n",
      "    ram_util_percent: 72.85925925925926\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036724730945120056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.99734814686092\n",
      "    mean_inference_ms: 1.721935396747196\n",
      "    mean_raw_obs_processing_ms: 1.6032445876934707\n",
      "  time_since_restore: 14825.61273241043\n",
      "  time_this_iter_s: 19.29051899909973\n",
      "  time_total_s: 14825.61273241043\n",
      "  timers:\n",
      "    learn_throughput: 1569.972\n",
      "    learn_time_ms: 636.954\n",
      "    load_throughput: 55562.747\n",
      "    load_time_ms: 17.998\n",
      "    sample_throughput: 38.908\n",
      "    sample_time_ms: 25701.663\n",
      "    update_time_ms: 2.041\n",
      "  timestamp: 1633805945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 640\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   640</td><td style=\"text-align: right;\">         14825.6</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               308</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 641000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 308.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1660\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9263122571839226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014330314593537471\n",
      "          policy_loss: -0.011664589618643124\n",
      "          total_loss: -0.0284130042211877\n",
      "          vf_explained_var: -0.016929317265748978\n",
      "          vf_loss: 6.347225369406968e-05\n",
      "    num_agent_steps_sampled: 641000\n",
      "    num_agent_steps_trained: 641000\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65\n",
      "    ram_util_percent: 72.57187499999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672459443455593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.004405813673483\n",
      "    mean_inference_ms: 1.721940352900923\n",
      "    mean_raw_obs_processing_ms: 1.6041191863458624\n",
      "  time_since_restore: 14847.852187871933\n",
      "  time_this_iter_s: 22.239455461502075\n",
      "  time_total_s: 14847.852187871933\n",
      "  timers:\n",
      "    learn_throughput: 1568.053\n",
      "    learn_time_ms: 637.734\n",
      "    load_throughput: 55390.532\n",
      "    load_time_ms: 18.054\n",
      "    sample_throughput: 39.683\n",
      "    sample_time_ms: 25199.94\n",
      "    update_time_ms: 2.052\n",
      "  timestamp: 1633805968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   641</td><td style=\"text-align: right;\">         14847.9</td><td style=\"text-align: right;\">641000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            308.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 642000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_18-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 308.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1662\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3463653610812294\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010397371679199354\n",
      "          policy_loss: -0.0475613996386528\n",
      "          total_loss: -0.05911751091480255\n",
      "          vf_explained_var: -0.2251402735710144\n",
      "          vf_loss: 0.0001290480681038591\n",
      "    num_agent_steps_sampled: 642000\n",
      "    num_agent_steps_trained: 642000\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.73333333333334\n",
      "    ram_util_percent: 72.51\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036724507258475356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00896512176871\n",
      "    mean_inference_ms: 1.7219434152473507\n",
      "    mean_raw_obs_processing_ms: 1.6043248941328778\n",
      "  time_since_restore: 14869.205254793167\n",
      "  time_this_iter_s: 21.35306692123413\n",
      "  time_total_s: 14869.205254793167\n",
      "  timers:\n",
      "    learn_throughput: 1568.16\n",
      "    learn_time_ms: 637.69\n",
      "    load_throughput: 53998.746\n",
      "    load_time_ms: 18.519\n",
      "    sample_throughput: 40.325\n",
      "    sample_time_ms: 24798.56\n",
      "    update_time_ms: 2.055\n",
      "  timestamp: 1633805989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 642\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   642</td><td style=\"text-align: right;\">         14869.2</td><td style=\"text-align: right;\">642000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            308.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 643000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 311.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1665\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7064593765470717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018536453255483663\n",
      "          policy_loss: -0.07502516441875035\n",
      "          total_loss: -0.08880617469549179\n",
      "          vf_explained_var: -0.5425310134887695\n",
      "          vf_loss: 0.00011287722166647048\n",
      "    num_agent_steps_sampled: 643000\n",
      "    num_agent_steps_trained: 643000\n",
      "    num_steps_sampled: 643000\n",
      "    num_steps_trained: 643000\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.39000000000001\n",
      "    ram_util_percent: 72.55999999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036724365504705224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01559804982691\n",
      "    mean_inference_ms: 1.721947642330546\n",
      "    mean_raw_obs_processing_ms: 1.60463628108937\n",
      "  time_since_restore: 14890.229319572449\n",
      "  time_this_iter_s: 21.024064779281616\n",
      "  time_total_s: 14890.229319572449\n",
      "  timers:\n",
      "    learn_throughput: 1566.052\n",
      "    learn_time_ms: 638.548\n",
      "    load_throughput: 53768.165\n",
      "    load_time_ms: 18.598\n",
      "    sample_throughput: 41.406\n",
      "    sample_time_ms: 24150.897\n",
      "    update_time_ms: 2.057\n",
      "  timestamp: 1633806010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 643000\n",
      "  training_iteration: 643\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   643</td><td style=\"text-align: right;\">         14890.2</td><td style=\"text-align: right;\">643000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            311.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-00-29\n",
      "  done: false\n",
      "  episode_len_mean: 312.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1667\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7699115263091194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01181873446512455\n",
      "          policy_loss: -0.01294419773750835\n",
      "          total_loss: -0.028494661011629636\n",
      "          vf_explained_var: -0.146320179104805\n",
      "          vf_loss: 0.0001270280105826613\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.682142857142864\n",
      "    ram_util_percent: 72.57499999999999\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672427298639911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.019735554583125\n",
      "    mean_inference_ms: 1.7219500143505702\n",
      "    mean_raw_obs_processing_ms: 1.6048437588937277\n",
      "  time_since_restore: 14909.222812652588\n",
      "  time_this_iter_s: 18.99349308013916\n",
      "  time_total_s: 14909.222812652588\n",
      "  timers:\n",
      "    learn_throughput: 1569.448\n",
      "    learn_time_ms: 637.167\n",
      "    load_throughput: 53524.65\n",
      "    load_time_ms: 18.683\n",
      "    sample_throughput: 42.353\n",
      "    sample_time_ms: 23610.899\n",
      "    update_time_ms: 2.054\n",
      "  timestamp: 1633806029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 644\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   644</td><td style=\"text-align: right;\">         14909.2</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            312.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 315.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1670\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6333085417747497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01436233003035849\n",
      "          policy_loss: 0.010213574384235674\n",
      "          total_loss: -0.0033717055287626053\n",
      "          vf_explained_var: -0.4183010458946228\n",
      "          vf_loss: 0.00029109699810053117\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_agent_steps_trained: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.765517241379314\n",
      "    ram_util_percent: 72.63448275862066\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0367241526509392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.025802034741474\n",
      "    mean_inference_ms: 1.7219533119992425\n",
      "    mean_raw_obs_processing_ms: 1.6051112574067445\n",
      "  time_since_restore: 14930.088337182999\n",
      "  time_this_iter_s: 20.865524530410767\n",
      "  time_total_s: 14930.088337182999\n",
      "  timers:\n",
      "    learn_throughput: 1570.984\n",
      "    learn_time_ms: 636.544\n",
      "    load_throughput: 53648.773\n",
      "    load_time_ms: 18.64\n",
      "    sample_throughput: 43.193\n",
      "    sample_time_ms: 23151.641\n",
      "    update_time_ms: 2.04\n",
      "  timestamp: 1633806050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 645\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   645</td><td style=\"text-align: right;\">         14930.1</td><td style=\"text-align: right;\">645000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            315.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 646000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 317.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1672\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7837755587365893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014142176796808608\n",
      "          policy_loss: -0.04324775189161301\n",
      "          total_loss: -0.05852588957382573\n",
      "          vf_explained_var: -0.12044397741556168\n",
      "          vf_loss: 0.0001405654453265015\n",
      "    num_agent_steps_sampled: 646000\n",
      "    num_agent_steps_trained: 646000\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61851851851853\n",
      "    ram_util_percent: 72.70740740740742\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672407375275443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.02962850741683\n",
      "    mean_inference_ms: 1.7219552117087937\n",
      "    mean_raw_obs_processing_ms: 1.6052750693403948\n",
      "  time_since_restore: 14949.074024915695\n",
      "  time_this_iter_s: 18.985687732696533\n",
      "  time_total_s: 14949.074024915695\n",
      "  timers:\n",
      "    learn_throughput: 1567.637\n",
      "    learn_time_ms: 637.903\n",
      "    load_throughput: 53773.542\n",
      "    load_time_ms: 18.597\n",
      "    sample_throughput: 44.563\n",
      "    sample_time_ms: 22440.19\n",
      "    update_time_ms: 2.056\n",
      "  timestamp: 1633806069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 646\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   646</td><td style=\"text-align: right;\">         14949.1</td><td style=\"text-align: right;\">646000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            317.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 647000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 321.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1675\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6703581796752083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012038912760173975\n",
      "          policy_loss: -0.06935657180017896\n",
      "          total_loss: -0.08384120729234483\n",
      "          vf_explained_var: -0.18715892732143402\n",
      "          vf_loss: 0.0001596636047502721\n",
      "    num_agent_steps_sampled: 647000\n",
      "    num_agent_steps_trained: 647000\n",
      "    num_steps_sampled: 647000\n",
      "    num_steps_trained: 647000\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.542857142857144\n",
      "    ram_util_percent: 72.78928571428571\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036723942743466825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.035030484662073\n",
      "    mean_inference_ms: 1.7219576780643258\n",
      "    mean_raw_obs_processing_ms: 1.6055241773305091\n",
      "  time_since_restore: 14968.420286655426\n",
      "  time_this_iter_s: 19.346261739730835\n",
      "  time_total_s: 14968.420286655426\n",
      "  timers:\n",
      "    learn_throughput: 1565.02\n",
      "    learn_time_ms: 638.97\n",
      "    load_throughput: 53946.032\n",
      "    load_time_ms: 18.537\n",
      "    sample_throughput: 45.975\n",
      "    sample_time_ms: 21750.775\n",
      "    update_time_ms: 2.063\n",
      "  timestamp: 1633806088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647000\n",
      "  training_iteration: 647\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   647</td><td style=\"text-align: right;\">         14968.4</td><td style=\"text-align: right;\">647000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            321.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_67f57_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_19-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 324.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1677\n",
      "  experiment_id: 9188d825d2554895b0ca0fd85ffa2a7a\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1710523009300232\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7168709794680277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009395367397608743\n",
      "          policy_loss: -0.06250214498076175\n",
      "          total_loss: -0.07788888747907347\n",
      "          vf_explained_var: -0.48133841156959534\n",
      "          vf_loss: 0.00017487067404241922\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 648\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.72592592592592\n",
      "    ram_util_percent: 72.87037037037038\n",
      "  pid: 202262\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03672383754818442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.038411158341614\n",
      "    mean_inference_ms: 1.7219589256851637\n",
      "    mean_raw_obs_processing_ms: 1.6056911275022228\n",
      "  time_since_restore: 14987.482815980911\n",
      "  time_this_iter_s: 19.06252932548523\n",
      "  time_total_s: 14987.482815980911\n",
      "  timers:\n",
      "    learn_throughput: 1568.739\n",
      "    learn_time_ms: 637.455\n",
      "    load_throughput: 53987.625\n",
      "    load_time_ms: 18.523\n",
      "    sample_throughput: 51.139\n",
      "    sample_time_ms: 19554.697\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1633806107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 648\n",
      "  trial_id: 67f57_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.01 GiB heap, 0.0/4.0 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_14-51-38<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_67f57_00000</td><td>RUNNING </td><td>192.168.3.5:202262</td><td style=\"text-align: right;\">   648</td><td style=\"text-align: right;\">         14987.5</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               324</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C32 pretrained (AngelaCNN + MLP) (3 noops after placement)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
